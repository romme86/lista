98479
{"version":0,"id":"2f2bf93fda09982d0176d2149f03648cd141a250013d741b28fcab7df2bbffc6","main":"/backend/backend.mjs","imports":{},"resolutions":{"/backend/backend.mjs":{"#package":"/package.json","../rpc-commands.mjs":"/rpc-commands.mjs","autobase":"/node_modules/autobase/index.js","b4a":"/node_modules/b4a/index.js","bare-crypto":"/node_modules/bare-crypto/index.js","bare-path":"/node_modules/bare-path/index.js","bare-rpc":"/node_modules/bare-rpc/index.js","bare-url":"/node_modules/bare-url/index.js","corestore":"/node_modules/corestore/index.js","hyperswarm":"/node_modules/hyperswarm/index.js"},"/node_modules/@hyperswarm/secret-stream/index.js":{"#package":"/node_modules/@hyperswarm/secret-stream/package.json","./lib/bridge":"/node_modules/@hyperswarm/secret-stream/lib/bridge.js","./lib/handshake":"/node_modules/@hyperswarm/secret-stream/lib/handshake.js","b4a":"/node_modules/b4a/index.js","hypercore-crypto":"/node_modules/hypercore-crypto/index.js","sodium-secretstream":"/node_modules/sodium-secretstream/index.js","sodium-universal":"/node_modules/sodium-universal/index.js","streamx":"/node_modules/streamx/index.js","timeout-refresh":"/node_modules/timeout-refresh/index.js","unslab":"/node_modules/unslab/index.js"},"/node_modules/@hyperswarm/secret-stream/lib/bridge.js":{"#package":"/node_modules/@hyperswarm/secret-stream/package.json","streamx":"/node_modules/streamx/index.js"},"/node_modules/@hyperswarm/secret-stream/lib/handshake.js":{"#package":"/node_modules/@hyperswarm/secret-stream/package.json","b4a":"/node_modules/b4a/index.js","noise-curve-ed":"/node_modules/noise-curve-ed/index.js","noise-handshake":"/node_modules/noise-handshake/noise.js","sodium-universal":"/node_modules/sodium-universal/index.js"},"/node_modules/@hyperswarm/secret-stream/package.json":{},"/node_modules/autobase/encoding/legacy.js":{"#package":"/node_modules/autobase/package.json","compact-encoding":"/node_modules/compact-encoding/index.js","index-encoder":"/node_modules/index-encoder/index.js"},"/node_modules/autobase/encoding/spec/autobase/index.js":{"#package":"/node_modules/autobase/package.json","../../legacy.js":"/node_modules/autobase/encoding/legacy.js","hyperschema/runtime":"/node_modules/hyperschema/runtime.cjs"},"/node_modules/autobase/index.js":{"#package":"/node_modules/autobase/package.json","./lib/active-writers.js":"/node_modules/autobase/lib/active-writers.js","./lib/append-batch.js":"/node_modules/autobase/lib/append-batch.js","./lib/apply-calls.js":"/node_modules/autobase/lib/apply-calls.js","./lib/apply-state.js":"/node_modules/autobase/lib/apply-state.js","./lib/boot.js":"/node_modules/autobase/lib/boot.js","./lib/caps.js":"/node_modules/autobase/lib/caps.js","./lib/encryption.js":"/node_modules/autobase/lib/encryption.js","./lib/fast-forward.js":"/node_modules/autobase/lib/fast-forward.js","./lib/linearizer.js":"/node_modules/autobase/lib/linearizer.js","./lib/local-state.js":"/node_modules/autobase/lib/local-state.js","./lib/messages.js":"/node_modules/autobase/lib/messages.js","./lib/store.js":"/node_modules/autobase/lib/store.js","./lib/system.js":"/node_modules/autobase/lib/system.js","./lib/timer.js":"/node_modules/autobase/lib/timer.js","./lib/updates.js":"/node_modules/autobase/lib/updates.js","./lib/values.js":"/node_modules/autobase/lib/values.js","./lib/wakeup.js":"/node_modules/autobase/lib/wakeup.js","./lib/writer.js":"/node_modules/autobase/lib/writer.js","b4a":"/node_modules/b4a/index.js","compact-encoding":"/node_modules/compact-encoding/index.js","core-coupler":"/node_modules/core-coupler/index.js","debounceify":"/node_modules/debounceify/index.js","hypercore":"/node_modules/hypercore/index.js","hypercore-id-encoding":"/node_modules/hypercore-id-encoding/index.js","nanoassert":"/node_modules/nanoassert/index.js","protomux-wakeup":"/node_modules/protomux-wakeup/index.js","ready-resource":"/node_modules/ready-resource/index.js","resolve-reject-promise":"/node_modules/resolve-reject-promise/index.js","safety-catch":"/node_modules/safety-catch/index.js","scope-lock":"/node_modules/scope-lock/index.js","signal-promise":"/node_modules/signal-promise/index.js"},"/node_modules/autobase/lib/active-writers.js":{"#package":"/node_modules/autobase/package.json","b4a":"/node_modules/b4a/index.js"},"/node_modules/autobase/lib/append-batch.js":{"#package":"/node_modules/autobase/package.json","signal-promise":"/node_modules/signal-promise/index.js"},"/node_modules/autobase/lib/apply-calls.js":{"#package":"/node_modules/autobase/package.json"},"/node_modules/autobase/lib/apply-state.js":{"#package":"/node_modules/autobase/package.json","./apply-calls.js":"/node_modules/autobase/lib/apply-calls.js","./caps.js":"/node_modules/autobase/lib/caps.js","./encryption.js":"/node_modules/autobase/lib/encryption.js","./fork.js":"/node_modules/autobase/lib/fork.js","./local-state.js":"/node_modules/autobase/lib/local-state.js","./messages.js":"/node_modules/autobase/lib/messages.js","./system.js":"/node_modules/autobase/lib/system.js","./updates.js":"/node_modules/autobase/lib/updates.js","./values.js":"/node_modules/autobase/lib/values.js","b4a":"/node_modules/b4a/index.js","compact-encoding":"/node_modules/compact-encoding/index.js","hypercore-crypto":"/node_modules/hypercore-crypto/index.js","hypercore/lib/multisig.js":"/node_modules/hypercore/lib/multisig.js","nanoassert":"/node_modules/nanoassert/index.js","ready-resource":"/node_modules/ready-resource/index.js","safety-catch":"/node_modules/safety-catch/index.js"},"/node_modules/autobase/lib/boot.js":{"#package":"/node_modules/autobase/package.json","./messages.js":"/node_modules/autobase/lib/messages.js","compact-encoding":"/node_modules/compact-encoding/index.js"},"/node_modules/autobase/lib/caps.js":{"#package":"/node_modules/autobase/package.json","hypercore-crypto":"/node_modules/hypercore-crypto/index.js"},"/node_modules/autobase/lib/clock.js":{"#package":"/node_modules/autobase/package.json","tiny-buffer-map":"/node_modules/tiny-buffer-map/index.js"},"/node_modules/autobase/lib/consensus.js":{"#package":"/node_modules/autobase/package.json","./clock.js":"/node_modules/autobase/lib/clock.js","tiny-buffer-map":"/node_modules/tiny-buffer-map/index.js"},"/node_modules/autobase/lib/encryption.js":{"#package":"/node_modules/autobase/package.json","./caps.js":"/node_modules/autobase/lib/caps.js","./messages.js":"/node_modules/autobase/lib/messages.js","./system.js":"/node_modules/autobase/lib/system.js","b4a":"/node_modules/b4a/index.js","compact-encoding":"/node_modules/compact-encoding/index.js","hypercore-crypto":"/node_modules/hypercore-crypto/index.js","hypercore/lib/default-encryption.js":"/node_modules/hypercore/lib/default-encryption.js","ready-resource":"/node_modules/ready-resource/index.js","resolve-reject-promise":"/node_modules/resolve-reject-promise/index.js","sodium-universal":"/node_modules/sodium-universal/index.js"},"/node_modules/autobase/lib/fast-forward.js":{"#package":"/node_modules/autobase/package.json","./encryption.js":"/node_modules/autobase/lib/encryption.js","./system.js":"/node_modules/autobase/lib/system.js","resolve-reject-promise":"/node_modules/resolve-reject-promise/index.js","safety-catch":"/node_modules/safety-catch/index.js"},"/node_modules/autobase/lib/fork.js":{"#package":"/node_modules/autobase/package.json","./encryption.js":"/node_modules/autobase/lib/encryption.js","./messages.js":"/node_modules/autobase/lib/messages.js","./system.js":"/node_modules/autobase/lib/system.js","compact-encoding":"/node_modules/compact-encoding/index.js"},"/node_modules/autobase/lib/linearizer.js":{"#package":"/node_modules/autobase/package.json","./clock.js":"/node_modules/autobase/lib/clock.js","./consensus.js":"/node_modules/autobase/lib/consensus.js","./topolist.js":"/node_modules/autobase/lib/topolist.js","b4a":"/node_modules/b4a/index.js","nanoassert":"/node_modules/nanoassert/index.js"},"/node_modules/autobase/lib/local-state.js":{"#package":"/node_modules/autobase/package.json","./messages.js":"/node_modules/autobase/lib/messages.js","b4a":"/node_modules/b4a/index.js","compact-encoding":"/node_modules/compact-encoding/index.js"},"/node_modules/autobase/lib/messages.js":{"#package":"/node_modules/autobase/package.json","../encoding/spec/autobase":"/node_modules/autobase/encoding/spec/autobase/index.js"},"/node_modules/autobase/lib/node-buffer.js":{"#package":"/node_modules/autobase/package.json"},"/node_modules/autobase/lib/store.js":{"#package":"/node_modules/autobase/package.json","./messages.js":"/node_modules/autobase/lib/messages.js","b4a":"/node_modules/b4a/index.js","compact-encoding":"/node_modules/compact-encoding/index.js","hypercore":"/node_modules/hypercore/index.js","hypercore-crypto":"/node_modules/hypercore-crypto/index.js"},"/node_modules/autobase/lib/system.js":{"#package":"/node_modules/autobase/package.json","./caps.js":"/node_modules/autobase/lib/caps.js","./messages.js":"/node_modules/autobase/lib/messages.js","b4a":"/node_modules/b4a/index.js","compact-encoding":"/node_modules/compact-encoding/index.js","hyperbee":"/node_modules/hyperbee/index.js","hypercore-crypto":"/node_modules/hypercore-crypto/index.js","ready-resource":"/node_modules/ready-resource/index.js","sub-encoder":"/node_modules/sub-encoder/index.js"},"/node_modules/autobase/lib/timer.js":{"#package":"/node_modules/autobase/package.json","safety-catch":"/node_modules/safety-catch/index.js"},"/node_modules/autobase/lib/topolist.js":{"#package":"/node_modules/autobase/package.json","b4a":"/node_modules/b4a/index.js","nanoassert":"/node_modules/nanoassert/index.js"},"/node_modules/autobase/lib/updates.js":{"#package":"/node_modules/autobase/package.json"},"/node_modules/autobase/lib/values.js":{"#package":"/node_modules/autobase/package.json","./caps.js":"/node_modules/autobase/lib/caps.js","./encryption.js":"/node_modules/autobase/lib/encryption.js","./messages.js":"/node_modules/autobase/lib/messages.js","b4a":"/node_modules/b4a/index.js","compact-encoding":"/node_modules/compact-encoding/index.js","hypercore-crypto":"/node_modules/hypercore-crypto/index.js"},"/node_modules/autobase/lib/wakeup.js":{"#package":"/node_modules/autobase/package.json","b4a":"/node_modules/b4a/index.js","compact-encoding":"/node_modules/compact-encoding/index.js","ready-resource":"/node_modules/ready-resource/index.js","safety-catch":"/node_modules/safety-catch/index.js"},"/node_modules/autobase/lib/writer.js":{"#package":"/node_modules/autobase/package.json","./linearizer.js":"/node_modules/autobase/lib/linearizer.js","./node-buffer.js":"/node_modules/autobase/lib/node-buffer.js","b4a":"/node_modules/b4a/index.js","compact-encoding":"/node_modules/compact-encoding/index.js","nanoassert":"/node_modules/nanoassert/index.js","ready-resource":"/node_modules/ready-resource/index.js","signal-promise":"/node_modules/signal-promise/index.js"},"/node_modules/autobase/package.json":{},"/node_modules/b4a/index.js":{"#package":"/node_modules/b4a/package.json"},"/node_modules/b4a/package.json":{},"/node_modules/bare-crypto/binding.js":{"#package":"/node_modules/bare-crypto/package.json",".":"linked:libbare-crypto.1.12.0.so"},"/node_modules/bare-crypto/index.js":{"#package":"/node_modules/bare-crypto/package.json","./lib/cipher":"/node_modules/bare-crypto/lib/cipher.js","./lib/constants":"/node_modules/bare-crypto/lib/constants.js","./lib/hash":"/node_modules/bare-crypto/lib/hash.js","./lib/hmac":"/node_modules/bare-crypto/lib/hmac.js","./lib/key":"/node_modules/bare-crypto/lib/key.js","./lib/pbkdf2":"/node_modules/bare-crypto/lib/pbkdf2.js","./lib/random":"/node_modules/bare-crypto/lib/random.js","./lib/signature":"/node_modules/bare-crypto/lib/signature.js","./web":"/node_modules/bare-crypto/web.js"},"/node_modules/bare-crypto/lib/cipher.js":{"#package":"/node_modules/bare-crypto/package.json","../binding":"/node_modules/bare-crypto/binding.js","./constants":"/node_modules/bare-crypto/lib/constants.js","bare-stream":"/node_modules/bare-stream/index.js"},"/node_modules/bare-crypto/lib/constants.js":{"#package":"/node_modules/bare-crypto/package.json","../binding":"/node_modules/bare-crypto/binding.js","./errors":"/node_modules/bare-crypto/lib/errors.js"},"/node_modules/bare-crypto/lib/errors.js":{"#package":"/node_modules/bare-crypto/package.json"},"/node_modules/bare-crypto/lib/hash.js":{"#package":"/node_modules/bare-crypto/package.json","../binding":"/node_modules/bare-crypto/binding.js","./constants":"/node_modules/bare-crypto/lib/constants.js","bare-stream":"/node_modules/bare-stream/index.js"},"/node_modules/bare-crypto/lib/hmac.js":{"#package":"/node_modules/bare-crypto/package.json","../binding":"/node_modules/bare-crypto/binding.js","./constants":"/node_modules/bare-crypto/lib/constants.js","bare-stream":"/node_modules/bare-stream/index.js"},"/node_modules/bare-crypto/lib/key.js":{"#package":"/node_modules/bare-crypto/package.json","../binding":"/node_modules/bare-crypto/binding.js","./constants":"/node_modules/bare-crypto/lib/constants.js"},"/node_modules/bare-crypto/lib/pbkdf2.js":{"#package":"/node_modules/bare-crypto/package.json","../binding":"/node_modules/bare-crypto/binding.js","./constants":"/node_modules/bare-crypto/lib/constants.js"},"/node_modules/bare-crypto/lib/random.js":{"#package":"/node_modules/bare-crypto/package.json","../binding":"/node_modules/bare-crypto/binding.js"},"/node_modules/bare-crypto/lib/signature.js":{"#package":"/node_modules/bare-crypto/package.json","../binding":"/node_modules/bare-crypto/binding.js","./constants":"/node_modules/bare-crypto/lib/constants.js"},"/node_modules/bare-crypto/lib/web/algorithm/ed25519.js":{"#package":"/node_modules/bare-crypto/package.json","../../..":"/node_modules/bare-crypto/index.js","../../../binding":"/node_modules/bare-crypto/binding.js","../../errors":"/node_modules/bare-crypto/lib/errors.js","../../key":"/node_modules/bare-crypto/lib/key.js","../crypto-key":"/node_modules/bare-crypto/lib/web/crypto-key.js"},"/node_modules/bare-crypto/lib/web/algorithm/hmac.js":{"#package":"/node_modules/bare-crypto/package.json","../../..":"/node_modules/bare-crypto/index.js","../../errors":"/node_modules/bare-crypto/lib/errors.js","../crypto-key":"/node_modules/bare-crypto/lib/web/crypto-key.js"},"/node_modules/bare-crypto/lib/web/algorithm/pbkdf2.js":{"#package":"/node_modules/bare-crypto/package.json","../../..":"/node_modules/bare-crypto/index.js","../../errors":"/node_modules/bare-crypto/lib/errors.js","../crypto-key":"/node_modules/bare-crypto/lib/web/crypto-key.js"},"/node_modules/bare-crypto/lib/web/algorithm/sha.js":{"#package":"/node_modules/bare-crypto/package.json","../../..":"/node_modules/bare-crypto/index.js"},"/node_modules/bare-crypto/lib/web/crypto-key.js":{"#package":"/node_modules/bare-crypto/package.json"},"/node_modules/bare-crypto/package.json":{},"/node_modules/bare-crypto/web.js":{"#package":"/node_modules/bare-crypto/package.json",".":"/node_modules/bare-crypto/index.js","./lib/errors":"/node_modules/bare-crypto/lib/errors.js","./lib/web/algorithm/ed25519":"/node_modules/bare-crypto/lib/web/algorithm/ed25519.js","./lib/web/algorithm/hmac":"/node_modules/bare-crypto/lib/web/algorithm/hmac.js","./lib/web/algorithm/pbkdf2":"/node_modules/bare-crypto/lib/web/algorithm/pbkdf2.js","./lib/web/algorithm/sha":"/node_modules/bare-crypto/lib/web/algorithm/sha.js","./lib/web/crypto-key":"/node_modules/bare-crypto/lib/web/crypto-key.js"},"/node_modules/bare-events/index.js":{"#package":"/node_modules/bare-events/package.json","./lib/errors":"/node_modules/bare-events/lib/errors.js"},"/node_modules/bare-events/lib/errors.js":{"#package":"/node_modules/bare-events/package.json"},"/node_modules/bare-events/package.json":{},"/node_modules/bare-fs/binding.js":{"#package":"/node_modules/bare-fs/package.json",".":"linked:libbare-fs.4.5.2.so"},"/node_modules/bare-fs/index.js":{"#package":"/node_modules/bare-fs/package.json","./binding":"/node_modules/bare-fs/binding.js","./lib/constants":"/node_modules/bare-fs/lib/constants.js","./lib/errors":"/node_modules/bare-fs/lib/errors.js","./promises":"/node_modules/bare-fs/promises.js","bare-events":"/node_modules/bare-events/index.js","bare-path":"/node_modules/bare-path/index.js","bare-stream":"/node_modules/bare-stream/index.js","bare-url":"/node_modules/bare-url/index.js","fast-fifo":"/node_modules/fast-fifo/index.js"},"/node_modules/bare-fs/lib/constants.js":{"#package":"/node_modules/bare-fs/package.json","../binding":"/node_modules/bare-fs/binding.js"},"/node_modules/bare-fs/lib/errors.js":{"#package":"/node_modules/bare-fs/package.json","bare-os":"/node_modules/bare-os/index.js"},"/node_modules/bare-fs/package.json":{},"/node_modules/bare-fs/promises.js":{"#package":"/node_modules/bare-fs/package.json",".":"/node_modules/bare-fs/index.js","bare-events":"/node_modules/bare-events/index.js"},"/node_modules/bare-os/binding.js":{"#package":"/node_modules/bare-os/package.json",".":"linked:libbare-os.3.6.2.so"},"/node_modules/bare-os/index.js":{"#package":"/node_modules/bare-os/package.json","./binding":"/node_modules/bare-os/binding.js","./lib/constants":"/node_modules/bare-os/lib/constants.js","./lib/errors":"/node_modules/bare-os/lib/errors.js"},"/node_modules/bare-os/lib/constants.js":{"#package":"/node_modules/bare-os/package.json","../binding":"/node_modules/bare-os/binding.js"},"/node_modules/bare-os/lib/errors.js":{"#package":"/node_modules/bare-os/package.json"},"/node_modules/bare-os/package.json":{},"/node_modules/bare-path/index.js":{"#package":"/node_modules/bare-path/package.json","./lib/posix":"/node_modules/bare-path/lib/posix.js","./lib/win32":"/node_modules/bare-path/lib/win32.js"},"/node_modules/bare-path/lib/constants.js":{"#package":"/node_modules/bare-path/package.json"},"/node_modules/bare-path/lib/posix.js":{"#package":"/node_modules/bare-path/package.json","./constants":"/node_modules/bare-path/lib/constants.js","./shared":"/node_modules/bare-path/lib/shared.js","./win32":"/node_modules/bare-path/lib/win32.js","bare-os":"/node_modules/bare-os/index.js"},"/node_modules/bare-path/lib/shared.js":{"#package":"/node_modules/bare-path/package.json","./constants":"/node_modules/bare-path/lib/constants.js"},"/node_modules/bare-path/lib/win32.js":{"#package":"/node_modules/bare-path/package.json","./constants":"/node_modules/bare-path/lib/constants.js","./posix":"/node_modules/bare-path/lib/posix.js","./shared":"/node_modules/bare-path/lib/shared.js","bare-os":"/node_modules/bare-os/index.js"},"/node_modules/bare-path/package.json":{},"/node_modules/bare-rpc/index.js":{"#package":"/node_modules/bare-rpc/package.json","./lib/command-router":"/node_modules/bare-rpc/lib/command-router.js","./lib/constants":"/node_modules/bare-rpc/lib/constants.js","./lib/incoming-request":"/node_modules/bare-rpc/lib/incoming-request.js","./lib/incoming-stream":"/node_modules/bare-rpc/lib/incoming-stream.js","./lib/messages":"/node_modules/bare-rpc/lib/messages.js","./lib/outgoing-request":"/node_modules/bare-rpc/lib/outgoing-request.js","./lib/outgoing-stream":"/node_modules/bare-rpc/lib/outgoing-stream.js","b4a":"/node_modules/b4a/index.js","compact-encoding":"/node_modules/compact-encoding/index.js","safety-catch":"/node_modules/safety-catch/index.js"},"/node_modules/bare-rpc/lib/command-router.js":{"#package":"/node_modules/bare-rpc/package.json","compact-encoding":"/node_modules/compact-encoding/index.js"},"/node_modules/bare-rpc/lib/constants.js":{"#package":"/node_modules/bare-rpc/package.json"},"/node_modules/bare-rpc/lib/errors.js":{"#package":"/node_modules/bare-rpc/package.json"},"/node_modules/bare-rpc/lib/incoming-request.js":{"#package":"/node_modules/bare-rpc/package.json","./errors":"/node_modules/bare-rpc/lib/errors.js","compact-encoding":"/node_modules/compact-encoding/index.js"},"/node_modules/bare-rpc/lib/incoming-stream.js":{"#package":"/node_modules/bare-rpc/package.json","./constants":"/node_modules/bare-rpc/lib/constants.js","bare-stream":"/node_modules/bare-stream/index.js"},"/node_modules/bare-rpc/lib/messages.js":{"#package":"/node_modules/bare-rpc/package.json","./constants":"/node_modules/bare-rpc/lib/constants.js","./errors":"/node_modules/bare-rpc/lib/errors.js","compact-encoding":"/node_modules/compact-encoding/index.js"},"/node_modules/bare-rpc/lib/outgoing-request.js":{"#package":"/node_modules/bare-rpc/package.json","./errors":"/node_modules/bare-rpc/lib/errors.js","compact-encoding":"/node_modules/compact-encoding/index.js"},"/node_modules/bare-rpc/lib/outgoing-stream.js":{"#package":"/node_modules/bare-rpc/package.json","./constants":"/node_modules/bare-rpc/lib/constants.js","bare-stream":"/node_modules/bare-stream/index.js"},"/node_modules/bare-rpc/package.json":{},"/node_modules/bare-stream/index.js":{"#package":"/node_modules/bare-stream/package.json","streamx":"/node_modules/streamx/index.js"},"/node_modules/bare-stream/package.json":{},"/node_modules/bare-url/binding.js":{"#package":"/node_modules/bare-url/package.json",".":"linked:libbare-url.2.3.2.so"},"/node_modules/bare-url/index.js":{"#package":"/node_modules/bare-url/package.json","./binding":"/node_modules/bare-url/binding.js","./lib/errors":"/node_modules/bare-url/lib/errors.js","./lib/url-search-params":"/node_modules/bare-url/lib/url-search-params.js","bare-path":"/node_modules/bare-path/index.js"},"/node_modules/bare-url/lib/errors.js":{"#package":"/node_modules/bare-url/package.json"},"/node_modules/bare-url/lib/url-search-params.js":{"#package":"/node_modules/bare-url/package.json"},"/node_modules/bare-url/package.json":{},"/node_modules/big-sparse-array/index.js":{"#package":"/node_modules/big-sparse-array/package.json"},"/node_modules/big-sparse-array/package.json":{},"/node_modules/bits-to-bytes/index.js":{"#package":"/node_modules/bits-to-bytes/package.json","b4a":"/node_modules/b4a/index.js"},"/node_modules/bits-to-bytes/package.json":{},"/node_modules/blind-relay/index.js":{"#package":"/node_modules/blind-relay/package.json","./lib/errors":"/node_modules/blind-relay/lib/errors.js","b4a":"/node_modules/b4a/index.js","bits-to-bytes":"/node_modules/bits-to-bytes/index.js","compact-encoding":"/node_modules/compact-encoding/index.js","compact-encoding-bitfield":"/node_modules/compact-encoding-bitfield/index.js","events":"/node_modules/bare-events/index.js","protomux":"/node_modules/protomux/index.js","sodium-universal":"/node_modules/sodium-universal/index.js","streamx":"/node_modules/streamx/index.js"},"/node_modules/blind-relay/lib/errors.js":{"#package":"/node_modules/blind-relay/package.json"},"/node_modules/blind-relay/package.json":{},"/node_modules/bogon/index.js":{"#package":"/node_modules/bogon/package.json","b4a":"/node_modules/b4a/index.js","compact-encoding":"/node_modules/compact-encoding/index.js","compact-encoding-net":"/node_modules/compact-encoding-net/index.js"},"/node_modules/bogon/package.json":{},"/node_modules/codecs/index.js":{"#package":"/node_modules/codecs/package.json","b4a":"/node_modules/b4a/index.js"},"/node_modules/codecs/package.json":{},"/node_modules/compact-encoding-bitfield/index.js":{"#package":"/node_modules/compact-encoding-bitfield/package.json","compact-encoding":"/node_modules/compact-encoding/index.js"},"/node_modules/compact-encoding-bitfield/package.json":{},"/node_modules/compact-encoding-net/index.js":{"#package":"/node_modules/compact-encoding-net/package.json","compact-encoding":"/node_modules/compact-encoding/index.js"},"/node_modules/compact-encoding-net/package.json":{},"/node_modules/compact-encoding/endian.js":{"#package":"/node_modules/compact-encoding/package.json"},"/node_modules/compact-encoding/index.js":{"#package":"/node_modules/compact-encoding/package.json","./endian":"/node_modules/compact-encoding/endian.js","./lexint":"/node_modules/compact-encoding/lexint.js","./raw":"/node_modules/compact-encoding/raw.js","b4a":"/node_modules/b4a/index.js"},"/node_modules/compact-encoding/lexint.js":{"#package":"/node_modules/compact-encoding/package.json"},"/node_modules/compact-encoding/package.json":{},"/node_modules/compact-encoding/raw.js":{"#package":"/node_modules/compact-encoding/package.json","./endian":"/node_modules/compact-encoding/endian.js","b4a":"/node_modules/b4a/index.js"},"/node_modules/core-coupler/index.js":{"#package":"/node_modules/core-coupler/package.json","safety-catch":"/node_modules/safety-catch/index.js"},"/node_modules/core-coupler/package.json":{},"/node_modules/corestore/index.js":{"#package":"/node_modules/corestore/package.json","./lib/audit.js":"/node_modules/corestore/lib/audit.js","b4a":"/node_modules/b4a/index.js","hypercore":"/node_modules/hypercore/index.js","hypercore-crypto":"/node_modules/hypercore-crypto/index.js","hypercore-errors":"/node_modules/hypercore-errors/index.js","hypercore-id-encoding":"/node_modules/hypercore-id-encoding/index.js","ready-resource":"/node_modules/ready-resource/index.js","sodium-universal":"/node_modules/sodium-universal/index.js","which-runtime":"/node_modules/which-runtime/index.js"},"/node_modules/corestore/lib/audit.js":{"#package":"/node_modules/corestore/package.json"},"/node_modules/corestore/package.json":{},"/node_modules/debounceify/index.js":{"#package":"/node_modules/debounceify/package.json"},"/node_modules/debounceify/package.json":{},"/node_modules/device-file/index.js":{"#package":"/node_modules/device-file/package.json","b4a":"/node_modules/b4a/index.js","fd-lock":"/node_modules/fd-lock/index.js","fs":"/node_modules/bare-fs/index.js","fs-native-extensions":"/node_modules/fs-native-extensions/index.js","path":"/node_modules/bare-path/index.js","ready-resource":"/node_modules/ready-resource/index.js"},"/node_modules/device-file/package.json":{},"/node_modules/dht-rpc/index.js":{"#package":"/node_modules/dht-rpc/package.json","./lib/commands":"/node_modules/dht-rpc/lib/commands.js","./lib/errors":"/node_modules/dht-rpc/lib/errors.js","./lib/io":"/node_modules/dht-rpc/lib/io.js","./lib/peer":"/node_modules/dht-rpc/lib/peer.js","./lib/query":"/node_modules/dht-rpc/lib/query.js","./lib/session":"/node_modules/dht-rpc/lib/session.js","b4a":"/node_modules/b4a/index.js","compact-encoding":"/node_modules/compact-encoding/index.js","events":"/node_modules/bare-events/index.js","kademlia-routing-table":"/node_modules/kademlia-routing-table/index.js","nat-sampler":"/node_modules/nat-sampler/index.js","sodium-universal":"/node_modules/sodium-universal/index.js","time-ordered-set":"/node_modules/time-ordered-set/index.js","udx-native":"/node_modules/udx-native/lib/udx.js"},"/node_modules/dht-rpc/lib/commands.js":{"#package":"/node_modules/dht-rpc/package.json"},"/node_modules/dht-rpc/lib/errors.js":{"#package":"/node_modules/dht-rpc/package.json"},"/node_modules/dht-rpc/lib/io.js":{"#package":"/node_modules/dht-rpc/package.json","./errors":"/node_modules/dht-rpc/lib/errors.js","./peer":"/node_modules/dht-rpc/lib/peer.js","b4a":"/node_modules/b4a/index.js","compact-encoding":"/node_modules/compact-encoding/index.js","fast-fifo":"/node_modules/fast-fifo/index.js","sodium-universal":"/node_modules/sodium-universal/index.js"},"/node_modules/dht-rpc/lib/peer.js":{"#package":"/node_modules/dht-rpc/package.json","b4a":"/node_modules/b4a/index.js","compact-encoding":"/node_modules/compact-encoding/index.js","compact-encoding-net":"/node_modules/compact-encoding-net/index.js","sodium-universal":"/node_modules/sodium-universal/index.js"},"/node_modules/dht-rpc/lib/query.js":{"#package":"/node_modules/dht-rpc/package.json","./commands":"/node_modules/dht-rpc/lib/commands.js","./peer":"/node_modules/dht-rpc/lib/peer.js","b4a":"/node_modules/b4a/index.js","streamx":"/node_modules/streamx/index.js"},"/node_modules/dht-rpc/lib/session.js":{"#package":"/node_modules/dht-rpc/package.json"},"/node_modules/dht-rpc/package.json":{},"/node_modules/events-universal/bare.js":{"#package":"/node_modules/events-universal/package.json","bare-events":"/node_modules/bare-events/index.js"},"/node_modules/events-universal/package.json":{},"/node_modules/fast-fifo/fixed-size.js":{"#package":"/node_modules/fast-fifo/package.json"},"/node_modules/fast-fifo/index.js":{"#package":"/node_modules/fast-fifo/package.json","./fixed-size":"/node_modules/fast-fifo/fixed-size.js"},"/node_modules/fast-fifo/package.json":{},"/node_modules/fd-lock/index.js":{"#package":"/node_modules/fd-lock/package.json","fs":"/node_modules/bare-fs/index.js","fs-native-extensions":"/node_modules/fs-native-extensions/index.js","ready-resource":"/node_modules/ready-resource/index.js","resource-on-exit":"/node_modules/resource-on-exit/index.js"},"/node_modules/fd-lock/package.json":{},"/node_modules/flat-tree/index.js":{"#package":"/node_modules/flat-tree/package.json"},"/node_modules/flat-tree/package.json":{},"/node_modules/fs-native-extensions/binding.js":{"#package":"/node_modules/fs-native-extensions/package.json",".":"linked:libfs-native-extensions.1.4.5.so","require-addon":"/node_modules/require-addon/lib/bare.js"},"/node_modules/fs-native-extensions/index.js":{"#package":"/node_modules/fs-native-extensions/package.json","./binding":"/node_modules/fs-native-extensions/binding.js","which-runtime":"/node_modules/which-runtime/index.js"},"/node_modules/fs-native-extensions/package.json":{},"/node_modules/hyperbee/index.js":{"#package":"/node_modules/hyperbee/package.json","./iterators/diff":"/node_modules/hyperbee/iterators/diff.js","./iterators/history":"/node_modules/hyperbee/iterators/history.js","./iterators/local":"/node_modules/hyperbee/iterators/local.js","./iterators/range":"/node_modules/hyperbee/iterators/range.js","./lib/extension":"/node_modules/hyperbee/lib/extension.js","./lib/messages":"/node_modules/hyperbee/lib/messages.js","b4a":"/node_modules/b4a/index.js","codecs":"/node_modules/codecs/index.js","debounceify":"/node_modules/debounceify/index.js","hypercore-errors":"/node_modules/hypercore-errors/index.js","mutexify/promise":"/node_modules/mutexify/promise.js","rache":"/node_modules/rache/index.js","ready-resource":"/node_modules/ready-resource/index.js","resolve-reject-promise":"/node_modules/resolve-reject-promise/index.js","safety-catch":"/node_modules/safety-catch/index.js","streamx":"/node_modules/streamx/index.js","unslab":"/node_modules/unslab/index.js"},"/node_modules/hyperbee/iterators/diff.js":{"#package":"/node_modules/hyperbee/package.json","b4a":"/node_modules/b4a/index.js"},"/node_modules/hyperbee/iterators/history.js":{"#package":"/node_modules/hyperbee/package.json"},"/node_modules/hyperbee/iterators/local.js":{"#package":"/node_modules/hyperbee/package.json"},"/node_modules/hyperbee/iterators/range.js":{"#package":"/node_modules/hyperbee/package.json","b4a":"/node_modules/b4a/index.js"},"/node_modules/hyperbee/lib/extension.js":{"#package":"/node_modules/hyperbee/package.json","./messages":"/node_modules/hyperbee/lib/messages.js"},"/node_modules/hyperbee/lib/messages.js":{"#package":"/node_modules/hyperbee/package.json","b4a":"/node_modules/b4a/index.js","protocol-buffers-encodings":"/node_modules/protocol-buffers-encodings/index.js"},"/node_modules/hyperbee/package.json":{},"/node_modules/hypercore-crypto/index.js":{"#package":"/node_modules/hypercore-crypto/package.json","b4a":"/node_modules/b4a/index.js","compact-encoding":"/node_modules/compact-encoding/index.js","sodium-universal":"/node_modules/sodium-universal/index.js"},"/node_modules/hypercore-crypto/package.json":{},"/node_modules/hypercore-errors/index.js":{"#package":"/node_modules/hypercore-errors/package.json","hypercore-id-encoding":"/node_modules/hypercore-id-encoding/index.js"},"/node_modules/hypercore-errors/package.json":{},"/node_modules/hypercore-id-encoding/index.js":{"#package":"/node_modules/hypercore-id-encoding/package.json","b4a":"/node_modules/b4a/index.js","z32":"/node_modules/z32/index.js"},"/node_modules/hypercore-id-encoding/package.json":{},"/node_modules/hypercore-storage/index.js":{"#package":"/node_modules/hypercore-storage/package.json","./lib/keys.js":"/node_modules/hypercore-storage/lib/keys.js","./lib/streams.js":"/node_modules/hypercore-storage/lib/streams.js","./lib/tx.js":"/node_modules/hypercore-storage/lib/tx.js","./lib/view.js":"/node_modules/hypercore-storage/lib/view.js","./migrations/0":"/node_modules/hypercore-storage/migrations/0/index.js","device-file":"/node_modules/device-file/index.js","fs":"/node_modules/bare-fs/index.js","path":"/node_modules/bare-path/index.js","resolve-reject-promise":"/node_modules/resolve-reject-promise/index.js","rocksdb-native":"/node_modules/rocksdb-native/index.js","scope-lock":"/node_modules/scope-lock/index.js"},"/node_modules/hypercore-storage/lib/block-dependency-stream.js":{"#package":"/node_modules/hypercore-storage/package.json","./keys":"/node_modules/hypercore-storage/lib/keys.js","streamx":"/node_modules/streamx/index.js"},"/node_modules/hypercore-storage/lib/close-error-stream.js":{"#package":"/node_modules/hypercore-storage/package.json","streamx":"/node_modules/streamx/index.js"},"/node_modules/hypercore-storage/lib/keys.js":{"#package":"/node_modules/hypercore-storage/package.json","b4a":"/node_modules/b4a/index.js","compact-encoding":"/node_modules/compact-encoding/index.js","index-encoder":"/node_modules/index-encoder/index.js"},"/node_modules/hypercore-storage/lib/streams.js":{"#package":"/node_modules/hypercore-storage/package.json","../spec/hyperschema":"/node_modules/hypercore-storage/spec/hyperschema/index.js","./block-dependency-stream.js":"/node_modules/hypercore-storage/lib/block-dependency-stream.js","./keys.js":"/node_modules/hypercore-storage/lib/keys.js","b4a":"/node_modules/b4a/index.js"},"/node_modules/hypercore-storage/lib/tx.js":{"#package":"/node_modules/hypercore-storage/package.json","../spec/hyperschema":"/node_modules/hypercore-storage/spec/hyperschema/index.js","./keys.js":"/node_modules/hypercore-storage/lib/keys.js","./view.js":"/node_modules/hypercore-storage/lib/view.js","b4a":"/node_modules/b4a/index.js","flat-tree":"/node_modules/flat-tree/index.js"},"/node_modules/hypercore-storage/lib/view.js":{"#package":"/node_modules/hypercore-storage/package.json","./close-error-stream.js":"/node_modules/hypercore-storage/lib/close-error-stream.js","b4a":"/node_modules/b4a/index.js","streamx":"/node_modules/streamx/index.js"},"/node_modules/hypercore-storage/migrations/0/index.js":{"#package":"/node_modules/hypercore-storage/package.json","../../lib/tx.js":"/node_modules/hypercore-storage/lib/tx.js","../../lib/view.js":"/node_modules/hypercore-storage/lib/view.js","./messages.js":"/node_modules/hypercore-storage/migrations/0/messages.js","b4a":"/node_modules/b4a/index.js","compact-encoding":"/node_modules/compact-encoding/index.js","flat-tree":"/node_modules/flat-tree/index.js","fs":"/node_modules/bare-fs/index.js","hypercore-crypto":"/node_modules/hypercore-crypto/index.js","path":"/node_modules/bare-path/index.js","streamx":"/node_modules/streamx/index.js"},"/node_modules/hypercore-storage/migrations/0/messages.js":{"#package":"/node_modules/hypercore-storage/package.json","b4a":"/node_modules/b4a/index.js","compact-encoding":"/node_modules/compact-encoding/index.js"},"/node_modules/hypercore-storage/package.json":{},"/node_modules/hypercore-storage/spec/hyperschema/index.js":{"#package":"/node_modules/hypercore-storage/package.json","hyperschema/runtime":"/node_modules/hyperschema/runtime.cjs"},"/node_modules/hypercore/index.js":{"#package":"/node_modules/hypercore/package.json","./lib/caps":"/node_modules/hypercore/lib/caps.js","./lib/core":"/node_modules/hypercore/lib/core.js","./lib/default-encryption":"/node_modules/hypercore/lib/default-encryption.js","./lib/download":"/node_modules/hypercore/lib/download.js","./lib/info":"/node_modules/hypercore/lib/info.js","./lib/inspect":"/node_modules/hypercore/lib/inspect.js","./lib/merkle-tree":"/node_modules/hypercore/lib/merkle-tree.js","./lib/replicator":"/node_modules/hypercore/lib/replicator.js","./lib/streams":"/node_modules/hypercore/lib/streams.js","./lib/verifier":"/node_modules/hypercore/lib/verifier.js","@hyperswarm/secret-stream":"/node_modules/@hyperswarm/secret-stream/index.js","b4a":"/node_modules/b4a/index.js","compact-encoding":"/node_modules/compact-encoding/index.js","events":"/node_modules/bare-events/index.js","hypercore-crypto":"/node_modules/hypercore-crypto/index.js","hypercore-errors":"/node_modules/hypercore-errors/index.js","hypercore-id-encoding":"/node_modules/hypercore-id-encoding/index.js","hypercore-storage":"/node_modules/hypercore-storage/index.js","is-options":"/node_modules/is-options/index.js","protomux":"/node_modules/protomux/index.js","safety-catch":"/node_modules/safety-catch/index.js","unslab":"/node_modules/unslab/index.js"},"/node_modules/hypercore/lib/audit.js":{"#package":"/node_modules/hypercore/package.json","./merkle-tree":"/node_modules/hypercore/lib/merkle-tree.js","b4a":"/node_modules/b4a/index.js","flat-tree":"/node_modules/flat-tree/index.js","hypercore-crypto":"/node_modules/hypercore-crypto/index.js"},"/node_modules/hypercore/lib/bit-interlude.js":{"#package":"/node_modules/hypercore/package.json","./compat":"/node_modules/hypercore/lib/compat.js","b4a":"/node_modules/b4a/index.js"},"/node_modules/hypercore/lib/bitfield.js":{"#package":"/node_modules/hypercore/package.json","./compat":"/node_modules/hypercore/lib/compat.js","b4a":"/node_modules/b4a/index.js","big-sparse-array":"/node_modules/big-sparse-array/index.js"},"/node_modules/hypercore/lib/caps.js":{"#package":"/node_modules/hypercore/package.json","b4a":"/node_modules/b4a/index.js","compact-encoding":"/node_modules/compact-encoding/index.js","hypercore-crypto":"/node_modules/hypercore-crypto/index.js","sodium-universal":"/node_modules/sodium-universal/index.js"},"/node_modules/hypercore/lib/compat.js":{"#package":"/node_modules/hypercore/package.json","quickbit-universal":"/node_modules/quickbit-universal/index.js","quickbit-universal/fallback":"/node_modules/quickbit-universal/fallback.js"},"/node_modules/hypercore/lib/copy-prologue.js":{"#package":"/node_modules/hypercore/package.json","./bitfield":"/node_modules/hypercore/lib/bitfield.js","b4a":"/node_modules/b4a/index.js","flat-tree":"/node_modules/flat-tree/index.js","hypercore-crypto":"/node_modules/hypercore-crypto/index.js","quickbit-universal":"/node_modules/quickbit-universal/index.js"},"/node_modules/hypercore/lib/core.js":{"#package":"/node_modules/hypercore/package.json","./audit":"/node_modules/hypercore/lib/audit.js","./bit-interlude":"/node_modules/hypercore/lib/bit-interlude.js","./bitfield":"/node_modules/hypercore/lib/bitfield.js","./copy-prologue":"/node_modules/hypercore/lib/copy-prologue.js","./merkle-tree":"/node_modules/hypercore/lib/merkle-tree.js","./mutex":"/node_modules/hypercore/lib/mutex.js","./remote-bitfield":"/node_modules/hypercore/lib/remote-bitfield.js","./replicator":"/node_modules/hypercore/lib/replicator.js","./session-state":"/node_modules/hypercore/lib/session-state.js","./verifier":"/node_modules/hypercore/lib/verifier.js","b4a":"/node_modules/b4a/index.js","hypercore-crypto":"/node_modules/hypercore-crypto/index.js","hypercore-errors":"/node_modules/hypercore-errors/index.js","unslab":"/node_modules/unslab/index.js","z32":"/node_modules/z32/index.js"},"/node_modules/hypercore/lib/default-encryption.js":{"#package":"/node_modules/hypercore/package.json","./caps":"/node_modules/hypercore/lib/caps.js","b4a":"/node_modules/b4a/index.js","compact-encoding":"/node_modules/compact-encoding/index.js","sodium-universal":"/node_modules/sodium-universal/index.js"},"/node_modules/hypercore/lib/download.js":{"#package":"/node_modules/hypercore/package.json"},"/node_modules/hypercore/lib/hotswap-queue.js":{"#package":"/node_modules/hypercore/package.json"},"/node_modules/hypercore/lib/info.js":{"#package":"/node_modules/hypercore/package.json"},"/node_modules/hypercore/lib/inspect.js":{"#package":"/node_modules/hypercore/package.json","b4a":"/node_modules/b4a/index.js"},"/node_modules/hypercore/lib/merkle-tree.js":{"#package":"/node_modules/hypercore/package.json","./caps":"/node_modules/hypercore/lib/caps.js","b4a":"/node_modules/b4a/index.js","flat-tree":"/node_modules/flat-tree/index.js","hypercore-crypto":"/node_modules/hypercore-crypto/index.js","hypercore-errors":"/node_modules/hypercore-errors/index.js","unslab":"/node_modules/unslab/index.js"},"/node_modules/hypercore/lib/messages.js":{"#package":"/node_modules/hypercore/package.json","./caps":"/node_modules/hypercore/lib/caps.js","b4a":"/node_modules/b4a/index.js","compact-encoding":"/node_modules/compact-encoding/index.js","hypercore-errors":"/node_modules/hypercore-errors/index.js","unslab":"/node_modules/unslab/index.js"},"/node_modules/hypercore/lib/multisig.js":{"#package":"/node_modules/hypercore/package.json","./merkle-tree":"/node_modules/hypercore/lib/merkle-tree.js","./messages":"/node_modules/hypercore/lib/messages.js","b4a":"/node_modules/b4a/index.js","compact-encoding":"/node_modules/compact-encoding/index.js","flat-tree":"/node_modules/flat-tree/index.js"},"/node_modules/hypercore/lib/mutex.js":{"#package":"/node_modules/hypercore/package.json"},"/node_modules/hypercore/lib/receiver-queue.js":{"#package":"/node_modules/hypercore/package.json","fast-fifo":"/node_modules/fast-fifo/index.js"},"/node_modules/hypercore/lib/remote-bitfield.js":{"#package":"/node_modules/hypercore/package.json","./compat":"/node_modules/hypercore/lib/compat.js","big-sparse-array":"/node_modules/big-sparse-array/index.js"},"/node_modules/hypercore/lib/replicator.js":{"#package":"/node_modules/hypercore/package.json","./caps":"/node_modules/hypercore/lib/caps.js","./hotswap-queue":"/node_modules/hypercore/lib/hotswap-queue.js","./merkle-tree":"/node_modules/hypercore/lib/merkle-tree.js","./messages":"/node_modules/hypercore/lib/messages.js","./mutex":"/node_modules/hypercore/lib/mutex.js","./receiver-queue":"/node_modules/hypercore/lib/receiver-queue.js","./remote-bitfield":"/node_modules/hypercore/lib/remote-bitfield.js","b4a":"/node_modules/b4a/index.js","flat-tree":"/node_modules/flat-tree/index.js","hypercore-errors":"/node_modules/hypercore-errors/index.js","random-array-iterator":"/node_modules/random-array-iterator/index.js","safety-catch":"/node_modules/safety-catch/index.js"},"/node_modules/hypercore/lib/session-state.js":{"#package":"/node_modules/hypercore/package.json","./bitfield":"/node_modules/hypercore/lib/bitfield.js","./merkle-tree":"/node_modules/hypercore/lib/merkle-tree.js","./mutex":"/node_modules/hypercore/lib/mutex.js","b4a":"/node_modules/b4a/index.js","flat-tree":"/node_modules/flat-tree/index.js","hypercore-crypto":"/node_modules/hypercore-crypto/index.js","hypercore-errors":"/node_modules/hypercore-errors/index.js","nanoassert":"/node_modules/nanoassert/index.js","quickbit-universal":"/node_modules/quickbit-universal/index.js"},"/node_modules/hypercore/lib/streams.js":{"#package":"/node_modules/hypercore/package.json","streamx":"/node_modules/streamx/index.js"},"/node_modules/hypercore/lib/verifier.js":{"#package":"/node_modules/hypercore/package.json","./caps":"/node_modules/hypercore/lib/caps.js","./messages":"/node_modules/hypercore/lib/messages.js","./multisig":"/node_modules/hypercore/lib/multisig.js","b4a":"/node_modules/b4a/index.js","compact-encoding":"/node_modules/compact-encoding/index.js","flat-tree":"/node_modules/flat-tree/index.js","hypercore-crypto":"/node_modules/hypercore-crypto/index.js","hypercore-errors":"/node_modules/hypercore-errors/index.js","unslab":"/node_modules/unslab/index.js"},"/node_modules/hypercore/package.json":{},"/node_modules/hyperdht/index.js":{"#package":"/node_modules/hyperdht/package.json","./lib/connect":"/node_modules/hyperdht/lib/connect.js","./lib/connection-pool":"/node_modules/hyperdht/lib/connection-pool.js","./lib/constants":"/node_modules/hyperdht/lib/constants.js","./lib/crypto":"/node_modules/hyperdht/lib/crypto.js","./lib/errors":"/node_modules/hyperdht/lib/errors.js","./lib/messages":"/node_modules/hyperdht/lib/messages.js","./lib/persistent":"/node_modules/hyperdht/lib/persistent.js","./lib/raw-stream-set":"/node_modules/hyperdht/lib/raw-stream-set.js","./lib/router":"/node_modules/hyperdht/lib/router.js","./lib/server":"/node_modules/hyperdht/lib/server.js","./lib/socket-pool":"/node_modules/hyperdht/lib/socket-pool.js","b4a":"/node_modules/b4a/index.js","compact-encoding":"/node_modules/compact-encoding/index.js","dht-rpc":"/node_modules/dht-rpc/index.js","hypercore-id-encoding":"/node_modules/hypercore-id-encoding/index.js","safety-catch":"/node_modules/safety-catch/index.js","sodium-universal":"/node_modules/sodium-universal/index.js","xache":"/node_modules/xache/index.js"},"/node_modules/hyperdht/lib/announcer.js":{"#package":"/node_modules/hyperdht/package.json","./constants":"/node_modules/hyperdht/lib/constants.js","./encode":"/node_modules/hyperdht/lib/encode.js","./messages":"/node_modules/hyperdht/lib/messages.js","./persistent":"/node_modules/hyperdht/lib/persistent.js","./sleeper":"/node_modules/hyperdht/lib/sleeper.js","compact-encoding":"/node_modules/compact-encoding/index.js","safety-catch":"/node_modules/safety-catch/index.js","signal-promise":"/node_modules/signal-promise/index.js"},"/node_modules/hyperdht/lib/connect.js":{"#package":"/node_modules/hyperdht/package.json","./constants":"/node_modules/hyperdht/lib/constants.js","./crypto":"/node_modules/hyperdht/lib/crypto.js","./errors":"/node_modules/hyperdht/lib/errors.js","./holepuncher":"/node_modules/hyperdht/lib/holepuncher.js","./noise-wrap":"/node_modules/hyperdht/lib/noise-wrap.js","./secure-payload":"/node_modules/hyperdht/lib/secure-payload.js","./semaphore":"/node_modules/hyperdht/lib/semaphore.js","./sleeper":"/node_modules/hyperdht/lib/sleeper.js","@hyperswarm/secret-stream":"/node_modules/@hyperswarm/secret-stream/index.js","b4a":"/node_modules/b4a/index.js","blind-relay":"/node_modules/blind-relay/index.js","bogon":"/node_modules/bogon/index.js","safety-catch":"/node_modules/safety-catch/index.js","unslab":"/node_modules/unslab/index.js"},"/node_modules/hyperdht/lib/connection-pool.js":{"#package":"/node_modules/hyperdht/package.json","./errors":"/node_modules/hyperdht/lib/errors.js","b4a":"/node_modules/b4a/index.js","events":"/node_modules/bare-events/index.js"},"/node_modules/hyperdht/lib/constants.js":{"#package":"/node_modules/hyperdht/package.json","hypercore-crypto":"/node_modules/hypercore-crypto/index.js"},"/node_modules/hyperdht/lib/crypto.js":{"#package":"/node_modules/hyperdht/package.json","b4a":"/node_modules/b4a/index.js","sodium-universal":"/node_modules/sodium-universal/index.js"},"/node_modules/hyperdht/lib/encode.js":{"#package":"/node_modules/hyperdht/package.json","b4a":"/node_modules/b4a/index.js","compact-encoding":"/node_modules/compact-encoding/index.js"},"/node_modules/hyperdht/lib/errors.js":{"#package":"/node_modules/hyperdht/package.json"},"/node_modules/hyperdht/lib/holepuncher.js":{"#package":"/node_modules/hyperdht/package.json","./constants":"/node_modules/hyperdht/lib/constants.js","./nat":"/node_modules/hyperdht/lib/nat.js","./sleeper":"/node_modules/hyperdht/lib/sleeper.js","b4a":"/node_modules/b4a/index.js"},"/node_modules/hyperdht/lib/messages.js":{"#package":"/node_modules/hyperdht/package.json","compact-encoding":"/node_modules/compact-encoding/index.js","compact-encoding-net":"/node_modules/compact-encoding-net/index.js"},"/node_modules/hyperdht/lib/nat.js":{"#package":"/node_modules/hyperdht/package.json","../lib/constants":"/node_modules/hyperdht/lib/constants.js"},"/node_modules/hyperdht/lib/noise-wrap.js":{"#package":"/node_modules/hyperdht/package.json","./constants":"/node_modules/hyperdht/lib/constants.js","./errors":"/node_modules/hyperdht/lib/errors.js","./messages":"/node_modules/hyperdht/lib/messages.js","@hyperswarm/secret-stream":"/node_modules/@hyperswarm/secret-stream/index.js","b4a":"/node_modules/b4a/index.js","compact-encoding":"/node_modules/compact-encoding/index.js","noise-curve-ed":"/node_modules/noise-curve-ed/index.js","noise-handshake":"/node_modules/noise-handshake/noise.js","sodium-universal":"/node_modules/sodium-universal/index.js"},"/node_modules/hyperdht/lib/persistent.js":{"#package":"/node_modules/hyperdht/package.json","./constants":"/node_modules/hyperdht/lib/constants.js","./encode":"/node_modules/hyperdht/lib/encode.js","./messages":"/node_modules/hyperdht/lib/messages.js","b4a":"/node_modules/b4a/index.js","compact-encoding":"/node_modules/compact-encoding/index.js","record-cache":"/node_modules/record-cache/index.js","sodium-universal":"/node_modules/sodium-universal/index.js","unslab":"/node_modules/unslab/index.js","xache":"/node_modules/xache/index.js"},"/node_modules/hyperdht/lib/raw-stream-set.js":{"#package":"/node_modules/hyperdht/package.json"},"/node_modules/hyperdht/lib/router.js":{"#package":"/node_modules/hyperdht/package.json","./constants":"/node_modules/hyperdht/lib/constants.js","./errors":"/node_modules/hyperdht/lib/errors.js","./messages":"/node_modules/hyperdht/lib/messages.js","b4a":"/node_modules/b4a/index.js","compact-encoding":"/node_modules/compact-encoding/index.js","safety-catch":"/node_modules/safety-catch/index.js","xache":"/node_modules/xache/index.js"},"/node_modules/hyperdht/lib/secure-payload.js":{"#package":"/node_modules/hyperdht/package.json","./messages":"/node_modules/hyperdht/lib/messages.js","b4a":"/node_modules/b4a/index.js","sodium-universal":"/node_modules/sodium-universal/index.js"},"/node_modules/hyperdht/lib/semaphore.js":{"#package":"/node_modules/hyperdht/package.json"},"/node_modules/hyperdht/lib/server.js":{"#package":"/node_modules/hyperdht/package.json","./announcer":"/node_modules/hyperdht/lib/announcer.js","./constants":"/node_modules/hyperdht/lib/constants.js","./crypto":"/node_modules/hyperdht/lib/crypto.js","./errors":"/node_modules/hyperdht/lib/errors.js","./holepuncher":"/node_modules/hyperdht/lib/holepuncher.js","./noise-wrap":"/node_modules/hyperdht/lib/noise-wrap.js","./secure-payload":"/node_modules/hyperdht/lib/secure-payload.js","@hyperswarm/secret-stream":"/node_modules/@hyperswarm/secret-stream/index.js","b4a":"/node_modules/b4a/index.js","blind-relay":"/node_modules/blind-relay/index.js","bogon":"/node_modules/bogon/index.js","events":"/node_modules/bare-events/index.js","safety-catch":"/node_modules/safety-catch/index.js"},"/node_modules/hyperdht/lib/sleeper.js":{"#package":"/node_modules/hyperdht/package.json"},"/node_modules/hyperdht/lib/socket-pool.js":{"#package":"/node_modules/hyperdht/package.json","b4a":"/node_modules/b4a/index.js"},"/node_modules/hyperdht/package.json":{},"/node_modules/hyperschema/package.json":{},"/node_modules/hyperschema/runtime.cjs":{"#package":"/node_modules/hyperschema/package.json","compact-encoding":"/node_modules/compact-encoding/index.js"},"/node_modules/hyperswarm/index.js":{"#package":"/node_modules/hyperswarm/package.json","./lib/connection-set":"/node_modules/hyperswarm/lib/connection-set.js","./lib/peer-discovery":"/node_modules/hyperswarm/lib/peer-discovery.js","./lib/peer-info":"/node_modules/hyperswarm/lib/peer-info.js","./lib/retry-timer":"/node_modules/hyperswarm/lib/retry-timer.js","b4a":"/node_modules/b4a/index.js","events":"/node_modules/bare-events/index.js","hyperdht":"/node_modules/hyperdht/index.js","shuffled-priority-queue":"/node_modules/shuffled-priority-queue/index.js","streamx":"/node_modules/streamx/index.js","unslab":"/node_modules/unslab/index.js"},"/node_modules/hyperswarm/lib/bulk-timer.js":{"#package":"/node_modules/hyperswarm/package.json"},"/node_modules/hyperswarm/lib/connection-set.js":{"#package":"/node_modules/hyperswarm/package.json","b4a":"/node_modules/b4a/index.js"},"/node_modules/hyperswarm/lib/peer-discovery.js":{"#package":"/node_modules/hyperswarm/package.json","b4a":"/node_modules/b4a/index.js","safety-catch":"/node_modules/safety-catch/index.js"},"/node_modules/hyperswarm/lib/peer-info.js":{"#package":"/node_modules/hyperswarm/package.json","b4a":"/node_modules/b4a/index.js","events":"/node_modules/bare-events/index.js","unslab":"/node_modules/unslab/index.js"},"/node_modules/hyperswarm/lib/retry-timer.js":{"#package":"/node_modules/hyperswarm/package.json","./bulk-timer":"/node_modules/hyperswarm/lib/bulk-timer.js"},"/node_modules/hyperswarm/package.json":{},"/node_modules/index-encoder/index.js":{"#package":"/node_modules/index-encoder/package.json","b4a":"/node_modules/b4a/index.js"},"/node_modules/index-encoder/package.json":{},"/node_modules/is-options/index.js":{"#package":"/node_modules/is-options/package.json","b4a":"/node_modules/b4a/index.js"},"/node_modules/is-options/package.json":{},"/node_modules/kademlia-routing-table/index.js":{"#package":"/node_modules/kademlia-routing-table/package.json","events":"/node_modules/bare-events/index.js"},"/node_modules/kademlia-routing-table/package.json":{},"/node_modules/mutexify/index.js":{"#package":"/node_modules/mutexify/package.json","queue-tick":"/node_modules/queue-tick/process-next-tick.js"},"/node_modules/mutexify/package.json":{},"/node_modules/mutexify/promise.js":{"#package":"/node_modules/mutexify/package.json",".":"/node_modules/mutexify/index.js"},"/node_modules/nanoassert/index.js":{"#package":"/node_modules/nanoassert/package.json"},"/node_modules/nanoassert/package.json":{},"/node_modules/nat-sampler/index.js":{"#package":"/node_modules/nat-sampler/package.json"},"/node_modules/nat-sampler/package.json":{},"/node_modules/noise-curve-ed/index.js":{"#package":"/node_modules/noise-curve-ed/package.json","b4a":"/node_modules/b4a/index.js","nanoassert":"/node_modules/nanoassert/index.js","sodium-universal":"/node_modules/sodium-universal/index.js"},"/node_modules/noise-curve-ed/package.json":{},"/node_modules/noise-handshake/cipher.js":{"#package":"/node_modules/noise-handshake/package.json","b4a":"/node_modules/b4a/index.js","sodium-universal":"/node_modules/sodium-universal/index.js"},"/node_modules/noise-handshake/dh.js":{"#package":"/node_modules/noise-handshake/package.json","b4a":"/node_modules/b4a/index.js","nanoassert":"/node_modules/nanoassert/index.js","sodium-universal":"/node_modules/sodium-universal/index.js"},"/node_modules/noise-handshake/hkdf.js":{"#package":"/node_modules/noise-handshake/package.json","./hmac":"/node_modules/noise-handshake/hmac.js","b4a":"/node_modules/b4a/index.js"},"/node_modules/noise-handshake/hmac.js":{"#package":"/node_modules/noise-handshake/package.json","b4a":"/node_modules/b4a/index.js","sodium-universal":"/node_modules/sodium-universal/index.js"},"/node_modules/noise-handshake/noise.js":{"#package":"/node_modules/noise-handshake/package.json","./hkdf":"/node_modules/noise-handshake/hkdf.js","./symmetric-state":"/node_modules/noise-handshake/symmetric-state.js","b4a":"/node_modules/b4a/index.js","nanoassert":"/node_modules/nanoassert/index.js"},"/node_modules/noise-handshake/package.json":{},"/node_modules/noise-handshake/symmetric-state.js":{"#package":"/node_modules/noise-handshake/package.json","./cipher":"/node_modules/noise-handshake/cipher.js","./dh":"/node_modules/noise-handshake/dh.js","./hkdf":"/node_modules/noise-handshake/hkdf.js","b4a":"/node_modules/b4a/index.js","nanoassert":"/node_modules/nanoassert/index.js","sodium-universal":"/node_modules/sodium-universal/index.js"},"/node_modules/protocol-buffers-encodings/index.js":{"#package":"/node_modules/protocol-buffers-encodings/package.json","b4a":"/node_modules/b4a/index.js","signed-varint":"/node_modules/signed-varint/index.js","varint":"/node_modules/varint/index.js"},"/node_modules/protocol-buffers-encodings/package.json":{},"/node_modules/protomux-wakeup/index.js":{"#package":"/node_modules/protomux-wakeup/package.json","./spec/hyperschema":"/node_modules/protomux-wakeup/spec/hyperschema/index.js","b4a":"/node_modules/b4a/index.js","hypercore-crypto":"/node_modules/hypercore-crypto/index.js","protomux":"/node_modules/protomux/index.js"},"/node_modules/protomux-wakeup/package.json":{},"/node_modules/protomux-wakeup/spec/hyperschema/index.js":{"#package":"/node_modules/protomux-wakeup/package.json","hyperschema/runtime":"/node_modules/hyperschema/runtime.cjs"},"/node_modules/protomux/index.js":{"#package":"/node_modules/protomux/package.json","b4a":"/node_modules/b4a/index.js","compact-encoding":"/node_modules/compact-encoding/index.js","queue-tick":"/node_modules/queue-tick/process-next-tick.js","safety-catch":"/node_modules/safety-catch/index.js","unslab":"/node_modules/unslab/index.js"},"/node_modules/protomux/package.json":{},"/node_modules/queue-tick/package.json":{},"/node_modules/queue-tick/process-next-tick.js":{"#package":"/node_modules/queue-tick/package.json","./queue-microtask":"/node_modules/queue-tick/queue-microtask.js"},"/node_modules/queue-tick/queue-microtask.js":{"#package":"/node_modules/queue-tick/package.json"},"/node_modules/quickbit-native/binding.js":{"#package":"/node_modules/quickbit-native/package.json",".":"linked:libquickbit-native.2.4.8.so","require-addon":"/node_modules/require-addon/lib/bare.js"},"/node_modules/quickbit-native/index.js":{"#package":"/node_modules/quickbit-native/package.json","./binding":"/node_modules/quickbit-native/binding.js"},"/node_modules/quickbit-native/package.json":{},"/node_modules/quickbit-universal/fallback.js":{"#package":"/node_modules/quickbit-universal/package.json","simdle-universal":"/node_modules/simdle-universal/index.js"},"/node_modules/quickbit-universal/index.js":{"#package":"/node_modules/quickbit-universal/package.json","./fallback":"/node_modules/quickbit-universal/fallback.js","quickbit-native":"/node_modules/quickbit-native/index.js"},"/node_modules/quickbit-universal/package.json":{},"/node_modules/rache/index.js":{"#package":"/node_modules/rache/package.json"},"/node_modules/rache/package.json":{},"/node_modules/random-array-iterator/index.js":{"#package":"/node_modules/random-array-iterator/package.json"},"/node_modules/random-array-iterator/package.json":{},"/node_modules/ready-resource/index.js":{"#package":"/node_modules/ready-resource/package.json","events":"/node_modules/bare-events/index.js"},"/node_modules/ready-resource/package.json":{},"/node_modules/record-cache/index.js":{"#package":"/node_modules/record-cache/package.json","b4a":"/node_modules/b4a/index.js"},"/node_modules/record-cache/package.json":{},"/node_modules/refcounter/index.js":{"#package":"/node_modules/refcounter/package.json"},"/node_modules/refcounter/package.json":{},"/node_modules/require-addon/lib/bare.js":{"#package":"/node_modules/require-addon/package.json"},"/node_modules/require-addon/package.json":{},"/node_modules/resolve-reject-promise/index.js":{"#package":"/node_modules/resolve-reject-promise/package.json"},"/node_modules/resolve-reject-promise/package.json":{},"/node_modules/resource-on-exit/index.js":{"#package":"/node_modules/resource-on-exit/package.json"},"/node_modules/resource-on-exit/package.json":{},"/node_modules/rocksdb-native/binding.js":{"#package":"/node_modules/rocksdb-native/package.json",".":"linked:librocksdb-native.3.11.4.so","require-addon":"/node_modules/require-addon/lib/bare.js"},"/node_modules/rocksdb-native/index.js":{"#package":"/node_modules/rocksdb-native/package.json","./lib/column-family":"/node_modules/rocksdb-native/lib/column-family.js","./lib/constants":"/node_modules/rocksdb-native/lib/constants.js","./lib/filter-policy":"/node_modules/rocksdb-native/lib/filter-policy.js","./lib/iterator":"/node_modules/rocksdb-native/lib/iterator.js","./lib/snapshot":"/node_modules/rocksdb-native/lib/snapshot.js","./lib/state":"/node_modules/rocksdb-native/lib/state.js"},"/node_modules/rocksdb-native/lib/batch.js":{"#package":"/node_modules/rocksdb-native/package.json","../binding":"/node_modules/rocksdb-native/binding.js","compact-encoding":"/node_modules/compact-encoding/index.js"},"/node_modules/rocksdb-native/lib/column-family.js":{"#package":"/node_modules/rocksdb-native/package.json","../binding":"/node_modules/rocksdb-native/binding.js","./constants":"/node_modules/rocksdb-native/lib/constants.js","./filter-policy":"/node_modules/rocksdb-native/lib/filter-policy.js"},"/node_modules/rocksdb-native/lib/constants.js":{"#package":"/node_modules/rocksdb-native/package.json"},"/node_modules/rocksdb-native/lib/filter-policy.js":{"#package":"/node_modules/rocksdb-native/package.json"},"/node_modules/rocksdb-native/lib/iterator.js":{"#package":"/node_modules/rocksdb-native/package.json","../binding":"/node_modules/rocksdb-native/binding.js","compact-encoding":"/node_modules/compact-encoding/index.js","streamx":"/node_modules/streamx/index.js"},"/node_modules/rocksdb-native/lib/snapshot.js":{"#package":"/node_modules/rocksdb-native/package.json","../binding":"/node_modules/rocksdb-native/binding.js"},"/node_modules/rocksdb-native/lib/state.js":{"#package":"/node_modules/rocksdb-native/package.json","../binding":"/node_modules/rocksdb-native/binding.js","./batch":"/node_modules/rocksdb-native/lib/batch.js","./column-family":"/node_modules/rocksdb-native/lib/column-family.js","compact-encoding":"/node_modules/compact-encoding/index.js","ready-resource":"/node_modules/ready-resource/index.js","refcounter":"/node_modules/refcounter/index.js","resolve-reject-promise":"/node_modules/resolve-reject-promise/index.js","signal-promise":"/node_modules/signal-promise/index.js"},"/node_modules/rocksdb-native/package.json":{},"/node_modules/safety-catch/index.js":{"#package":"/node_modules/safety-catch/package.json"},"/node_modules/safety-catch/package.json":{},"/node_modules/scope-lock/index.js":{"#package":"/node_modules/scope-lock/package.json"},"/node_modules/scope-lock/package.json":{},"/node_modules/shuffled-priority-queue/index.js":{"#package":"/node_modules/shuffled-priority-queue/package.json","unordered-set":"/node_modules/unordered-set/index.js"},"/node_modules/shuffled-priority-queue/package.json":{},"/node_modules/signal-promise/index.js":{"#package":"/node_modules/signal-promise/package.json"},"/node_modules/signal-promise/package.json":{},"/node_modules/signed-varint/index.js":{"#package":"/node_modules/signed-varint/package.json","varint":"/node_modules/varint/index.js"},"/node_modules/signed-varint/package.json":{},"/node_modules/simdle-native/binding.js":{"#package":"/node_modules/simdle-native/package.json",".":"linked:libsimdle-native.1.3.9.so","require-addon":"/node_modules/require-addon/lib/bare.js"},"/node_modules/simdle-native/index.js":{"#package":"/node_modules/simdle-native/package.json","./binding":"/node_modules/simdle-native/binding.js","b4a":"/node_modules/b4a/index.js"},"/node_modules/simdle-native/package.json":{},"/node_modules/simdle-universal/fallback.js":{"#package":"/node_modules/simdle-universal/package.json","./scalar":"/node_modules/simdle-universal/scalar.js","b4a":"/node_modules/b4a/index.js"},"/node_modules/simdle-universal/index.js":{"#package":"/node_modules/simdle-universal/package.json","./fallback":"/node_modules/simdle-universal/fallback.js","simdle-native":"/node_modules/simdle-native/index.js"},"/node_modules/simdle-universal/package.json":{},"/node_modules/simdle-universal/scalar.js":{"#package":"/node_modules/simdle-universal/package.json"},"/node_modules/sodium-native/binding.js":{"#package":"/node_modules/sodium-native/package.json",".":"linked:libsodium-native.5.0.10.so","require-addon":"/node_modules/require-addon/lib/bare.js"},"/node_modules/sodium-native/index.js":{"#package":"/node_modules/sodium-native/package.json","./binding":"/node_modules/sodium-native/binding.js","which-runtime":"/node_modules/which-runtime/index.js"},"/node_modules/sodium-native/package.json":{},"/node_modules/sodium-secretstream/index.js":{"#package":"/node_modules/sodium-secretstream/package.json","b4a":"/node_modules/b4a/index.js","sodium-universal":"/node_modules/sodium-universal/index.js"},"/node_modules/sodium-secretstream/package.json":{},"/node_modules/sodium-universal/index.js":{"#package":"/node_modules/sodium-universal/package.json","sodium-native":"/node_modules/sodium-native/index.js"},"/node_modules/sodium-universal/package.json":{},"/node_modules/streamx/index.js":{"#package":"/node_modules/streamx/package.json","events-universal":"/node_modules/events-universal/bare.js","fast-fifo":"/node_modules/fast-fifo/index.js","text-decoder":"/node_modules/text-decoder/index.js"},"/node_modules/streamx/package.json":{},"/node_modules/sub-encoder/index.js":{"#package":"/node_modules/sub-encoder/package.json","b4a":"/node_modules/b4a/index.js","codecs":"/node_modules/codecs/index.js"},"/node_modules/sub-encoder/package.json":{},"/node_modules/text-decoder/index.js":{"#package":"/node_modules/text-decoder/package.json","./lib/pass-through-decoder":"/node_modules/text-decoder/lib/pass-through-decoder.js","./lib/utf8-decoder":"/node_modules/text-decoder/lib/utf8-decoder.js"},"/node_modules/text-decoder/lib/pass-through-decoder.js":{"#package":"/node_modules/text-decoder/package.json","b4a":"/node_modules/b4a/index.js"},"/node_modules/text-decoder/lib/utf8-decoder.js":{"#package":"/node_modules/text-decoder/package.json","b4a":"/node_modules/b4a/index.js"},"/node_modules/text-decoder/package.json":{},"/node_modules/time-ordered-set/index.js":{"#package":"/node_modules/time-ordered-set/package.json"},"/node_modules/time-ordered-set/package.json":{},"/node_modules/timeout-refresh/browser.js":{"#package":"/node_modules/timeout-refresh/package.json"},"/node_modules/timeout-refresh/index.js":{"#package":"/node_modules/timeout-refresh/package.json","./browser":"/node_modules/timeout-refresh/browser.js","./node":"/node_modules/timeout-refresh/node.js"},"/node_modules/timeout-refresh/node.js":{"#package":"/node_modules/timeout-refresh/package.json"},"/node_modules/timeout-refresh/package.json":{},"/node_modules/tiny-buffer-map/index.js":{"#package":"/node_modules/tiny-buffer-map/package.json","b4a":"/node_modules/b4a/index.js"},"/node_modules/tiny-buffer-map/package.json":{},"/node_modules/udx-native/binding.js":{"#package":"/node_modules/udx-native/package.json",".":"linked:libudx-native.1.19.2.so","require-addon":"/node_modules/require-addon/lib/bare.js"},"/node_modules/udx-native/lib/ip.js":{"#package":"/node_modules/udx-native/package.json"},"/node_modules/udx-native/lib/network-interfaces.js":{"#package":"/node_modules/udx-native/package.json","../binding":"/node_modules/udx-native/binding.js","b4a":"/node_modules/b4a/index.js","events":"/node_modules/bare-events/index.js"},"/node_modules/udx-native/lib/socket.js":{"#package":"/node_modules/udx-native/package.json","../binding":"/node_modules/udx-native/binding.js","./ip":"/node_modules/udx-native/lib/ip.js","b4a":"/node_modules/b4a/index.js","events":"/node_modules/bare-events/index.js"},"/node_modules/udx-native/lib/stream.js":{"#package":"/node_modules/udx-native/package.json","../binding":"/node_modules/udx-native/binding.js","./ip":"/node_modules/udx-native/lib/ip.js","b4a":"/node_modules/b4a/index.js","streamx":"/node_modules/streamx/index.js"},"/node_modules/udx-native/lib/udx.js":{"#package":"/node_modules/udx-native/package.json","../binding":"/node_modules/udx-native/binding.js","./ip":"/node_modules/udx-native/lib/ip.js","./network-interfaces":"/node_modules/udx-native/lib/network-interfaces.js","./socket":"/node_modules/udx-native/lib/socket.js","./stream":"/node_modules/udx-native/lib/stream.js","b4a":"/node_modules/b4a/index.js"},"/node_modules/udx-native/package.json":{},"/node_modules/unordered-set/index.js":{"#package":"/node_modules/unordered-set/package.json"},"/node_modules/unordered-set/package.json":{},"/node_modules/unslab/index.js":{"#package":"/node_modules/unslab/package.json","b4a":"/node_modules/b4a/index.js"},"/node_modules/unslab/package.json":{},"/node_modules/varint/decode.js":{"#package":"/node_modules/varint/package.json"},"/node_modules/varint/encode.js":{"#package":"/node_modules/varint/package.json"},"/node_modules/varint/index.js":{"#package":"/node_modules/varint/package.json","./decode.js":"/node_modules/varint/decode.js","./encode.js":"/node_modules/varint/encode.js","./length.js":"/node_modules/varint/length.js"},"/node_modules/varint/length.js":{"#package":"/node_modules/varint/package.json"},"/node_modules/varint/package.json":{},"/node_modules/which-runtime/index.js":{"#package":"/node_modules/which-runtime/package.json"},"/node_modules/which-runtime/package.json":{},"/node_modules/xache/index.js":{"#package":"/node_modules/xache/package.json"},"/node_modules/xache/package.json":{},"/node_modules/z32/index.js":{"#package":"/node_modules/z32/package.json","b4a":"/node_modules/b4a/index.js"},"/node_modules/z32/package.json":{},"/package.json":{},"/rpc-commands.mjs":{"#package":"/package.json"}},"addons":["linked:libbare-crypto.1.12.0.so","linked:libbare-fs.4.5.2.so","linked:libbare-os.3.6.2.so","linked:libbare-url.2.3.2.so","linked:libfs-native-extensions.1.4.5.so","linked:libquickbit-native.2.4.8.so","linked:librocksdb-native.3.11.4.so","linked:libsimdle-native.1.3.9.so","linked:libsodium-native.5.0.10.so","linked:libudx-native.1.19.2.so"],"assets":[],"files":{"/backend/backend.mjs":{"offset":0,"length":17167,"mode":420},"/node_modules/@hyperswarm/secret-stream/index.js":{"offset":17167,"length":16722,"mode":420},"/node_modules/@hyperswarm/secret-stream/lib/bridge.js":{"offset":33889,"length":1273,"mode":420},"/node_modules/@hyperswarm/secret-stream/lib/handshake.js":{"offset":35162,"length":1950,"mode":420},"/node_modules/@hyperswarm/secret-stream/package.json":{"offset":37112,"length":1132,"mode":420},"/node_modules/autobase/encoding/legacy.js":{"offset":38244,"length":8848,"mode":420},"/node_modules/autobase/encoding/spec/autobase/index.js":{"offset":47092,"length":18918,"mode":420},"/node_modules/autobase/index.js":{"offset":66010,"length":58148,"mode":420},"/node_modules/autobase/lib/active-writers.js":{"offset":124158,"length":736,"mode":420},"/node_modules/autobase/lib/append-batch.js":{"offset":124894,"length":1426,"mode":420},"/node_modules/autobase/lib/apply-calls.js":{"offset":126320,"length":2275,"mode":420},"/node_modules/autobase/lib/apply-state.js":{"offset":128595,"length":45236,"mode":420},"/node_modules/autobase/lib/boot.js":{"offset":173831,"length":3461,"mode":420},"/node_modules/autobase/lib/caps.js":{"offset":177292,"length":505,"mode":420},"/node_modules/autobase/lib/clock.js":{"offset":177797,"length":749,"mode":420},"/node_modules/autobase/lib/consensus.js":{"offset":178546,"length":11754,"mode":420},"/node_modules/autobase/lib/encryption.js":{"offset":190300,"length":9444,"mode":420},"/node_modules/autobase/lib/fast-forward.js":{"offset":199744,"length":5714,"mode":420},"/node_modules/autobase/lib/fork.js":{"offset":205458,"length":4230,"mode":420},"/node_modules/autobase/lib/linearizer.js":{"offset":209688,"length":9696,"mode":420},"/node_modules/autobase/lib/local-state.js":{"offset":219384,"length":2013,"mode":420},"/node_modules/autobase/lib/messages.js":{"offset":221397,"length":827,"mode":420},"/node_modules/autobase/lib/node-buffer.js":{"offset":222224,"length":1563,"mode":420},"/node_modules/autobase/lib/store.js":{"offset":223787,"length":14107,"mode":420},"/node_modules/autobase/lib/system.js":{"offset":237894,"length":20176,"mode":420},"/node_modules/autobase/lib/timer.js":{"offset":258070,"length":2937,"mode":420},"/node_modules/autobase/lib/topolist.js":{"offset":261007,"length":5108,"mode":420},"/node_modules/autobase/lib/updates.js":{"offset":266115,"length":1148,"mode":420},"/node_modules/autobase/lib/values.js":{"offset":267263,"length":1539,"mode":420},"/node_modules/autobase/lib/wakeup.js":{"offset":268802,"length":4034,"mode":420},"/node_modules/autobase/lib/writer.js":{"offset":272836,"length":8901,"mode":420},"/node_modules/autobase/package.json":{"offset":281737,"length":2249,"mode":420},"/node_modules/b4a/index.js":{"offset":283986,"length":4054,"mode":420},"/node_modules/b4a/package.json":{"offset":288040,"length":1140,"mode":420},"/node_modules/bare-crypto/binding.js":{"offset":289180,"length":33,"mode":420},"/node_modules/bare-crypto/index.js":{"offset":289213,"length":1611,"mode":420},"/node_modules/bare-crypto/lib/cipher.js":{"offset":290824,"length":7366,"mode":420},"/node_modules/bare-crypto/lib/constants.js":{"offset":298190,"length":2313,"mode":420},"/node_modules/bare-crypto/lib/errors.js":{"offset":300503,"length":957,"mode":420},"/node_modules/bare-crypto/lib/hash.js":{"offset":301460,"length":829,"mode":420},"/node_modules/bare-crypto/lib/hmac.js":{"offset":302289,"length":1015,"mode":420},"/node_modules/bare-crypto/lib/key.js":{"offset":303304,"length":1067,"mode":420},"/node_modules/bare-crypto/lib/pbkdf2.js":{"offset":304371,"length":713,"mode":420},"/node_modules/bare-crypto/lib/random.js":{"offset":305084,"length":1707,"mode":420},"/node_modules/bare-crypto/lib/signature.js":{"offset":306791,"length":1010,"mode":420},"/node_modules/bare-crypto/lib/web/algorithm/ed25519.js":{"offset":307801,"length":6918,"mode":420},"/node_modules/bare-crypto/lib/web/algorithm/hmac.js":{"offset":314719,"length":4925,"mode":420},"/node_modules/bare-crypto/lib/web/algorithm/pbkdf2.js":{"offset":319644,"length":1467,"mode":420},"/node_modules/bare-crypto/lib/web/algorithm/sha.js":{"offset":321111,"length":299,"mode":420},"/node_modules/bare-crypto/lib/web/crypto-key.js":{"offset":321410,"length":970,"mode":420},"/node_modules/bare-crypto/package.json":{"offset":322380,"length":1475,"mode":420},"/node_modules/bare-crypto/web.js":{"offset":323855,"length":7393,"mode":420},"/node_modules/bare-events/index.js":{"offset":331248,"length":8033,"mode":420},"/node_modules/bare-events/lib/errors.js":{"offset":339281,"length":671,"mode":420},"/node_modules/bare-events/package.json":{"offset":339952,"length":1399,"mode":420},"/node_modules/bare-fs/binding.js":{"offset":341351,"length":33,"mode":420},"/node_modules/bare-fs/index.js":{"offset":341384,"length":48267,"mode":420},"/node_modules/bare-fs/lib/constants.js":{"offset":389651,"length":1494,"mode":420},"/node_modules/bare-fs/lib/errors.js":{"offset":391145,"length":1163,"mode":420},"/node_modules/bare-fs/package.json":{"offset":392308,"length":1546,"mode":420},"/node_modules/bare-fs/promises.js":{"offset":393854,"length":1866,"mode":420},"/node_modules/bare-os/binding.js":{"offset":395720,"length":33,"mode":420},"/node_modules/bare-os/index.js":{"offset":395753,"length":2433,"mode":420},"/node_modules/bare-os/lib/constants.js":{"offset":398186,"length":113,"mode":420},"/node_modules/bare-os/lib/errors.js":{"offset":398299,"length":479,"mode":420},"/node_modules/bare-os/package.json":{"offset":398778,"length":1023,"mode":420},"/node_modules/bare-path/index.js":{"offset":399801,"length":306,"mode":420},"/node_modules/bare-path/lib/constants.js":{"offset":400107,"length":247,"mode":420},"/node_modules/bare-path/lib/posix.js":{"offset":400354,"length":5991,"mode":420},"/node_modules/bare-path/lib/shared.js":{"offset":406345,"length":1888,"mode":420},"/node_modules/bare-path/lib/win32.js":{"offset":408233,"length":13427,"mode":420},"/node_modules/bare-path/package.json":{"offset":421660,"length":796,"mode":420},"/node_modules/bare-rpc/index.js":{"offset":422456,"length":10199,"mode":420},"/node_modules/bare-rpc/lib/command-router.js":{"offset":432655,"length":1073,"mode":420},"/node_modules/bare-rpc/lib/constants.js":{"offset":433728,"length":270,"mode":420},"/node_modules/bare-rpc/lib/errors.js":{"offset":433998,"length":597,"mode":420},"/node_modules/bare-rpc/lib/incoming-request.js":{"offset":434595,"length":1241,"mode":420},"/node_modules/bare-rpc/lib/incoming-stream.js":{"offset":435836,"length":1263,"mode":420},"/node_modules/bare-rpc/lib/messages.js":{"offset":437099,"length":3934,"mode":420},"/node_modules/bare-rpc/lib/outgoing-request.js":{"offset":441033,"length":1670,"mode":420},"/node_modules/bare-rpc/lib/outgoing-stream.js":{"offset":442703,"length":2374,"mode":420},"/node_modules/bare-rpc/package.json":{"offset":445077,"length":1208,"mode":420},"/node_modules/bare-stream/index.js":{"offset":446285,"length":7658,"mode":420},"/node_modules/bare-stream/package.json":{"offset":453943,"length":1307,"mode":420},"/node_modules/bare-url/binding.js":{"offset":455250,"length":33,"mode":420},"/node_modules/bare-url/index.js":{"offset":455283,"length":9261,"mode":420},"/node_modules/bare-url/lib/errors.js":{"offset":464544,"length":778,"mode":420},"/node_modules/bare-url/lib/url-search-params.js":{"offset":465322,"length":3867,"mode":420},"/node_modules/bare-url/package.json":{"offset":469189,"length":1105,"mode":420},"/node_modules/big-sparse-array/index.js":{"offset":470294,"length":2471,"mode":420},"/node_modules/big-sparse-array/package.json":{"offset":472765,"length":636,"mode":420},"/node_modules/bits-to-bytes/index.js":{"offset":473401,"length":3083,"mode":420},"/node_modules/bits-to-bytes/package.json":{"offset":476484,"length":708,"mode":420},"/node_modules/blind-relay/index.js":{"offset":477192,"length":10851,"mode":420},"/node_modules/blind-relay/lib/errors.js":{"offset":488043,"length":1043,"mode":420},"/node_modules/blind-relay/package.json":{"offset":489086,"length":1046,"mode":420},"/node_modules/bogon/index.js":{"offset":490132,"length":3961,"mode":420},"/node_modules/bogon/package.json":{"offset":494093,"length":657,"mode":420},"/node_modules/codecs/index.js":{"offset":494750,"length":2416,"mode":420},"/node_modules/codecs/package.json":{"offset":497166,"length":655,"mode":420},"/node_modules/compact-encoding-bitfield/index.js":{"offset":497821,"length":1963,"mode":420},"/node_modules/compact-encoding-bitfield/package.json":{"offset":499784,"length":827,"mode":420},"/node_modules/compact-encoding-net/index.js":{"offset":500611,"length":4109,"mode":420},"/node_modules/compact-encoding-net/package.json":{"offset":504720,"length":734,"mode":420},"/node_modules/compact-encoding/endian.js":{"offset":505454,"length":105,"mode":420},"/node_modules/compact-encoding/index.js":{"offset":505559,"length":23634,"mode":420},"/node_modules/compact-encoding/lexint.js":{"offset":529193,"length":2851,"mode":420},"/node_modules/compact-encoding/package.json":{"offset":532044,"length":801,"mode":420},"/node_modules/compact-encoding/raw.js":{"offset":532845,"length":4093,"mode":420},"/node_modules/core-coupler/index.js":{"offset":536938,"length":2243,"mode":420},"/node_modules/core-coupler/package.json":{"offset":539181,"length":599,"mode":420},"/node_modules/corestore/index.js":{"offset":539780,"length":17862,"mode":420},"/node_modules/corestore/lib/audit.js":{"offset":557642,"length":475,"mode":420},"/node_modules/corestore/package.json":{"offset":558117,"length":1085,"mode":420},"/node_modules/debounceify/index.js":{"offset":559202,"length":585,"mode":420},"/node_modules/debounceify/package.json":{"offset":559787,"length":567,"mode":420},"/node_modules/device-file/index.js":{"offset":560354,"length":5900,"mode":420},"/node_modules/device-file/package.json":{"offset":566254,"length":1063,"mode":420},"/node_modules/dht-rpc/index.js":{"offset":567317,"length":25403,"mode":420},"/node_modules/dht-rpc/lib/commands.js":{"offset":592720,"length":82,"mode":420},"/node_modules/dht-rpc/lib/errors.js":{"offset":592802,"length":719,"mode":420},"/node_modules/dht-rpc/lib/io.js":{"offset":593521,"length":15926,"mode":420},"/node_modules/dht-rpc/lib/peer.js":{"offset":609447,"length":631,"mode":420},"/node_modules/dht-rpc/lib/query.js":{"offset":610078,"length":9913,"mode":420},"/node_modules/dht-rpc/lib/session.js":{"offset":619991,"length":1103,"mode":420},"/node_modules/dht-rpc/package.json":{"offset":621094,"length":1356,"mode":420},"/node_modules/events-universal/bare.js":{"offset":622450,"length":40,"mode":420},"/node_modules/events-universal/package.json":{"offset":622490,"length":908,"mode":420},"/node_modules/fast-fifo/fixed-size.js":{"offset":623398,"length":875,"mode":420},"/node_modules/fast-fifo/index.js":{"offset":624273,"length":972,"mode":420},"/node_modules/fast-fifo/package.json":{"offset":625245,"length":682,"mode":420},"/node_modules/fd-lock/index.js":{"offset":625927,"length":1661,"mode":420},"/node_modules/fd-lock/package.json":{"offset":627588,"length":1099,"mode":420},"/node_modules/flat-tree/index.js":{"offset":628687,"length":8164,"mode":420},"/node_modules/flat-tree/package.json":{"offset":636851,"length":631,"mode":420},"/node_modules/fs-native-extensions/binding.js":{"offset":637482,"length":90,"mode":420},"/node_modules/fs-native-extensions/index.js":{"offset":637572,"length":5718,"mode":420},"/node_modules/fs-native-extensions/package.json":{"offset":643290,"length":1699,"mode":420},"/node_modules/hyperbee/index.js":{"offset":644989,"length":46120,"mode":420},"/node_modules/hyperbee/iterators/diff.js":{"offset":691109,"length":5368,"mode":420},"/node_modules/hyperbee/iterators/history.js":{"offset":696477,"length":1642,"mode":420},"/node_modules/hyperbee/iterators/local.js":{"offset":698119,"length":1180,"mode":420},"/node_modules/hyperbee/iterators/range.js":{"offset":699299,"length":4734,"mode":420},"/node_modules/hyperbee/lib/extension.js":{"offset":704033,"length":3445,"mode":420},"/node_modules/hyperbee/lib/messages.js":{"offset":707478,"length":28162,"mode":420},"/node_modules/hyperbee/package.json":{"offset":735640,"length":1216,"mode":420},"/node_modules/hypercore-crypto/index.js":{"offset":736856,"length":5040,"mode":420},"/node_modules/hypercore-crypto/package.json":{"offset":741896,"length":771,"mode":420},"/node_modules/hypercore-errors/index.js":{"offset":742667,"length":4896,"mode":420},"/node_modules/hypercore-errors/package.json":{"offset":747563,"length":688,"mode":420},"/node_modules/hypercore-id-encoding/index.js":{"offset":748251,"length":923,"mode":420},"/node_modules/hypercore-id-encoding/package.json":{"offset":749174,"length":799,"mode":420},"/node_modules/hypercore-storage/index.js":{"offset":749973,"length":27551,"mode":420},"/node_modules/hypercore-storage/lib/block-dependency-stream.js":{"offset":777524,"length":2557,"mode":420},"/node_modules/hypercore-storage/lib/close-error-stream.js":{"offset":780081,"length":276,"mode":420},"/node_modules/hypercore-storage/lib/keys.js":{"offset":780357,"length":7454,"mode":420},"/node_modules/hypercore-storage/lib/streams.js":{"offset":787811,"length":4820,"mode":420},"/node_modules/hypercore-storage/lib/tx.js":{"offset":792631,"length":8416,"mode":420},"/node_modules/hypercore-storage/lib/view.js":{"offset":801047,"length":8378,"mode":420},"/node_modules/hypercore-storage/migrations/0/index.js":{"offset":809425,"length":20125,"mode":420},"/node_modules/hypercore-storage/migrations/0/messages.js":{"offset":829550,"length":26210,"mode":420},"/node_modules/hypercore-storage/package.json":{"offset":855760,"length":1248,"mode":420},"/node_modules/hypercore-storage/spec/hyperschema/index.js":{"offset":857008,"length":13108,"mode":420},"/node_modules/hypercore/index.js":{"offset":870116,"length":33855,"mode":420},"/node_modules/hypercore/lib/audit.js":{"offset":903971,"length":3834,"mode":420},"/node_modules/hypercore/lib/bit-interlude.js":{"offset":907805,"length":4257,"mode":420},"/node_modules/hypercore/lib/bitfield.js":{"offset":912062,"length":11854,"mode":420},"/node_modules/hypercore/lib/caps.js":{"offset":923916,"length":1537,"mode":420},"/node_modules/hypercore/lib/compat.js":{"offset":925453,"length":477,"mode":420},"/node_modules/hypercore/lib/copy-prologue.js":{"offset":925930,"length":5999,"mode":420},"/node_modules/hypercore/lib/core.js":{"offset":931929,"length":24419,"mode":420},"/node_modules/hypercore/lib/default-encryption.js":{"offset":956348,"length":3453,"mode":420},"/node_modules/hypercore/lib/download.js":{"offset":959801,"length":1278,"mode":420},"/node_modules/hypercore/lib/hotswap-queue.js":{"offset":961079,"length":1498,"mode":420},"/node_modules/hypercore/lib/info.js":{"offset":962577,"length":1517,"mode":420},"/node_modules/hypercore/lib/inspect.js":{"offset":964094,"length":1983,"mode":420},"/node_modules/hypercore/lib/merkle-tree.js":{"offset":966077,"length":33910,"mode":420},"/node_modules/hypercore/lib/messages.js":{"offset":999987,"length":27633,"mode":420},"/node_modules/hypercore/lib/multisig.js":{"offset":1027620,"length":2851,"mode":420},"/node_modules/hypercore/lib/mutex.js":{"offset":1030471,"length":1028,"mode":420},"/node_modules/hypercore/lib/receiver-queue.js":{"offset":1031499,"length":1413,"mode":420},"/node_modules/hypercore/lib/remote-bitfield.js":{"offset":1032912,"length":8079,"mode":420},"/node_modules/hypercore/lib/replicator.js":{"offset":1040991,"length":80781,"mode":420},"/node_modules/hypercore/lib/session-state.js":{"offset":1121772,"length":32225,"mode":420},"/node_modules/hypercore/lib/streams.js":{"offset":1153997,"length":3034,"mode":420},"/node_modules/hypercore/lib/verifier.js":{"offset":1157031,"length":9678,"mode":420},"/node_modules/hypercore/package.json":{"offset":1166709,"length":2197,"mode":420},"/node_modules/hyperdht/index.js":{"offset":1168906,"length":16099,"mode":420},"/node_modules/hyperdht/lib/announcer.js":{"offset":1185005,"length":7533,"mode":420},"/node_modules/hyperdht/lib/connect.js":{"offset":1192538,"length":23668,"mode":420},"/node_modules/hyperdht/lib/connection-pool.js":{"offset":1216206,"length":3050,"mode":420},"/node_modules/hyperdht/lib/constants.js":{"offset":1219256,"length":1211,"mode":420},"/node_modules/hyperdht/lib/crypto.js":{"offset":1220467,"length":632,"mode":420},"/node_modules/hyperdht/lib/encode.js":{"offset":1221099,"length":446,"mode":420},"/node_modules/hyperdht/lib/errors.js":{"offset":1221545,"length":3712,"mode":420},"/node_modules/hyperdht/lib/holepuncher.js":{"offset":1225257,"length":10003,"mode":420},"/node_modules/hyperdht/lib/messages.js":{"offset":1235260,"length":11266,"mode":420},"/node_modules/hyperdht/lib/nat.js":{"offset":1246526,"length":4714,"mode":420},"/node_modules/hyperdht/lib/noise-wrap.js":{"offset":1251240,"length":1669,"mode":420},"/node_modules/hyperdht/lib/persistent.js":{"offset":1252909,"length":8086,"mode":420},"/node_modules/hyperdht/lib/raw-stream-set.js":{"offset":1260995,"length":1511,"mode":420},"/node_modules/hyperdht/lib/router.js":{"offset":1262506,"length":6978,"mode":420},"/node_modules/hyperdht/lib/secure-payload.js":{"offset":1269484,"length":1491,"mode":420},"/node_modules/hyperdht/lib/semaphore.js":{"offset":1270975,"length":1577,"mode":420},"/node_modules/hyperdht/lib/server.js":{"offset":1272552,"length":21156,"mode":420},"/node_modules/hyperdht/lib/sleeper.js":{"offset":1293708,"length":690,"mode":420},"/node_modules/hyperdht/lib/socket-pool.js":{"offset":1294398,"length":4343,"mode":420},"/node_modules/hyperdht/package.json":{"offset":1298741,"length":2011,"mode":420},"/node_modules/hyperschema/package.json":{"offset":1300752,"length":1556,"mode":420},"/node_modules/hyperschema/runtime.cjs":{"offset":1302308,"length":54,"mode":420},"/node_modules/hyperswarm/index.js":{"offset":1302362,"length":18429,"mode":420},"/node_modules/hyperswarm/lib/bulk-timer.js":{"offset":1320791,"length":728,"mode":420},"/node_modules/hyperswarm/lib/connection-set.js":{"offset":1321519,"length":823,"mode":420},"/node_modules/hyperswarm/lib/peer-discovery.js":{"offset":1322342,"length":9286,"mode":420},"/node_modules/hyperswarm/lib/peer-info.js":{"offset":1331628,"length":2688,"mode":420},"/node_modules/hyperswarm/lib/retry-timer.js":{"offset":1334316,"length":1889,"mode":420},"/node_modules/hyperswarm/package.json":{"offset":1336205,"length":1228,"mode":420},"/node_modules/index-encoder/index.js":{"offset":1337433,"length":6762,"mode":420},"/node_modules/index-encoder/package.json":{"offset":1344195,"length":729,"mode":420},"/node_modules/is-options/index.js":{"offset":1344924,"length":140,"mode":420},"/node_modules/is-options/package.json":{"offset":1345064,"length":605,"mode":420},"/node_modules/kademlia-routing-table/index.js":{"offset":1345669,"length":4145,"mode":420},"/node_modules/kademlia-routing-table/package.json":{"offset":1349814,"length":967,"mode":420},"/node_modules/mutexify/index.js":{"offset":1350781,"length":536,"mode":420},"/node_modules/mutexify/package.json":{"offset":1351317,"length":679,"mode":420},"/node_modules/mutexify/promise.js":{"offset":1351996,"length":324,"mode":420},"/node_modules/nanoassert/index.js":{"offset":1352320,"length":438,"mode":420},"/node_modules/nanoassert/package.json":{"offset":1352758,"length":647,"mode":420},"/node_modules/nat-sampler/index.js":{"offset":1353405,"length":1550,"mode":420},"/node_modules/nat-sampler/package.json":{"offset":1354955,"length":608,"mode":420},"/node_modules/noise-curve-ed/index.js":{"offset":1355563,"length":1642,"mode":420},"/node_modules/noise-curve-ed/package.json":{"offset":1357205,"length":808,"mode":420},"/node_modules/noise-handshake/cipher.js":{"offset":1358013,"length":2817,"mode":420},"/node_modules/noise-handshake/dh.js":{"offset":1360830,"length":1439,"mode":420},"/node_modules/noise-handshake/hkdf.js":{"offset":1362269,"length":1092,"mode":420},"/node_modules/noise-handshake/hmac.js":{"offset":1363361,"length":1273,"mode":420},"/node_modules/noise-handshake/noise.js":{"offset":1364634,"length":6437,"mode":420},"/node_modules/noise-handshake/package.json":{"offset":1371071,"length":638,"mode":420},"/node_modules/noise-handshake/symmetric-state.js":{"offset":1371709,"length":2206,"mode":420},"/node_modules/protocol-buffers-encodings/index.js":{"offset":1373915,"length":6489,"mode":420},"/node_modules/protocol-buffers-encodings/package.json":{"offset":1380404,"length":719,"mode":420},"/node_modules/protomux-wakeup/index.js":{"offset":1381123,"length":16824,"mode":420},"/node_modules/protomux-wakeup/package.json":{"offset":1397947,"length":972,"mode":420},"/node_modules/protomux-wakeup/spec/hyperschema/index.js":{"offset":1398919,"length":3335,"mode":420},"/node_modules/protomux/index.js":{"offset":1402254,"length":18996,"mode":420},"/node_modules/protomux/package.json":{"offset":1421250,"length":816,"mode":420},"/node_modules/queue-tick/package.json":{"offset":1422066,"length":669,"mode":420},"/node_modules/queue-tick/process-next-tick.js":{"offset":1422735,"length":160,"mode":420},"/node_modules/queue-tick/queue-microtask.js":{"offset":1422895,"length":108,"mode":420},"/node_modules/quickbit-native/binding.js":{"offset":1423003,"length":90,"mode":420},"/node_modules/quickbit-native/index.js":{"offset":1423093,"length":4683,"mode":420},"/node_modules/quickbit-native/package.json":{"offset":1427776,"length":1104,"mode":420},"/node_modules/quickbit-universal/fallback.js":{"offset":1428880,"length":10204,"mode":420},"/node_modules/quickbit-universal/index.js":{"offset":1439084,"length":442,"mode":420},"/node_modules/quickbit-universal/package.json":{"offset":1439526,"length":908,"mode":420},"/node_modules/rache/index.js":{"offset":1440434,"length":2463,"mode":420},"/node_modules/rache/package.json":{"offset":1442897,"length":602,"mode":420},"/node_modules/random-array-iterator/index.js":{"offset":1443499,"length":1001,"mode":420},"/node_modules/random-array-iterator/package.json":{"offset":1444500,"length":700,"mode":420},"/node_modules/ready-resource/index.js":{"offset":1445200,"length":1091,"mode":420},"/node_modules/ready-resource/package.json":{"offset":1446291,"length":812,"mode":420},"/node_modules/record-cache/index.js":{"offset":1447103,"length":3668,"mode":420},"/node_modules/record-cache/package.json":{"offset":1450771,"length":612,"mode":420},"/node_modules/refcounter/index.js":{"offset":1451383,"length":611,"mode":420},"/node_modules/refcounter/package.json":{"offset":1451994,"length":451,"mode":420},"/node_modules/require-addon/lib/bare.js":{"offset":1452445,"length":45,"mode":420},"/node_modules/require-addon/package.json":{"offset":1452490,"length":1449,"mode":420},"/node_modules/resolve-reject-promise/index.js":{"offset":1453939,"length":443,"mode":420},"/node_modules/resolve-reject-promise/package.json":{"offset":1454382,"length":581,"mode":420},"/node_modules/resource-on-exit/index.js":{"offset":1454963,"length":361,"mode":420},"/node_modules/resource-on-exit/package.json":{"offset":1455324,"length":496,"mode":420},"/node_modules/rocksdb-native/binding.js":{"offset":1455820,"length":90,"mode":420},"/node_modules/rocksdb-native/index.js":{"offset":1455910,"length":4708,"mode":420},"/node_modules/rocksdb-native/lib/batch.js":{"offset":1460618,"length":8812,"mode":420},"/node_modules/rocksdb-native/lib/column-family.js":{"offset":1469430,"length":2674,"mode":420},"/node_modules/rocksdb-native/lib/constants.js":{"offset":1472104,"length":94,"mode":420},"/node_modules/rocksdb-native/lib/filter-policy.js":{"offset":1472198,"length":437,"mode":420},"/node_modules/rocksdb-native/lib/iterator.js":{"offset":1472635,"length":4311,"mode":420},"/node_modules/rocksdb-native/lib/snapshot.js":{"offset":1476946,"length":503,"mode":420},"/node_modules/rocksdb-native/lib/state.js":{"offset":1477449,"length":9762,"mode":420},"/node_modules/rocksdb-native/package.json":{"offset":1487211,"length":1539,"mode":420},"/node_modules/safety-catch/index.js":{"offset":1488750,"length":506,"mode":420},"/node_modules/safety-catch/package.json":{"offset":1489256,"length":547,"mode":420},"/node_modules/scope-lock/index.js":{"offset":1489803,"length":1821,"mode":420},"/node_modules/scope-lock/package.json":{"offset":1491624,"length":611,"mode":420},"/node_modules/shuffled-priority-queue/index.js":{"offset":1492235,"length":2607,"mode":420},"/node_modules/shuffled-priority-queue/package.json":{"offset":1494842,"length":691,"mode":420},"/node_modules/signal-promise/index.js":{"offset":1495533,"length":1252,"mode":420},"/node_modules/signal-promise/package.json":{"offset":1496785,"length":503,"mode":420},"/node_modules/signed-varint/index.js":{"offset":1497288,"length":435,"mode":420},"/node_modules/signed-varint/package.json":{"offset":1497723,"length":523,"mode":420},"/node_modules/simdle-native/binding.js":{"offset":1498246,"length":90,"mode":420},"/node_modules/simdle-native/index.js":{"offset":1498336,"length":3474,"mode":420},"/node_modules/simdle-native/package.json":{"offset":1501810,"length":1115,"mode":420},"/node_modules/simdle-universal/fallback.js":{"offset":1502925,"length":5137,"mode":420},"/node_modules/simdle-universal/index.js":{"offset":1508062,"length":103,"mode":420},"/node_modules/simdle-universal/package.json":{"offset":1508165,"length":879,"mode":420},"/node_modules/simdle-universal/scalar.js":{"offset":1509044,"length":469,"mode":420},"/node_modules/sodium-native/binding.js":{"offset":1509513,"length":89,"mode":420},"/node_modules/sodium-native/index.js":{"offset":1509602,"length":66328,"mode":420},"/node_modules/sodium-native/package.json":{"offset":1575930,"length":1356,"mode":420},"/node_modules/sodium-secretstream/index.js":{"offset":1577286,"length":2257,"mode":420},"/node_modules/sodium-secretstream/package.json":{"offset":1579543,"length":657,"mode":420},"/node_modules/sodium-universal/index.js":{"offset":1580200,"length":42,"mode":420},"/node_modules/sodium-universal/package.json":{"offset":1580242,"length":1217,"mode":420},"/node_modules/streamx/index.js":{"offset":1581459,"length":33350,"mode":420},"/node_modules/streamx/package.json":{"offset":1614809,"length":836,"mode":420},"/node_modules/sub-encoder/index.js":{"offset":1615645,"length":1968,"mode":420},"/node_modules/sub-encoder/package.json":{"offset":1617613,"length":908,"mode":420},"/node_modules/text-decoder/index.js":{"offset":1618521,"length":1378,"mode":420},"/node_modules/text-decoder/lib/pass-through-decoder.js":{"offset":1619899,"length":273,"mode":420},"/node_modules/text-decoder/lib/utf8-decoder.js":{"offset":1620172,"length":2529,"mode":420},"/node_modules/text-decoder/package.json":{"offset":1622701,"length":987,"mode":420},"/node_modules/time-ordered-set/index.js":{"offset":1623688,"length":1444,"mode":420},"/node_modules/time-ordered-set/package.json":{"offset":1625132,"length":666,"mode":420},"/node_modules/timeout-refresh/browser.js":{"offset":1625798,"length":1098,"mode":420},"/node_modules/timeout-refresh/index.js":{"offset":1626896,"length":184,"mode":420},"/node_modules/timeout-refresh/node.js":{"offset":1627080,"length":928,"mode":420},"/node_modules/timeout-refresh/package.json":{"offset":1628008,"length":619,"mode":420},"/node_modules/tiny-buffer-map/index.js":{"offset":1628627,"length":936,"mode":420},"/node_modules/tiny-buffer-map/package.json":{"offset":1629563,"length":650,"mode":420},"/node_modules/udx-native/binding.js":{"offset":1630213,"length":90,"mode":420},"/node_modules/udx-native/lib/ip.js":{"offset":1630303,"length":2204,"mode":420},"/node_modules/udx-native/lib/network-interfaces.js":{"offset":1632507,"length":1341,"mode":420},"/node_modules/udx-native/lib/socket.js":{"offset":1633848,"length":7557,"mode":420},"/node_modules/udx-native/lib/stream.js":{"offset":1641405,"length":12706,"mode":420},"/node_modules/udx-native/lib/udx.js":{"offset":1654111,"length":2868,"mode":420},"/node_modules/udx-native/package.json":{"offset":1656979,"length":1615,"mode":420},"/node_modules/unordered-set/index.js":{"offset":1658594,"length":677,"mode":420},"/node_modules/unordered-set/package.json":{"offset":1659271,"length":654,"mode":420},"/node_modules/unslab/index.js":{"offset":1659925,"length":913,"mode":420},"/node_modules/unslab/package.json":{"offset":1660838,"length":613,"mode":420},"/node_modules/varint/decode.js":{"offset":1661451,"length":508,"mode":420},"/node_modules/varint/encode.js":{"offset":1661959,"length":452,"mode":420},"/node_modules/varint/index.js":{"offset":1662411,"length":134,"mode":420},"/node_modules/varint/length.js":{"offset":1662545,"length":471,"mode":420},"/node_modules/varint/package.json":{"offset":1663016,"length":511,"mode":420},"/node_modules/which-runtime/index.js":{"offset":1663527,"length":1231,"mode":420},"/node_modules/which-runtime/package.json":{"offset":1664758,"length":602,"mode":420},"/node_modules/xache/index.js":{"offset":1665360,"length":2378,"mode":420},"/node_modules/xache/package.json":{"offset":1667738,"length":584,"mode":420},"/node_modules/z32/index.js":{"offset":1668322,"length":2654,"mode":420},"/node_modules/z32/package.json":{"offset":1670976,"length":701,"mode":420},"/package.json":{"offset":1671677,"length":1741,"mode":420},"/rpc-commands.mjs":{"offset":1673418,"length":344,"mode":420}}}
// /* global Bare, BareKit */

import RPC from 'bare-rpc'
import URL from 'bare-url'
import { join } from 'bare-path'
import {
    RPC_RESET,
    RPC_MESSAGE,
    RPC_UPDATE,
    RPC_ADD,
    RPC_DELETE,
    RPC_GET_KEY,
    RPC_JOIN_KEY,
    RPC_ADD_FROM_BACKEND,
    RPC_UPDATE_FROM_BACKEND,
    RPC_DELETE_FROM_BACKEND,
    SYNC_LIST
} from '../rpc-commands.mjs'
import b4a from 'b4a'
import Autobase from 'autobase'
import Corestore from 'corestore'
import Hyperswarm from 'hyperswarm'
const { IPC } = BareKit
import { randomBytes } from 'bare-crypto'

console.error('bare backend is rocking.')

const storagePath = join(URL.fileURLToPath(Bare.argv[0]), 'lista') || './data'
const peerKeysString = Bare.argv[1] || '' // Comma-separated peer keys
const baseKeyHex = Bare.argv[2] || '' // Optional Autobase key (to join an existing base)

// Initialize Corestore
let store

// P2P state
let swarm = null
let autobase = null
let discovery = null
let currentTopic = null

// Handshake swarm for writer key exchange
let chatSwarm = null
let chatTopic = null
const knownWriters = new Set()
let addedStaticPeers = false

// Track connected replication peers
let peerCount = 0

// RPC instance (assigned later, but referenced by helper fns)
let rpc = null

// Optional Autobase key from argv (initial base)
let baseKey = null
if (baseKeyHex) {
    try {
        baseKey = Buffer.from(baseKeyHex.trim(), 'hex')
        console.error('Using existing Autobase key from argv[2]:', baseKeyHex.trim())
    } catch (err) {
        console.error('Invalid base key hex, creating new base instead:', err.message)
        baseKey = null
    }
}

// Generate unique ID (used only for addItem)
function generateId () {
    return randomBytes(16).toString('hex')
}

function sendHandshakeMessage (conn, msg) {
    const line = JSON.stringify(msg) + '\n'
    conn.write(line)
}

async function handleHandshakeMessage (msg) {
    if (!autobase) return
    if (!msg || msg.type !== 'writer-key') return

    const remoteKeyHex = msg.key
    if (!remoteKeyHex || typeof remoteKeyHex !== 'string') return

    if (knownWriters.has(remoteKeyHex)) return
    knownWriters.add(remoteKeyHex)

    // Only a writer can add other writers.
    if (!autobase.writable) {
        console.error('Not writable here, cannot add remote writer yet')
        return
    }

    console.error('Adding remote writer via autobase:', remoteKeyHex)

    await autobase.append({
        type: 'add-writer',
        key: remoteKeyHex
    })
}

async function setupHandshakeChannel (conn) {
    if (!autobase) {
        console.error('setupHandshakeChannel called before Autobase is initialized')
        return
    }

    // Send our writer key immediately
    await autobase.ready()
    const myWriterKeyHex = autobase.local.key.toString('hex')
    sendHandshakeMessage(conn, {
        type: 'writer-key',
        key: myWriterKeyHex
    })

    let buffer = ''
    conn.on('data', (chunk) => {
        buffer += chunk.toString()
        let idx
        while ((idx = buffer.indexOf('\n')) !== -1) {
            const line = buffer.slice(0, idx)
            buffer = buffer.slice(idx + 1)
            if (!line.trim()) continue

            // Fast-path: hypercore protocol frames and other binary garbage
            // are not going to start with '{', so just ignore them.
            if (line[0] !== '{') {
                continue
            }

            let msg
            try {
                msg = JSON.parse(line)
            } catch (e) {
                console.warn('invalid JSON from peer (handshake, ignored):', line)
                continue
            }

            handleHandshakeMessage(msg)
        }
    })
}

function setupChatSwarm (chatTopic) {
    if (!autobase) {
        console.error('setupChatSwarm called before Autobase is initialized')
        return
    }
    chatSwarm = new Hyperswarm()
    console.error('setting up chat swarm with topic:', chatTopic.toString('hex'))
    chatSwarm.on('connection', (conn, info) => {
        console.error('Handshake connection (chat swarm) with peer', info?.peer)
        conn.on('error', (err) => {
            console.error('Chat swarm connection error:', err)
        })
        setupHandshakeChannel(conn)
    })

    chatSwarm.on('error', (err) => {
        console.error('Chat swarm error:', err)
    })

    chatSwarm.join(chatTopic, { server: true, client: true })
    console.error('Handshake chat swarm joined on topic:', chatTopic.toString('hex'))
}

function broadcastPeerCount () {
    if (!rpc) return
    try {
        const req = rpc.request(RPC_MESSAGE)
        req.send(JSON.stringify({ type: 'peer-count', count: peerCount }))
    } catch (e) {
        console.error('Failed to broadcast peer count', e)
    }
}

async function initAutobase (newBaseKey) {
    // 1. Clean up previous Autobase instance (if any)
    if (autobase) {
        try {
            autobase.removeAllListeners('append')
            if (typeof autobase.close === 'function') {
                console.error('Closing previous Autobase instance...')
                await autobase.close()
            } else {
                console.error('Previous Autobase has no close() method, skipping close')
            }
        } catch (e) {
            console.error('Error while closing previous Autobase:', e)
        }
        autobase = null
    }

    // 2. Tear down networking bound to old store
    if (discovery) {
        try { await discovery.destroy() } catch (e) { console.error(e) }
        discovery = null
    }
    if (chatSwarm) {
        try { await chatSwarm.destroy() } catch (e) { console.error(e) }
        chatSwarm = null
    }

    // 3. Close old store
    if (store) {
        try {
            await store.close()
        } catch (e) {
            console.error('Error closing Corestore:', e)
        }
    }

    // 4. Create fresh Corestore
    store = new Corestore(storagePath)
    await store.ready()

    baseKey = newBaseKey || null
    console.error(
        'initializing a new autobase with key:',
        baseKey ? baseKey.toString('hex') : '(new base)'
    )

    autobase = new Autobase(store, baseKey, { apply, open, valueEncoding: 'json' })

    console.error('Calling autobase.ready()...')
    await autobase.ready()
    console.error(
        'autobase.ready() resolved. Autobase ready, writable?',
        autobase.writable,
        ' key:',
        autobase.key?.toString('hex'),
        ' local writer key:',
        autobase.local?.key?.toString('hex')
    )
    if (autobase) {
        const req = rpc.request(RPC_GET_KEY)
        req.send(autobase.key?.toString('hex'))
    }
    // Re-attach the append listener to the *new* instance
    autobase.on('append', async () => {
        console.error('New data appended, rebuilding view...')
        // await rebuildView()
    })

    // Add static peers only once
    if (!addedStaticPeers && peerKeysString) {
        const peerKeys = peerKeysString.split(',').filter(k => k.trim())
        for (const keyHex of peerKeys) {
            try {
                const peerKey = Buffer.from(keyHex.trim(), 'hex')
                const peerCore = store.get({ key: peerKey })
                await peerCore.ready()
                await autobase.addInput(peerCore)
                console.error('Added peer writer from argv[1]:', keyHex.trim())
            } catch (err) {
                console.error('Failed to add peer from argv[1]:', keyHex, err.message)
            }
        }
        addedStaticPeers = true
    }

    // Reset peer count on new base
    peerCount = 0
    broadcastPeerCount()

    // --- Update replication swarm topic for this base ---
    const firstLocalAutobaseKey = randomBytes(32)
    const topic = autobase.key || firstLocalAutobaseKey
    console.error('Discovery topic (replication swarm):', topic.toString('hex'))

    // Switch discovery to new topic
    if (discovery) {
        try {
            await discovery.destroy()
        } catch (e) {
            console.error('Error destroying previous discovery:', e)
        }
    }

    swarm = new Hyperswarm()
    swarm.on('error', (err) => {
        console.error('Replication swarm error:', err)
    })

    swarm.on('connection', (conn) => {
        console.error('New peer connected (replication swarm)', b4a.from(conn.publicKey), conn.publicKey)
        conn.on('error', (err) => {
            console.error('Replication connection error:', err)
        })
        peerCount++
        broadcastPeerCount()

        conn.on('close', () => {
            peerCount = Math.max(0, peerCount - 1)
            broadcastPeerCount()
        })


        if (autobase) {
            autobase.replicate(conn)
        } else {
            console.error('No Autobase yet to replicate with')
        }
    })

    discovery = swarm.join(topic, { server: true, client: true })
    await discovery.flushed()
    console.error('Joined replication swarm for current base')

    // Restart chat swarm with new topic
    if (chatSwarm) {
        try {
            await chatSwarm.destroy()
        } catch (e) {
            console.error('Error destroying previous chat swarm:', e)
        }
        chatSwarm = null
    }
    setupChatSwarm(baseKey != null ? baseKey : autobase.key)
}

async function joinNewBase (baseKeyHexStr) {
    if (!baseKeyHexStr || typeof baseKeyHexStr !== 'string') {
        console.error('joinNewBase: invalid baseKey', baseKeyHexStr)
        return
    }

    try {
        const newKey = Buffer.from(baseKeyHexStr.trim(), 'hex')
        if (newKey.length !== 32) {
            console.error('joinNewBase: baseKey must be 32 bytes, got', newKey.length)
            return
        }
        console.error('Joining new Autobase key at runtime:', baseKeyHexStr.trim())
        await initAutobase(newKey).then(() => {
            console.error('Backend ready')
        }).catch((err) => {
            console.error('initAutobase failed at startup:', err)
        })
    } catch (e) {
        console.error('joinNewBase failed:', e)
    }
}

// Create RPC server
rpc = new RPC(IPC, async (req, error) => {
    console.error('got a request from react', req)
    if (error) {
        console.error('got an error from react', error)
    }
    try {
        switch (req.command) {
            case RPC_ADD: {
                const text = JSON.parse(b4a.toString(req.data))
                await addItem(text)
                break
            }
            case RPC_UPDATE: {
                const data = JSON.parse(req.data.toString())
                await updateItem(data.item)
                break
            }
            case RPC_DELETE: {
                const data = JSON.parse(req.data.toString())
                await deleteItem(data.item)
                break
            }
            case RPC_GET_KEY: {
                console.error('command RPC_GET_KEY')
                if (!autobase) {
                    console.error('RPC_GET_KEY requested before Autobase is ready')
                    break
                }
                const keyReq = rpc.request(RPC_GET_KEY)
                keyReq.send(autobase.local.key.toString('hex'))
                break
            }
            case RPC_JOIN_KEY: {
                console.error('command RPC_JOIN_KEY')
                const data = JSON.parse(req.data.toString())
                console.error('Joining new base key from RPC:', data.key)
                await joinNewBase(data.key)
                break
            }
        }
    } catch (err) {
        console.error('Error handling RPC request:', err)
    }
})

// Initialize Autobase for the initial baseKey (from argv or new)
await initAutobase(baseKey).then(() => {
    console.error('Backend ready')
}).catch((err) => {
    console.error('initAutobase failed at startup:', err)
})

// Backend ready
console.error('Backend ready')

// Cleanup on teardown
Bare.on('teardown', async () => {
    console.error('Backend shutting down...')
    try {
        await swarm.destroy()
    } catch (e) {
        console.error('Error destroying replication swarm:', e)
    }
    if (chatSwarm) {
        try {
            await chatSwarm.destroy()
        } catch (e) {
            console.error('Error destroying chat swarm:', e)
        }
    }
    if (discovery) {
        try {
            await discovery.destroy()
        } catch (e) {
            console.error('Error destroying discovery:', e)
        }
    }
    try {
        await store.close()
    } catch (e) {
        console.error('Error closing store:', e)
    }
    console.error('Backend shutdown complete')
})

function open (store) {
    const view = store.get({
        name: 'test',
        valueEncoding: 'json'
    })
    console.error('opening store...', view)
    return view
}

async function apply (nodes, view, host) {
    console.error('apply started')
    for (const { value } of nodes) {
        if (!value) continue

        // Handle writer membership updates coming from handshake
        if (value.type === 'add-writer' && typeof value.key === 'string') {
            try {
                const writerKey = Buffer.from(value.key, 'hex')
                await host.addWriter(writerKey, { indexer: false })
                console.error('Added writer from add-writer op:', value.key)
            } catch (err) {
                console.error('Failed to add writer from add-writer op:', err)
            }
            continue
        }

        if (value.type === 'add') {
            if (!validateItem(value.value)) {
                console.error('Invalid item schema in add operation:', value.value)
                continue
            }
            console.error('Applying add operation for item:', value.value)
            const addReq = rpc.request(RPC_ADD_FROM_BACKEND)
            addReq.send(JSON.stringify(value.value))
            continue
        }

        if (value.type === 'delete') {
            if (!validateItem(value.value)) {
                console.error('Invalid item schema in delete operation:', value.value)
                continue
            }
            console.error('Applying delete operation for item:', value.value)
            const deleteReq = rpc.request(RPC_DELETE_FROM_BACKEND)
            deleteReq.send(JSON.stringify(value.value))
            continue
        }

        if (value.type === 'update') {
            if (!validateItem(value.value)) {
                console.error('Invalid item schema in update operation:', value.value)
                continue
            }
            console.error('Applying update operation for item:', value.value)
            const updateReq = rpc.request(RPC_UPDATE_FROM_BACKEND)
            updateReq.send(JSON.stringify(value.value))
            continue
        }

        if (value.type === 'list') {
            if (!Array.isArray(value.value)) {
                console.error('Invalid list operation payload, expected array:', value.value)
                continue
            }
            console.error('Applying list operation for items:', value.value)
            const updateReq = rpc.request(SYNC_LIST)
            updateReq.send(JSON.stringify(value.value))
            continue
        }

        // All other values are appended to the view (for future use)
        await view.append(value)
    }
}

// Simple inline schema validation matching the mobile ListEntry
function validateItem (item) {
    if (typeof item !== 'object' || item === null) return false
    if (typeof item.text !== 'string') return false
    if (typeof item.isDone !== 'boolean') return false
    if (typeof item.timeOfCompletion !== 'number') return false
    return true
}

// Add item operation (backend creates the canonical item)
async function addItem (text, listId) {
    if (!autobase) {
        console.error('addItem called before Autobase is initialized')
        return
    }

    console.error('command RPC_ADD addItem text', text)

    const item = {
        id: generateId(),                    // extra metadata, frontend can ignore
        text,
        isDone: false,
        listId: listId || null,
        timeOfCompletion: 0,
        updatedAt: Date.now(),
        timestamp: Date.now(),
    }

    const op = {
        type: 'add',
        value: item
    }

    await autobase.append(op)
    console.error('Added item:', text)
}

// Update item operation: AUTONOMOUS, NO BACKEND MEMORY
async function updateItem (item) {
    if (!autobase) {
        console.error('updateItem called before Autobase is initialized')
        return
    }

    console.error('command RPC_UPDATE updateItem item', item)

    const op = {
        type: 'update',
        value: item
    }

    await autobase.append(op)
    console.error('Updated item:', item.text)
}

// Delete item operation: AUTONOMOUS, NO BACKEND MEMORY
async function deleteItem (item) {
    if (!autobase) {
        console.error('deleteItem called before Autobase is initialized')
        return
    }

    console.error('command RPC_DELETE deleteItem item', item)

    const op = {
        type: 'delete',
        value: item
    }

    await autobase.append(op)
    console.error('Deleted item:', item.text)
}
const { Pull, Push, HEADERBYTES, KEYBYTES, ABYTES } = require('sodium-secretstream')
const sodium = require('sodium-universal')
const crypto = require('hypercore-crypto')
const { Duplex, Writable, getStreamError } = require('streamx')
const b4a = require('b4a')
const Timeout = require('timeout-refresh')
const unslab = require('unslab')
const Bridge = require('./lib/bridge')
const Handshake = require('./lib/handshake')

const IDHEADERBYTES = HEADERBYTES + 32
const [NS_INITIATOR, NS_RESPONDER, NS_SEND] = crypto.namespace('hyperswarm/secret-stream', 3)
const MAX_ATOMIC_WRITE = 256 * 256 * 256 - 1

module.exports = class NoiseSecretStream extends Duplex {
  constructor(isInitiator, rawStream, opts = {}) {
    super({ mapWritable: toBuffer })

    if (typeof isInitiator !== 'boolean') {
      throw new Error('isInitiator should be a boolean')
    }

    this.noiseStream = this
    this.isInitiator = isInitiator
    this.rawStream = null

    this.publicKey = opts.publicKey || null
    this.remotePublicKey = opts.remotePublicKey || null
    this.handshakeHash = null
    this.connected = false
    this.keepAlive = opts.keepAlive || 0
    this.timeout = 0
    this.enableSend = opts.enableSend !== false

    // pointer for upstream to set data here if they want
    this.userData = null

    let openedDone = null
    this.opened = new Promise((resolve) => {
      openedDone = resolve
    })

    this.rawBytesWritten = 0
    this.rawBytesRead = 0

    // metadata used by 'hyperdht'
    this.relay = null
    this.puncher = null

    // unwrapped raw stream
    this._rawStream = null

    // handshake state
    this._handshake = null
    this._handshakePattern = opts.pattern || null
    this._handshakeDone = null

    // message parsing state
    this._state = 0
    this._len = 0
    this._tmp = 1
    this._message = null

    this._openedDone = openedDone
    this._startDone = null
    this._drainDone = null
    this._outgoingPlain = null
    this._outgoingWrapped = null
    this._utp = null
    this._setup = true
    this._ended = 2
    this._encrypt = null
    this._decrypt = null
    this._timeoutTimer = null
    this._keepAliveTimer = null
    this._sendState = null

    if (opts.autoStart !== false) this.start(rawStream, opts)

    // wiggle it to trigger open immediately (TODO add streamx option for this)
    this.resume()
    this.pause()
  }

  static keyPair(seed) {
    return Handshake.keyPair(seed)
  }

  static id(handshakeHash, isInitiator, id) {
    return streamId(handshakeHash, isInitiator, id)
  }

  setTimeout(ms) {
    if (!ms) ms = 0

    this._clearTimeout()
    this.timeout = ms

    if (!ms || this.rawStream === null) return

    this._timeoutTimer = Timeout.once(ms, destroyTimeout, this)
    this._timeoutTimer.unref()
  }

  setKeepAlive(ms) {
    if (!ms) ms = 0

    this._clearKeepAlive()

    this.keepAlive = ms

    if (!ms || this.rawStream === null) return

    this._keepAliveTimer = Timeout.on(ms, sendKeepAlive, this)
    this._keepAliveTimer.unref()
  }

  sendKeepAlive() {
    const empty = this.alloc(0)
    this.write(empty)
  }

  start(rawStream, opts = {}) {
    if (rawStream) {
      this.rawStream = rawStream
      this._rawStream = rawStream
      if (typeof this.rawStream.setContentSize === 'function') {
        this._utp = rawStream
      }
    } else {
      this.rawStream = new Bridge(this)
      this._rawStream = this.rawStream.reverse
    }

    this.rawStream.on('error', this._onrawerror.bind(this))
    this.rawStream.on('close', this._onrawclose.bind(this))

    this._startHandshake(opts.handshake, opts.keyPair || null)
    this._continueOpen(null)

    if (this.destroying) return

    if (opts.data) this._onrawdata(opts.data)
    if (opts.ended) this._onrawend()

    if (this.keepAlive > 0 && this._keepAliveTimer === null) {
      this.setKeepAlive(this.keepAlive)
    }

    if (this.timeout > 0 && this._timeoutTimer === null) {
      this.setTimeout(this.timeout)
    }
  }

  async flush() {
    if ((await this.opened) === false) return false
    if ((await Writable.drained(this)) === false) return false
    if (this.destroying) return false

    if (this.rawStream !== null && this.rawStream.flush) {
      return await this.rawStream.flush()
    }

    return true
  }

  _continueOpen(err) {
    if (err) this.destroy(err)
    if (this._startDone === null) return
    const done = this._startDone
    this._startDone = null
    this._open(done)
  }

  _onkeypairpromise(p) {
    const self = this
    const cont = this._continueOpen.bind(this)

    p.then(onkeypair, cont)

    function onkeypair(kp) {
      self._onkeypair(kp)
      cont(null)
    }
  }

  _onkeypair(keyPair) {
    const pattern = this._handshakePattern || 'XX'
    const remotePublicKey = this.remotePublicKey

    this._handshake = new Handshake(this.isInitiator, keyPair, remotePublicKey, pattern)
    this.publicKey = this._handshake.keyPair.publicKey
  }

  _startHandshake(handshake, keyPair) {
    if (handshake) {
      const { tx, rx, hash, publicKey, remotePublicKey } = handshake
      this._setupSecretStream(tx, rx, hash, publicKey, remotePublicKey)
      return
    }

    if (!keyPair) keyPair = Handshake.keyPair()

    if (typeof keyPair.then === 'function') {
      this._onkeypairpromise(keyPair)
    } else {
      this._onkeypair(keyPair)
    }
  }

  _onrawerror(err) {
    this.destroy(err)
  }

  _onrawclose() {
    if (this._ended !== 0) this.destroy()
  }

  _onrawdata(data) {
    let offset = 0

    if (this._timeoutTimer !== null) {
      this._timeoutTimer.refresh()
    }

    do {
      switch (this._state) {
        case 0: {
          while (this._tmp !== 0x1000000 && offset < data.byteLength) {
            const v = data[offset++]
            this._len += this._tmp * v
            this._tmp *= 256
          }

          if (this._tmp === 0x1000000) {
            this._tmp = 0
            this._state = 1
            const unprocessed = data.byteLength - offset
            if (unprocessed < this._len && this._utp !== null)
              this._utp.setContentSize(this._len - unprocessed)
          }

          break
        }

        case 1: {
          const missing = this._len - this._tmp
          const end = missing + offset

          if (this._message === null && end <= data.byteLength) {
            this._message = data.subarray(offset, end)
            offset += missing
            this._incoming()
            break
          }

          const unprocessed = data.byteLength - offset

          if (this._message === null) {
            this._message = b4a.allocUnsafe(this._len)
          }

          b4a.copy(data, this._message, this._tmp, offset)
          this._tmp += unprocessed

          if (end <= data.byteLength) {
            offset += missing
            this._incoming()
          } else {
            offset += unprocessed
          }

          break
        }
      }
    } while (offset < data.byteLength && !this.destroying)
  }

  _onrawend() {
    this._ended--
    this.push(null)
  }

  _onrawdrain() {
    const drain = this._drainDone
    if (drain === null) return
    this._drainDone = null
    drain()
  }

  _read(cb) {
    this.rawStream.resume()
    cb(null)
  }

  _incoming() {
    const message = this._message

    this._state = 0
    this._len = 0
    this._tmp = 1
    this._message = null

    if (this._setup === true) {
      if (this._handshake) {
        this._onhandshakert(this._handshake.recv(message))
      } else {
        if (message.byteLength !== IDHEADERBYTES) {
          this.destroy(new Error('Invalid header message received'))
          return
        }

        const remoteId = message.subarray(0, 32)
        const expectedId = streamId(this.handshakeHash, !this.isInitiator)
        const header = message.subarray(32)

        if (!b4a.equals(expectedId, remoteId)) {
          this.destroy(new Error('Invalid header received'))
          return
        }

        this._decrypt.init(header)
        this._setup = false // setup is now done
      }
      return
    }

    if (message.byteLength < ABYTES) {
      this.destroy(new Error('Invalid message received'))
      return
    }

    this.rawBytesRead += message.byteLength

    const plain = message.subarray(1, message.byteLength - ABYTES + 1)

    try {
      this._decrypt.next(message, plain)
    } catch (err) {
      this.destroy(err)
      return
    }

    // If keep alive is selective, eat the empty buffers (ie assume the other side has it enabled also)
    if (plain.byteLength === 0 && this.keepAlive !== 0) return

    if (this.push(plain) === false) {
      this.rawStream.pause()
    }
  }

  _onhandshakert(h) {
    if (this._handshakeDone === null) return

    if (h !== null) {
      if (h.data) this._rawStream.write(h.data)
      if (!h.tx) return
    }

    const done = this._handshakeDone
    const publicKey = this._handshake.keyPair.publicKey

    this._handshakeDone = null
    this._handshake = null

    if (h === null) return done(new Error('Noise handshake failed'))

    this._setupSecretStream(h.tx, h.rx, h.hash, publicKey, h.remotePublicKey)
    this._resolveOpened(true)
    done(null)
  }

  _setupSecretStream(tx, rx, handshakeHash, publicKey, remotePublicKey) {
    const buf = b4a.allocUnsafeSlow(3 + IDHEADERBYTES)
    writeUint24le(IDHEADERBYTES, buf)

    this._encrypt = new Push(unslab(tx.subarray(0, KEYBYTES)), undefined, buf.subarray(3 + 32))
    this._decrypt = new Pull(unslab(rx.subarray(0, KEYBYTES)))

    this.publicKey = publicKey
    this.remotePublicKey = remotePublicKey
    this.handshakeHash = handshakeHash

    const id = buf.subarray(3, 3 + 32)
    streamId(handshakeHash, this.isInitiator, id)

    // initialize secretbox state for unordered messages
    this._setupSecretSend(handshakeHash)

    this.emit('handshake')
    // if rawStream is a bridge, also emit it there
    if (this.rawStream !== this._rawStream) this.rawStream.emit('handshake')

    if (this.destroying) return

    this._rawStream.write(buf)
  }

  _setupSecretSend(handshakeHash) {
    this._sendState = b4a.allocUnsafeSlow(32 + 32 + 8 + 8)
    const encrypt = this._sendState.subarray(0, 32) // secrets
    const decrypt = this._sendState.subarray(32, 64)
    const counter = this._sendState.subarray(64, 72) // nonce
    const initial = this._sendState.subarray(72)

    const inputs = this.isInitiator
      ? [
          [NS_INITIATOR, NS_SEND],
          [NS_RESPONDER, NS_SEND]
        ]
      : [
          [NS_RESPONDER, NS_SEND],
          [NS_INITIATOR, NS_SEND]
        ]

    sodium.crypto_generichash_batch(encrypt, inputs[0], handshakeHash)
    sodium.crypto_generichash_batch(decrypt, inputs[1], handshakeHash)

    sodium.randombytes_buf(initial)
    counter.set(initial)
  }

  _open(cb) {
    // no autostart or no handshake yet
    if (this._rawStream === null || (this._handshake === null && this._encrypt === null)) {
      this._startDone = cb
      return
    }

    this._rawStream.on('data', this._onrawdata.bind(this))
    this._rawStream.on('end', this._onrawend.bind(this))
    this._rawStream.on('drain', this._onrawdrain.bind(this))

    if (this.enableSend) this._rawStream.on('message', this._onmessage.bind(this))

    if (this._encrypt !== null) {
      this._resolveOpened(true)
      return cb(null)
    }

    this._handshakeDone = cb

    if (this.isInitiator) this._onhandshakert(this._handshake.send())
  }

  _predestroy() {
    if (this.rawStream) {
      const error = getStreamError(this)
      this.rawStream.destroy(error)
    }

    if (this._startDone !== null) {
      const done = this._startDone
      this._startDone = null
      done(new Error('Stream destroyed'))
    }

    if (this._handshakeDone !== null) {
      const done = this._handshakeDone
      this._handshakeDone = null
      done(new Error('Stream destroyed'))
    }

    if (this._drainDone !== null) {
      const done = this._drainDone
      this._drainDone = null
      done(new Error('Stream destroyed'))
    }
  }

  _write(data, cb) {
    let wrapped = this._outgoingWrapped

    if (data !== this._outgoingPlain) {
      wrapped = b4a.allocUnsafe(data.byteLength + 3 + ABYTES)
      wrapped.set(data, 4)
    } else {
      this._outgoingWrapped = this._outgoingPlain = null
    }

    if (wrapped.byteLength - 3 > MAX_ATOMIC_WRITE) {
      return cb(
        new Error(
          'Message is too large for an atomic write. Max size is ' + MAX_ATOMIC_WRITE + ' bytes.'
        )
      )
    }
    this.rawBytesWritten += wrapped.byteLength

    writeUint24le(wrapped.byteLength - 3, wrapped)
    // offset 4 so we can do it in-place
    this._encrypt.next(wrapped.subarray(4, 4 + data.byteLength), wrapped.subarray(3))

    if (this._keepAliveTimer !== null) this._keepAliveTimer.refresh()

    if (this._rawStream.write(wrapped) === false) {
      this._drainDone = cb
    } else {
      cb(null)
    }
  }

  _final(cb) {
    this._clearKeepAlive()
    this._ended--
    this._rawStream.end()
    cb(null)
  }

  _resolveOpened(val) {
    if (this._openedDone === null) return
    const opened = this._openedDone
    this._openedDone = null
    opened(val)
    if (!val) return
    this.connected = true
    this.emit('connect')
  }

  _clearTimeout() {
    if (this._timeoutTimer === null) return
    this._timeoutTimer.destroy()
    this._timeoutTimer = null
    this.timeout = 0
  }

  _clearKeepAlive() {
    if (this._keepAliveTimer === null) return
    this._keepAliveTimer.destroy()
    this._keepAliveTimer = null
    this.keepAlive = 0
  }

  _destroy(cb) {
    this._clearKeepAlive()
    this._clearTimeout()
    this._resolveOpened(false)
    cb(null)
  }

  _boxMessage(buffer) {
    const MB = sodium.crypto_secretbox_MACBYTES // 16
    const NB = sodium.crypto_secretbox_NONCEBYTES // 24

    const counter = this._sendState.subarray(64, 72)
    sodium.sodium_increment(counter)
    if (b4a.equals(counter, this._sendState.subarray(72))) {
      this.destroy(new Error('udp send nonce exchausted'))
      return
    }

    const secret = this._sendState.subarray(0, 32)
    const envelope = b4a.allocUnsafe(8 + MB + buffer.byteLength)
    const nonce = envelope.subarray(0, NB)
    const ciphertext = envelope.subarray(8)

    b4a.fill(nonce, 0) // pad suffix
    nonce.set(counter)

    sodium.crypto_secretbox_easy(ciphertext, buffer, nonce, secret)
    return envelope
  }

  send(buffer) {
    if (!this._sendState) return
    if (!this.rawStream?.send) return // udx-stream expected

    const message = this._boxMessage(buffer)
    return this.rawStream.send(message)
  }

  trySend(buffer) {
    if (!this._sendState) return
    if (!this.rawStream?.trySend) return // udx-stream expected

    const message = this._boxMessage(buffer)
    this.rawStream.trySend(message)
  }

  _onmessage(buffer) {
    if (!this._sendState) return // messages before handshake are dropped

    const MB = sodium.crypto_secretbox_MACBYTES // 16
    const NB = sodium.crypto_secretbox_NONCEBYTES // 24

    if (buffer.byteLength < NB) return // Invalid message

    const nonce = b4a.allocUnsafe(NB)
    b4a.fill(nonce, 0)
    nonce.set(buffer.subarray(0, 8))

    const secret = this._sendState.subarray(32, 64)
    const ciphertext = buffer.subarray(8)
    const plain = buffer.subarray(8, buffer.byteLength - MB)

    if (ciphertext.byteLength < MB) return // invalid message

    const success = sodium.crypto_secretbox_open_easy(plain, ciphertext, nonce, secret)

    if (success) this.emit('message', plain)
  }

  alloc(len) {
    const buf = b4a.allocUnsafe(len + 3 + ABYTES)
    this._outgoingWrapped = buf
    this._outgoingPlain = buf.subarray(4, buf.byteLength - ABYTES + 1)
    return this._outgoingPlain
  }

  toJSON() {
    return {
      isInitiator: this.isInitiator,
      publicKey: this.publicKey && b4a.toString(this.publicKey, 'hex'),
      remotePublicKey: this.remotePublicKey && b4a.toString(this.remotePublicKey, 'hex'),
      connected: this.connected,
      destroying: this.destroying,
      destroyed: this.destroyed,
      rawStream: this.rawStream && this.rawStream.toJSON ? this.rawStream.toJSON() : null
    }
  }
}

function writeUint24le(n, buf) {
  buf[0] = n & 255
  buf[1] = (n >>> 8) & 255
  buf[2] = (n >>> 16) & 255
}

function streamId(handshakeHash, isInitiator, out = b4a.allocUnsafe(32)) {
  sodium.crypto_generichash(out, isInitiator ? NS_INITIATOR : NS_RESPONDER, handshakeHash)
  return out
}

function toBuffer(data) {
  return typeof data === 'string' ? b4a.from(data) : data
}

function destroyTimeout() {
  this.destroy(new Error('Stream timed out'))
}

function sendKeepAlive() {
  const empty = this.alloc(0)
  this.write(empty)
}
const { Duplex, Writable } = require('streamx')

class ReversePassThrough extends Duplex {
  constructor(s) {
    super()
    this._stream = s
    this._ondrain = null
  }

  _write(data, cb) {
    if (this._stream.push(data) === false) {
      this._stream._ondrain = cb
    } else {
      cb(null)
    }
  }

  _final(cb) {
    this._stream.push(null)
    cb(null)
  }

  _read(cb) {
    const ondrain = this._ondrain
    this._ondrain = null
    if (ondrain) ondrain()
    cb(null)
  }
}

module.exports = class Bridge extends Duplex {
  constructor(noiseStream) {
    super()

    this.noiseStream = noiseStream

    this._ondrain = null
    this.reverse = new ReversePassThrough(this)
  }

  get publicKey() {
    return this.noiseStream.publicKey
  }

  get remotePublicKey() {
    return this.noiseStream.remotePublicKey
  }

  get handshakeHash() {
    return this.noiseStream.handshakeHash
  }

  flush() {
    return Writable.drained(this)
  }

  _read(cb) {
    const ondrain = this._ondrain
    this._ondrain = null
    if (ondrain) ondrain()
    cb(null)
  }

  _write(data, cb) {
    if (this.reverse.push(data) === false) {
      this.reverse._ondrain = cb
    } else {
      cb(null)
    }
  }

  _final(cb) {
    this.reverse.push(null)
    cb(null)
  }
}
const sodium = require('sodium-universal')
const curve = require('noise-curve-ed')
const Noise = require('noise-handshake')
const b4a = require('b4a')

const EMPTY = b4a.alloc(0)

module.exports = class Handshake {
  constructor(isInitiator, keyPair, remotePublicKey, pattern) {
    this.isInitiator = isInitiator
    this.keyPair = keyPair
    this.noise = new Noise(pattern, isInitiator, keyPair, { curve })
    this.noise.initialise(EMPTY, remotePublicKey)
    this.destroyed = false
  }

  static keyPair(seed) {
    const publicKey = b4a.alloc(32)
    const secretKey = b4a.alloc(64)
    if (seed) sodium.crypto_sign_seed_keypair(publicKey, secretKey, seed)
    else sodium.crypto_sign_keypair(publicKey, secretKey)
    return { publicKey, secretKey }
  }

  recv(data) {
    try {
      this.noise.recv(data)
      if (this.noise.complete) return this._return(null)
      return this.send()
    } catch {
      this.destroy()
      return null
    }
  }

  // note that the data returned here is framed so we don't have to do an extra copy
  // when sending it...
  send() {
    try {
      const data = this.noise.send()
      const wrap = b4a.allocUnsafe(data.byteLength + 3)

      writeUint24le(data.byteLength, wrap)
      wrap.set(data, 3)

      return this._return(wrap)
    } catch {
      this.destroy()
      return null
    }
  }

  destroy() {
    if (this.destroyed) return
    this.destroyed = true
  }

  _return(data) {
    const tx = this.noise.complete ? b4a.toBuffer(this.noise.tx) : null
    const rx = this.noise.complete ? b4a.toBuffer(this.noise.rx) : null
    const hash = this.noise.complete ? b4a.toBuffer(this.noise.hash) : null
    const remotePublicKey = this.noise.complete ? b4a.toBuffer(this.noise.rs) : null

    return {
      data,
      remotePublicKey,
      hash,
      tx,
      rx
    }
  }
}

function writeUint24le(n, buf) {
  buf[0] = n & 255
  buf[1] = (n >>> 8) & 255
  buf[2] = (n >>> 16) & 255
}
{
  "name": "@hyperswarm/secret-stream",
  "version": "6.9.1",
  "description": "Secret stream backed by Noise and libsodium's secretstream",
  "main": "index.js",
  "files": [
    "index.js",
    "lib/**.js"
  ],
  "dependencies": {
    "b4a": "^1.1.0",
    "hypercore-crypto": "^3.3.1",
    "noise-curve-ed": "^2.0.1",
    "noise-handshake": "^4.0.0",
    "sodium-secretstream": "^1.1.0",
    "sodium-universal": "^5.0.0",
    "streamx": "^2.14.0",
    "timeout-refresh": "^2.0.0",
    "unslab": "^1.3.0"
  },
  "devDependencies": {
    "brittle": "^3.3.0",
    "prettier": "^3.6.2",
    "prettier-config-holepunch": "^2.0.0",
    "udx-native": "^1.13.2"
  },
  "scripts": {
    "format": "prettier --write .",
    "lint": "prettier --check .",
    "test": "brittle test.js"
  },
  "repository": {
    "type": "git",
    "url": "https://github.com/holepunchto/hyperswarm-secret-stream.git"
  },
  "author": "Mathias Buus (@mafintosh)",
  "license": "Apache-2.0",
  "bugs": {
    "url": "https://github.com/holepunchto/hyperswarm-secret-stream/issues"
  },
  "homepage": "https://github.com/holepunchto/hyperswarm-secret-stream"
}
const c = require('compact-encoding')
const IndexEncoder = require('index-encoder')

const Checkout = {
  preencode(state, m) {
    c.fixed32.preencode(state, m.key)
    c.uint.preencode(state, m.length)
  },
  encode(state, m) {
    c.fixed32.encode(state, m.key)
    c.uint.encode(state, m.length)
  },
  decode(state) {
    return {
      key: c.fixed32.decode(state),
      length: c.uint.decode(state)
    }
  }
}

const Clock = c.array(Checkout)

const IndexCheckpoint = {
  preencode(state, m) {
    c.fixed64.preencode(state, m.signature)
    c.uint.preencode(state, m.length)
  },
  encode(state, m) {
    c.fixed64.encode(state, m.signature)
    c.uint.encode(state, m.length)
  },
  decode(state) {
    return {
      signature: c.fixed64.decode(state),
      length: c.uint.decode(state)
    }
  }
}

const KeyV0 = {
  preencode(state, m) {
    c.fixed32.preencode(state, m.key)
  },
  encode(state, m) {
    c.fixed32.encode(state, m.key)
  },
  decode(state) {
    return {
      key: c.fixed32.decode(state),
      length: -1
    }
  }
}

const KeysV0 = c.array(KeyV0)

const WakeupV0 = {
  preencode(state, m) {
    c.uint.preencode(state, 0) // version
    c.uint.preencode(state, m.type)

    if (m.type === 1) {
      KeysV0.preencode(state, m.writers)
    }
  },
  encode(state, m) {
    c.uint.encode(state, 0) // version
    c.uint.encode(state, m.type)

    if (m.type === 1) {
      KeysV0.encode(state, m.writers)
    }
  },
  decode(state) {
    const v = c.uint.decode(state)
    if (v !== 0) throw new Error('Unsupported version: ' + v)

    const type = c.uint.decode(state)
    const m = { version: 0, type, writers: null }

    if (m.type === 1) {
      m.writers = KeysV0.decode(state)
    }

    return m
  }
}

const Wakeup = {
  preencode(state, m) {
    if (m.version === 0) return WakeupV0.preencode(state, m)

    c.uint.preencode(state, 1) // version
    c.uint.preencode(state, m.type)

    if (m.type === 1) {
      Clock.preencode(state, m.writers)
    }
  },
  encode(state, m) {
    if (m.version === 0) return WakeupV0.encode(state, m)

    c.uint.encode(state, 1) // version
    c.uint.encode(state, m.type)

    if (m.type === 1) {
      Clock.encode(state, m.writers)
    }
  },
  decode(state) {
    const start = state.start
    const v = c.uint.decode(state)

    if (v > 1) throw new Error('Unsupported version: ' + v)

    if (v === 0) {
      state.start = start
      return WakeupV0.decode(state)
    }

    const type = c.uint.decode(state)
    const m = { version: 1, type, writers: null }

    if (m.type === 1) {
      m.writers = Clock.decode(state)
    }

    return m
  }
}

const BootRecordV0 = {
  preencode() {
    throw new Error('version 0 records cannot be encoded')
  },
  encode() {
    throw new Error('version 0 records cannot be encoded')
  },
  decode(state) {
    const indexed = Checkout.decode(state)
    const heads = Clock.decode(state)

    // one cause initial recover is not ff recovery
    return {
      version: 0,
      key: indexed.key,
      systemLength: indexed.length,
      indexersUpdated: false,
      fastForwarding: false,
      recoveries: 1,
      migrating: false,
      heads
    }
  }
}

const Checkpointer = {
  preencode(state, idx) {
    c.uint.preencode(state, idx.checkpointer)
    if (idx.checkpoint !== null) IndexCheckpoint.preencode(state, idx.checkpoint)
  },
  encode(state, idx) {
    c.uint.encode(state, idx.checkpointer)
    if (idx.checkpoint !== null) IndexCheckpoint.encode(state, idx.checkpoint)
  },
  decode(state) {
    const checkpointer = c.uint.decode(state)
    const checkpoint = checkpointer ? null : IndexCheckpoint.decode(state)

    return {
      checkpointer,
      checkpoint
    }
  }
}

const CheckpointerArray = c.array(Checkpointer)

const Indexer = {
  preencode(state, m) {
    c.uint.preencode(state, m.signature)
    c.fixed32.preencode(state, m.namespace)
    c.fixed32.preencode(state, m.publicKey)
  },
  encode(state, m) {
    c.uint.encode(state, m.signature)
    c.fixed32.encode(state, m.namespace)
    c.fixed32.encode(state, m.publicKey)
  },
  decode(state) {
    return {
      signature: c.uint.decode(state),
      namespace: c.fixed32.decode(state),
      publicKey: c.fixed32.decode(state)
    }
  }
}

const Indexers = c.array(Indexer)

const DigestV0 = {
  preencode(state, m) {
    c.uint.preencode(state, m.pointer)
    if (m.pointer === 0) {
      Indexers.preencode(state, m.indexers)
    }
  },
  encode(state, m) {
    c.uint.encode(state, m.pointer)
    if (m.pointer === 0) {
      Indexers.encode(state, m.indexers)
    }
  },
  decode(state) {
    const pointer = c.uint.decode(state)
    return {
      pointer,
      indexers: pointer === 0 ? Indexers.decode(state) : null
    }
  }
}

const Digest = {
  preencode(state, m) {
    c.uint.preencode(state, m.pointer)
    if (m.pointer === 0) {
      c.fixed32.preencode(state, m.key)
    }
  },
  encode(state, m) {
    c.uint.encode(state, m.pointer)
    if (m.pointer === 0) {
      c.fixed32.encode(state, m.key)
    }
  },
  decode(state) {
    const pointer = c.uint.decode(state)
    return {
      pointer,
      key: pointer === 0 ? c.fixed32.decode(state) : null
    }
  }
}

const Node = {
  preencode(state, m) {
    Clock.preencode(state, m.heads)
    c.uint.preencode(state, m.batch)
    c.buffer.preencode(state, m.value)
  },
  encode(state, m) {
    Clock.encode(state, m.heads)
    c.uint.encode(state, m.batch)
    c.buffer.encode(state, m.value)
  },
  decode(state, m) {
    return {
      heads: Clock.decode(state),
      batch: c.uint.decode(state),
      value: c.buffer.decode(state)
    }
  }
}

const Additional = {
  preencode(state, m) {
    c.uint.preencode(state, m.pointer)
    if (m.pointer === 0) {
      AdditionalData.preencode(state, m.data)
    }
  },
  encode(state, m) {
    c.uint.encode(state, m.pointer)
    if (m.pointer === 0) {
      AdditionalData.encode(state, m.data)
    }
  },
  decode(state) {
    const pointer = c.uint.decode(state)
    return {
      pointer,
      data: pointer === 0 ? AdditionalData.decode(state) : null
    }
  }
}

const AdditionalData = {
  preencode(state, m) {
    c.uint.preencode(state, 0)
  },
  encode(state, m) {
    c.uint.encode(state, 0) // empty for now, for the future
  },
  decode(state) {
    const flags = c.uint.decode(state)
    return {
      encryptionId: flags & 1 ? c.fixed32.decode(state) : null, // to help validate the encryption key used
      abi: flags & 2 ? c.uint.decode(state) : 0
    }
  }
}

const OplogMessageV1 = {
  preencode(state, m) {
    throw new Error('Encoding not supported')
  },
  encode(state, m) {
    throw new Error('Encoding not supported')
  },
  decode(state) {
    const maxSupportedVersion = c.uint.decode(state)

    const flags = c.uint.decode(state)

    const isCheckpointer = (flags & 1) !== 0

    const chk = isCheckpointer ? CheckpointerArray.decode(state) : null
    const digest = isCheckpointer ? Digest.decode(state) : null

    const node = Node.decode(state)

    return {
      version: 1,
      maxSupportedVersion,
      digest,
      checkpoint: chk ? { system: chk[0], encryption: null, user: chk.slice(1) } : null,
      optimistic: (flags & 2) !== 0,
      node
    }
  }
}

const OplogMessageV0 = {
  preencode(state, m) {
    throw new Error('Encoding not supported')
  },
  encode(state, m) {
    throw new Error('Encoding not supported')
  },
  decode(state) {
    const flags = c.uint.decode(state)

    const isCheckpointer = (flags & 1) !== 0

    if (isCheckpointer) DigestV0.decode(state)
    const chk = isCheckpointer ? CheckpointerArray.decode(state) : null
    const node = Node.decode(state)
    Additional.decode(state)
    const maxSupportedVersion = state.start < state.end ? c.uint.decode(state) : 0

    return {
      version: 0,
      maxSupportedVersion,
      digest: null,
      checkpoint: chk ? { system: chk[0], encryption: null, user: chk.slice(1) } : null,
      optimistic: null,
      node
    }
  }
}

// prefix 0 is reserved for future manifest
const LINEARIZER_PREFIX = 1

const LinearizerKey = {
  preencode(state, seq) {
    IndexEncoder.UINT.preencode(state, LINEARIZER_PREFIX)
    IndexEncoder.UINT.preencode(state, seq)
  },
  encode(state, seq) {
    IndexEncoder.UINT.encode(state, LINEARIZER_PREFIX)
    IndexEncoder.UINT.encode(state, seq)
  },
  decode(state) {
    IndexEncoder.UINT.decode(state)
    return IndexEncoder.UINT.decode(state)
  }
}

function infoLegacyMap(info) {
  return {
    version: info.version,
    members: info.members,
    pendingIndexers: info.pendingIndexers,
    indexers: info.indexers,
    heads: info.heads,
    views: info.views,
    encryptionLength: 0,
    entropy: null
  }
}

module.exports = {
  Wakeup,
  BootRecordV0,
  OplogMessageV0,
  OplogMessageV1,
  LinearizerKey,
  infoLegacyMap
}
// This file is autogenerated by the hyperschema compiler
// Schema Version: 1
/* eslint-disable camelcase */
/* eslint-disable quotes */

const { c } = require('hyperschema/runtime')
const external0 = require('../../legacy.js')

const VERSION = 1

// eslint-disable-next-line no-unused-vars
let version = VERSION

// @autobase/checkout
const encoding0 = {
  preencode(state, m) {
    c.fixed32.preencode(state, m.key)
    c.uint.preencode(state, m.length)
  },
  encode(state, m) {
    c.fixed32.encode(state, m.key)
    c.uint.encode(state, m.length)
  },
  decode(state) {
    const r0 = c.fixed32.decode(state)
    const r1 = c.uint.decode(state)

    return {
      key: r0,
      length: r1
    }
  }
}

// @autobase/clock
const encoding1 = c.array(encoding0)

// @autobase/index-checkpoint
const encoding2 = {
  preencode(state, m) {
    c.fixed64.preencode(state, m.signature)
    c.uint.preencode(state, m.length)
  },
  encode(state, m) {
    c.fixed64.encode(state, m.signature)
    c.uint.encode(state, m.length)
  },
  decode(state) {
    const r0 = c.fixed64.decode(state)
    const r1 = c.uint.decode(state)

    return {
      signature: r0,
      length: r1
    }
  }
}

const encoding3 = external0.Wakeup

const encoding4 = external0.BootRecordV0

// @autobase/boot-record-raw
const encoding5 = {
  preencode(state, m) {
    c.fixed32.preencode(state, m.key)
    c.uint.preencode(state, m.systemLength)
    state.end++ // max flag is 4 so always one byte

    if (m.recoveries) c.uint.preencode(state, m.recoveries)
  },
  encode(state, m) {
    const flags = (m.indexersUpdated ? 1 : 0) | (m.fastForwarding ? 2 : 0) | (m.recoveries ? 4 : 0)

    c.fixed32.encode(state, m.key)
    c.uint.encode(state, m.systemLength)
    c.uint.encode(state, flags)

    if (m.recoveries) c.uint.encode(state, m.recoveries)
  },
  decode(state, version) {
    if (version === undefined) version = c.uint.decode(state)
    const r0 = c.fixed32.decode(state)
    const r1 = c.uint.decode(state)
    const flags = c.uint.decode(state)

    return {
      version,
      key: r0,
      systemLength: r1,
      indexersUpdated: (flags & 1) !== 0,
      fastForwarding: (flags & 2) !== 0,
      recoveries: (flags & 4) !== 0 ? c.uint.decode(state) : 0
    }
  }
}

// @autobase/boot-record
const encoding6 = {
  preencode(state, m) {
    c.uint.preencode(state, m.version)
    switch (m.version) {
      case 0:
        encoding4.preencode(state, m)
        break
      case 1:
      case 2:
      case 3:
        encoding5.preencode(state, m)
        break
      default:
        throw new Error('Unsupported version')
    }
  },
  encode(state, m) {
    c.uint.encode(state, m.version)
    switch (m.version) {
      case 0:
        encoding4.encode(state, m)
        break
      case 1:
      case 2:
      case 3:
        encoding5.encode(state, m)
        break
      default:
        throw new Error('Unsupported version')
    }
  },
  decode(state) {
    const version = c.uint.decode(state)
    switch (version) {
      case 0: {
        const decoded = encoding4.decode(state, version)
        return decoded
      }
      case 1:
      case 2:
      case 3: {
        const decoded = encoding5.decode(state, version)
        return decoded
      }
      default:
        throw new Error('Unsupported version')
    }
  }
}

// @autobase/checkpointer
const encoding7 = {
  preencode(state, m) {
    state.end++ // max flag is 2 so always one byte

    if (m.checkpointer) c.uint.preencode(state, m.checkpointer)
    if (m.checkpoint) encoding2.preencode(state, m.checkpoint)
  },
  encode(state, m) {
    const flags = (m.checkpointer ? 1 : 0) | (m.checkpoint ? 2 : 0)

    c.uint.encode(state, flags)

    if (m.checkpointer) c.uint.encode(state, m.checkpointer)
    if (m.checkpoint) encoding2.encode(state, m.checkpoint)
  },
  decode(state) {
    const flags = c.uint.decode(state)

    return {
      checkpointer: (flags & 1) !== 0 ? c.uint.decode(state) : 0,
      checkpoint: (flags & 2) !== 0 ? encoding2.decode(state) : null
    }
  }
}

// @autobase/checkpoint.user
const encoding8_2 = c.array(encoding7)

// @autobase/checkpoint
const encoding8 = {
  preencode(state, m) {
    state.end++ // max flag is 4 so always one byte

    if (m.system) encoding7.preencode(state, m.system)
    if (m.encryption) encoding7.preencode(state, m.encryption)
    if (m.user) encoding8_2.preencode(state, m.user)
  },
  encode(state, m) {
    const flags = (m.system ? 1 : 0) | (m.encryption ? 2 : 0) | (m.user ? 4 : 0)

    c.uint.encode(state, flags)

    if (m.system) encoding7.encode(state, m.system)
    if (m.encryption) encoding7.encode(state, m.encryption)
    if (m.user) encoding8_2.encode(state, m.user)
  },
  decode(state) {
    const flags = c.uint.decode(state)

    return {
      system: (flags & 1) !== 0 ? encoding7.decode(state) : null,
      encryption: (flags & 2) !== 0 ? encoding7.decode(state) : null,
      user: (flags & 4) !== 0 ? encoding8_2.decode(state) : null
    }
  }
}

// @autobase/digest
const encoding9 = {
  preencode(state, m) {
    state.end++ // max flag is 2 so always one byte

    if (m.pointer) c.uint.preencode(state, m.pointer)
    if (m.key) c.fixed32.preencode(state, m.key)
  },
  encode(state, m) {
    const flags = (m.pointer ? 1 : 0) | (m.key ? 2 : 0)

    c.uint.encode(state, flags)

    if (m.pointer) c.uint.encode(state, m.pointer)
    if (m.key) c.fixed32.encode(state, m.key)
  },
  decode(state) {
    const flags = c.uint.decode(state)

    return {
      pointer: (flags & 1) !== 0 ? c.uint.decode(state) : 0,
      key: (flags & 2) !== 0 ? c.fixed32.decode(state) : null
    }
  }
}

// @autobase/node
const encoding10 = {
  preencode(state, m) {
    encoding1.preencode(state, m.heads)
    c.uint.preencode(state, m.batch)
    c.buffer.preencode(state, m.value)
  },
  encode(state, m) {
    encoding1.encode(state, m.heads)
    c.uint.encode(state, m.batch)
    c.buffer.encode(state, m.value)
  },
  decode(state) {
    const r0 = encoding1.decode(state)
    const r1 = c.uint.decode(state)
    const r2 = c.buffer.decode(state)

    return {
      heads: r0,
      batch: r1,
      value: r2
    }
  }
}

// @autobase/trace.system
const encoding11_0 = c.array(c.uint)
// @autobase/trace.encryption
const encoding11_1 = encoding11_0

// @autobase/trace
const encoding11 = {
  preencode(state, m) {
    encoding11_0.preencode(state, m.system)
    encoding11_1.preencode(state, m.encryption)
    encoding11_2.preencode(state, m.user)
  },
  encode(state, m) {
    encoding11_0.encode(state, m.system)
    encoding11_1.encode(state, m.encryption)
    encoding11_2.encode(state, m.user)
  },
  decode(state) {
    const r0 = encoding11_0.decode(state)
    const r1 = encoding11_1.decode(state)
    const r2 = encoding11_2.decode(state)

    return {
      system: r0,
      encryption: r1,
      user: r2
    }
  }
}

const encoding12 = external0.OplogMessageV0

const encoding13 = external0.OplogMessageV1

// @autobase/oplog-message-v2.checkpoint
const encoding14_1 = c.frame(encoding8)
// @autobase/oplog-message-v2.digest
const encoding14_2 = c.frame(encoding9)
// @autobase/oplog-message-v2.trace
const encoding14_4 = c.frame(encoding11)

// @autobase/oplog-message-v2
const encoding14 = {
  preencode(state, m) {
    encoding10.preencode(state, m.node)
    state.end++ // max flag is 8 so always one byte

    if (m.checkpoint) encoding14_1.preencode(state, m.checkpoint)
    if (m.digest) encoding14_2.preencode(state, m.digest)
    if (m.trace) encoding14_4.preencode(state, m.trace)
  },
  encode(state, m) {
    const flags =
      (m.checkpoint ? 1 : 0) | (m.digest ? 2 : 0) | (m.optimistic ? 4 : 0) | (m.trace ? 8 : 0)

    encoding10.encode(state, m.node)
    c.uint.encode(state, flags)

    if (m.checkpoint) encoding14_1.encode(state, m.checkpoint)
    if (m.digest) encoding14_2.encode(state, m.digest)
    if (m.trace) encoding14_4.encode(state, m.trace)
  },
  decode(state, version) {
    if (version === undefined) version = c.uint.decode(state)
    const r0 = encoding10.decode(state)
    const flags = c.uint.decode(state)

    return {
      version,
      node: r0,
      checkpoint: (flags & 1) !== 0 ? encoding14_1.decode(state) : null,
      digest: (flags & 2) !== 0 ? encoding14_2.decode(state) : null,
      optimistic: (flags & 4) !== 0,
      trace: (flags & 8) !== 0 ? encoding14_4.decode(state) : null
    }
  }
}

// @autobase/oplog-message
const encoding15 = {
  preencode(state, m) {
    c.uint.preencode(state, m.version)
    switch (m.version) {
      case 0:
        encoding12.preencode(state, m)
        break
      case 1:
        encoding13.preencode(state, m)
        break
      case 2:
        encoding14.preencode(state, m)
        break
      default:
        throw new Error('Unsupported version')
    }
  },
  encode(state, m) {
    c.uint.encode(state, m.version)
    switch (m.version) {
      case 0:
        encoding12.encode(state, m)
        break
      case 1:
        encoding13.encode(state, m)
        break
      case 2:
        encoding14.encode(state, m)
        break
      default:
        throw new Error('Unsupported version')
    }
  },
  decode(state) {
    const version = c.uint.decode(state)
    switch (version) {
      case 0: {
        const decoded = encoding12.decode(state, version)
        return decoded
      }
      case 1: {
        const decoded = encoding13.decode(state, version)
        return decoded
      }
      case 2: {
        const decoded = encoding14.decode(state, version)
        return decoded
      }
      default:
        throw new Error('Unsupported version')
    }
  }
}

// @autobase/info-v1.pendingIndexers
const encoding16_1 = c.array(c.fixed32)

// @autobase/info-v1
const encoding16 = {
  preencode(state, m) {
    c.uint.preencode(state, m.members)
    encoding16_1.preencode(state, m.pendingIndexers)
    encoding1.preencode(state, m.indexers)
    encoding1.preencode(state, m.heads)
    encoding1.preencode(state, m.views)
  },
  encode(state, m) {
    c.uint.encode(state, m.members)
    encoding16_1.encode(state, m.pendingIndexers)
    encoding1.encode(state, m.indexers)
    encoding1.encode(state, m.heads)
    encoding1.encode(state, m.views)
  },
  decode(state, version) {
    if (version === undefined) version = c.uint.decode(state)
    const r0 = c.uint.decode(state)
    const r1 = encoding16_1.decode(state)
    const r2 = encoding1.decode(state)
    const r3 = encoding1.decode(state)
    const r4 = encoding1.decode(state)

    return {
      version,
      members: r0,
      pendingIndexers: r1,
      indexers: r2,
      heads: r3,
      views: r4
    }
  }
}

// @autobase/info-v2.pendingIndexers
const encoding17_1 = encoding16_1

// @autobase/info-v2
const encoding17 = {
  preencode(state, m) {
    c.uint.preencode(state, m.members)
    encoding17_1.preencode(state, m.pendingIndexers)
    encoding1.preencode(state, m.indexers)
    encoding1.preencode(state, m.heads)
    encoding1.preencode(state, m.views)
    c.uint.preencode(state, m.encryptionLength)
    state.end++ // max flag is 1 so always one byte

    if (m.entropy) c.fixed32.preencode(state, m.entropy)
  },
  encode(state, m) {
    const flags = m.entropy ? 1 : 0

    c.uint.encode(state, m.members)
    encoding17_1.encode(state, m.pendingIndexers)
    encoding1.encode(state, m.indexers)
    encoding1.encode(state, m.heads)
    encoding1.encode(state, m.views)
    c.uint.encode(state, m.encryptionLength)
    c.uint.encode(state, flags)

    if (m.entropy) c.fixed32.encode(state, m.entropy)
  },
  decode(state, version) {
    if (version === undefined) version = c.uint.decode(state)
    const r0 = c.uint.decode(state)
    const r1 = encoding17_1.decode(state)
    const r2 = encoding1.decode(state)
    const r3 = encoding1.decode(state)
    const r4 = encoding1.decode(state)
    const r5 = c.uint.decode(state)
    const flags = c.uint.decode(state)

    return {
      version,
      members: r0,
      pendingIndexers: r1,
      indexers: r2,
      heads: r3,
      views: r4,
      encryptionLength: r5,
      entropy: (flags & 1) !== 0 ? c.fixed32.decode(state) : null
    }
  }
}

// @autobase/info
const encoding18 = {
  preencode(state, m) {
    c.uint.preencode(state, m.version)
    switch (m.version) {
      case 0:
      case 1:
        encoding16.preencode(state, m)
        break
      case 2:
        encoding17.preencode(state, m)
        break
      default:
        throw new Error('Unsupported version')
    }
  },
  encode(state, m) {
    c.uint.encode(state, m.version)
    switch (m.version) {
      case 0:
      case 1:
        encoding16.encode(state, m)
        break
      case 2:
        encoding17.encode(state, m)
        break
      default:
        throw new Error('Unsupported version')
    }
  },
  decode(state) {
    const version = c.uint.decode(state)
    switch (version) {
      case 0:
      case 1: {
        const decoded = encoding16.decode(state, version)
        const map = external0.infoLegacyMap
        return map(decoded)
      }
      case 2: {
        const decoded = encoding17.decode(state, version)
        return decoded
      }
      default:
        throw new Error('Unsupported version')
    }
  }
}

// @autobase/member
const encoding19 = {
  preencode(state, m) {
    state.end++ // max flag is 2 so always one byte
    c.uint.preencode(state, m.length)
  },
  encode(state, m) {
    const flags = (m.isIndexer ? 1 : 0) | (m.isRemoved ? 2 : 0)

    c.uint.encode(state, flags)
    c.uint.encode(state, m.length)
  },
  decode(state) {
    const flags = c.uint.decode(state)

    return {
      isIndexer: (flags & 1) !== 0,
      isRemoved: (flags & 2) !== 0,
      length: c.uint.decode(state)
    }
  }
}

const encoding20 = external0.LinearizerKey

// @autobase/linearizer-update
const encoding21 = {
  preencode(state, m) {
    c.fixed32.preencode(state, m.key)
    c.uint.preencode(state, m.length)
    c.uint.preencode(state, m.batch)
    c.uint.preencode(state, m.systemLength)
    state.end++ // max flag is 1 so always one byte
  },
  encode(state, m) {
    const flags = m.indexers ? 1 : 0

    c.fixed32.encode(state, m.key)
    c.uint.encode(state, m.length)
    c.uint.encode(state, m.batch)
    c.uint.encode(state, m.systemLength)
    c.uint.encode(state, flags)
  },
  decode(state) {
    const r0 = c.fixed32.decode(state)
    const r1 = c.uint.decode(state)
    const r2 = c.uint.decode(state)
    const r3 = c.uint.decode(state)
    const flags = c.uint.decode(state)

    return {
      key: r0,
      length: r1,
      batch: r2,
      systemLength: r3,
      indexers: (flags & 1) !== 0
    }
  }
}

// @autobase/encryption-descriptor
const encoding22 = {
  preencode(state, m) {
    c.uint.preencode(state, m.type)
    c.buffer.preencode(state, m.payload)
  },
  encode(state, m) {
    c.uint.encode(state, m.type)
    c.buffer.encode(state, m.payload)
  },
  decode(state) {
    const r0 = c.uint.decode(state)
    const r1 = c.buffer.decode(state)

    return {
      type: r0,
      payload: r1
    }
  }
}

// @autobase/manifest-data
const encoding23 = {
  preencode(state, m) {
    c.uint.preencode(state, m.version)
    state.end++ // max flag is 2 so always one byte

    if (m.legacyBlocks) c.uint.preencode(state, m.legacyBlocks)
    if (m.namespace) c.fixed32.preencode(state, m.namespace)
  },
  encode(state, m) {
    const flags = (m.legacyBlocks ? 1 : 0) | (m.namespace ? 2 : 0)

    c.uint.encode(state, m.version)
    c.uint.encode(state, flags)

    if (m.legacyBlocks) c.uint.encode(state, m.legacyBlocks)
    if (m.namespace) c.fixed32.encode(state, m.namespace)
  },
  decode(state) {
    const r0 = c.uint.decode(state)
    const flags = c.uint.decode(state)

    return {
      version: r0,
      legacyBlocks: (flags & 1) !== 0 ? c.uint.decode(state) : 0,
      namespace: (flags & 2) !== 0 ? c.fixed32.decode(state) : null
    }
  }
}

// @autobase/user-view-trace.blocks
const encoding24_1 = encoding11_0

// @autobase/user-view-trace
const encoding24 = {
  preencode(state, m) {
    c.uint.preencode(state, m.view)
    encoding24_1.preencode(state, m.blocks)
  },
  encode(state, m) {
    c.uint.encode(state, m.view)
    encoding24_1.encode(state, m.blocks)
  },
  decode(state) {
    const r0 = c.uint.decode(state)
    const r1 = encoding24_1.decode(state)

    return {
      view: r0,
      blocks: r1
    }
  }
}

// @autobase/trace.user, deferred due to recusive use
const encoding11_2 = c.array(encoding24)

function setVersion(v) {
  version = v
}

function encode(name, value, v = VERSION) {
  version = v
  return c.encode(getEncoding(name), value)
}

function decode(name, buffer, v = VERSION) {
  version = v
  return c.decode(getEncoding(name), buffer)
}

function getEnum(name) {
  switch (name) {
    default:
      throw new Error('Enum not found ' + name)
  }
}

function getEncoding(name) {
  switch (name) {
    case '@autobase/checkout':
      return encoding0
    case '@autobase/clock':
      return encoding1
    case '@autobase/index-checkpoint':
      return encoding2
    case '@autobase/wakeup':
      return encoding3
    case '@autobase/boot-record-v0':
      return encoding4
    case '@autobase/boot-record-raw':
      return encoding5
    case '@autobase/boot-record':
      return encoding6
    case '@autobase/checkpointer':
      return encoding7
    case '@autobase/checkpoint':
      return encoding8
    case '@autobase/digest':
      return encoding9
    case '@autobase/node':
      return encoding10
    case '@autobase/trace':
      return encoding11
    case '@autobase/oplog-message-v0':
      return encoding12
    case '@autobase/oplog-message-v1':
      return encoding13
    case '@autobase/oplog-message-v2':
      return encoding14
    case '@autobase/oplog-message':
      return encoding15
    case '@autobase/info-v1':
      return encoding16
    case '@autobase/info-v2':
      return encoding17
    case '@autobase/info':
      return encoding18
    case '@autobase/member':
      return encoding19
    case '@autobase/linearizer-key':
      return encoding20
    case '@autobase/linearizer-update':
      return encoding21
    case '@autobase/encryption-descriptor':
      return encoding22
    case '@autobase/manifest-data':
      return encoding23
    case '@autobase/user-view-trace':
      return encoding24
    default:
      throw new Error('Encoder not found ' + name)
  }
}

function getStruct(name, v = VERSION) {
  const enc = getEncoding(name)
  return {
    preencode(state, m) {
      version = v
      enc.preencode(state, m)
    },
    encode(state, m) {
      version = v
      enc.encode(state, m)
    },
    decode(state) {
      version = v
      return enc.decode(state)
    }
  }
}

const resolveStruct = getStruct // compat

module.exports = {
  resolveStruct,
  getStruct,
  getEnum,
  getEncoding,
  encode,
  decode,
  setVersion,
  version
}
const b4a = require('b4a')
const ReadyResource = require('ready-resource')
const debounceify = require('debounceify')
const c = require('compact-encoding')
const safetyCatch = require('safety-catch')
const hypercoreId = require('hypercore-id-encoding')
const assert = require('nanoassert')
const SignalPromise = require('signal-promise')
const CoreCoupler = require('core-coupler')
const ProtomuxWakeup = require('protomux-wakeup')
const rrp = require('resolve-reject-promise')
const Hypercore = require('hypercore')
const ScopeLock = require('scope-lock')

const LocalState = require('./lib/local-state.js')
const Linearizer = require('./lib/linearizer.js')
const SystemView = require('./lib/system.js')
const { EncryptionView } = require('./lib/encryption.js')
const UpdateChanges = require('./lib/updates.js')
const messages = require('./lib/messages.js')
const Timer = require('./lib/timer.js')
const Writer = require('./lib/writer.js')
const { encodeValue, decodeValue } = require('./lib/values.js')
const ActiveWriters = require('./lib/active-writers.js')
const AutoWakeup = require('./lib/wakeup.js')

const FastForward = require('./lib/fast-forward.js')
const AutoStore = require('./lib/store.js')
const ApplyState = require('./lib/apply-state.js')
const AppendBatch = require('./lib/append-batch.js')
const { PublicApplyCalls } = require('./lib/apply-calls.js')
const boot = require('./lib/boot.js')
const { MAX_AUTOBASE_VERSION, BOOT_RECORD_VERSION } = require('./lib/caps.js')

const inspect = Symbol.for('nodejs.util.inspect.custom')
const INTERRUPT = new Error('Apply interrupted')
const BINARY_ENCODING = c.from('binary')

const RECOVERIES = 3

// default is to automatically ack
const DEFAULT_ACK_INTERVAL = 10_000
const DEFAULT_ACK_THRESHOLD = 4

const REMOTE_ADD_BATCH = 64
const REMOTE_ADD_BATCH_SANITY = 4096
const MIN_FF_WAIT = 300_000 // wait at least 5min before attempting to ff again after failure

class WakeupHandler {
  constructor(base, discoveryKey) {
    this.active = true
    this.discoveryKey = discoveryKey
    this.base = base
  }

  onpeeractive(peer, session) {
    this.base._bumpWakeupPeer(peer)
  }

  onlookup(req, peer, session) {
    const wakeup = this.base._getWakeup()
    if (wakeup.length === 0) return
    session.announce(peer, wakeup)
  }

  onannounce(wakeup, peer, session) {
    if (this.base.isFastForwarding()) {
      this.base._needsWakeupRequest = true
      return
    }
    this.base.hintWakeup(wakeup)
  }
}

module.exports = class Autobase extends ReadyResource {
  constructor(store, bootstrap, handlers = {}) {
    if (Array.isArray(bootstrap)) bootstrap = bootstrap[0] // TODO: just a quick compat, lets remove soon

    if (bootstrap && typeof bootstrap !== 'string' && !b4a.isBuffer(bootstrap)) {
      handlers = bootstrap
      bootstrap = null
    }

    super()

    const key = bootstrap ? toKey(bootstrap) : null

    this.id = null
    this.key = key
    this.discoveryKey = null
    this.backoff = handlers.backoff || null

    this.keyPair = null
    this.valueEncoding = c.from(handlers.valueEncoding || 'binary')
    this.store = store
    this.globalCache = store.globalCache || null
    this.migrated = false

    this.encrypted = handlers.encrypted || !!handlers.encryptionKey
    this.encrypt = !!handlers.encrypt
    this.encryptionKey = null
    this.encryption = null

    this.activeBatch = null // maintained by the append-batch

    this.local = null
    this.localWriter = null
    this.isIndexer = false

    this.activeWriters = new ActiveWriters()
    this.linearizer = null
    this.updating = false

    this.nukeTip = !!handlers.nukeTip

    this.wakeupOwner = !handlers.wakeup
    this.wakeupCapability = null
    this.wakeupProtocol = handlers.wakeup || new ProtomuxWakeup()
    this.wakeupSession = null

    this._primaryBootstrap = null

    this.fastForwardEnabled = handlers.fastForward !== false
    this.fastForwarding = null
    this.fastForwardTo = null
    this.fastForwardFailedAt = 0
    this.fastForwardMinimum = FastForward.MINIMUM

    this.bigBatches = !!handlers.bigBatches

    this._bootstrapWriters = [] // might contain dups, but thats ok
    this._bootstrapWritersChanged = false

    this._flushSignal = new SignalPromise()
    this._flushing = 0

    this._checkWriters = []
    this._optimistic = -1
    this._appended = 0
    this._appending = null
    this._wakeup = new AutoWakeup(this)
    this._wakeupHints = new Map()
    this._wakeupPeerBound = this._wakeupPeer.bind(this)
    this._coupler = null

    this._updateLocalCore = null
    this._lock = new ScopeLock()

    this._needsWakeupRequest = false
    this._needsWakeup = true
    this._needsWakeupHeads = true

    this._updates = []
    this._handlers = handlers || {}
    this._warn = emitWarning.bind(this)
    this._lastError = null

    this._draining = false
    this._writable = null
    this._advancing = null
    this._interrupting = false
    this._caughtup = false

    this.paused = false

    this._bump = debounceify(() => {
      this._advancing = this._advance()
      return this._advancing
    })

    this._onremotewriterchangeBound = this._onremotewriterchange.bind(this)
    this._onlocalwriterchangeBound = this._onlocalwriterchange.bind(this)

    this._preopen = null

    this._hasOpen = !!this._handlers.open
    this._hasApply = !!this._handlers.apply
    this._hasOptimisticApply = !!this._handlers.optimistic
    this._hasUpdate = !!this._handlers.update
    this._hasClose = !!this._handlers.close

    this._viewStore = new AutoStore(this)
    this._applyState = null

    this.view = null
    this.core = null
    this.version = -1
    this.interrupted = null
    this.recoveries = RECOVERIES

    const { ackInterval = DEFAULT_ACK_INTERVAL, ackThreshold = DEFAULT_ACK_THRESHOLD } = handlers

    this._ackInterval = ackInterval
    this._ackThreshold = ackThreshold
    this._ackTickThreshold = ackThreshold
    this._ackTick = 0

    this._ackTimer = null
    this._acking = false

    this._waiting = new SignalPromise()

    this.view = this._hasOpen
      ? this._handlers.open(this._viewStore, new PublicApplyCalls(this))
      : null
    this.core = this._viewStore.get({ name: '_system' })

    if (this.fastForwardEnabled && isObject(handlers.fastForward)) {
      this._runFastForward(
        new FastForward(this, handlers.fastForward.key, { verified: false })
      ).catch(noop)
    }

    this.ready().catch(safetyCatch)
  }

  [inspect](depth, opts) {
    let indent = ''
    if (typeof opts.indentationLvl === 'number') {
      while (indent.length < opts.indentationLvl) indent += ' '
    }

    return indent + 'Autobase { ... }'
  }

  // just compat, use .key
  get bootstrap() {
    return this.key
  }

  // TODO: compat, will be removed
  get bootstraps() {
    return [this.bootstrap]
  }

  get appending() {
    return this._appending !== null && this._appending.length > 0
  }

  get writable() {
    return this.localWriter !== null && !this.localWriter.isRemoved
  }

  get ackable() {
    return this.localWriter !== null && this.localWriter.isActiveIndexer
  }

  get signedLength() {
    return this.core.signedLength
  }

  get indexedLength() {
    return this._applyState ? this._applyState.indexedLength : 0
  }

  get length() {
    return this.core.length
  }

  get flushing() {
    return this._flushing > 0
  }

  hash() {
    return this.core.treeHash()
  }

  // deprecated, use .core.key
  getSystemKey() {
    return this.core.key
  }

  get system() {
    return this._applyState && this._applyState.system
  }

  // deprecated
  async getIndexedInfo() {
    if (this.opened === false) await this.ready()
    return (
      this._applyState && this._applyState.system.getIndexedInfo(this._applyState.indexedLength)
    )
  }

  _isActiveIndexer() {
    return this.localWriter ? this.localWriter.isActiveIndexer : false
  }

  replicate(isInitiator, opts) {
    const stream = this.store.replicate(isInitiator, opts)
    this.wakeupProtocol.addStream(stream)
    return stream
  }

  heads() {
    if (!this._applyState || !this._applyState.opened) return []
    const nodes = new Array(this._applyState.system.heads.length)
    for (let i = 0; i < this._applyState.system.heads.length; i++)
      nodes[i] = this._applyState.system.heads[i]
    return nodes.sort(compareNodes)
  }

  async export() {
    const exporting = [this.store.storage.export(this.local.discoveryKey), this._viewStore.export()]

    const [local, views] = await Promise.all(exporting)

    return {
      local,
      views
    }
  }

  hintWakeup(hints) {
    if (!Array.isArray(hints)) hints = [hints]
    for (const { key, length } of hints) {
      const hex = b4a.toString(key, 'hex')
      const prev = this._wakeupHints.get(hex)
      if (!prev || length === -1 || prev < length) this._wakeupHints.set(hex, length)
    }
    this._queueBump()
  }

  setBigBatches(bool = true) {
    this.bigBatches = bool
  }

  _queueBump() {
    this._bump().catch(safetyCatch)
  }

  async _runPreOpen() {
    if (this._handlers.wait) await this._handlers.wait()

    await this.store.ready()

    this.keyPair = (await this._handlers.keyPair) || null

    if (this._handlers.encryptionKey !== null) {
      this.encryptionKey = await this._handlers.encryptionKey
    }

    const result = await boot(this.store, this.key, {
      encryptionKey: this.encryptionKey,
      encrypt: this.encrypt,
      keyPair: this.keyPair
    })

    this._primaryBootstrap = result.bootstrap
    this.local = result.local

    this.key = result.bootstrap.key
    this.discoveryKey = result.bootstrap.discoveryKey
    this.id = result.bootstrap.id

    this.encryptionKey = result.encryptionKey

    if (this.encrypted) {
      assert(this.encryptionKey !== null, 'Encryption key is expected')
    }

    if (this.encryptionKey) {
      this.encryption = new EncryptionView(this, null)

      this.local.setEncryption(this.getWriterEncryption())
      this._primaryBootstrap.setEncryption(this.getWriterEncryption())
    }

    if (this.nukeTip) await this._nukeTip()

    this.local.on('append', this._onlocalwriterchangeBound)

    this.wakeupCapability = (await this._handlers.wakeupCapability) || {
      key: this.key,
      discoveryKey: this.discoveryKey
    }
    this.setWakeup(this.wakeupCapability.key, this.wakeupCapability.discoveryKey)
  }

  async _nukeTipBatch(key, length) {
    const core = this.store.get({ key, active: false })
    await core.ready()

    const batch = core.session({ name: 'batch' })
    await batch.ready()
    if (batch.length > length) await batch.truncate(length)
    await batch.close()

    await core.close()
  }

  // TODO: not atomic atm, so more of a (very useful) debug helper
  async _nukeTip() {
    const pointer = await this.local.getUserData('autobase/boot')
    if (!pointer) return

    const boot = c.decode(messages.BootRecord, pointer)

    await LocalState.clear(this.local)

    await this._nukeTipBatch(boot.key, boot.systemLength)

    const core = this.store.get({ key: boot.key, active: false, encryption: null })
    const encCore = await EncryptionView.setSystemEncryption(this, core)

    const batch = core.session({ name: 'batch' })
    await batch.ready()

    const info = await SystemView.getIndexedInfo(batch, boot.systemLength)
    await batch.close()

    for (const view of info.views) {
      // ensure any views ref'ed by system are consistent as well
      await this._nukeTipBatch(view.key, view.length)
    }

    if (boot.heads) this.hintWakeup(boot.heads)
    if (this.local.length) this.hintWakeup([{ key: this.local.key, length: this.local.length }])

    await core.close()
    if (encCore) await encCore.close()
  }

  _bumpWakeupPeer(peer) {
    if (this._coupler) this._coupler.update(peer.stream)
  }

  async _rotateLocalWriter(newLocal) {
    assert(!this.appending, 'Cannot rotate a newLocal writer if an append is in progress')

    const oldLocal = this.local

    if (this._applyState) await this._applyState.close()

    this.local = newLocal
    this.local.on('append', this._onlocalwriterchangeBound)

    this.localWriter = null
    this._updateLocalCore = null

    const store = this._viewStore.atomize()

    const local = store.getLocal()
    await local.ready()

    await LocalState.moveTo(oldLocal, local)

    await local.setUserData('referrer', this.key)
    if (this.encryptionKey) await local.setUserData('autobase/encryption', this.encryptionKey)

    await store.flush()
    await store.close()

    await this._primaryBootstrap.setUserData('autobase/local', local.key)

    await this._clearWriters()
    await oldLocal.close()

    // done, soft reboot

    await this._viewStore.updateLocal()

    await this._clearWriters()

    this._applyState = new ApplyState(this)
    await this._applyState.ready()
    await this._makeLinearizerFromViewState()

    this._caughtup = false

    this._rebooted()

    this.emit('rotate-local-writer')
  }

  async setLocal(key, { keyPair } = {}) {
    if (!this.opened) await this.ready()

    const manifest = keyPair
      ? { version: this.store.manifestVersion, signers: [{ publicKey: keyPair.publicKey }] }
      : null
    if (!key) key = Hypercore.key(manifest)
    // If the keys are the same, no need to rotate
    if (b4a.equals(key, this.local.key)) return

    const encryption = this.encryptionKey ? this.getWriterEncryption() : null

    const local = this.store.get({
      key,
      manifest,
      active: false,
      exclusive: true,
      encryption,
      valueEncoding: messages.OplogMessage
    })
    await local.ready()

    this._updateLocalCore = local

    let runs = 0
    while (!this._interrupting && this.appending && runs++ < 16) await this.update()
    await this._bump()
  }

  setWakeup(cap, discoveryKey) {
    if (this.wakeupSession) this.wakeupSession.destroy()
    if (!discoveryKey && b4a.equals(cap, this.key)) discoveryKey = this.discoveryKey
    this.wakeupSession = this.wakeupProtocol.session(
      cap,
      new WakeupHandler(this, discoveryKey || null)
    )

    // incase this session has active peers already, bump the coupler
    for (const peer of this.wakeupSession.peers) {
      if (peer.active) this._bumpWakeupPeer(peer)
    }
  }

  async _getMigrationPointer(key, length) {
    const core = this.store.get({ key, active: false, encryption: null })
    const encCore = await EncryptionView.setSystemEncryption(this, core)

    const min = core.manifest && core.manifest.prologue ? core.manifest.prologue.length : 0

    for (let i = length - 1; i >= min; i--) {
      if (!(await core.has(i))) continue

      const sys = new SystemView(core, { checkout: i + 1 })
      await sys.ready()

      let good = true

      for (const v of sys.views) {
        const vc = this.store.get({ key: v.key, active: false })
        await vc.ready()
        if (vc.length < v.length) good = false
        await vc.close()
      }

      await sys.close()
      if (!good) continue

      return i + 1
    }

    await core.close()
    if (encCore) await encCore.close()

    return min
  }

  // migrating from 6 -> latest
  async _migrate6(key, length) {
    const core = this.store.get({ key, active: false })
    await core.ready()
    const batch = core.session({ name: 'batch', overwrite: true, checkout: length })
    await batch.ready()
    await batch.close()
    await core.close()
  }

  // called by view-store for bootstrapping
  async _getSystemInfo() {
    const boot = await this._getBootRecord()
    if (!boot.key) return null

    const migrated = !!boot.heads
    const indexedLength = boot.systemLength

    if (migrated) {
      // ensure system batch is consistent on initial migration
      await this._migrate6(boot.key, indexedLength)
    }

    const core = this.store.get({ key: boot.key, encryption: null, active: false })
    await core.ready()

    const batch = core.session({ name: 'batch' })
    const encCore = await EncryptionView.setSystemEncryption(this, batch)

    const info = await SystemView.getIndexedInfo(batch, indexedLength)

    if (encCore) await encCore.close()
    await batch.close()
    await core.close()

    if (info.version > MAX_AUTOBASE_VERSION) {
      throw new Error('Autobase upgrade required.')
    }

    // just compat
    if (migrated) {
      this.migrated = true

      for (const view of info.views) {
        // ensure any views ref'ed by system are consistent as well
        await this._migrate6(view.key, view.length)
      }

      if (boot.heads) this.hintWakeup(boot.heads)
      if (this.local.length) this.hintWakeup([{ key: this.local.key, length: this.local.length }])
    }

    return {
      key: boot.key,
      indexers: info.indexers,
      views: info.views,
      entropy: info.entropy
    }
  }

  // called by the apply state for bootstrapping
  async _getBootRecord() {
    await this._preopen

    const pointer = await this.local.getUserData('autobase/boot')

    const boot = pointer
      ? c.decode(messages.BootRecord, pointer)
      : {
          version: BOOT_RECORD_VERSION,
          key: null,
          systemLength: 0,
          indexersUpdated: false,
          fastForwarding: false,
          recoveries: RECOVERIES,
          heads: null
        }

    if (boot.heads) {
      const len = await this._getMigrationPointer(boot.key, boot.systemLength)
      if (len !== boot.systemLength)
        this._warn(
          new Error(
            'Invalid pointer in migration, correcting (' + len + ' vs ' + boot.systemLength + ')'
          )
        )
      boot.systemLength = len
    }

    return boot
  }

  static async getBootRecord(store, key) {
    const result = await boot(store, key, { exclusive: false })
    await result.bootstrap.close()
    await result.local.close()
    return result.boot
  }

  _interrupt(reason) {
    assert(!!this._applyState.applying, 'Interrupt is only allowed in apply')
    this._interrupting = true
    if (reason) this.interrupted = reason
    throw INTERRUPT
  }

  async flush() {
    if (this._flushing === 0) return
    return this._flushSignal.wait()
  }

  async advance() {
    if (this.opened === false) await this.ready()
    await this._advancing
  }

  recouple() {
    if (this._coupler) this._coupler.destroy()
    const core = this._viewStore.getSystemCore()
    this._coupler = new CoreCoupler(core, this._wakeupPeerBound)
  }

  _updateBootstrapWriters() {
    const writers = this.linearizer.getBootstrapWriters()

    // first clear all, but without applying it for churn reasons
    for (const writer of this._bootstrapWriters) {
      writer.isBootstrap = false
      writer.isCoupled = false
    }

    // all passed are bootstraps
    for (const writer of writers) {
      writer.isCoupled = true
      writer.setBootstrap(true)
    }

    // reset activity on old ones, all should be in sync now
    for (const writer of this._bootstrapWriters) {
      if (writer.isBootstrap === false) writer.setBootstrap(false)
    }

    this._bootstrapWriters = writers
    this._bootstrapWritersChanged = false
  }

  async _openLinearizer() {
    if (this._applyState.system.bootstrapping) {
      await this._makeLinearizer(null)
      this._bootstrapLinearizer()
      return
    }

    await this._makeLinearizerFromViewState()
  }

  async _catchupApplyState() {
    if (await this._applyState.shouldMigrate()) {
      await this._migrate()
    } else if (!(await this._applyState.catchup(this.linearizer))) {
      return false
    }

    this._caughtup = true
    return true
  }

  async _open() {
    this._preopen = this._runPreOpen()
    await this._preopen

    if (this.closing) return

    this._applyState = new ApplyState(this)

    try {
      await this._applyState.ready()
    } catch (err) {
      if (this.closing) return

      try {
        await this._applyState.close()
      } catch {}

      try {
        await this.core.ready()
      } catch {}

      this._applyState = null
      if (this.closing) return

      throw err
    }

    try {
      await this._openLinearizer()
      await this.core.ready()
      await this._wakeup.ready()
    } catch (err) {
      if (this.closing) return
      throw err
    }

    if (this.core.length - this._applyState.indexedLength > this._ackTickThreshold) {
      this._ackTick = this._ackTickThreshold
    }
    if (this.localWriter && this._ackInterval) {
      this._startAckTimer()
    }

    this._updateBootstrapWriters()

    this.recouple()
    this._queueFastForward()

    // queue a full bump that handles wakeup etc (not legal to wait for that here)
    this._queueBump()
  }

  async _close() {
    await this.flush() // flush all pending non network io
    this._interrupting = true
    await Promise.resolve() // defer one tick

    if (this.wakeupSession) this.wakeupSession.destroy()
    if (this.wakeupOwner) this.wakeupProtocol.destroy()

    if (this.fastForwarding) await this.fastForwarding.close()

    if (this._coupler) this._coupler.destroy()
    this._coupler = null
    this._waiting.notify(null)

    await this.activeWriters.clear()

    const closing = this._advancing ? this._advancing.catch(safetyCatch) : null

    await this._clearWriters()
    if (this._primaryBootstrap) await this._primaryBootstrap.close()
    await this.local.close()

    if (this._ackTimer) {
      this._ackTimer.stop()
      await this._ackTimer.flush()
    }

    await this._wakeup.close()

    if (this._hasClose) await this._handlers.close(this.view)
    if (this._applyState) await this._applyState.close()

    await this._viewStore.close()
    await this.core.close()
    await this.store.close()

    if (this._writable) this._writable.resolve(false)

    await closing
  }

  getLastError() {
    return this._lastError
  }

  _onError(err) {
    if (this.closing) return

    this._lastError = err

    if (err === INTERRUPT) {
      this.emit('interrupt', this.interrupted)
      this.emit('update')
      return
    }

    this.close().catch(safetyCatch)

    // if no one is listening we should crash! we cannot rely on the EE here
    // as this is wrapped in a promise so instead of nextTick throw it
    if (ReadyResource.listenerCount(this, 'error') === 0) {
      crashSoon(err)
      return
    }

    this.emit('error', err)
  }

  async _closeWriter(w) {
    this.activeWriters.delete(w)
    await w.close()
  }

  async _gcWriters() {
    // just return early, why not
    if (this._checkWriters.length === 0) return

    while (this._checkWriters.length > 0) {
      const w = this._checkWriters.pop()

      // doesnt hurt
      w.updateActivity()

      if (!w.flushed()) continue

      const unqueued = this._wakeup.unqueue(w.core.key, w.core.length)

      if (!unqueued || w.isActiveIndexer) continue
      if (this.localWriter === w) continue

      await this._closeWriter(w)
    }

    await this._wakeup.flush()
  }

  _startAckTimer() {
    if (this._ackTimer) return
    this._ackTimer = new Timer(this._backgroundAck.bind(this), this._ackInterval)
    this._bumpAckTimer()
  }

  _bumpAckTimer() {
    if (!this._ackTimer) return
    this._ackTimer.bump()
  }

  async update() {
    if (this.opened === false) await this.ready()

    try {
      await this._bump()
      if (this._acking) await this._bump() // if acking just rebump incase it was triggered from above...
    } catch (err) {
      if (this._interrupting) return
      throw err
    }
  }

  // runs in bg, not allowed to throw
  // TODO: refactor so this only moves the writer affected to a updated set
  async _onremotewriterchange() {
    this._bumpAckTimer()

    try {
      await this._bump()
    } catch {}
  }

  _onlocalwriterchange() {
    if (!this.localWriter || this.localWriter.isRemoved) this._queueBump()
  }

  _onwakeup() {
    this._needsWakeup = true
    this._queueBump()
  }

  isFastForwarding() {
    if (this.fastForwardTo !== null) return true
    return this.fastForwardEnabled && this.fastForwarding !== null
  }

  _backgroundAck() {
    return this.ack(true)
  }

  async ack(bg = false) {
    if (this.opened === false) await this.ready()
    if (this.localWriter === null || this._acking || this._interrupting || this._appending !== null)
      return

    if (this._applyState === null) {
      try {
        await this._bump()
      } catch {}
      if (this._applyState === null || this._interrupting) return
    }

    const applyState = this._applyState
    if (applyState.opened === false) await applyState.ready()

    const isPendingIndexer = applyState.isLocalPendingIndexer()

    // if no one is waiting for our index manifest, wait for FF before pushing an ack
    if ((!isPendingIndexer && this.isFastForwarding()) || this._interrupting) return

    const isIndexer = applyState.isLocalIndexer() || isPendingIndexer
    if (!isIndexer) return

    this._acking = true

    try {
      await this._bump()
    } catch (err) {
      if (!this._interrupting) throw err
    }

    if (this._interrupting || !this.localWriter || this.localWriter.closed) {
      this._acking = false
      return
    }

    // avoid lumping acks together due to the bump wait here
    if (this._ackTimer && bg) await this._ackTimer.asapStandalone()
    if (this._interrupting || applyState.closing) {
      this._acking = false
      return
    }

    const alwaysWrite = isPendingIndexer || (await applyState.shouldWrite())
    if (this._interrupting || applyState.closing) {
      this._acking = false
      return
    }

    const hasPendingIndexers = applyState.system.indexers.length > this.linearizer.indexers.length

    if (alwaysWrite || this.linearizer.shouldAck(this.localWriter, hasPendingIndexers)) {
      try {
        if (this.localWriter && !this.localWriter.closed) await this.append(null)
      } catch (err) {
        if (!this._interrupting) throw err
      }
    }

    if (!this._interrupting) {
      this._updateAckThreshold()
      this._bumpAckTimer()
    }

    this._acking = false
  }

  views() {
    if (this._applyState) return this._applyState.store.listViews()
    return this._viewStore.listViews()
  }

  batch() {
    return new AppendBatch(this)
  }

  async append(value, opts) {
    if (this.opened === false) await this.ready()
    if (this._advancing !== null) await this._advancing
    while (this.activeBatch) await this.activeBatch.flushed()
    return this._appendBatch(value, opts)
  }

  async _appendBatch(value, opts) {
    if (this._interrupting) throw new Error('Autobase is closing')
    if (value && this.valueEncoding !== BINARY_ENCODING)
      value = normalize(this.valueEncoding, value)

    const optimistic = !!opts && !!opts.optimistic && !!value

    // we wanna allow acks so interdexers can flush
    if (
      !optimistic &&
      (this.localWriter === null || (this.localWriter.isRemoved && value !== null))
    ) {
      throw new Error('Not writable')
    }

    if (this._appending === null) this._appending = []

    let len = 0
    if (Array.isArray(value)) {
      for (const v of value) len = this._append(v)
    } else {
      len = this._append(value)
    }

    const localLength = this.local.length

    if (optimistic) this._optimistic = this._appending.length - 1
    const target = this._appended + this._appending.length

    // await in case append is in current tick
    if (this._advancing) await this._advancing

    let runs = 0

    // bump until we've flushed the nodes
    while (this._appended < target && !this._interrupting) {
      await this._bump()
      // safety
      if (runs++ >= 16 && this.localWriter && this.localWriter.idle()) break
    }

    if (this._advancing) await this._advancing
    if (this._interrupting) throw new Error('Autobase is closing')

    return localLength + len
  }

  _append(value) {
    // if prev value is an ack that hasnt been flushed, skip it
    if (this._appending.length > 0) {
      if (value === null) return this._appending.length
      if (this._appending[this._appending.length - 1] === null) {
        this._appending.pop()
      }
    }
    return this._appending.push(value)
  }

  static decodeValue(value, opts) {
    return decodeValue(value, opts)
  }

  static encodeValue(value, opts) {
    return encodeValue(value, opts)
  }

  static async getLocalKey(store, opts = {}) {
    const core = opts.keyPair
      ? store.get({ ...opts, active: false })
      : store.get({ ...opts, name: 'local', active: false })
    await core.ready()
    const key = core.key
    await core.close()
    return key
  }

  static getLocalCore(store, handlers, encryptionKey) {
    const encryption = !encryptionKey ? null : { key: encryptionKey }
    const opts = {
      ...handlers,
      compat: false,
      active: false,
      exclusive: true,
      valueEncoding: messages.OplogMessage,
      encryption
    }
    return opts.keyPair ? store.get(opts) : store.get({ ...opts, name: 'local' })
  }

  static async getUserData(core) {
    const view = await core.getUserData('autobase/view')

    return {
      referrer: await core.getUserData('referrer'),
      view: view ? b4a.toString(view) : null
    }
  }

  static async isAutobase(core, opts = {}) {
    const block = await core.get(0, opts)
    if (!block) throw new Error('Core is empty.')
    if (!b4a.isBuffer(block)) return isAutobaseMessage(block)

    try {
      const m = c.decode(messages.OplogMessage, block)
      return isAutobaseMessage(m)
    } catch {
      return false
    }
  }

  // no guarantees where the user data is stored, just that its associated with the base
  async setUserData(key, val) {
    await this._preopen
    const core = this._primaryBootstrap === null ? this.local : this._primaryBootstrap

    await core.setUserData(key, val)
  }

  async getUserData(key) {
    await this._preopen
    const core = this._primaryBootstrap === null ? this.local : this._primaryBootstrap

    return await core.getUserData(key)
  }

  _needsLocalWriter() {
    return this.localWriter === null || this.localWriter.closed
  }

  // no guarantees about writer.isActiveIndexer property here
  async _getWriterByKey(key, len, seen, allowGC, isAdded, system) {
    assert(this._draining === true || (this.opening && !this.opened) || this._optimistic > -1)

    await this._lock.lock()

    if (this._interrupting) {
      this._lock.unlock()
      throw new Error('Autobase is closing')
    }

    try {
      let w = this.activeWriters.get(key)

      const alreadyActive = !!w
      const sys = system || this._applyState.system
      const writerInfo = await sys.get(key)

      if (len === -1) {
        if (!allowGC && writerInfo === null) {
          if (w) w.isRemoved = !isAdded
          return null
        }

        len = writerInfo === null ? 0 : writerInfo.length
      }

      const isActive = writerInfo !== null && (isAdded || !writerInfo.isRemoved)
      const isRemoved = !isActive

      if (w) {
        w.isRemoved = isRemoved
      } else {
        w = this._makeWriter(key, len, isActive, isRemoved)
        if (!w) return null
      }

      if (isRemoved && sys.bootstrapping && b4a.equals(w.core.key, this.key)) {
        w.isRemoved = false
      }

      if (this._isLocalCore(w.core) && this._needsLocalWriter()) {
        this._setLocalWriter(w)
      }

      w.seen(seen)

      if (alreadyActive) return w

      await w.ready()

      if (this._isLocalCore(w.core) && this._needsLocalWriter()) {
        this._setLocalWriter(w)
      }

      if (allowGC && w.flushed()) {
        this._wakeup.unqueue(key, len)
        if (w !== this.localWriter) {
          await w.close()
          return w
        }
      }

      this.activeWriters.add(w)
      this._checkWriters.push(w)

      assert(w.opened)
      assert(!w.closed)

      w.updateActivity()
      return w
    } finally {
      this._lock.unlock()
    }
  }

  _updateAll() {
    const p = []
    for (const w of this.activeWriters) p.push(w.update(null).catch(safetyCatch))
    return Promise.all(p)
  }

  getWriterEncryption() {
    if (!this.encryptionKey) return null
    return this.encryption.getWriterEncryption()
  }

  _makeWriterCore(key) {
    if (this.closing) throw new Error('Autobase is closing')
    if (this._interrupting) throw INTERRUPT()

    const local = b4a.equals(key, this.local.key)
    const encryption = this.getWriterEncryption()

    const core = local
      ? this.local.session({ valueEncoding: messages.OplogMessage, encryption, active: false })
      : this.store.get({
          key,
          compat: false,
          writable: false,
          valueEncoding: messages.OplogMessage,
          encryption,
          active: false
        })

    return core
  }

  _makeWriter(key, length, isActive, isRemoved) {
    const core = this._makeWriterCore(key)
    const w = new Writer(this, core, length, isRemoved)

    if (this._isLocalCore(core)) {
      if (isActive) this._setLocalWriter(w) // only set active writer
      return w
    }

    core.on('append', this._onremotewriterchangeBound)
    core.on('download', this._onremotewriterchangeBound)
    core.on('manifest', this._onremotewriterchangeBound)

    return w
  }

  _updateLinearizer(indexers, heads) {
    // only current active indexers are reset to true below
    for (const w of this.activeWriters) w.isActiveIndexer = false
    if (this.localWriter) this.localWriter.isActiveIndexer = false

    for (const writer of indexers) writer.isActiveIndexer = true

    if (this._isActiveIndexer() && !this.isIndexer) {
      this._setLocalIndexer()
    } else if (!this._isActiveIndexer() && this.isIndexer) {
      this._clearLocalIndexer()
    }

    this.linearizer = new Linearizer(indexers, { heads, writers: this.activeWriters })

    this._updateAckThreshold()
  }

  async _updateLocalWriter(sys) {
    if (this.localWriter !== null && !this.localWriter.closed) return
    await this._getWriterByKey(this.local.key, -1, 0, true, false, sys)
  }

  async _bootstrapLinearizer() {
    const bootstrap = this._makeWriter(this.key, 0, true, false)

    this.activeWriters.add(bootstrap)
    this._checkWriters.push(bootstrap)
    await bootstrap.ready()
    this._ensureWakeup(bootstrap)

    this._updateLinearizer([bootstrap], [])
  }

  async _makeLinearizer(sys) {
    if (sys === null) {
      return this._bootstrapLinearizer()
    }

    if (this.opened || (await sys.hasLocal(this.local.key))) {
      await this._updateLocalWriter(sys)
    }

    const indexers = []

    for (const head of sys.indexers) {
      const writer = await this._getWriterByKey(head.key, head.length, 0, false, false, sys)
      indexers.push(writer)
    }

    if (!this._isActiveIndexer()) {
      for (const key of sys.pendingIndexers) {
        if (b4a.equals(key, this.local.key)) {
          this._setLocalIndexer()
          break
        }
      }
    }

    this._updateLinearizer(indexers, sys.heads)

    for (const { key, length } of sys.heads) {
      await this._getWriterByKey(key, length, 0, false, false, sys)
    }
  }

  async _clearWriters() {
    await this.activeWriters.clear()
    if (this.localWriter !== null) await this.localWriter.close()
    this._checkWriters = []
  }

  async _makeLinearizerFromViewState() {
    const sys = await this._applyState.getIndexedSystem()
    await this._makeLinearizer(sys)
    await sys.close()
  }

  async _runForceFastForward() {
    await this.core.ready()
    if (this.closing) return
    const rec = this._applyState ? await this._applyState.recoverAt() : null
    await this._runFastForward(
      new FastForward(this, this.core.key, { force: true, length: rec ? rec.length : 0 })
    )
  }

  // general repair method for trying to recover
  async repair() {
    await this.forceFastForward()
  }

  async forceFastForward() {
    if (this.isFastForwarding()) return
    await this._runForceFastForward()
    await this.update()
  }

  async _applyFastForward() {
    if (
      !this.fastForwardTo.force &&
      this.fastForwardTo.length < this.core.length + this.fastForwardTo.minimum
    ) {
      this.fastForwardTo = null
      this._updateActivity()
      // still not worth it
      return
    }

    const changes = this._hasUpdate ? new UpdateChanges(this) : null
    if (changes) changes.track(this._applyState)

    // close existing state
    if (this._applyState) await this._applyState.close()

    const { key, length, views, indexers, manifestVersion, entropy } = this.fastForwardTo

    const from = this.core.signedLength
    const ffed = new Set()

    this._flushing++
    try {
      const store = this._viewStore.atomize()

      const migrated = !b4a.equals(key, this.core.key)

      const systemRef = await this._viewStore.findViewByKey(key, indexers, manifestVersion, entropy)
      ffed.add(systemRef)

      if (migrated) {
        await this._applyFastForwardMigration(systemRef, { key, length })
      } else {
        await systemRef.catchup(store.atom, length)
      }

      for (const v of views) {
        const ref = await this._viewStore.findViewByKey(v.key, indexers, manifestVersion, entropy)
        if (!ref) continue // unknown, view ignored
        ffed.add(ref)

        if (migrated) {
          await this._applyFastForwardMigration(ref, v)
        } else {
          await ref.catchup(store.atom, v.length)
        }
      }

      if (ffed.size !== store.getViewCount()) {
        for (const ref of store.getViews()) {
          if (ffed.has(ref)) continue
          await ref.catchup(store.atom, 0) // its gone
        }
      }

      // migrate zero length cores
      if (migrated) {
        const manifests = await this._viewStore.getIndexerManifests(indexers)

        for (const [name, ref] of this._viewStore.byName) {
          if (ffed.has(ref)) continue
          await this._migrateView(manifests, null, name, 0, manifestVersion, entropy, [], null)
        }
      }

      const value = c.encode(messages.BootRecord, {
        version: BOOT_RECORD_VERSION,
        key,
        systemLength: length,
        indexersUpdated: false,
        fastForwarding: true,
        recoveries: RECOVERIES
      })

      const local = store.getLocal()
      await local.ready()
      await local.setUserData('autobase/boot', value)

      await LocalState.clear(local)

      if (changes) changes.finalise()

      await store.flush()
      await store.close()
    } finally {
      if (--this._flushing === 0) this._flushSignal.notify()
    }

    const to = this.core.signedLength

    for (const ref of ffed) await ref.release()

    this.recoveries = RECOVERIES
    this.fastForwardTo = null
    this._queueFastForward()

    await this._clearWriters()

    this._applyState = new ApplyState(this)
    await this._applyState.ready()

    if (changes) await this._handlers.update(this._applyState.view, changes)

    if (await this._applyState.shouldMigrate()) {
      await this._migrate()
    } else {
      await this._makeLinearizerFromViewState()
      await this._applyState.catchup(this.linearizer)
    }

    if (!this.localWriter || this.localWriter.closed) {
      await this._updateLocalWriter(this._applyState.system)
    }

    this._caughtup = true

    this._rebooted()
    this.emit('fast-forward', to, from)
  }

  // TODO: not atomic in regards to the ff, fix that
  async _applyFastForwardMigration(ref, v) {
    const next = this.store.get(v.key)
    await next.ready()

    const prologue = next.manifest && next.manifest.prologue

    if (prologue && prologue.length > 0 && ref.core.length >= prologue.length) {
      try {
        await next.core.copyPrologue(ref.core.state)
      } catch {
        // we might be missing some nodes for this, just ignore, only an optimisation
      }
    }

    const batch = next.session({ name: 'batch', overwrite: true, checkout: v.length })
    await batch.ready()

    // remake the batch, reset from our prologue in case it replicated inbetween
    // TODO: we should really have an HC function for this

    await ref.batch.state.moveTo(batch, batch.length)
    await batch.close()

    ref.migrated(this, next)
  }

  async _migrateView(
    indexerManifests,
    source,
    name,
    indexedLength,
    manifestVersion,
    entropy,
    linked,
    manifestData
  ) {
    const ref = this._viewStore.byName.get(name)

    const prologue =
      indexedLength === 0
        ? null
        : { length: indexedLength, hash: await source.treeHash(indexedLength) }

    if (manifestData === null && ref.core.manifest.userData !== null) {
      manifestData = ref.core.manifest.userData
    }

    const next = this._viewStore.getViewCore(
      indexerManifests,
      name,
      prologue,
      manifestVersion,
      entropy,
      linked,
      manifestData
    )
    await next.ready()

    if (indexedLength > 0) {
      await next.core.copyPrologue(source.state)
    }

    // remake the batch, reset from our prologue in case it replicated inbetween
    // TODO: we should really have an HC function for this
    const batch = next.session({ name: 'batch', overwrite: true, checkout: indexedLength })
    await batch.ready()

    if (source !== null) {
      while (batch.length < source.length) {
        await batch.append(await source.get(batch.length))
      }
    }

    await ref.batch.state.moveTo(batch, batch.length)
    await batch.close()

    ref.migrated(this, next)

    return ref
  }

  async _premigrate() {
    this._flushing++
    try {
      const length = this._applyState.indexedLength
      const system = this._applyState.system

      const info = await system.getIndexedInfo(length)
      const indexerManifests = await this._viewStore.getIndexerManifests(info.indexers)

      const { views, systemView, encryptionView } = this._applyState

      const manifestVersion = this._applyState.system.core.manifest.version

      for (const view of views) {
        const v = this._applyState.getViewFromSystem(view, info)
        const indexedLength = v ? v.length : 0

        const ref = this._viewStore.getViewByName(view.name)
        const source = ref.getCore()
        await source.ready()

        await this._migrateView(
          indexerManifests,
          source,
          view.name,
          indexedLength,
          manifestVersion,
          info.entropy,
          null,
          null
        )
      }

      const source = encryptionView.ref.getCore()
      await source.ready()

      const enc = await this._migrateView(
        indexerManifests,
        source,
        '_encryption',
        info.encryptionLength,
        manifestVersion,
        info.entropy,
        null,
        null
      )

      const linked = [enc.core.key]

      const sysCore = systemView.ref.getCore()
      const ref = await this._migrateView(
        indexerManifests,
        sysCore,
        '_system',
        length,
        manifestVersion,
        info.entropy,
        linked,
        null,
        null
      )

      return ref.core.key
    } finally {
      if (--this._flushing === 0) this._flushSignal.notify()
    }
  }

  async _migrate() {
    return this._reboot(await this._premigrate())
  }

  async _reboot(key) {
    await this._clearWriters()
    await this._makeLinearizerFromViewState()

    await this._applyState.finalize(key)

    this._applyState = new ApplyState(this)

    await this._applyState.ready()
    await this._applyState.catchup(this.linearizer)

    // end soft shutdown

    this._queueFastForward()

    this._rebooted()
  }

  _rebooted() {
    this.recouple()
    this._updateActivity()

    this.emit('reboot')

    // ensure we re-evalute our state
    this._bootstrapWritersChanged = true
    this._needsWakeup = true

    this.updating = true
    this._queueBump()
  }

  _setLocalWriter(w) {
    this.localWriter = w
    if (this._ackInterval) this._startAckTimer()
  }

  _unsetLocalWriter() {
    if (!this.localWriter) return

    this._closeWriter(this.localWriter)
    if (this.localWriter.isActiveIndexer) this._clearLocalIndexer()

    this.localWriter = null
  }

  _setLocalIndexer() {
    assert(this.localWriter !== null)

    this.isIndexer = true
    this.emit('is-indexer')
  }

  _clearLocalIndexer() {
    assert(this.localWriter !== null)

    if (this._ackTimer) this._ackTimer.stop()

    this.isIndexer = false
    this._ackTimer = null

    this.emit('is-non-indexer')
  }

  _isLocalCore(core) {
    return core.writable && core.id === this.local.id
  }

  _addLocalHeads() {
    // not writable atm, will prop be writable in a subsequent bump tho
    if (!this.localWriter || this.localWriter.closed) return null
    // safety, localwriter is still processing, should prop be an assertion
    if (!this.localWriter.idle()) return null

    const length = this._optimistic === -1 ? this._appending.length : this._optimistic || 1

    const nodes = new Array(length)
    for (let i = 0; i < length; i++) {
      const heads = this.linearizer.getHeads()
      const deps = new Set(this.linearizer.heads)
      const batch = length - i
      const value = this._appending[i]

      const node = this.localWriter.append(value, heads, batch, deps, this._optimistic === 0)

      this.linearizer.addHead(node)
      nodes[i] = node
    }

    return nodes
  }

  _flushLocalHeads(length) {
    this._appending = length === this._appending.length ? null : this._appending.slice(length)
    this._appended += length

    if (this._optimistic > -1 && this._optimistic < length) this._optimistic = -1
  }

  async _addRemoteHeads() {
    let added = 0

    while (
      added < REMOTE_ADD_BATCH ||
      (this.bigBatches && !this._indexersIdle() && added < REMOTE_ADD_BATCH_SANITY)
    ) {
      await this._updateAll()

      let advanced = 0

      for (const w of this.activeWriters) {
        let node = w.advance()
        if (node === null) continue

        advanced += node.batch

        while (true) {
          this.linearizer.addHead(node)
          if (node.batch === 1) break
          node = w.advance()
        }
      }

      if (advanced === 0) {
        if (this.bigBatches && !this._indexersIdle() && (await this._waitForIndexers())) continue
        break
      }

      added += advanced
    }

    return added
  }

  async _waitForIndexers() {
    const promises = []

    for (const w of this.linearizer.indexers) {
      if (w.idle()) continue
      if (await w.core.has(w.length)) continue
      promises.push(w.core.get(w.length, { timeout: 3000 }))
    }

    for (const result of await Promise.allSettled(promises)) {
      if (result.status !== 'rejected') return true
    }

    return false
  }

  _indexersIdle() {
    for (const w of this.linearizer.indexers) {
      if (!w.idle()) return false
    }
    return true
  }

  async _drain() {
    const writable = this.writable

    while (!this._interrupting && !this.paused) {
      if (this.fastForwardTo !== null) {
        await this._applyFastForward()
        continue // revaluate conditions...
      }

      // we defer this to post ready so its not blocking reading the views
      if (this._caughtup === false) {
        // if the catchup fails, just force ff it to recover, less user pain
        if (!(await this._catchupApplyState())) {
          await this._runForceFastForward()
        }
        continue
      }

      const remoteAdded = await this._addRemoteHeads()
      const localNodes = this._appending !== null ? this._addLocalHeads() : null

      if (this._interrupting) return

      if (remoteAdded > 0 || localNodes !== null) {
        this.updating = true
      }

      const u = this.linearizer.update()
      const update = u
        ? await this._applyState.update(u, localNodes)
        : { reboot: false, migrated: false }

      if (localNodes) this._flushLocalHeads(localNodes.length)

      if (!update.reboot) {
        if (this._applyState.shouldFlush()) {
          await this._applyState.flush()
          this.updating = true
        }

        if (this._checkWriters.length > 0) {
          await this._gcWriters()
          continue // rerun the update loop as a writer might have been added
        }
        if (remoteAdded >= REMOTE_ADD_BATCH) continue
        break
      }

      await this._gcWriters()
      await this._reboot(this._applyState.key)
    }

    // emit state changes post drain
    if (writable !== this.writable) {
      if (this.writable && this._writable) this._writable.resolve(true)
      this.emit(writable ? 'unwritable' : 'writable')
    }
  }

  _wakeupPeer(stream) {
    if (!this.wakeupSession) return
    const wakeup = this._getWakeup()
    if (wakeup.length === 0) return
    this.wakeupSession.announceByStream(stream, wakeup)
  }

  _getWakeup() {
    const writers = []

    for (const w of this.activeWriters) {
      if (w.isActiveIndexer || w.flushed()) continue
      writers.push({ key: w.core.key, length: w.length })
    }

    return writers
  }

  async _wakeupWriter(key, length) {
    this._ensureWakeup(await this._getWriterByKey(key, -1, length, true, false, null))
  }

  // ensure wakeup on an existing writer (the writer calls this in addition to above)
  _ensureWakeup(w) {
    if (w === null || w.isBootstrap === true) return
    w.setBootstrap(true) // even if turn false at end of drain, hypercore makes them linger a bit so no churn
    this._bootstrapWriters.push(w)
    this._bootstrapWritersChanged = true
  }

  async _drainWakeup() {
    const promises = []

    // warmup all the below gets
    if (this._needsWakeup) {
      for (const { key, length } of this._wakeup) {
        const w = this.activeWriters.get(key)
        if (w) {
          if (w.length < length) w.seen(length)
          continue
        }
        promises.push(this._applyState.system.get(key))
      }

      if (this._needsWakeupHeads) {
        for (const { key } of await this._applyState.system.heads) {
          if (this.activeWriters.has(key)) continue
          promises.push(this._applyState.system.get(key))
        }
      }
    }
    for (const [hex, length] of this._wakeupHints) {
      const key = b4a.from(hex, 'hex')
      if (length !== -1) {
        const w = this.activeWriters.get(key)
        if (w) {
          if (w.length < length) w.seen(length)
          continue
        }
      }
      promises.push(this._applyState.system.get(key))
    }

    await Promise.allSettled(promises)

    if (this._needsWakeup === true) {
      this._needsWakeup = false

      for (const { key, length } of this._wakeup) {
        if (this.activeWriters.has(key)) continue
        await this._wakeupWriter(key, length)
      }

      if (this._needsWakeupHeads === true) {
        this._needsWakeupHeads = false

        for (const { key } of await this._applyState.system.heads) {
          if (this.activeWriters.has(key)) continue
          await this._wakeupWriter(key, 0)
        }
      }
    }

    for (const [hex, length] of this._wakeupHints) {
      const key = b4a.from(hex, 'hex')
      if (this.activeWriters.has(key)) continue
      if (length !== -1) {
        const info = await this._applyState.system.get(key)
        if (info && length <= info.length) continue // stale hint
      }
      await this._wakeupWriter(key, length === -1 ? 0 : length)
    }

    this._wakeupHints.clear()
  }

  pause() {
    this.paused = true
  }

  resume() {
    this.paused = false
    this._queueBump()
  }

  waitForWritable() {
    if (this.writable) return Promise.resolve(true)
    if (!this._writable) this._writable = rrp()
    return this._writable.promise
  }

  async _drainWithInterupt() {
    while (true) {
      try {
        await this._drain()
        return
      } catch (err) {
        if (this.closing || !this.fastForwardTo) throw err
        if (Date.now() - this.fastForwardFailedAt < MIN_FF_WAIT) throw err
        this.fastForwardFailedAt = Date.now()
        // if an FF is enabled retry
      }
    }
  }

  async _advance() {
    if (this.opened === false) await this.ready()
    if (this.paused || this._interrupting) return

    this._draining = true

    if (this._updateLocalCore !== null) {
      await this._rotateLocalWriter(this._updateLocalCore)
    }

    const local = this.local.length

    try {
      await this._drainWithInterupt()

      // must run post drain so the linearizer is caught up
      if (this._caughtup && (this._needsWakeup === true || this._wakeupHints.size > 0))
        await this._drainWakeup()

      // check if we should update local writer
      if (!this.localWriter || this.localWriter.closed) {
        await this._updateLocalWriter(this._applyState.system)
      }

      this._draining = false
    } catch (err) {
      this._onError(err)
      return
    }

    if (this._interrupting) return

    if (this.localWriter && !this.localWriter.closed) {
      if (this._applyState.isLocalPendingIndexer()) this.ack().catch(noop)
      else if (this._triggerAckAsap()) this._ackTimer.asap()
    }

    // keep bootstraps in sync with linearizer
    if (this.updating === true || this._bootstrapWritersChanged === true) {
      this._updateBootstrapWriters()
    }

    if (this.updating === true) {
      this.updating = false

      if (local !== this.local.length) this._resetAckTick()
      else this._ackTick++

      if (!this._interrupting) this.emit('update')
      this._waiting.notify(null)
    }

    if (!this._interrupting) await this._gcWriters()
  }

  _triggerAckAsap() {
    if (!this._ackTimer) return false

    // flush if threshold is reached and we are not already acking
    if (this._ackTickThreshold && !this._acking && this._ackTick >= this._ackTickThreshold) {
      if (this._ackTimer) {
        for (const w of this.linearizer.indexers) {
          if (w.core.length > w.length) return false // wait for the normal ack cycle in this case
        }
      }

      return true
    }

    return false
  }

  _queueFastForward() {
    if (!this.core.opened) return
    // should have a better way to get this
    const latestSignedLength = this.core.core.state.length

    if (!this.fastForwardEnabled || this.fastForwarding !== null || this._interrupting) return

    if (latestSignedLength - this.core.length < this.fastForwardMinimum) return
    if (this.fastForwardTo !== null) return
    if (Date.now() - this.fastForwardFailedAt < MIN_FF_WAIT) return

    this._runFastForward(
      new FastForward(this, this.core.key, { minimum: this.fastForwardMinimum })
    ).catch(noop)
  }

  _queueStaticFastForward(key) {
    if (!this.fastForwardEnabled || this.fastForwarding !== null || this._interrupting) return
    if (this.fastForwardTo !== null) return
    if (Date.now() - this.fastForwardFailedAt < MIN_FF_WAIT) return

    this._runFastForward(new FastForward(this, key, { verified: false })).catch(noop)
  }

  _updateActivity() {
    this.activeWriters.updateActivity()
    if (this._applyState) {
      if (this.isFastForwarding()) this._applyState.pause()
      else this._applyState.resume()
    }

    // in case we ignored wakeups case we were fast forwarding, request them now if not
    if (this._needsWakeupRequest && !this.isFastForwarding() && this.wakeupSession) {
      this._needsWakeupRequest = false

      this.wakeupSession.broadcastLookup({})
    }
  }

  _preferFastForward() {
    this.fastForwardMinimum = 1
    this._queueFastForward()
  }

  _postApply() {
    this.fastForwardMinimum = FastForward.MINIMUM
  }

  async _runFastForward(ff) {
    this.fastForwarding = ff

    this._updateActivity()

    const result = await ff.upgrade()
    await ff.close()

    if (this.fastForwarding === ff) this.fastForwarding = null

    if (!result) {
      if (ff.failed) this.fastForwardFailedAt = Date.now()
      else this._queueFastForward()
      this._updateActivity()
      return
    }

    this.fastForwardFailedAt = 0
    this.fastForwardTo = result

    if (this._applyState && this._applyState.applying && this.fastForwardMinimum === 1) {
      await this._applyState.close()
    }

    this._bumpAckTimer()
    this._queueBump()
  }

  // triggered from apply
  async _addWriter(key, sys) {
    // just compat for old version
    assert(!!this._applyState.applying, 'System changes are only allowed in apply')

    const writer =
      (await this._getWriterByKey(key, -1, 0, false, true, sys)) ||
      this._makeWriter(key, 0, true, false)
    await writer.ready()

    if (!this.activeWriters.has(key)) {
      this.activeWriters.add(writer)
      this._checkWriters.push(writer)
      this._ensureWakeup(writer)
    }

    // fetch any nodes needed for dependents
    this._queueBump()
  }

  // triggered from apply
  _removeWriter(key) {
    // just compat for old version
    const w = this.activeWriters.get(key)
    if (w) w.isRemoved = true

    this._queueBump()
  }

  removeable(key) {
    return this._applyState ? this._applyState.removeable(key) : false
  }

  _updateAckThreshold() {
    if (this._ackThreshold === 0) return
    if (this._ackTimer) this._ackTimer.bau()
    this._ackTick = 0
    this._ackTickThreshold = random2over1(this.linearizer.indexers.length * this._ackThreshold)
  }

  _resetAckTick() {
    this._ackTick = 0
    if (this._ackTimer) this._ackTimer.bau()
  }

  _shiftWriter(w) {
    w.shift()
    if (w.flushed()) this._checkWriters.push(w)
  }
}

function toKey(k) {
  return b4a.isBuffer(k) ? k : hypercoreId.decode(k)
}

function isAutobaseMessage(msg) {
  return msg.checkpoint ? msg.checkpoint.length > 0 : msg.checkpoint === null
}

function compareNodes(a, b) {
  return b4a.compare(a.key, b.key)
}

function random2over1(n) {
  return Math.floor(n + Math.random() * n)
}

function noop() {}

function crashSoon(err) {
  queueMicrotask(() => {
    throw err
  })
  throw err
}

function isObject(obj) {
  return typeof obj === 'object' && obj !== null
}

function emitWarning(err) {
  safetyCatch(err)
  this.emit('warning', err)
}

function normalize(valueEncoding, value) {
  const state = { buffer: null, start: 0, end: 0 }
  valueEncoding.preencode(state, value)
  state.buffer = b4a.allocUnsafe(state.end)
  valueEncoding.encode(state, value)
  state.start = 0
  return valueEncoding.decode(state)
}
const b4a = require('b4a')

module.exports = class ActiveWriters {
  constructor() {
    this.map = new Map()
  }

  get size() {
    return this.map.size
  }

  [Symbol.iterator]() {
    return this.map.values()
  }

  get(key) {
    return this.map.get(b4a.toString(key, 'hex')) || null
  }

  has(key) {
    return this.get(key) !== null
  }

  add(writer) {
    this.map.set(b4a.toString(writer.core.key, 'hex'), writer)
  }

  delete(writer) {
    this.map.delete(b4a.toString(writer.core.key, 'hex'))
  }

  updateActivity() {
    for (const w of this.map.values()) w.updateActivity()
  }

  clear() {
    const p = []
    for (const w of this.map.values()) p.push(w.close())
    this.map.clear()

    return Promise.all(p)
  }
}
const SignalPromise = require('signal-promise')

module.exports = class AppendBatch {
  constructor(base) {
    this.base = base
    this.blocks = []
    this.closed = false
    this.flushing = false
    this._flushed = null
  }

  async _acquire() {
    while (this.base.activeBatch !== null && this.base.activeBatch !== this)
      await this.base.activeBatch.flushed()
    this.base.activeBatch = this
  }

  async append(value) {
    if (this.base.opened === false) await this.base.ready()
    if (this.base._advancing !== null) await this.base._advancing

    if (this.closed) throw new Error('Batch is closed')
    if (this.base.activeBatch !== this) await this._acquire()
    if (this.closed) throw new Error('Batch is closed')

    this.blocks.push(value)
    return this.base.local.length + this.blocks.length
  }

  async flush() {
    if (this.closed) throw new Error('Batch is closed')
    if (this.flushing) return this.flushed()
    this.flushing = true
    if (this.blocks.length) await this.base._appendBatch(this.blocks)
    return this.close()
  }

  flushed() {
    if (this.closed) return Promise.resolve()
    if (this._flushed) return this._flushed.wait()
    this._flushed = new SignalPromise()
    return this._flushed.wait()
  }

  close() {
    if (this.base.activeBatch !== this) return
    this.base.activeBatch = null
    this.closed = true
    if (this._flushed) this._flushed.notify(null)
  }
}
class PublicApplyCalls {
  constructor(base) {
    this.base = base
    this.internal = false
  }

  get discoveryKey() {
    return this.base.discoveryKey
  }

  get key() {
    return this.base.key
  }

  get id() {
    return this.base.id
  }

  async addWriter() {
    throw new Error('Not allowed on the public view')
  }

  async ackWriter() {
    throw new Error('Not allowed on the public view')
  }

  async removeWriter() {
    throw new Error('Not allowed on the public view')
  }

  preferFastForward() {
    throw new Error('Not allowed on the public view')
  }

  interrupt() {
    throw new Error('Not allowed on the public view')
  }

  removeable() {
    throw new Error('Not allowed on the public view')
  }

  fork() {
    throw new Error('Not allowed on the public view')
  }

  anchor() {
    throw new Error('Not allowed on the public view')
  }
}

class PrivateApplyCalls extends PublicApplyCalls {
  constructor(state) {
    super(state.base)
    this.state = state
    this.internal = true
  }

  get system() {
    return this.state.system
  }

  async addWriter(key, { indexer = true, isIndexer = indexer } = {}) {
    // just compat for old version
    await this.state.system.add(key, { isIndexer })
    await this.base._addWriter(key, this.state.system)
  }

  async ackWriter(key) {
    await this.state.system.ack(key)
  }

  async removeWriter(key) {
    // just compat for old version
    if (!this.state.removeable(key)) {
      throw new Error('Not allowed to remove the last indexer')
    }

    await this.state.system.remove(key)
    this.base._removeWriter(key)
  }

  preferFastForward() {
    this.base._preferFastForward()
  }

  interrupt(reason) {
    this.base._interrupt(reason)
  }

  removeable(key) {
    return this.state.removeable(key)
  }

  async fork(indexerKeys, system) {
    if (!(await this.state.validateFork(indexerKeys, system))) return false

    const indexers = indexerKeys.map(toIndexer)

    this.state.pendingFork = { indexers, length: system.length }

    return true
  }

  async createAnchor() {
    return await this.state.createAnchor()
  }
}

module.exports = { PublicApplyCalls, PrivateApplyCalls }

function toIndexer(key) {
  return { key, length: 0 } // length is not used, just for api compat
}
const ReadyResource = require('ready-resource')
const assert = require('nanoassert')
const c = require('compact-encoding')
const crypto = require('hypercore-crypto')
const b4a = require('b4a')
const safetyCatch = require('safety-catch')

const { partialSignature } = require('hypercore/lib/multisig.js')

const SystemView = require('./system.js')
const { AutobaseEncryption } = require('./encryption.js')
const UpdateChanges = require('./updates.js')
const messages = require('./messages.js')
const { PrivateApplyCalls } = require('./apply-calls.js')
const { encodeValue } = require('./values.js')
const Fork = require('./fork.js')
const LocalState = require('./local-state.js')
const { OPLOG_VERSION, BOOT_RECORD_VERSION } = require('./caps.js')

// todo: expose this as an option
const SHOULD_SIGN_THRESHOLD = 0

class CheckpointCore {
  constructor(view, core, signer, paused) {
    this.view = view
    this.core = core
    this.signer = signer
    this.length = 0
    this.digest = null
    this.signatures = { system: null, encryption: null, user: [] }
    this.closed = false
    this.paused = paused
  }

  pause() {
    this.paused = true
  }

  resume() {
    if (!this.paused) return
    this.paused = false
    this.updateBackground()
  }

  async update() {
    if (await this.updateCheckpoints()) await this.view.maybeSigned()
  }

  async updateCheckpoints() {
    if (this.core.length <= this.length || this.closed || this.paused) return false

    const length = this.core.length
    const value = await this.core.get(length - 1)

    if (length <= this.length || this.closed || !value.digest || this.paused) return false

    const [digest, checkpoints] = await Promise.all([
      this._inflateDigest(length, value.digest),
      this._inflateAllCheckpoints(length, value.checkpoint)
    ])

    if (length <= this.length || this.closed || !digest || !checkpoints || this.paused) return false
    for (let i = 0; i < checkpoints.length; i++) {
      if (checkpoints[i] === null) return false
    }

    this.length = length
    this.digest = digest
    this.signatures = checkpoints

    return true
  }

  updateBackground() {
    return this.update().catch(safetyCatch)
  }

  updateInternalSignatures(length, signatures) {
    this.signatures.system = updateSignature(length, signatures.system, this.signatures.system)
    this.signatures.encryption = updateSignature(
      length,
      signatures.encryption,
      this.signatures.encryption
    )
  }

  updateUserSignatures(length, signatures) {
    this.signatures.user = updateSignatures(length, signatures, this.signatures.user)
  }

  makeCheckpoints(length) {
    return {
      system: makeCheckpoint(length, this.signatures.system),
      encryption: makeCheckpoint(length, this.signatures.encryption),
      user: makeCheckpoints(length, this.signatures.user)
    }
  }

  makeDigest(length, key) {
    if (this.digest && (this.digest.key === key || b4a.equals(this.digest.key, key))) {
      return { key: null, pointer: length - this.digest.at }
    }
    this.digest = { key, at: length }
    return { key, pointer: 0 }
  }

  async ready() {
    await this.core.ready()
    this.core.on('append', this.updateBackground.bind(this))
    if (this.core.writable) await this.updateCheckpoints()
    this.updateBackground()
  }

  signedLength() {
    return this.signatures.system ? this.signatures.system.length : 0
  }

  close() {
    this.closed = true
    return this.core.close()
  }

  async _inflateDigest(length, dig) {
    if (!dig) return null

    if (dig.pointer === 0) {
      return { key: dig.key, at: length }
    }

    const len = length - dig.pointer

    if (this.digest && len === this.digest.at) return this.digest
    if (len <= 0) return null

    const { digest } = await this.core.get(len - 1)
    if (!digest || !digest.key) return null

    return { key: digest.key, at: len }
  }

  _sameSignatures(b, a) {
    return (
      sameSignature(a.system, b.system) &&
      sameSignature(a.encryption, b.encryption) &&
      sameSignatures(a.user, b.user)
    )
  }

  async _inflateAllCheckpoints(length, checkpoints) {
    if (!checkpoints) return { system: null, encryption: null, user: [] }
    if (this._sameSignatures(checkpoints, this.signatures)) return this.signatures

    const system = this._inflateSystemCheckpoint(checkpoints.system, length)
    const encryption = this._inflateEncryptionCheckpoint(checkpoints.encryption, length)

    const user = checkpoints.user ? new Array(checkpoints.user.length) : []

    for (let i = 0; i < user.length; i++) {
      const chk = checkpoints.user[i]
      user[i] = this._inflateUserCheckpoint(chk, i, length)
    }

    const [systemCheckpoint, encryptionCheckpoint, userCheckpoints] = await Promise.all([
      system,
      encryption,
      Promise.all(user)
    ])

    return {
      system: systemCheckpoint,
      encryption: encryptionCheckpoint,
      user: userCheckpoints
    }
  }

  async _inflateSystemCheckpoint(chk, coreLength) {
    if (chk.checkpoint) {
      const { signature, length } = chk.checkpoint
      return { signature, length, at: coreLength }
    }

    const len = coreLength - chk.checkpointer
    if (this.signatures.system && this.signatures.system.at === len) {
      return this.signatures.system
    }

    if (len <= 0) return null

    const { checkpoint } = await this.core.get(len - 1)
    if (!checkpoint || !checkpoint.system || !checkpoint.system.checkpoint) return null

    const { signature, length } = checkpoint.system.checkpoint
    return { signature, length, at: len }
  }

  async _inflateEncryptionCheckpoint(chk, coreLength) {
    if (!chk) return { length: 0, signature: null, at: 0 }

    if (chk.checkpoint) {
      const { signature, length } = chk.checkpoint
      return { signature, length, at: coreLength }
    }

    const len = coreLength - chk.checkpointer
    if (this.signatures.encryption && this.signatures.encryption.at === len) {
      return this.signatures.encryption
    }

    if (len <= 0) return null

    const { checkpoint } = await this.core.get(len - 1)
    if (!checkpoint || !checkpoint.encryption || !checkpoint.encryption.checkpoint) return null

    const { signature, length } = checkpoint.encryption.checkpoint
    return { signature, length, at: len }
  }

  async _inflateUserCheckpoint(chk, i, coreLength) {
    if (chk.checkpoint) {
      const { signature, length } = chk.checkpoint
      return { signature, length, at: coreLength }
    }

    const len = coreLength - chk.checkpointer
    if (i < this.signatures.user.length && this.signatures.user[i].at === len) {
      return this.signatures.user[i]
    }

    if (len <= 0) return null

    const { checkpoint } = await this.core.get(len - 1)
    if (!checkpoint || i >= checkpoint.user.length || !checkpoint.user[i].checkpoint) return null

    const { signature, length } = checkpoint.user[i].checkpoint
    return { signature, length, at: len }
  }
}

module.exports = class ApplyState extends ReadyResource {
  constructor(base) {
    super()

    this.base = base
    this.encryption = this.base.encryption
    this.valueEncoding = base.valueEncoding
    this.store = base._viewStore.atomize()
    this.key = null
    this.system = null
    this.view = null
    this.views = []
    this.systemView = null
    this.encryptionView = null
    this.hostcalls = null
    this.changes = base._hasUpdate ? new UpdateChanges(base) : null

    this.updates = []

    this.local = null
    this.localState = null

    this.fastForwarding = false
    this.indexersUpdated = false
    this.quorum = 0
    this.needsIndexedLengthUpdate = false
    this.interrupted = false
    this.applying = null
    this.applyBatch = null

    this.localCheckpoint = null
    this.localIndexer = false

    this.systemUpgrade = null

    this.checkpoints = []
    this.pendingViews = null
    this.pendingFork = null
    this.dirty = false
  }

  shouldFlush() {
    return this.dirty
  }

  async shouldWrite() {
    if (!this.localIndexer || !this.localCheckpoint || !this.localCheckpoint.core.opened) {
      return false
    }

    const info = await this.system.getIndexedInfo(this.indexedLength)
    if (!info.views.length) return false

    const { encryption, user } = this.localCheckpoint.signatures

    // todo: does this condition hold for soft-fork?
    if (info.views.length > user.length) return true

    for (let i = 0; i < info.views.length; i++) {
      if (user[i].length + SHOULD_SIGN_THRESHOLD < info.views[i].length) return true
    }

    // always flush encryption signature
    if (info.encryptionLength !== 0) {
      if (!encryption || encryption.length < info.encryptionLength) return true
    }

    return false
  }

  get indexedLength() {
    return this.systemView ? this.systemView.indexedLength : 0
  }

  get systemRef() {
    return this.systemView ? this.systemView.ref : null
  }

  async validateFork(indexerKeys, system) {
    if (!b4a.equals(system.key, this.system.core.key)) return false
    if (this.pendingIndexedLength() < system.length) return false
    for (const key of indexerKeys) {
      const info = await this.system.get(key)

      // writer should be active and we need manifest
      if (!info || !info.length || info.isRemoved) return false
    }
    return true
  }

  async shouldMigrate() {
    if (!this.fastForwarding && !this.indexersUpdated) return false
    if (this.system.indexers.length === 0) return false // genesis

    // sanity, prefer migration
    if (!this.system.core.manifest || !this.system.core.manifest.signers) return true

    // easy mode
    if (this.system.indexers.length !== this.system.core.manifest.signers.length) {
      return true
    }

    const manifests = await this.store.getIndexerManifests(this.system.indexers)

    for (let i = 0; i < this.system.core.manifest.signers.length; i++) {
      const { publicKey } = this.system.core.manifest.signers[i]
      if (!b4a.equals(publicKey, manifests[i].signers[0].publicKey)) return true
    }

    return false
  }

  isLocalPendingIndexer() {
    if (this.system.pendingIndexers.length === 0) return false
    const key = this.base.local.key
    for (const k of this.system.pendingIndexers) {
      if (b4a.equals(k, key)) return !b4a.equals(this.base.key, key) && this.base.local.length === 0
    }
    return false
  }

  removeable(key) {
    if (this.system.indexers.length !== 1) return true
    return !b4a.equals(this.system.indexers[0].key, key)
  }

  isLocalIndexer() {
    return !!this.localCheckpoint
  }

  _createCheckpointCore(key, active, signer, paused) {
    const encryption = this.base.getWriterEncryption(key)

    const session = this.base.store.get({
      key,
      valueEncoding: messages.OplogMessage,
      encryption,
      active
    })

    return new CheckpointCore(this, session, signer, paused)
  }

  async _openInternalView(name, indexedLength) {
    const core = this.store.get({ name })
    await core.ready()
    await core.setUserData('referrer', this.base.key)
    await core.setUserData('autobase/view', b4a.from(name))

    const ref = this.store.getViewByName(name)

    return { ref, core, indexedLength }
  }

  async _open() {
    try {
      await this._boot()
    } catch (err) {
      await this._free()
      throw err
    }
  }

  async _boot() {
    const boot = await this.base._getBootRecord()

    this.systemView = await this._openInternalView('_system', boot.systemLength)
    this.encryptionView = await this._openInternalView('_encryption', -1)

    const sysCore = this.systemView.core
    if (sysCore.manifest.version >= 2) {
      if (this.encryption !== null) await this.encryption.reload(this.encryptionView.core)
    }

    const system = new SystemView(sysCore)
    await system.ready()

    // reset so we dont track _system or _encryption
    this.store.opened = []

    this.hostcalls = new PrivateApplyCalls(this)
    const view = this.base._hasOpen ? this.base._handlers.open(this.store, this.hostcalls) : null

    this.view = view
    this.system = system

    // ensure all are ready
    for (const v of this.store.opened) await v.atomicBatch.ready()

    this.key = system.core.key

    this.fastForwarding = boot.fastForwarding
    this.indexersUpdated = boot.indexersUpdated
    this.quorum = sysCore.manifest ? sysCore.manifest.quorum : 0

    const added = new Set()

    for (let i = 0; i < system.views.length; i++) {
      const { key, length } = system.views[i]
      const v = await this.store.findViewByKey(
        key,
        system.indexers,
        sysCore.manifest.version,
        system.entropy
      )

      if (v === null) {
        this.views.push({ name: null, key, length, core: null, ref: null, mappedIndex: i })
        continue
      }

      await v.atomicBatch.ready()
      await v.core.setUserData('referrer', this.base.key)
      await v.core.setUserData('autobase/view', b4a.from(v.name))

      this.views.push({ name: v.name, key, length, core: v.atomicBatch, ref: v, mappedIndex: i })

      added.add(v)
    }

    for (const v of this.store.opened) {
      if (added.has(v)) continue
      const core = v.atomicBatch

      this.views.push({
        name: v.name,
        key: core.key,
        length: core.length,
        core,
        ref: v,
        mappedIndex: -1
      })
    }

    this.local = this.store.getLocal()
    await this.local.ready()

    this.localState = new LocalState(this.local)

    let isLocalIndexer = false

    const paused = this.base.isFastForwarding()
    const inf = await this.system.getIndexedInfo()

    for (let i = 0; i < inf.indexers.length; i++) {
      const idx = inf.indexers[i]
      const chk = this._createCheckpointCore(idx.key, true, i, paused)
      await chk.ready()
      this.checkpoints.push(chk)
      if (b4a.equals(idx.key, this.base.local.key)) isLocalIndexer = true
    }

    const shouldSign = this.quorum > 0 && isLocalIndexer

    if (shouldSign) {
      this.localIndexer = true
      await this._startLocalCheckpoint()
    }

    await this._refreshWriters()

    // deferred
    this.maybeSigned().catch(noop)
  }

  async _startLocalCheckpoint() {
    for (const chk of this.checkpoints) {
      if (chk.core.id === this.base.local.id) {
        this.localCheckpoint = chk
        return
      }
    }

    this.localCheckpoint = this._createCheckpointCore(this.base.local.key, false, 0, false)
    await this.localCheckpoint.ready()
  }

  interrupt() {
    this.interrupted = true
    this.pause()
  }

  pause() {
    for (const chk of this.checkpoints) {
      if (chk !== this.localCheckpoint) chk.pause()
    }
  }

  resume() {
    for (const chk of this.checkpoints) {
      if (chk !== this.localCheckpoint) chk.resume()
    }
  }

  _checkSystemUpgrade() {
    if (this.systemUpgrade) return

    const tally = new Map()

    for (const chk of this.checkpoints) {
      if (chk.signedLength() <= this.system.core.signedLength) continue
      if (!chk.digest) continue

      const id = b4a.toString(chk.digest.key, 'hex')
      const cnt = (tally.get(id) || 0) + 1

      if (cnt >= this.quorum) {
        this.systemUpgrade = chk.digest.key
        this.base._queueStaticFastForward(this.systemUpgrade)
        return
      }

      tally.set(id, cnt)
    }
  }

  mapIndexToView(index) {
    for (const { mappedIndex, ref } of this.views) {
      if (mappedIndex === index) return ref
    }
  }

  async maybeSigned() {
    if (this.opened === false) await this.ready()
    if (this.interrupted) return

    const thres = this.checkpoints.length - this.quorum
    if (thres < 0 || this.checkpoints.length <= thres) return

    this.checkpoints.sort(cmpCheckpoints)

    const signableLength = this.checkpoints[thres].signedLength()
    if (signableLength <= this.system.core.signedLength) return

    const expected = this.system.core.key

    if (signableLength > this.indexedLength) {
      if (!b4a.equals(expected, this.checkpoints[thres].digest.key)) this._checkSystemUpgrade()
      this.needsIndexedLengthUpdate = true
      return
    }

    for (let i = thres; i < this.checkpoints.length; i++) {
      const chk = this.checkpoints[i]
      // we must agree on what the system is obvs
      if (!b4a.equals(expected, chk.digest.key)) {
        this._checkSystemUpgrade()
        return
      }
      // if we dont have the indexed state ourself, then no way for us to verify / patch
      if (!chk.signatures.system || chk.signatures.system.length > this.indexedLength) {
        this.needsIndexedLengthUpdate = true
        return
      }
    }

    this.needsIndexedLengthUpdate = false

    const chk = []
    for (let i = thres; i < this.checkpoints.length; i++) {
      chk.push({ signer: this.checkpoints[i].signer, signatures: this.checkpoints[i].signatures })
    }

    const { system } = this.checkpoints[thres].signatures

    await this._assembleMultisig(system.length, chk)
  }

  async _assembleMultisig(signableLength, checkpoints) {
    const views = new Array(this.views.length + 2) // +2 is system + encryption

    const sys = await this.system.getIndexedInfo(signableLength)

    if (this.interrupted) return

    if (this.pendingViews && this.pendingViews[0].length >= signableLength) {
      return
    }

    views[0] = createCheckpointSignature(signableLength, this.systemView, checkpoints.length)

    if (sys.encryptionLength) {
      views[1] = createCheckpointSignature(
        sys.encryptionLength,
        this.encryptionView,
        checkpoints.length
      )
    }

    const offset = 2 // system + encryption

    for (let i = offset; i < views.length; i++) {
      const view = this.views[i - offset]
      const core = view.core

      const v = this.getViewFromSystem(view, sys)
      const length = v ? v.length : 0

      if (!core || length <= core.signedLength) continue

      const viewIndex = view.mappedIndex + offset
      views[viewIndex] = createCheckpointSignature(length, view, checkpoints.length)
    }

    for (let i = 0; i < checkpoints.length; i++) {
      addCheckpointSignatures(views, checkpoints, i)
    }

    const promises = []
    for (const view of views) {
      if (!view) continue

      for (let i = 0; i < view.signatures.length; i++) {
        promises.push(setPartialSignature(view, i))
      }
    }

    await Promise.all(promises)

    // check that the state still looks good, couple replicate inbetween...
    for (const v of views) {
      if (!v) continue

      for (let i = 0; i < v.signatures.length; i++) {
        if (!v.partials[i]) return
      }
    }

    if (this.interrupted) return

    if (this.pendingViews && this.pendingViews[0].length >= signableLength) {
      return
    }
    // skipped system for whatever reason...
    if (!views[0]) return

    this.pendingViews = views
    this.dirty = true

    // TODO: only needed if not updating, wont crash so keeping for now, just less efficient
    this.base._queueBump()
  }

  _close() {
    if (this.interrupted) return
    return this._free()
  }

  async _free() {
    this.interrupted = true
    if (this.applying) this._postApply()
    for (const chk of this.checkpoints) await chk.close()
    if (this.localCheckpoint) await this.localCheckpoint.close()
    if (this.view && this.base._hasClose) await this.base._handlers.close(this.view)
    if (this.system) await this.system.close()

    const promises = []
    for (const v of this.views) {
      if (v.ref) promises.push(v.ref.release())
    }

    if (this.systemView) promises.push(this.systemView.ref.release())
    if (this.encryptionView) promises.push(this.encryptionView.ref.release())

    await Promise.all(promises)
    await this.store.close()
  }

  _pushUpdate(u) {
    u.version = 1
    u.seq = this.updates.length === 0 ? 0 : this.updates[this.updates.length - 1].seq + 1
    u.systemLength = this.systemView.core.length
    this.updates.push(u)
    this.localState.insertUpdate(u)
  }

  async catchup(linearizer) {
    if (!this.opened) await this.ready()
    if (!this.system.heads.length) return true

    const writers = new Map()

    // load linearizer...
    const updates = await this.localState.listUpdates()
    const sys = await this.system.checkout(this.indexedLength)

    for (const node of updates) {
      const hex = b4a.toString(node.key, 'hex')

      let w = writers.get(hex)

      if (w === undefined) {
        // TODO: we actually have all the writer info already but our current methods make it hard to reuse that
        w = await this.base._getWriterByKey(node.key, -1, 0, true, false, sys)
        writers.set(hex, w)
      }

      if (w.length >= node.length) {
        this.base._warn(new Error('Update expects writer to be consumed here'))
        return false
      }

      while (w.length < node.length) {
        await w.update(sys)

        const next = w.advance()
        if (!next) {
          this.base._warn(new Error('Node must exist for catchup'))
          return false
        }

        linearizer.addHead(next)
      }
    }

    await sys.close()

    this.updates = updates

    if (this.indexersUpdated) {
      this.indexersUpdated = false
      // if we updated the indexers, invalidate all the internal state and reapply it
      await this.truncate(this.indexedLength)
      await this._rollbackViews()

      if (this.tx === null) this.tx = this.local.state.storage.write()

      while (
        this.updates.length > 0 &&
        this.updates[this.updates.length - 1].systemLength >= this.indexedLength
      ) {
        const u = this.updates.pop()
        this.localState.deleteUpdate(u)
      }
    } else {
      // otherwise we know the internal state is correct, so we carry on
      linearizer.update()
    }

    // must refresh the writers here so isRemoved is up to date
    await this._refreshWriters()
    return true
  }

  async getIndexedSystem() {
    if (this.opened === false) await this.ready()

    const indexedLength = this.pendingFork ? this.pendingFork.length : this.indexedLength

    const sys = await this.system.checkout(indexedLength)
    await sys.ready()
    return sys
  }

  getViewFromSystem(view, sys = this.system) {
    if (view.mappedIndex === -1 || view.mappedIndex >= sys.views.length) return null
    return sys.views[view.mappedIndex]
  }

  async recoverAt() {
    if (!this.systemRef) await this.ready()

    await this.systemRef.core.ready()

    const lt = this.systemRef.core.core.state.length

    for await (const { length, info } of SystemView.flushes(this.systemRef.core.session(), {
      reverse: true,
      lt
    })) {
      let consistent = true

      for (let i = 0; i < info.views.length; i++) {
        if (i >= this.views.length) continue
        if (!b4a.equals(info.views[i].key, this.views[i].key)) continue

        if (info.views[i].length > this.views[i].core.core.state.length) {
          consistent = false
          break
        }
      }

      if (consistent) {
        return {
          length,
          force: true,
          key: this.system.core.key,
          indexers: info.indexers,
          views: info.views
        }
      }
    }

    return null
  }

  bootstrap() {
    return this.system.add(this.base.key, { isIndexer: true, isPending: false })
  }

  async undo(popped) {
    if (!popped) return

    let indexersUpdated = false
    while (popped > 0) {
      const u = this.updates.pop()
      this.localState.deleteUpdate(u)
      popped -= u.batch
      if (u.indexers) indexersUpdated = true
    }

    const u = this.updates.length === 0 ? null : this.updates[this.updates.length - 1]
    const systemLength = u ? u.systemLength : this.indexedLength

    await this.truncate(systemLength)
    if (indexersUpdated) await this._rollbackViews()
  }

  async truncate(systemLength) {
    if (this.opened === false) await this.ready()
    if (systemLength === this.system.core.length) return

    await this.system.core.truncate(systemLength)

    const migrated = await this.system.update()

    if (this.encryptionView.core.length !== this.system.encryptionLength) {
      await this.encryptionView.core.truncate(this.system.encryptionLength)
    }

    for (const view of this.views) {
      if (!view.core) continue

      const v = this.getViewFromSystem(view)

      if (v && view.core.length === v.length) continue
      if (v === null) view.mappedIndex = -1 // unmap

      await view.core.truncate(v ? v.length : 0)
    }

    if (!migrated) return

    await this._refreshWriters()
    await this._rollbackViews()
  }

  async _refreshWriters() {
    // TODO: add the seq at which we updated the state to the writer instance.
    // then we know if it was truncated out without the lookup, meaning faster truncations
    for (const w of this.base.activeWriters) {
      const data = await this.system.get(w.core.key)
      const bootstrapper = this.system.core.length === 0 && b4a.equals(w.core.key, this.base.key)
      const isRemoved = data ? data.isRemoved : !bootstrapper
      w.isRemoved = isRemoved
    }
  }

  async _updateSystem() {
    if (!(await this.system.update())) return
    await this._refreshWriters()
    await this._rollbackViews()
  }

  async _signViewCore(core, length) {
    const s = await core.signable(length, 0)
    const signature = crypto.sign(s, this.base.local.keyPair.secretKey)
    return { signature, length }
  }

  async _signInternalViewCores(sys) {
    return {
      system: await this._signViewCore(this.systemView.core, this.systemView.indexedLength),
      encryption: await this._signViewCore(this.encryptionView.core, sys.encryptionLength)
    }
  }

  async _signUserViewCores(sys) {
    const promises = new Array(sys.views.length)

    for (let i = 0; i < this.views.length; i++) {
      const view = this.views[i]

      const v = this.getViewFromSystem(view, sys)
      const indexedLength = v ? v.length : 0

      if (!indexedLength) continue

      const viewIndex = view.mappedIndex
      promises[viewIndex] = this._signViewCore(view.ref.atomicBatch, indexedLength)
    }

    return Promise.all(promises)
  }

  async finalize(key) {
    this.localState.setBootRecord({
      version: BOOT_RECORD_VERSION,
      key,
      systemLength: this.systemView.indexedLength,
      indexersUpdated: true,
      fastForwarding: false,
      recoveries: this.base.recoveries
    })

    await this.localState.flush()

    await this.store.flush()
    await this.close()
  }

  async _flush(localNodes) {
    if (localNodes) await this._appendLocalNodes(localNodes)

    this.localState.setBootRecord({
      version: BOOT_RECORD_VERSION,
      key: this.key,
      systemLength: this.systemView.indexedLength,
      indexersUpdated: false,
      fastForwarding: false,
      recoveries: this.base.recoveries
    })

    await this.localState.flush()

    if (this.localCheckpoint) {
      // we have to flush here so the chkpoint can update
      // could run the checkpoint on the batch but also no big dead
      // as the "should we sign" check is rerun on boot...
      await this.store.flush()
      await this.localCheckpoint.update()
    }

    if (this.pendingViews) {
      const views = this.pendingViews
      this.pendingViews = null

      for (let i = views.length - 1; i >= 0; i--) {
        const v = views[i]
        if (!v || v.length <= v.core.signedLength) continue
        const signature = v.core.core.verifier.assemble(v.partials)
        try {
          await v.ref.commit(this.store.atom, v.length, signature)
        } catch (err) {
          // TODO: we should prevalidate all signatures instead of during commit so it can be all or nothing, just slighly
          // friendlier ux. missing hc api for that
          if (!this.base.closing) this.base._warn(err)
          break
        }
      }
    }

    await this.store.flush()

    this.fastForwarding = false
    if (this.pendingViews === null) this.dirty = false
  }

  _indexUpdates(indexed) {
    let shift = 0
    while (indexed > 0) indexed -= this.updates[shift++].batch

    const last = this.updates[shift - 1]

    this.systemView.indexedLength = last.systemLength

    for (let i = 0; i < shift; i++) this.localState.deleteUpdate(this.updates[i])
    this.updates.splice(0, shift)

    if (!this.needsIndexedLengthUpdate) return
    this.maybeSigned().catch(noop)
  }

  pendingIndexedLength() {
    let indexed = this.applying.indexed.length
    if (!this.applying || !this.updates.length || !indexed) return this.indexedLength

    let pos = 0
    while (indexed > 0 && pos < this.updates.length) {
      indexed -= this.updates[pos++].batch
    }

    const last = this.updates[pos - 1]
    return last.systemLength
  }

  async _assertNode(node, batch) {
    // helpful dag helper, so kept here

    const v = (await this.system.get(node.writer.core.key)) || { length: 0, isRemoved: false }
    const expected = v.length + batch
    if (node.length === expected) return

    console.trace(
      'INVALID_INSERTION',
      'length=',
      node.length,
      'key=',
      node.writer.core.key,
      'local=',
      node.writer.core.writable,
      'batch=',
      batch,
      'dag=',
      v
    )

    process.exit(1)
  }

  _postApply() {
    this.applyBatch = this.applying = null
    this.base._postApply()
  }

  async _optimisticApply(u, node, indexed) {
    const checkpoint = this.system.checkpoint()

    this.system.addHead(node)

    const applyBatch = []
    const key = node.writer.core.key

    applyBatch.push({
      indexed,
      optimistic: true,
      from: node.writer.core,
      length: node.length,
      value: node.value,
      heads: node.actualHeads
    })

    const pre = await this.system.get(key)
    const preLength = pre ? pre.length : 0

    let failed = false

    this.applying = u
    this.applyBatch = applyBatch
    try {
      await this.base._handlers.apply(applyBatch, this.view, this.hostcalls)
    } catch (err) {
      if (this.base.closing) throw err
      failed = true
    }
    this._postApply()

    if (!failed) {
      const post = await this.system.get(key)
      // check if acked by addWriter/removeWriter/ackWriter
      if (!post || preLength === post.length) failed = true
    }

    if (!failed) {
      // technically we only need to to this is the writer was removed, but hey, tricky logic
      await this._refreshWriters()
      return true
    }

    // it failed! rollback...

    this.system.applyCheckpoint(checkpoint)

    if (this.encryptionView.core.length !== this.system.encryptionLength) {
      await this.encryptionView.core.truncate(this.system.encryptionLength)
    }

    for (const view of this.views) {
      const viewLength = view.core ? view.core.length : view.length
      const viewLookup = this.getViewFromSystem(view)
      const viewSystemLength = viewLookup ? viewLookup.length : 0
      if (viewSystemLength === viewLength) continue
      await view.core.truncate(viewSystemLength)
    }

    await this._refreshWriters()

    await this._rollbackViews()
    return false
  }

  _startTrace(node) {
    this.systemRef.tracer.start()
    this.encryptionView.ref.tracer.start()
    for (let i = 0; i < this.views.length; i++) this.views[i].ref.tracer.start()
  }

  _endTrace(node) {
    const trace = {
      system: this.systemRef.tracer.end(),
      encryption: this.encryptionView.ref.tracer.end(),
      user: []
    }

    for (let i = 0; i < this.views.length; i++) {
      const blocks = this.views[i].ref.tracer.end()
      if (blocks.length) trace.user.push({ view: i, blocks })
    }

    if (trace.system.length || trace.encryption.length || trace.user.length) node.trace = trace
  }

  async update(u, localNodes) {
    if (this.changes !== null) this.changes.track(this)

    let batch = 0
    let applyBatch = []
    let indexersUpdated = 0

    let j = 0
    let i = 0
    let forkedAt = -1

    while (i < Math.min(u.indexed.length, u.shared)) {
      const node = u.indexed[i++]

      if (node.batch > 1) continue
      this.base._shiftWriter(node.writer)

      const update = this.updates[j++]

      if (update.indexers) {
        indexersUpdated = i
        break
      }
    }

    if (u.undo) await this.undo(u.undo)

    if (this.system.bootstrapping) await this.bootstrap()

    await this._updateSystem()

    // Organize by view
    const warmupSystemView = []
    const warmupEncryptionView = []
    const warmupUserView = new Array(this.views.length)

    for (let k = u.shared; k < u.length; k++) {
      const indexed = k < u.indexed.length
      const node = indexed ? u.indexed[k] : u.tip[k - u.indexed.length]
      if (!node.trace) continue

      if (node.trace.system.length) warmupSystemView.push(...node.trace.system)
      if (node.trace.encryption.length) warmupEncryptionView.push(...node.trace.encryption)

      for (const trace of node.trace.user) {
        // Assert (as warning) that trace view should only be currently indexed views
        if (trace.view >= this.views.length) {
          console.warn(
            'Oplog block trace contains OOB view index',
            'trace view',
            trace.view,
            'views.length',
            this.views.length
          )
          continue
        }

        warmupUserView[trace.view] = warmupUserView[trace.view] || []
        warmupUserView[trace.view].push(...trace.blocks)
      }
    }

    // Request per view
    if (warmupSystemView.length) this.systemView.core.download({ blocks: warmupSystemView })
    if (warmupEncryptionView.length)
      this.encryptionView.core.download({ blocks: warmupEncryptionView })
    for (let k = 0; k < warmupUserView.length; k++) {
      const blocks = warmupUserView[k]
      if (!blocks) continue

      const view = this.views[k]
      view.core.download({ blocks })
    }

    for (i = u.shared; i < u.length; i++) {
      if (this.base.backoff !== null && this.base.backoff.backoff()) await this.base.backoff.wait()

      const indexed = i < u.indexed.length
      const node = indexed ? u.indexed[i] : u.tip[i - u.indexed.length]

      batch++

      if (node.writer.core.writable) this._startTrace(node)

      const optimist =
        node.writer.isRemoved &&
        node.optimistic &&
        batch === 1 &&
        this.base._hasOptimisticApply === true &&
        (await this._optimisticApply(u, node, indexed))

      if (!optimist) {
        if (node.writer.isRemoved && !node.writer.isActiveIndexer) {
          if (node.batch > 1) continue
          // in case someone is linking this node and they are not removed
          const u = {
            seq: 0,
            key: node.writer.core.key,
            length: node.length,
            batch,
            systemLength: 0,
            indexers: false
          }
          this._pushUpdate(u)
          batch = 0
          assert(applyBatch.length === 0, 'Apply batch should not have been modified')
          continue
        }

        // in prod we prop need to disable this assertion as its pretty expensive,
        // but it catches a lot of bugs, so here for debugs
        // await this._assertNode(node, batch)

        const deps = node.causalDependencies()

        // oldest -> newest
        for (let i = deps.length - 1; i >= 1; i--) {
          const d = deps[i]
          if (await this.system.linkable(d.writer.core.key, d.length)) {
            this.system.addHead(deps[i])
          }
        }

        this.system.addHead(deps[0])

        if (node.value !== null && !node.writer.isRemoved) {
          applyBatch.push({
            indexed,
            optimistic: false,
            from: node.writer.core,
            length: node.length,
            value: node.value,
            heads: node.actualHeads
          })
        }

        if (node.batch > 1) continue

        if (applyBatch.length && this.base._hasApply === true) {
          this.applying = u
          this.applyBatch = applyBatch
          await this.base._handlers.apply(applyBatch, this.view, this.hostcalls)
          if (forkedAt === -1 && this.pendingFork) forkedAt = i
          this._postApply()
        }
      }

      const update = {
        seq: 0,
        key: node.writer.core.key,
        length: node.length,
        batch,
        systemLength: 0,
        indexers: false
      }

      update.indexers = !!this.system.indexerUpdate

      if (this.system.indexerUpdate) await this._generateNextViews()

      await this.system.flush(this.views)
      await this.system.update()

      if (node.writer.core.writable) this._endTrace(node)

      batch = 0
      applyBatch = []

      this._pushUpdate(update)

      if (!indexed || indexersUpdated) continue

      this.base._shiftWriter(node.writer)

      if (update.indexers) {
        indexersUpdated = i + 1
      }
    }

    if (this.pendingFork) {
      const pending = this.pendingFork
      if (pending.length > this.indexedLength) this._indexUpdates(u.indexed.length)

      const fork = new Fork(this.base, this.store, this, pending)
      const forked = await fork.upgrade()

      if (!forked) throw new Error('Fork failed')

      await this._flush(localNodes)

      return {
        reboot: true,
        migrated: false
      }
    }

    if (u.indexed.length) {
      this._indexUpdates(indexersUpdated || u.indexed.length)
    }

    const migrated = indexersUpdated !== 0
    if (migrated) this.key = await this.base._premigrate()

    if (this.changes !== null) {
      this.changes.finalise()
      await this.base._handlers.update(this.view, this.changes)
    }

    await this._flush(localNodes)

    return {
      reboot: migrated,
      migrated
    }
  }

  flush() {
    return this._flush(null)
  }

  async _appendLocalNodes(localNodes) {
    if (localNodes.length === 0) return // just in case

    const blocks = new Array(localNodes.length)
    const local = this.local

    if (!local.opened) await local.ready()

    for (let i = 0; i < blocks.length; i++) {
      const { value, heads, batch, optimistic, trace } = localNodes[i]

      blocks[i] = {
        version: OPLOG_VERSION,
        maxSupportedVersion: this.base.maxSupportedVersion,
        checkpoint: null,
        digest: null,
        optimistic,
        node: {
          heads,
          batch,
          value: value === null ? null : c.encode(this.base.valueEncoding, value)
        },
        trace
      }
    }

    if (this.localIndexer) {
      const signedLength = this.localCheckpoint.signedLength()
      const local = this.local

      let signedAt = 0
      let internalSignatures = null
      let signatures = null

      if (this.indexedLength > signedLength) {
        const sys = await this.system.getIndexedInfo(this.indexedLength)
        internalSignatures = await this._signInternalViewCores(sys)
        signatures = await this._signUserViewCores(sys)
        signedAt = local.length + blocks.length
      }

      for (let i = 0; i < blocks.length; i++) {
        const length = local.length + i + 1
        if (length === signedAt) {
          this.localCheckpoint.updateInternalSignatures(length, internalSignatures)
          this.localCheckpoint.updateUserSignatures(length, signatures)
        }

        const blk = blocks[i]

        blk.checkpoint = this.localCheckpoint.makeCheckpoints(length)
        blk.digest = this.localCheckpoint.makeDigest(length, this.key)
      }
    } else {
      if (!this.localCheckpoint) await this._startLocalCheckpoint()

      for (let i = 0; i < blocks.length; i++) {
        const length = local.length + i + 1
        blocks[i].digest = this.localCheckpoint.makeDigest(length, this.key)
      }
    }

    await local.append(blocks)
  }

  async _rollbackViews() {
    // encryption view key is not in system so no need to handle here
    for (const view of this.views) {
      const v = this.getViewFromSystem(view)

      if (v) {
        view.key = v.key
      } else {
        await this._resetView(view)
      }
    }
  }

  async createAnchor() {
    const node = this.applyBatch[this.applyBatch.length - 1]
    const key = node.from.key
    const length = node.length

    const info = await this.system.get(key, { unflushed: true })

    if (!info || info.length < length) throw new Error('Anchor node is not in system')

    const state = { start: 0, end: 40, buffer: b4a.alloc(40) }
    c.fixed32.encode(state, key)
    c.uint64.encode(state, length)

    const namespace = crypto.hash(state.buffer)
    const manifestData = c.encode(messages.ManifestData, { version: 0, legacyBlocks: 0, namespace })

    const padding = this.encryption ? AutobaseEncryption.PADDING : 0
    const block = encodeValue(null, { heads: [{ key, length }], padding })
    if (this.encryption) await this.encryption.encryptAnchor(block, namespace)

    const root = { index: 0, size: block.byteLength, hash: crypto.data(block) }
    const hash = crypto.tree([root])
    const prologue = { hash, length: 1 }

    const core = this.store.createAnchorCore(prologue, manifestData)
    await core.ready()

    if (core.length === 0) await core.append(block, { writable: true, maxLength: 1 })

    await this.system.add(core.key, { isIndexer: false, length: 0 })
    await this.base._addWriter(core.key, this.system)

    const anchor = { key: core.key, length: core.length }

    await core.close()
    await this.base.hintWakeup(anchor)

    return anchor
  }

  async _resetView(view) {
    const manifests = await this.store.getIndexerManifests(this.system.indexers)

    const manifestData = view.core ? view.core.manifest.userData : null
    view.key = await this.store.createView(
      manifests,
      view.name,
      null,
      this.system.core.manifest.version,
      this.system.entropy,
      null,
      manifestData
    ) // will never be system so no linked
    view.length = 0
  }

  async _generateNextViews() {
    const sys = this.system

    // note, this is very state dependent so can ONLY be called at the exact time an indexer upgrade occurs
    const manifests = await this.store.getIndexerManifests(sys.indexers)
    const entropy = sys.version < 2 ? null : sys.getEntropy(sys.indexers, sys.flushLength())

    for (const v of this.views) {
      const manifestData = v.core ? v.core.manifest.userData : null

      const prologue = await getPrologue(v)
      const key = await this.store.createView(
        manifests,
        v.name,
        prologue,
        sys.core.manifest.version,
        entropy,
        null,
        manifestData
      )

      v.key = key
    }
  }
}

async function getPrologue(view) {
  const length = view.core ? view.core.length : view.length
  if (!length) return null

  const hash = await view.core.treeHash(length)

  return {
    hash,
    length
  }
}

function noop() {}

function cmpCheckpoints(a, b) {
  return a.signedLength() - b.signedLength()
}

function sameSignature(a, b) {
  if (!a || !b) return a === b
  return a.length === b.length
}

function sameSignatures(a, b) {
  if (!a || !b) return a === b
  if (a.length !== b.length) return false
  for (let i = 0; i < a.length; i++) {
    if (a[i].length !== b[i].length) return false
  }
  return true
}

function createCheckpointSignature(length, view, checkpoints) {
  return {
    length,
    core: view.core,
    ref: view.ref,
    signatures: new Array(checkpoints),
    partials: new Array(checkpoints)
  }
}

function addCheckpointSignatures(pending, checkpoints, index) {
  const { signer, signatures } = checkpoints[index]

  pending[0].signatures[index] = {
    signature: signatures.system.signature,
    length: signatures.system.length,
    signer
  }

  if (pending[1]) {
    pending[1].signatures[index] = {
      signature: signatures.encryption.signature,
      length: signatures.encryption.length,
      signer
    }
  }

  for (let i = 2; i < pending.length; i++) {
    if (!pending[i]) continue

    const { length, signature } = signatures.user[i - 2]

    pending[i].signatures[index] = {
      signature,
      length,
      signer
    }
  }
}

async function setPartialSignature(view, index) {
  const sig = view.signatures[index]
  if (sig.length < view.length && sig.length > 0) await view.core.get(sig.length - 1)

  view.partials[index] = await partialSignature(
    view.core,
    sig.signer,
    view.length,
    sig.length,
    sig.signature
  )
}

function updateSignature(length, sig, existing) {
  return sig ? { signature: sig.signature, length: sig.length, at: length } : existing
}

function updateSignatures(length, signatures, existing) {
  const updated = new Array(signatures.length)

  for (let i = 0; i < updated.length; i++) {
    updated[i] = updateSignature(length, signatures[i], i < existing.length ? existing[i] : null)
  }

  return updated
}

function makeCheckpoint(length, chk) {
  if (!chk) return null

  const checkpointer = length - chk.at

  return {
    checkpointer,
    checkpoint: checkpointer === 0 ? { signature: chk.signature, length: chk.length } : null
  }
}

function makeCheckpoints(length, signatures) {
  if (!signatures.length) return null

  const checkpoints = new Array(signatures.length)

  for (let i = 0; i < signatures.length; i++) {
    checkpoints[i] = makeCheckpoint(length, signatures[i])
  }

  return checkpoints
}
const c = require('compact-encoding')
const messages = require('./messages.js')

module.exports = async function boot(
  corestore,
  key,
  { encrypt, encryptionKey, keyPair, exclusive = true } = {}
) {
  const result = {
    key: null,
    local: null,
    bootstrap: null,
    encryptionKey: null,
    boot: null
  }

  const manifest = keyPair
    ? { version: corestore.manifestVersion, signers: [{ publicKey: keyPair.publicKey }] }
    : null

  if (key) {
    result.key = key

    const bootstrap = corestore.get({ key, active: false, valueEncoding: messages.OplogMessage })
    await bootstrap.ready()

    const localKey = await bootstrap.getUserData('autobase/local')

    if (keyPair) {
      result.local = corestore.get({
        keyPair,
        active: false,
        exclusive,
        valueEncoding: messages.OplogMessage,
        manifest
      })
    } else {
      if (bootstrap.writable && !localKey) {
        result.local = bootstrap.session({
          active: false,
          exclusive,
          valueEncoding: messages.OplogMessage
        })
      } else {
        const local = localKey
          ? corestore.get({
              key: localKey,
              active: false,
              exclusive,
              valueEncoding: messages.OplogMessage
            })
          : corestore.get({
              name: 'local',
              active: false,
              exclusive,
              valueEncoding: messages.OplogMessage
            })

        await local.ready()
        result.local = local
      }
    }

    result.bootstrap = bootstrap
  } else {
    result.local = keyPair
      ? corestore.get({
          keyPair,
          manifest,
          active: false,
          exclusive,
          valueEncoding: messages.OplogMessage
        })
      : corestore.get({
          name: 'local',
          active: false,
          exclusive,
          valueEncoding: messages.OplogMessage
        })
    await result.local.ready()

    const key = await result.local.getUserData('referrer')
    if (key) {
      result.key = key
      result.bootstrap = corestore.get({ key, active: false, valueEncoding: messages.OplogMessage })
      await result.bootstrap.ready()
    } else {
      result.key = result.local.key
      result.bootstrap = result.local.session({
        active: false,
        valueEncoding: messages.OplogMessage
      })
      await result.bootstrap.setUserData('autobase/local', result.local.key)
    }
  }

  if (key || keyPair) {
    await result.bootstrap.setUserData('referrer', result.key)
    await result.bootstrap.setUserData('autobase/local', result.local.key)
    await result.local.setUserData('referrer', result.key)
  }

  const [encryptionKeyBuffer, pointer] = await Promise.all([
    result.local.getUserData('autobase/encryption'),
    result.local.getUserData('autobase/boot')
  ])

  if (pointer) {
    result.boot = c.decode(messages.BootRecord, pointer)
  }

  if (encryptionKeyBuffer) {
    result.encryptionKey = encryptionKeyBuffer
  }

  if (!result.encryptionKey && (encryptionKey || encrypt)) {
    if (!encryptionKey)
      encryptionKey = (await corestore.createKeyPair('autobase/encryption')).secretKey.subarray(
        0,
        32
      )
    await result.bootstrap.setUserData('autobase/encryption', encryptionKey) // legacy support
    await result.local.setUserData('autobase/encryption', encryptionKey)
    result.encryptionKey = encryptionKey
  }

  return result
}
const crypto = require('hypercore-crypto')

const OPLOG_VERSION = 2
const DEFAULT_AUTOBASE_VERSION = 1 // default
const MAX_AUTOBASE_VERSION = 2 // optional fork support
const BOOT_RECORD_VERSION = 3

const [NS_SIGNER_NAMESPACE, NS_VIEW_BLOCK_KEY, NS_HASH_KEY, NS_ENCRYPTION] = crypto.namespace(
  'autobase',
  4
)

module.exports = {
  OPLOG_VERSION,
  DEFAULT_AUTOBASE_VERSION,
  MAX_AUTOBASE_VERSION,
  BOOT_RECORD_VERSION,
  NS_SIGNER_NAMESPACE,
  NS_VIEW_BLOCK_KEY,
  NS_HASH_KEY,
  NS_ENCRYPTION
}
const BufferMap = require('tiny-buffer-map')

// This is basically just a Map atm, but leaving it as an abstraction for now
// in case we wanna optimize it for our exact usecase

module.exports = class Clock {
  constructor() {
    this.seen = new BufferMap()
  }

  get size() {
    return this.seen.size
  }

  has(key) {
    return this.seen.has(key)
  }

  includes(key, length) {
    return this.seen.has(key) && this.seen.get(key) >= length
  }

  get(key) {
    return this.seen.get(key) || 0
  }

  set(key, len) {
    this.seen.set(key, len)
    return len
  }

  add(clock) {
    for (const [key, l] of clock) {
      if (this.get(key) < l) this.set(key, l)
    }
  }

  [Symbol.iterator]() {
    return this.seen[Symbol.iterator]()
  }
}
const BufferMap = require('tiny-buffer-map')

const Clock = require('./clock.js')

const UNSEEN = 0
const NEWER = 1
const ACKED = 2

// Consensus machine for Autobase. Sort DAG nodes using
// vector clocks to determine a globally consistent view

module.exports = class Consensus {
  constructor(indexers) {
    this.merges = new Set()
    this.majority = (indexers.length >>> 1) + 1
    this.indexers = indexers
    this.removed = new Clock()
    this.updated = false

    this.writers = new BufferMap()
    for (const idx of this.indexers) {
      this.writers.set(idx.core.key, idx)
    }
  }

  addHead(node) {
    if (!node.writer.isActiveIndexer) return
    if (this._isMerge(node)) this.merges.add(node)
    this.updated = true
    return node
  }

  /* Indexer Only DAG methods */

  _tails(node, tails) {
    const tailSet = new Set()
    for (const t of tails) {
      if (node.clock.includes(t.writer.core.key, t.length)) tailSet.add(t)
    }

    return tailSet
  }

  _tailsAndMerges(node, tails) {
    const all = this._tails(node, tails)
    for (const m of this.merges) {
      if (m !== node && node.clock.includes(m.writer.core.key, m.length)) {
        all.add(m)
      }
    }
    return all
  }

  _isMerge(node) {
    if (!node.writer.isActiveIndexer) return false

    const deps = []

    for (const idx of this.indexers) {
      let seq = node.clock.get(idx.core.key) - 1

      if (idx === node.writer) seq--

      const head = idx.get(seq)
      if (!head || this.removed.includes(head.writer.core.key, head.length)) continue

      let isDep = true
      for (let i = 0; i < deps.length; i++) {
        const d = deps[i]
        if (d === head) continue

        if (d.clock.includes(head.writer.core.key, head.length)) {
          isDep = false
          break
        }

        if (head.clock.includes(d.writer.core.key, d.length)) {
          const popped = deps.pop()
          if (d === popped) continue
          deps[i--] = popped
        }
      }

      if (isDep) deps.push(head)
    }

    return deps.length > 1
  }

  _indexerTails() {
    const tails = new Set()
    for (const idx of this.indexers) {
      const length = this.removed.has(idx.core.key) ? this.removed.get(idx.core.key) : idx.indexed

      const head = idx.get(length)
      if (!head || this.removed.includes(head.writer.core.key, head.length)) continue

      let isTail = true
      for (const t of tails) {
        if (head.clock.includes(t.writer.core.key, t.length)) {
          isTail = false
          break
        }

        if (t.clock.includes(head.writer.core.key, head.length)) {
          tails.delete(t)
        }
      }

      if (isTail) tails.add(head)
    }

    return tails
  }

  // parent is newer if for any node in parent's view,
  // either node can see object or object can see node
  _strictlyNewer(object, parent) {
    if (!parent.clock.includes(object.writer.core.key, object.length)) return false

    for (const [key, latest] of parent.clock) {
      const oldest = this.removed.get(key)
      if (latest <= oldest) continue // check quickly if we removed it

      // get the NEXT node from the writer from the objects pov, adjust if its removed
      let length = object.clock.get(key)
      if (length <= oldest) length = oldest

      // sanity check, likely not needed as someone has checked this before, but it's free
      if (latest < length) return false

      // if the same, they have both seen it, continue
      if (latest === length) continue

      const writer = this.writers.get(key)

      // might not be in the removed set but the writer can tell us if it was indexed...
      const next = writer && writer.get(length >= writer.indexed ? length : writer.indexed)

      // no next, its been indexed, both seen it
      if (!next) continue

      // if the NEXT node has seen the object its fine - newer
      if (next.clock.includes(object.writer.core.key, object.length)) continue

      // otherwise the parent must also NOT have seen the next node
      if (latest < next.length) continue

      return false
    }

    return true
  }

  _acks(target) {
    const acks = target.writer.isActiveIndexer ? [target] : [] // TODO: can be cached on the target node in future (ie if we add one we dont have to check it again)

    for (const idx of this.indexers) {
      if (idx === target.writer) continue

      let next = target.clock.get(idx.core.key)
      if (next < idx.nodes.offset) next = idx.nodes.offset

      const nextIndexNode = idx.get(next >= idx.indexed ? next : idx.indexed)

      // no node - no ack
      if (!nextIndexNode) continue

      // if the next index node does not see the target, no ack
      if (!nextIndexNode.clock.includes(target.writer.core.key, target.length)) continue

      // if the next index node is not strictly newer, skip to avoid ambig...
      if (!this._strictlyNewer(target, nextIndexNode)) continue

      acks.push(nextIndexNode)
    }

    return acks
  }

  acksFromNode(target, view) {
    const acks = new Set()

    if (!view || !view.clock.includes(target.writer.core.key, target.length)) return acks

    acks.add(view.writer)

    for (const idx of this.indexers) {
      if (idx === view.writer) continue

      const length = view.clock.get(idx.core.key)
      if (!length) continue

      if (target.clock.includes(idx.core.key, length)) continue

      const head = idx.get(length - 1)
      if (!head) continue

      if (head.clock.includes(target.writer.core.key, target.length)) {
        acks.add(idx)
      }
    }

    return acks
  }

  _ackedAt(acks, parent) {
    let seen = 0
    let missing = acks.length

    for (const node of acks) {
      missing--

      if (!parent.clock.includes(node.writer.core.key, node.length)) {
        if (seen + missing < this.majority) return false
        continue
      }

      if (++seen >= this.majority) return true
    }

    return false
  }

  confirms(indexer, target, acks, length) {
    if (!length || this.removed.get(indexer.core.key) >= length) return UNSEEN
    // def feels like there is a smarter way of doing this part
    // ie we just wanna find a node from the indexer that is strictly newer than target
    // and seens a maj of the acks - thats it

    let jump = true
    let newer = true

    for (let i = length - 1; i >= 0; i--) {
      const head = indexer.get(i)
      if (head === null) return UNSEEN

      let seen = 0

      for (const node of acks) {
        // if (node.writer === indexer) continue
        if (!head.clock.includes(node.writer.core.key, node.length)) continue
        if (++seen >= this.majority) break
      }

      if (!newer && seen < this.majority) {
        break
      }

      if (!this._strictlyNewer(target, head)) {
        // all strictly newer nodes are clustered together so bisect until we find the cluster
        if (jump) {
          jump = false

          let t = length - 1
          let b = 0

          while (t > b) {
            const mid = (t + b) >>> 1
            const node = indexer.get(mid)

            if (
              node === null ||
              !node.clock.includes(target.writer.core.key, target.length) ||
              this._strictlyNewer(target, node)
            ) {
              b = mid + 1
            } else {
              t = mid - 1
            }
          }

          // + 2 in case we are off by one and the i--. its fine, just an optimisation
          if (b + 1 < i) i = b + 2
        }

        newer = false
        continue
      } else if (seen < this.majority) {
        return NEWER
      }

      return ACKED
    }

    return UNSEEN
  }

  _isConfirmed(target, parent = null) {
    const acks = this._acks(target)
    const confs = new Set()

    if (acks.length < this.majority) return false
    let allNewer = true

    for (const indexer of this.indexers) {
      const length = parent
        ? parent.writer === indexer
          ? parent.length - 1
          : parent.clock.get(indexer.core.key)
        : indexer.length

      const result = this.confirms(indexer, target, acks, length)

      if (result === ACKED) {
        confs.add(indexer)
        if (confs.size >= this.majority) {
          return true
        }
      }

      if (result === UNSEEN) allNewer = false
    }

    if (parent) return this._isConfirmableAt(target, parent, acks, confs)

    return allNewer
  }

  _isConfirmableAt(target, parent, acks, confs) {
    if (!this._ackedAt(acks, parent)) return false

    let potential = confs.size

    for (const indexer of this.indexers) {
      if (confs.has(indexer)) continue

      const length = parent.clock.get(indexer.core.key)
      const isSeen = target.clock.includes(indexer.core.key, length)

      // if the target has seen the latest node, it can freely be used to confirm the target later
      // otherwise, check if a newer node is strictly newer...
      if (!isSeen) {
        const head = indexer.get(length - 1)

        // the next indexer head HAS to be strictly newer - meaning the current one has to be also.
        if (
          head &&
          !this.removed.includes(head.writer.core.key, head.length) &&
          !this._strictlyNewer(target, head)
        ) {
          continue
        }
      }

      if (++potential >= this.majority) return true
    }

    return false
  }

  // this can get called multiple times for same node
  remove(node) {
    this.merges.delete(node)
    this.removed.set(node.writer.core.key, node.length)
    return node
  }

  shift() {
    if (!this.updated) return []

    const tails = this._indexerTails()

    for (const tail of tails) {
      if (this._isConfirmed(tail)) {
        return [this.remove(tail)]
      }
    }

    for (const merge of this.merges) {
      if (this._isConfirmed(merge)) {
        return this._yieldNext(merge, tails)
      }
    }

    this.updated = false
    return []
  }

  // yields next indexer node
  _yieldNext(node, tails) {
    // only stop when we find a tail
    while (!tails.has(node)) {
      let next = null

      // for merges check if one fork is confirmed
      for (const t of this._tailsAndMerges(node, tails)) {
        if (this._isConfirmed(t, node)) {
          next = t
          break
        }
      }

      if (next) {
        node = next
        continue
      }

      // otherwise yield all tails
      const tailSet = []
      for (const t of this._tails(node, tails)) {
        tailSet.push(this.remove(t))
      }

      return tailSet
    }

    return [this.remove(node)]
  }

  shouldAck(writer) {
    for (const t of this._indexerTails()) {
      if (t.writer === writer) continue
      if (this._shouldAckNode(t, writer)) return true
    }

    return false
  }

  _shouldAckNode(target, writer) {
    const head = writer.head()
    const next = target.clock.get(writer.core.key)
    const nextIndexNode = writer.get(next >= writer.indexed ? next : writer.indexed)

    // if we have no next node and we didn't write target then ack
    if (!nextIndexNode && writer !== target.writer) return true

    // shortcuts if we have next node
    if (nextIndexNode) {
      // if the next node does not see the target, should ack
      if (!nextIndexNode.clock.includes(target.writer.core.key, target.length)) {
        return !head.clock.includes(target.writer.core.key, target.length)
      }

      // if the next node is not strictly newer, no point acking
      if (!this._strictlyNewer(target, nextIndexNode)) return false
    }

    // now check if we can double confirm
    const acks = this._acks(target)

    // need enough to double confirm
    if (acks.length >= this.majority) {
      return this.confirms(writer, target, acks, writer.length) === UNSEEN
    }

    return false
  }
}
const HypercoreEncryption = require('hypercore/lib/default-encryption.js')
const sodium = require('sodium-universal')
const crypto = require('hypercore-crypto')
const c = require('compact-encoding')
const b4a = require('b4a')
const rrp = require('resolve-reject-promise')
const ReadyResource = require('ready-resource')

const SystemView = require('./system.js')
const { EncryptionDescriptor, ManifestData } = require('./messages.js')
const { NS_VIEW_BLOCK_KEY, NS_HASH_KEY, NS_ENCRYPTION } = require('./caps.js')

const nonce = b4a.alloc(sodium.crypto_stream_NONCEBYTES)
const hash = nonce.subarray(0, sodium.crypto_generichash_BYTES_MIN)

class AutobaseEncryption {
  static PADDING = 8

  constructor(encryption) {
    this.encryption = encryption

    this.compat = null
    this.keys = null
    this.keysById = new Map()
  }

  get id() {
    return this.keys ? this.keys.id : 0
  }

  padding() {
    return AutobaseEncryption.PADDING
  }

  isCompat() {
    return false
  }

  load(keys) {
    if (this.keys === null) this.keys = keys
  }

  async update(ctx) {
    if (this.id !== 0 && this.id === this.encryption.id) return
    const keys = await this.get(this.encryption.id, ctx)
    if (keys) this.keys = keys
  }

  async get(id, ctx) {
    if (this.keysById.has(id)) return this.keysById.get(id)

    const keys = await this.getKeys(id, ctx)
    this.keysById.set(id, keys)

    return keys
  }

  async getKeys(id, ctx) {
    const entropy = await this.encryption.get(id)
    if (!entropy) return null

    const block = this.blockKey(entropy, ctx)
    const hash = crypto.hash([NS_HASH_KEY, block])

    return {
      id,
      block,
      hash
    }
  }

  blockKey(entropy, ctx) {
    return this.encryption.blockKey(entropy, ctx)
  }

  async _ensureCompat(ctx) {
    if (!this.compat) this.compat = this.compatKeys(ctx)
  }

  compatKeys() {
    throw new Error('Compatability method is not specified')
  }

  async encrypt(index, block, fork, ctx) {
    if (this.isCompat(ctx, index)) {
      this._ensureCompat(ctx)
      return HypercoreEncryption.encrypt(
        index,
        block,
        fork,
        this.compat.block,
        this.compat.blinding
      )
    }

    await this.update(ctx)

    encryptBlock(index, block, this.keys.id, this.keys.block, this.keys.hash)
  }

  async decrypt(index, block, ctx) {
    if (this.isCompat(ctx, index)) {
      this._ensureCompat(ctx)
      return HypercoreEncryption.decrypt(index, block, this.compat.block)
    }

    const padding = block.subarray(0, AutobaseEncryption.PADDING)
    block = block.subarray(AutobaseEncryption.PADDING)

    const type = padding[0]
    switch (type) {
      case 0:
        return block // unencrypted

      case 1:
        break

      default:
        throw new Error('Unrecognised encryption type')
    }

    const id = c.uint32.decode({ start: 4, end: 8, buffer: padding })

    const keys = await this.get(id, ctx)

    c.uint64.encode({ start: 0, end: 8, buffer: nonce }, index)
    nonce.set(padding, 8, 16)

    // Decrypt the block using the full nonce
    decrypt(block, nonce, keys.block)
  }
}

class ViewEncryption extends AutobaseEncryption {
  constructor(encryption, name) {
    super(encryption)
    this.name = name
  }

  isCompat(ctx, index) {
    if (ctx.manifest.version <= 1) return true
    if (!ctx.manifest.userData) return false
    const { legacyBlocks } = c.decode(ManifestData, ctx.manifest.userData)
    return index < legacyBlocks
  }

  compatKeys() {
    const { bootstrap, encryptionKey } = this.encryption.base
    const block = getCompatBlockKey(bootstrap, encryptionKey, this.name)
    return {
      block,
      blinding: crypto.hash(block)
    }
  }

  blockKey(entropy) {
    return getCompatBlockKey(this.encryption.base.bootstrap, entropy, this.name)
  }
}

class WriterEncryption extends AutobaseEncryption {
  isCompat(ctx) {
    return ctx.manifest.version <= 1
  }

  compatKeys(ctx) {
    return HypercoreEncryption.deriveKeys(this.encryption.base.encryptionKey, ctx.key)
  }

  blockKey(entropy, ctx) {
    if (ctx.manifest.userData) {
      const userData = c.decode(ManifestData, ctx.manifest.userData)
      if (userData.namespace !== null) {
        return this.encryption.blockKey(entropy, { key: userData.namespace })
      }
    }

    return this.encryption.blockKey(entropy, ctx)
  }
}

class EncryptionView extends ReadyResource {
  constructor(base, core) {
    super()

    this.base = base
    this.core = core || null

    this.sessions = new Map()

    this._initialising = null
  }

  async _open() {
    await this.initialised()
    await this.core.ready()
  }

  initialised() {
    if (this.core !== null) return this.core.ready()
    if (this._initialising) return this._initialising

    this._initialising = rrp()

    return this._initialising.promise
  }

  _close() {
    if (this._initialising) {
      this._initialising.reject(new Error('Encryption closed'))
      this._initialising = null
    }

    if (this.core) return this.core.close()
  }

  get bootstrapped() {
    return !!(this.core && this.core.length > 0)
  }

  get id() {
    return this.core ? this.core.length : 0
  }

  _createPayload(key) {
    return key
  }

  async reload(core) {
    if (this.core) await this.core.close()

    this.core = core
    await this.core.ready()

    if (this._initialising) {
      this._initialising.resolve()
      this._initialising = null
    }
  }

  unpack(type, payload) {
    if (type > 0) throw new Error('Unsupported version')
    return payload
  }

  async update(key) {
    const payload = await this._createPayload(key)
    const desc = { type: 0, payload }

    await this.core.append(c.encode(EncryptionDescriptor, desc))
  }

  async _preload() {
    await this.ready()
    return this.id()
  }

  getViewEncryption(name) {
    if (this.sessions.has(name)) return this.sessions.get(name)

    const encryption = new ViewEncryption(this, name)
    this.sessions.set(name, encryption)

    return encryption
  }

  getWriterEncryption() {
    return new WriterEncryption(this)
  }

  async encryptAnchor(block, namespace) {
    const entropy = await this.get(this.id)
    if (!entropy) return null

    const blockKey = this.blockKey(entropy, { key: namespace })
    const hashKey = crypto.hash([NS_HASH_KEY, block])

    encryptBlock(0, block, this.id, blockKey, hashKey)
  }

  blockKey(entropy, ctx) {
    return getBlockKey(this.base.bootstrap, this.base.encryptionKey, entropy, ctx.key)
  }

  async get(encryptionId) {
    if (encryptionId === 0) return SystemView.GENESIS_ENTROPY

    if (!this.core) await this.initialised()

    const index = encryptionId - 1
    const desc = await this.core.get(index)
    const { type, payload } = c.decode(EncryptionDescriptor, desc)

    const key = this.unpack(type, payload)
    return key
  }

  getSystemEncryption() {
    return this.getViewEncryption('_system')
  }

  static namespace(entropy) {
    return crypto.hash([NS_ENCRYPTION, entropy])
  }

  static getBlockKey(bootstrap, encryptionKey, entropy, hypercoreKey) {
    return getBlockKey(bootstrap, encryptionKey, entropy, hypercoreKey)
  }

  static getSystemEncryption(base, core) {
    const view = new EncryptionView(base, core)
    return view.getSystemEncryption()
  }

  static async setSystemEncryption(base, core, opts) {
    if (base.encryptionKey === null) return null

    await core.ready()
    if (core.manifest.version === 1) {
      const key = getCompatBlockKey(base.bootstrap, base.encryptionKey, '_system')
      return core.setEncryptionKey(key, { block: true })
    }

    if (!core.manifest.linked.length) {
      throw new Error('System manifest does not link encryption view')
    }

    const enc = base.store.get({ key: core.manifest.linked[0], active: false })
    const encryption = EncryptionView.getSystemEncryption(base, enc)

    await core.setEncryption(encryption, opts)

    return enc
  }
}

module.exports = {
  AutobaseEncryption,
  EncryptionView
}

function encrypt(block, nonce, key) {
  sodium.crypto_stream_xor(block, block, nonce, key)
}

function decrypt(block, nonce, key) {
  return encrypt(block, nonce, key) // symmetric
}

function getBlockKey(bootstrap, encryptionKey, entropy, hypercoreKey) {
  return (
    encryptionKey &&
    crypto.hash([NS_VIEW_BLOCK_KEY, bootstrap, encryptionKey, entropy, hypercoreKey])
  )
}

function getCompatBlockKey(bootstrap, encryptionKey, name) {
  if (typeof name === 'string') return getCompatBlockKey(bootstrap, encryptionKey, b4a.from(name))
  return encryptionKey && crypto.hash([NS_VIEW_BLOCK_KEY, bootstrap, encryptionKey, name])
}

function blockhash(block, padding, hashKey) {
  sodium.crypto_generichash(hash, block, hashKey)
  padding.set(hash.subarray(0, 8)) // copy first 8 bytes of hash
  hash.fill(0) // clear nonce buffer
}

function encryptBlock(index, block, id, blockKey, hashKey) {
  const padding = block.subarray(0, AutobaseEncryption.PADDING)
  block = block.subarray(AutobaseEncryption.PADDING)

  blockhash(block, padding, hashKey)
  c.uint32.encode({ start: 4, end: 8, buffer: padding }, id)

  c.uint64.encode({ start: 0, end: 8, buffer: nonce }, index)

  padding[0] = 1 // version in plaintext

  nonce.set(padding, 8, 16)

  // The combination of index, key id, fork id and block hash is very likely
  // to be unique for a given Hypercore and therefore our nonce is suitable
  encrypt(block, nonce, blockKey)
}
const rrp = require('resolve-reject-promise')
const safetyCatch = require('safety-catch')

const SystemView = require('./system.js')
const { EncryptionView } = require('./encryption.js')

const DEFAULT_OP_TIMEOUT = 5_000
const DEFAULT_MIN_FF = 16

module.exports = class FastForward {
  constructor(
    base,
    key,
    {
      timeout = DEFAULT_OP_TIMEOUT,
      verified = true,
      minimum = DEFAULT_MIN_FF,
      force = false,
      length = 0
    } = {}
  ) {
    this.base = base
    this.key = key
    this.timeout = timeout
    this.force = force
    this.core = base.store.get({ key, active: true, encryption: null })
    this.length = length
    this.views = []
    this.encryption = null
    this.entropy = null
    this.indexers = []
    this.minimum = minimum
    this.verified = verified
    this.waiting = new Set()
    this.cores = [this.core]
    this.system = null
    this.destroyed = false
    this.upgrading = null
    this.failed = false
  }

  static MINIMUM = DEFAULT_MIN_FF

  async upgrade() {
    try {
      if (!this.upgrading) this.upgrading = this._upgrade()

      if (!(await this.upgrading)) return null

      return {
        length: this.length,
        force: this.force,
        key: this.key,
        indexers: this.indexers,
        views: this.views,
        entropy: this.entropy,
        manifestVersion: this.core.manifest.version,
        minimum: this.minimum
      }
    } catch (err) {
      safetyCatch(err)
      this.failed = true
      return null
    } finally {
      await this.close()
    }
  }

  _waitForAppend(core) {
    if (core.length > 0) return Promise.resolve()

    const promise = rrp()
    const timeout = setTimeout(this.close.bind(this), this.timeout)
    const wait = { promise, timeout }

    this.waiting.add(wait)

    core.once('append', () => {
      this.waiting.delete(wait)
      clearTimeout(timeout)
      promise.resolve()
    })

    return promise.promise
  }

  _minLength() {
    return this.base.core.length + this.minimum
  }

  async _upgrade() {
    await this.core.ready()

    if (!this.verified) await this._waitForAppend(this.core)
    if (this.destroyed) return false

    if (this.core.manifest.linked) {
      const appends = []

      for (const key of this.core.manifest.linked) {
        const core = this.base.store.get({ key, active: true })
        this.cores.push(core)

        await core.ready()
        appends.push(this._waitForAppend(core))
      }

      await Promise.all(appends)
    }

    if (this.length === 0) this.length = this.core.length

    // note we use the persisted length here, as we might as well continue that work as thats ~ the same length
    if (!this.force && this.length < this._minLength()) return false
    if (this.length === 0) return false

    await this.core.get(this.length - 1, { timeout: this.timeout })
    await this.base.ready()

    if (this.destroyed) return false

    if (this.base.encryptionKey && !(await this._ensureEncryption())) return false
    if (this.destroyed) return false

    this.system = new SystemView(this.core, { checkout: this.length })
    await this.system.ready()
    if (this.destroyed) return false

    this.entropy = this.system.entropy

    if (this.core.manifest.version >= 2) {
      const linked = this.core.manifest.linked

      if (this.system.encryptionLength === 0 || linked === null || !linked.length) {
        return false
      }

      // technically diverges from system.views to include encryption here, but makes sense
      this.views.push({
        key: linked[0],
        length: this.system.encryptionLength
      })
    }

    const promises = []

    // ensure local key is locally available always
    promises.push(this.system.get(this.base.local.key, { timeout: this.timeout }))

    if (this.encryption && this.encryption.core) {
      promises.push(
        this.encryption.core.get(this.system.encryptionLength - 1, { timeout: this.timeout })
      )
    }

    for (const v of this.system.views) {
      this.views.push(v)
      const core = this.base.store.get({ key: v.key, active: true })
      this.cores.push(core)
      if (!v.length) continue // encryption view can be 0 length
      promises.push(core.get(v.length - 1, { timeout: this.timeout }))
    }

    for (const idx of this.system.indexers) {
      this.indexers.push(idx)
      if (idx.length === 0) continue // need to make sure we have manifest...
      const core = this.base.store.get({ key: idx.key, active: true })
      this.cores.push(core)
      promises.push(core.get(idx.length - 1, { timeout: this.timeout }))
      promises.push(this.system.get(idx.key, { timeout: this.timeout }))
    }

    for (const head of this.system.heads) {
      promises.push(this.system.get(head.key, { timeout: this.timeout }))
    }

    await Promise.all(promises)
    if (this.destroyed) return false

    return true
  }

  async _ensureEncryption() {
    // only need encryption if we migrated
    if (this.core.encryption !== null) return true

    const encCore = this.core.manifest.version <= 1 ? null : this.cores[1]

    // expects internal views to be loaded in order
    this.encryption = new EncryptionView(this.base, encCore)
    const encryption = this.encryption.getViewEncryption('_system')

    await this.core.setEncryption(encryption)

    return true
  }

  async close() {
    this.destroyed = true
    for (const { promise, timeout } of this.waiting) {
      clearTimeout(timeout)
      promise.resolve() // silently bail
    }
    if (this.system) await this.system.close()
    if (this.encryption) await this.encryption.close()
    for (const core of this.cores) await core.close()
    await this.core.close()
  }
}
const c = require('compact-encoding')

const SystemView = require('./system.js')
const { EncryptionView } = require('./encryption.js')
const { ManifestData } = require('./messages.js')

module.exports = class Fork {
  constructor(base, store, state, fork) {
    this.base = base
    this.store = store
    this.state = state
    this.indexers = fork.indexers
    this.length = fork.length
    this.system = null
    this.encryption = null
    this.manifests = null
    this.cores = []
  }

  async upgrade() {
    try {
      await this._upgrade()
      return true
    } catch {
      return false
    } finally {
      await this.close()
    }
  }

  async _upgrade() {
    this.manifests = await this.store.getIndexerManifests(this.indexers)

    this.system = new SystemView(await this._get('_system', this.length))
    this.encryption = new EncryptionView(this, await this._get('_encryption', -1))

    await this.system.ready()
    await this.encryption.ready()

    this.entropy = this.system.getEntropy(this.indexers, this.length)
    this.manifestVersion = Math.max(this.system.core.manifest.version, 2) // >= 2 to signal new encryption

    const views = []
    for (const view of this.state.views) {
      const v = this.getViewFromSystem(view, this.system)
      const indexedLength = v ? v.length : 0

      const session = await this._get(view.name, indexedLength)
      await session.ready()

      const next = await this.createView(view.name, session, null)
      await next.ready()

      await this._migrate(view.ref, next, view.core, indexedLength)

      // update key
      views.push({ key: next.key, length: indexedLength })
    }

    // internal views are explicitly migrated

    const entropy = EncryptionView.namespace(this.entropy)
    await this.encryption.update(entropy)

    const system = this.store.getViewByName('_system')
    const encryption = this.store.getViewByName('_encryption')

    const enc = await this.createView('_encryption', this.encryption.core, null)
    await enc.ready()

    await this._migrate(encryption, enc, this.encryption.core, this.encryption.core.length)

    await this.system.fork(this.indexers, this.manifests, this.encryption.core.length, views)

    const sys = await this.createView('_system', this.system.core, [enc.key])
    await sys.ready()

    await this._migrate(system, sys, this.system.core, this.system.core.length)
  }

  async createView(name, core, linked) {
    const prologue = await getPrologue(core)
    const manifestData = this._manifestData(core)

    return this.store.getViewCore(
      this.manifests,
      name,
      prologue,
      this.manifestVersion,
      this.entropy,
      linked,
      manifestData
    )
  }

  async _get(name, length) {
    const core = this.store.get({ name })
    this.cores.push(core)

    if (length !== -1) await core.truncate(length)

    return core
  }

  getViewFromSystem(view) {
    if (view.mappedIndex === -1 || view.mappedIndex >= this.system.views.length) return null
    return this.system.views[view.mappedIndex]
  }

  _manifestData(session) {
    if (session.manifest.version > 1) return session.manifest.userData
    return c.encode(ManifestData, { version: 0, legacyBlocks: session.length })
  }

  async _migrate(ref, next, source, length) {
    if (length > 0) {
      await next.core.copyPrologue(source.state)
    }

    // remake the batch, reset from our prologue in case it replicated inbetween
    // TODO: we should really have an HC function for this
    const batch = next.session({ name: 'batch', overwrite: true, checkout: length })
    await batch.ready()

    if (source !== null) {
      while (batch.length < source.length) {
        await batch.append(await source.get(batch.length))
      }
    }

    await ref.batch.state.moveTo(batch, batch.length)
    await batch.close()

    ref.migrated(this.base, next)
  }

  async close() {
    this.destroyed = true
    if (this.system) await this.system.close()
    if (this.encryption) await this.encryption.close()
    for (const core of this.cores) await core.close()
  }
}

async function getPrologue(core) {
  if (!core.length) return null

  return {
    hash: await core.treeHash(),
    length: core.length
  }
}
const b4a = require('b4a')
const assert = require('nanoassert')

const Clock = require('./clock.js')
const Consensus = require('./consensus.js')
const Topolist = require('./topolist.js')

class Node {
  constructor(writer, length, value, heads, batch, dependencies, optimistic, trace) {
    this.writer = writer
    this.length = length
    this.value = value
    this.heads = heads
    this.actualHeads = heads.slice(0) // TODO: we should remove this and just not mutate heads...

    this.dependents = new Set()
    this.dependencies = dependencies
    this.index = 0 // used by topolist

    this.optimistic = optimistic && batch === 1 && !!value

    this.batch = batch
    this.dropped = 0
    this.trace = trace

    this.clock = new Clock()

    this.yielded = false
    this.yielding = false
  }

  isTail() {
    return this.dependencies.size === this.dropped
  }

  causalDependencies(idx) {
    const order = [this]
    const stack = [this]

    let visited = null

    while (stack.length > 0) {
      const node = stack.pop()
      for (const dep of node.dependencies) {
        if (!dep.writer.isRemoved || dep.writer.isActiveIndexer) continue

        // TODO: replace with "coloring" but ok for now, tiny set
        if (visited === null) visited = new Set()
        if (visited.has(dep)) continue
        visited.add(dep)

        stack.push(dep)
        order.push(dep)
      }
    }

    return order
  }

  clear() {
    this.clock = null
    this.dependents = null
    return this
  }

  reset() {
    this.yielded = false
    this.yielding = false
    for (const dep of this.dependents) dep.dependencies.add(this)
    this.dependents.clear()
  }

  active() {
    for (const dep of this.dependencies) {
      if (dep.yielded) {
        this.dependencies.delete(dep) // nodes might be yielded during buffering
      } else {
        dep.dependents.add(this)
        this.clock.add(dep.clock)
      }
    }

    if (this.writer.isActiveIndexer) this.clock.set(this.writer.core.key, this.length)
  }

  tieBreak(node) {
    return tieBreak(this, node)
  }

  hasDependency(dep) {
    for (const h of this.actualHeads) {
      if (sameNode(h, dep)) return true
    }
    return false
  }

  get ref() {
    return this.writer.core.key.toString('hex').slice(0, 2) + ':' + this.length
  }
}

module.exports = class Linearizer {
  constructor(indexers, { heads = [], writers = new Map() } = {}) {
    this.heads = new Set()
    this.tails = new Set()
    this.tip = new Topolist()
    this.size = 0 // useful for debugging
    this.updated = false
    this.indexersUpdated = false
    this.writers = writers

    this.consensus = new Consensus(indexers)
    this._initialHeads = heads.slice(0)
    this._strictlyAdded = null

    for (const { key, length } of heads) {
      this.consensus.removed.set(key, length)
    }
  }

  get indexers() {
    return this.consensus.indexers
  }

  static createNode(writer, length, value, heads, batch, dependencies, optimistic, trace) {
    return new Node(writer, length, value, heads, batch, dependencies, optimistic, trace)
  }

  // returns the global links of the dag, use this to link against the current state of the dag
  // TODO: rename to heads() and move the sets to _ props
  getHeads() {
    const heads = this._initialHeads.slice(0)
    for (const node of this.heads) heads.push({ key: node.writer.core.key, length: node.length })
    return heads
  }

  // TODO: might contain dups atm, nbd for how we use it, returns an array of writers you can "pull"
  // to get the full dag view at any time
  getBootstrapWriters() {
    const writers = []

    for (const head of this.heads) writers.push(head.writer)
    for (let i = 0; i < this.consensus.indexers.length; i++)
      writers.push(this.consensus.indexers[i])

    return writers
  }

  addHead(node) {
    node.active()

    // 99.99% of the time _initialHeads is empty...
    if (this._initialHeads.length > 0) this._updateInitialHeads(node)

    if (node.isTail()) {
      this.tails.add(node)
    }

    for (const head of this.heads) {
      if (node.hasDependency(head)) {
        this.heads.delete(head)
      }
    }

    this.tip.add(node)
    if (node.writer.isActiveIndexer) this.consensus.addHead(node)

    this.size++
    this.heads.add(node)

    this.updated = true

    return node
  }

  update() {
    if (!this.updated) return null
    this.updated = false

    // get the indexed nodes
    const indexed = []
    while (true) {
      const nodes = this.consensus.shift()
      if (!nodes.length) break

      this._yield(nodes, indexed)
    }

    return this.tip.flush(indexed)
  }

  _updateInitialHeads(node) {
    for (const head of node.actualHeads) {
      for (let i = 0; i < this._initialHeads.length; i++) {
        const { key, length } = this._initialHeads[i]
        if (length !== head.length || !b4a.equals(key, head.key)) continue
        this._initialHeads.splice(i--, 1)
      }
    }
  }

  /* Ack methods */

  shouldAck(writer, pending = false) {
    if (!writer || !writer.isActiveIndexer) return false

    // all indexers have to flushed to the dag before we ack as a quick "debounce"
    for (const w of this.indexers) {
      if (w.length !== w.available) return false
    }

    let isHead = false

    // if ANY head is not an indexer ack
    for (const head of this.heads) {
      if (!head.writer.isActiveIndexer) return true
      if (head.writer === writer) isHead = true
    }

    if (this.heads.size === 1 && isHead) {
      return false // never self-ack!
    }

    const visited = new Set()

    // check if there is non-null value
    let valueCheck = false

    for (const tail of this.tails) {
      if (pending || this._nonNull(tail, visited)) {
        valueCheck = true
        break
      }
    }

    if (!valueCheck) return false

    if (this.consensus.shouldAck(writer)) return true

    return this._shouldAckHeads(writer, pending)
  }

  // check if there is any value above this node
  _nonNull(target, visited) {
    const stack = [target]

    while (stack.length) {
      const node = stack.pop()

      if (visited.has(node)) continue
      if (node.value !== null) return true

      visited.add(node)

      for (const dep of node.dependents) {
        stack.push(dep)
      }
    }

    return false
  }

  // ack if any head is closer to confirming a value
  _shouldAckHeads(writer, pending) {
    const prev = writer.head()

    for (const head of this.heads) {
      // only check other writers heads
      if (head.writer === writer) continue

      const stack = [head]
      const visited = new Set()

      while (stack.length) {
        const node = stack.pop()

        if (visited.has(node)) continue
        visited.add(node)

        if (pending || node.value !== null) {
          const acks = this.consensus.acksFromNode(node, head)
          const prevAcks = this.consensus.acksFromNode(node, prev)

          // head sees more acks
          if (acks.size > prevAcks.size) return true

          for (const idx of acks) {
            // head sees acks that writer does not
            if (!prevAcks.has(idx)) return true
          }

          // both seen, no point going any further down
          if (prevAcks.size && acks.size) continue
        }

        stack.push(...node.dependencies)
      }
    }

    return false
  }

  /* Full DAG methods */

  _yield(nodes, indexed = []) {
    const offset = indexed.length
    const tails = []

    // determine which nodes are yielded
    while (nodes.length) {
      const node = nodes.pop()

      if (node.yielding) continue
      node.yielding = true

      if (node.isTail()) tails.push(node)

      nodes.push(...node.dependencies)
    }

    while (tails.length) {
      let tail = tails.pop()

      for (tail of this._removeBatch(tail)) {
        Topolist.add(tail, indexed, offset)
      }

      for (const dep of tail.dependents) {
        if (dep.isTail() && dep.yielding) tails.push(dep)
      }
    }

    return indexed
  }

  _isFutureTail(node) {
    let dropped = node.dropped

    // a tail has no unyielded dependencies
    for (const dep of node.dependencies) {
      if (dep.yielded) continue
      if (dropped === 0) return false
      dropped--
    }

    return true
  }

  _removeNode(node) {
    this.tails.delete(node)
    this.heads.delete(node)
    this.consensus.remove(node)

    // update the tailset
    for (const d of node.dependents) {
      if (d.yielding && d.dependencies.has(node))
        d.dropped++ // keep links for the indexed batch
      else d.dependencies.delete(node)
      if (this._isFutureTail(d)) this.tails.add(d)
    }

    node.yielded = true
    this.size--

    if (this.heads.size === 0) {
      // in case of a single writer the dag might drain immediately...
      this._initialHeads.push({ key: node.writer.core.key, length: node.length })
    }

    return node
  }

  _removeBatch(node) {
    const batch = [this._removeNode(node)]

    while (node.batch !== 1) {
      // its a batch!
      if (node.dependents.size === 0) {
        // bad batch node, auto correct
        const next = node.writer.get(node.length)
        if (next && next.batch === node.batch - 1) node.dependents.add(next)
      }

      assert(node.dependents.size === 1, 'Batch is linked partially, which is not allowed')

      node = getFirst(node.dependents)
      batch.push(this._removeNode(node))
    }

    return batch
  }
}

function tieBreak(a, b) {
  return Topolist.compare(a, b) < 0 // lowest key wis
}

function getFirst(set) {
  return set[Symbol.iterator]().next().value
}

function sameNode(a, b) {
  return b4a.equals(a.key, b.writer.core.key) && a.length === b.length
}
const c = require('compact-encoding')
const b4a = require('b4a')

const messages = require('./messages.js')

const GTE = b4a.from([messages.LINEARIZER_PREFIX])
const LT = b4a.from([messages.LINEARIZER_PREFIX + 1])

module.exports = class LocalState {
  constructor(core) {
    this.core = core
    this.tx = null
  }

  static clear(core) {
    const tx = core.state.storage.write()
    tx.deleteLocalRange(GTE, LT)
    return tx.flush()
  }

  static async moveTo(src, dst) {
    const boot = await src.getUserData('autobase/boot')
    const tx = dst.state.storage.write()

    tx.deleteLocalRange(GTE, LT)

    for await (const data of src.state.storage.createLocalStream({ gte: GTE, lt: LT })) {
      tx.putLocal(data.key, data.value)
    }

    tx.putUserData('autobase/boot', boot)

    await tx.flush()
  }

  setBootRecord(boot) {
    if (this.tx === null) this.tx = this.core.state.storage.write()

    this.tx.putUserData('autobase/boot', c.encode(messages.BootRecord, boot))
  }

  async listUpdates() {
    const updates = []

    for await (const data of this.core.state.storage.createLocalStream({ gte: GTE, lt: LT })) {
      const update = c.decode(messages.LinearizerUpdate, data.value)
      const seq = c.decode(messages.LinearizerKey, data.key)
      update.seq = seq
      updates.push(update)
    }

    return updates
  }

  clearUpdates() {
    if (this.tx === null) this.tx = this.core.state.storage.write()

    this.tx.deleteLocalRange(GTE, LT)
  }

  deleteUpdate(update) {
    if (this.tx === null) this.tx = this.core.state.storage.write()

    this.tx.deleteLocal(c.encode(messages.LinearizerKey, update.seq))
  }

  insertUpdate(update) {
    if (this.tx === null) this.tx = this.core.state.storage.write()

    this.tx.putLocal(
      c.encode(messages.LinearizerKey, update.seq),
      c.encode(messages.LinearizerUpdate, update)
    )
  }

  flush() {
    if (!this.tx) return Promise.resolve()

    const flushing = this.tx.flush()
    this.tx = null
    return flushing
  }
}
const schema = require('../encoding/spec/autobase')

module.exports = {
  Wakeup: schema.resolveStruct('@autobase/wakeup'),
  Clock: schema.resolveStruct('@autobase/clock'),
  Checkout: schema.resolveStruct('@autobase/checkout'),
  BootRecord: schema.resolveStruct('@autobase/boot-record'),
  OplogMessage: schema.resolveStruct('@autobase/oplog-message'),
  Checkpoint: schema.resolveStruct('@autobase/checkpoint'),
  Info: schema.resolveStruct('@autobase/info'),
  Member: schema.resolveStruct('@autobase/member'),
  ManifestData: schema.resolveStruct('@autobase/manifest-data'),
  LINEARIZER_PREFIX: 1,
  LinearizerKey: schema.resolveStruct('@autobase/linearizer-key'),
  LinearizerUpdate: schema.resolveStruct('@autobase/linearizer-update'),
  EncryptionDescriptor: schema.resolveStruct('@autobase/encryption-descriptor')
}
const DEFAULT_SIZE = 32

module.exports = class NodeBuffer {
  constructor(offset, hwm) {
    this.hwm = hwm || DEFAULT_SIZE
    this.defaultHwm = this.hwm
    this.mask = this.hwm - 1
    this.top = 0
    this.btm = 0
    this.buffer = new Array(this.hwm)
    this.offset = offset || 0
    this.length = this.offset
  }

  get size() {
    return this.length - this.offset
  }

  isEmpty() {
    return this.length === this.offset
  }

  isFull() {
    return this.size === this.buffer.length
  }

  grow() {
    this.hwm <<= 1

    const size = this.size
    const buffer = new Array(this.hwm)
    const mask = this.hwm - 1

    for (let i = 0; i < size; i++) {
      buffer[i] = this.buffer[(this.btm + i) & this.mask]
    }

    this.mask = mask
    this.top = size
    this.btm = 0
    this.buffer = buffer
  }

  push(data) {
    if (this.isFull()) this.grow()

    this.buffer[this.top] = data
    this.top = (this.top + 1) & this.mask

    return this.length++
  }

  shift() {
    if (this.isEmpty()) return null

    const last = this.buffer[this.btm]

    this.buffer[this.btm] = undefined
    this.btm = (this.btm + 1) & this.mask
    this.offset++

    // reset on empty
    if (this.isEmpty() && this.hwm !== this.defaultHwm) {
      this.buffer = new Array(this.defaultHwm)
      this.hwm = this.buffer.length
      this.mask = this.hwm - 1
      this.top = this.btm = 0
    }

    return last
  }

  get(seq) {
    if (seq < this.offset || seq >= this.length) return null
    return this.buffer[(this.btm + (seq - this.offset)) & this.mask]
  }
}
const b4a = require('b4a')
const c = require('compact-encoding')
const crypto = require('hypercore-crypto')
const Hypercore = require('hypercore')

const messages = require('./messages.js')

const DEFAULT_MANIFEST_VERSION = 1
const INDEX_VERSION = 1
const MAX_TRACE_PER_VIEW = 256

const EMPTY = b4a.alloc(0)

const [NS_SIGNER_NAMESPACE] = crypto.namespace('autobase', 1)

class ViewCore {
  constructor(name, core, base) {
    this.name = name
    this.core = null
    this.batch = null
    this.atomicBatch = null
    this.tracer = new Tracer()

    this.migrated(base, core)
  }

  getCore() {
    return this.atomicBatch || this.batch || this.core
  }

  migrated(base, core) {
    // TODO: close old core if present, for now we just close when the autobase is closed indirectly
    // atm its unsafe to do as moveTo has a bug due to a missing read lock in hc
    this.core = core

    if (this.name === '_system') {
      const ff = base._queueFastForward.bind(base)
      this.core.on('append', ff)
      this.core.ready().then(ff, ff)
    }
  }

  async matchesKey(key) {
    if (!this.core.opened) await this.core.ready()
    return b4a.equals(this.core.key, key)
  }

  async matchesNamespace(target) {
    if (!this.core.opened) await this.core.ready()

    if (this.core.manifest && this.core.manifest.signers.length > 0) {
      const ns = this.core.manifest.signers[0].namespace
      if (b4a.equals(ns, target)) return true
    }

    return false
  }

  async commit(atom, length, signature) {
    // TODO: is this how its supposed be done atomic wise?
    await this.core.commit(this._getAtomicBatch(atom), { length, signature })
  }

  async release() {
    if (!this.atomicBatch) return
    await this.atomicBatch.ready()
    const sessions = this.atomicBatch.state.sessions
    for (let i = sessions.length - 1; i >= 0; i--) {
      if (!sessions[i]) break // closed before us
      await sessions[i].close()
    }
    await this.atomicBatch.close()
    this.atomicBatch = null
  }

  _getAtomicBatch(atom) {
    if (this.atomicBatch === null) {
      this.atomicBatch = this.batch.session({ atom, writable: true })
    }

    return this.atomicBatch
  }

  async catchup(atom, length) {
    await this.release()
    const batch = this._getAtomicBatch(atom)
    await batch.ready()
    await batch.state.catchup(length)
  }

  createSession(atom, valueEncoding) {
    if (this.batch === null) {
      this.batch = this.core.session({ name: 'batch', writable: true })
    }

    const s = atom
      ? this._getAtomicBatch(atom).session({
          valueEncoding,
          writable: true,
          onseq: this.tracer.onseq.bind(this.tracer)
        })
      : this.batch.session({ valueEncoding, writable: false })

    return s
  }
}

class Tracer {
  constructor() {
    this.enabled = false
    this.seen = new Set()
  }

  start() {
    this.enabled = true
    this.seen.clear()
  }

  end() {
    this.enabled = false
    return [...this.seen.values()]
  }

  onseq(seq, core) {
    if (!this.enabled) return
    if (this.seen.size >= MAX_TRACE_PER_VIEW) return
    if (seq >= core.signedLength) return

    this.seen.add(seq)
  }
}

class AutoStore {
  constructor(base, byName) {
    this.base = base
    this.store = base.store
    this.byName = byName || new Map()
    this.opened = []
    this.atom = null
    this.local = null
  }

  async close() {
    if (this.local) await this.local.close()
    for (const v of this.byName.values()) {
      await v.release()
    }
    if (this.atom) return
    for (const v of this.byName.values()) {
      if (v.core) await v.core.close()
      if (v.batch) await v.batch.close()
    }
  }

  async export() {
    const views = []

    for (const [name, value] of this.byName) {
      const core = await this.store.storage.export(value.core.discoveryKey, { batches: true })
      views.push({ name, core })
    }

    return views
  }

  atomize() {
    const store = new AutoStore(this.base, this.byName)
    store.atom = this.store.storage.createAtom()
    return store
  }

  async updateLocal() {
    if (!this.local) return
    await this.local.close()
    this.local = null
  }

  flush() {
    return this.atom ? this.atom.flush() : Promise.resolve()
  }

  get(opts, moreOpts) {
    if (typeof opts === 'string') opts = { name: opts }
    if (moreOpts) opts = { ...opts, ...moreOpts }

    const { name, valueEncoding = null } = opts

    if (!name) throw new Error('name is required')
    return this.getViewByName(name).createSession(this.atom, valueEncoding)
  }

  getViewByName(name) {
    let view = this.byName.get(name)

    if (!view) {
      const preload = this._preload(name)
      const core = this.base.store.get({ preload })
      view = new ViewCore(name, core, this.base)
      this.byName.set(name, view)
    }

    if (this.opened.indexOf(view) === -1) {
      this.opened.push(view)
    }

    return view
  }

  getLocal() {
    if (this.local !== null) return this.local

    this.local = this.base.local.session({
      atom: this.atom,
      valueEncoding: messages.OplogMessage,
      encryption: this.base.getWriterEncryption(this.base.local.key),
      active: false
    })

    return this.local
  }

  getViews() {
    return [...this.byName.values()]
  }

  getViewCount() {
    return this.byName.size
  }

  getSystemView() {
    return this.getViewByName('_system')
  }

  getSystemCore() {
    return this.getSystemView().core
  }

  getEncryption(name) {
    if (!this.base.encryptionKey) return null
    if (name === '_encryption') return null

    return this.base.encryption.getViewEncryption(name)
  }

  async getIndexerManifests(entries) {
    const manifests = []
    for (const { key } of entries) {
      const core = this.store.get({ key, active: false })
      await core.ready()
      if (!core.manifest) continue // danger but only for bootstrapping...
      manifests.push(core.manifest)
      await core.close()
    }
    return manifests
  }

  getViewCore(indexerManifests, name, prologue, manifestVersion, entropy, linked, manifestData) {
    const manifest = this._createManifest(
      indexerManifests,
      name,
      prologue,
      manifestVersion,
      entropy,
      linked,
      manifestData
    )

    return this.base.store.get({
      manifest,
      exclusive: false,
      encryption: this.getEncryption(name)
    })
  }

  listViews() {
    const views = []
    for (const [name, ref] of this.byName) {
      const core = ref.batch || ref.core
      views.push({ name, key: core.key, length: core.length, signedLength: core.signedLength })
    }
    return views
  }

  createAnchorCore(prologue, manifestData) {
    const manifest = {
      version: 2,
      hash: 'blake2b',
      prologue,
      allowPatch: false,
      quorum: 0,
      signers: [],
      userData: manifestData,
      linked: null
    }

    const core = this.store.get({
      manifest,
      active: false
    })

    return core
  }

  async createView(
    indexerManifests,
    name,
    prologue,
    manifestVersion,
    entropy,
    linked,
    manifestData
  ) {
    const manifest = this._createManifest(
      indexerManifests,
      name,
      prologue,
      manifestVersion,
      entropy,
      linked,
      manifestData
    )

    const core = this.store.get({
      key: Hypercore.key(manifest),
      manifest,
      active: false
    })

    await core.ready()
    const key = core.key

    await core.close()
    return key
  }

  async _preload(name) {
    await Promise.resolve()

    if (!this.base.opening) throw new Error('Autobase failed to open')
    await this.base._preopen

    const boot = (await this.base._getSystemInfo()) || {
      key: this.getBootstrapSystemKey(),
      indexers: [],
      views: [],
      entropy: null
    }
    const indexerManifests = await this.getIndexerManifests(boot.indexers)

    // no system, everything is fresh
    if (boot.indexers.length === 0) {
      return this._freshCorePreload(indexerManifests, name, boot.entropy)
    }

    // asking for the system, just return it, easy
    if (name === '_system') {
      return {
        key: boot.key,
        exclusive: false,
        encryption: this.getEncryption(name)
      }
    }

    if (name === '_encryption') {
      const core = this.getSystemCore()
      await core.ready()

      if (!core.manifest.linked) {
        return this._freshCorePreload(indexerManifests, name, boot.entropy)
      }

      return {
        key: core.manifest.linked[0],
        exclusive: false,
        encryption: null
      }
    }

    // infer which view
    const v = await this.findViewByName(indexerManifests, boot.views, name, boot.entropy)

    // new view, fresh
    if (v === null) {
      return this._freshCorePreload(indexerManifests, name, boot.entropy)
    }

    return {
      key: v.key,
      exclusive: false,
      encryption: this.getEncryption(name)
    }
  }

  _freshCorePreload(indexerManifests, name, entropy) {
    if (name === '_system') return this._freshSystemPreload(indexerManifests, entropy)

    return {
      manifest: this._createManifest(
        indexerManifests,
        name,
        null,
        DEFAULT_MANIFEST_VERSION,
        entropy,
        [],
        null
      ),
      exclusive: true,
      encryption: this.getEncryption(name)
    }
  }

  _freshSystemPreload(indexerManifests, entropy) {
    return {
      manifest: this._createSystemManifest(
        indexerManifests,
        null,
        DEFAULT_MANIFEST_VERSION,
        entropy,
        null
      ),
      exclusive: true,
      encryption: this.getEncryption('_system')
    }
  }

  _deriveStaticHash(name) {
    // key doesnt matter...
    return crypto.hash([this.base.key, b4a.from(name)])
  }

  _deriveNamespace(name, entropy) {
    const encryptionId = crypto.hash(this.base.encryptionKey || EMPTY)
    const version = c.encode(c.uint, INDEX_VERSION)
    const bootstrap = this.base.key

    return crypto.hash([
      NS_SIGNER_NAMESPACE,
      version,
      bootstrap,
      encryptionId,
      entropy,
      b4a.from(name)
    ])
  }

  async _getCoreManifest(key) {
    const core = this.store.get({ key, active: false })
    await core.ready()
    const manifest = core.manifest
    await core.close()
    return manifest
  }

  getBootstrapSystemKey() {
    return Hypercore.key(this._createSystemManifest([], null, DEFAULT_MANIFEST_VERSION, null))
  }

  async findViewByKey(key, indexers, manifestVersion, entropy) {
    for (const v of this.byName.values()) {
      if (await v.matchesKey(key)) return v
    }

    const manifest = await this._getCoreManifest(key)
    const target = manifest && manifest.signers.length ? manifest.signers[0].namespace : null

    if (target) {
      for (const v of this.byName.values()) {
        if (await v.matchesNamespace(target)) return v
      }
    }

    const indexerManifests = await this.getIndexerManifests(indexers)

    if (entropy === null) {
      entropy = indexerManifests[0].signers[0].namespace
    }

    if (target) {
      for (const v of this.byName.values()) {
        const namespace = this._deriveNamespace(v.name, entropy)
        if (b4a.equals(namespace, target)) return v
      }
    }

    // prob the empty prologue, we dont have manifest for those in FF if len=0
    for (const v of this.byName.values()) {
      const manifestData = v.core ? v.core.manifest.userData : null
      const manifest = this._createManifest(
        indexerManifests,
        v.name,
        null,
        manifestVersion,
        entropy,
        [],
        manifestData
      )
      const manifestKey = Hypercore.key(manifest)

      if (!b4a.equals(manifestKey, key)) continue

      // we didnt have the core! this is because it is empty, ensure its on disk
      const core = this.store.get({ key: manifestKey, manifest, active: false })
      await core.ready()
      await core.close()

      return v
    }

    return null
  }

  async findViewByName(indexerManifests, views, name, entropy) {
    if (indexerManifests.length === 0) return null

    if (entropy === null) entropy = indexerManifests[0].signers[0].namespace
    const namespace = this._deriveNamespace(name, entropy)

    for (const v of views) {
      const manifest = await this._getCoreManifest(v.key)
      if (manifest.signers.length === 0) continue

      const signer = manifest.signers[0]

      if (b4a.equals(signer.namespace, namespace)) return v
    }

    return null
  }

  _createManifest(indexerManifests, name, prologue, version, entropy, linked, manifestData) {
    if (!indexerManifests.length) {
      prologue = {
        hash: this._deriveStaticHash(name),
        length: 0
      }
    } else if (prologue && prologue.length === 0) {
      // just in case
      prologue = null
    }

    const signers = []

    if (indexerManifests.length && entropy === null) {
      entropy = indexerManifests[0].signers[0].namespace
    }

    for (const manifest of indexerManifests) {
      const signer = manifest.signers[0]

      signers.push({
        namespace: this._deriveNamespace(name, entropy),
        signature: 'ed25519',
        publicKey: signer.publicKey
      })
    }

    return {
      version,
      hash: 'blake2b',
      prologue,
      allowPatch: true,
      quorum: Math.min(signers.length, (signers.length >> 1) + 1),
      signers,
      userData: manifestData,
      linked: version > 1 ? linked : null
    }
  }

  _createSystemManifest(indexerManifests, prologue, version, entropy, manifestData) {
    if (prologue !== null) throw new Error('Can only derive fresh system core')

    const linked = []

    if (version > DEFAULT_MANIFEST_VERSION) {
      const encManifest = this._createManifest(
        indexerManifests,
        '_encryption',
        null,
        version,
        entropy,
        [],
        manifestData
      )
      linked.push(Hypercore.key(encManifest))
    }

    return this._createManifest(
      indexerManifests,
      '_system',
      null,
      version,
      entropy,
      linked,
      manifestData
    )
  }
}

module.exports = AutoStore
const Hyperbee = require('hyperbee')
const SubEncoder = require('sub-encoder')
const ReadyResource = require('ready-resource')
const b4a = require('b4a')
const c = require('compact-encoding')
const crypto = require('hypercore-crypto')

const { Info, Member } = require('./messages.js')
const { DEFAULT_AUTOBASE_VERSION } = require('./caps.js')

const subs = new SubEncoder()

const DIGEST = subs.sub(b4a.from([0]))
const MEMBERS = subs.sub(b4a.from([1]))

const INFO_KEY = DIGEST.encode('info')

const [GENESIS_ENTROPY, NS_ENTROPY] = crypto.namespace('autobase/entropy', 2)

module.exports = class SystemView extends ReadyResource {
  constructor(core, { checkout = 0, empty = false } = {}) {
    super()

    this.core = core

    // sessions is a workaround for batches not having sessions atm...
    this.db = new Hyperbee(core, {
      keyEncoding: 'binary',
      extension: false,
      checkout,
      sessions: typeof core.session === 'function'
    })

    this.version = DEFAULT_AUTOBASE_VERSION
    this.members = 0
    this.pendingIndexers = []
    this.indexers = []
    this.heads = []
    this.views = []
    this.encryptionLength = 0
    this.entropy = null

    this.indexerUpdate = false

    this._empty = empty
    this._fork = 0
    this._length = 0
    this._changes = []
    this._indexerMap = new Map()
    this._clockUpdates = new Map()
  }

  static GENESIS_ENTROPY = GENESIS_ENTROPY

  static async getIndexedInfo(core, length) {
    const sys = new this(core.session())

    try {
      return await sys.getIndexedInfo(length)
    } finally {
      await sys.close()
    }
  }

  static async *flushes(core, { reverse, lt = core.length, gte = 0, wait = true } = {}) {
    if (lt <= 0) return

    // ensure block
    await core.get(lt - 1)
    const sys = new SystemView(core)

    try {
      for await (const data of sys.db.createHistoryStream({ lt, gte, wait, reverse: true })) {
        if (!b4a.equals(data.key, INFO_KEY)) continue
        const info = c.decode(Info, data.value)
        yield { length: data.seq + 1, info }
      }
    } finally {
      await sys.close()
    }
  }

  get bootstrapping() {
    return this.members === 0
  }

  async checkout(length) {
    const checkout = new SystemView(this.core.session(), {
      checkout: length,
      empty: length === 0
    })

    await checkout.ready()

    return checkout
  }

  async _open() {
    // this should NEVER fail, if so we have a bug elsewhere (should always be consistent)
    const info = this._empty
      ? null
      : await this.db.get('info', {
          valueEncoding: Info,
          keyEncoding: DIGEST,
          update: false,
          wait: false
        })
    await this._reset(info)
  }

  async _close() {
    await this.db.close()
  }

  async getIndexedInfo(length = this.core.signedLength) {
    if (this.opened === false) await this.ready()

    if (length === this.core.length) {
      return {
        version: this.version,
        members: this.members,
        pendingIndexers: this.pendingIndexers,
        indexers: this.indexers,
        heads: this.heads,
        views: this.views,
        encryptionLength: this.encryptionLength,
        entropy: this.entropy
      }
    }

    const node = length === 0 ? null : await this.db.getBySeq(length - 1)
    if (node === null) {
      return {
        version: DEFAULT_AUTOBASE_VERSION,
        members: 0,
        pendingIndexers: [],
        indexers: [],
        heads: [],
        views: [],
        encryptionLength: 0,
        entropy: null
      }
    }

    return c.decode(Info, node.value)
  }

  static sameIndexers(a, b) {
    if (a.length !== b.length) return false

    for (let i = 0; i < a.length; i++) {
      if (!b4a.equals(a[i].key, b[i].core.key)) return false
    }

    return true
  }

  sameIndexers(indexers) {
    return SystemView.sameIndexers(this.indexers, indexers)
  }

  async history(since) {
    const checkout = this.db.checkout(since)
    const seen = new Map()

    const nodes = []
    const updates = []

    const checkoutNode = since === 0 ? null : await this.db.getBySeq(since - 1)

    let prevInfo = checkoutNode === null ? null : c.decode(Info, checkoutNode.value)
    let updateBatch = 0

    for await (const data of this.db.createHistoryStream({ gte: since })) {
      if (b4a.equals(data.key, INFO_KEY)) {
        const info = c.decode(Info, data.value)

        updates.push({
          batch: updateBatch,
          indexers: prevInfo === null || !sameIndexers(info, prevInfo),
          systemLength: data.seq + 1
        })

        prevInfo = info
        updateBatch = 0
        continue
      }

      const key = data.key.subarray(2)
      const hex = b4a.toString(key, 'hex')
      const len = c.decode(Member, data.value).length

      if (!seen.has(hex)) {
        const node = await checkout.get(data.key)
        if (node === null) {
          seen.set(hex, 0)
        } else {
          const { length } = c.decode(Member, node.value)
          seen.set(hex, length)
        }
      }

      const prev = seen.get(hex)
      const batch = len - prev
      seen.set(hex, len)

      if (batch === 0) continue

      updateBatch += batch

      if (nodes.length > 0) {
        const top = nodes[nodes.length - 1]
        if (b4a.equals(top.key, key)) {
          top.length = len
          top.batch += batch
          continue
        }
      }

      nodes.push({
        key,
        length: len,
        batch
      })
    }

    await checkout.close()

    return { updates, nodes }
  }

  async update() {
    if (this.opened === false) await this.ready()

    if (this._fork === this.core.fork && this._length === this.core.length) return false

    await this._reset(
      this._empty ? null : await this.db.get('info', { valueEncoding: Info, keyEncoding: DIGEST })
    )
    return true
  }

  async _reset(info) {
    this.version = info === null ? DEFAULT_AUTOBASE_VERSION : info.value.version
    this.members = info === null ? 0 : info.value.members
    this.pendingIndexers = info === null ? [] : info.value.pendingIndexers
    this.indexers = info === null ? [] : info.value.indexers
    this.heads = info === null ? [] : info.value.heads
    this.views = info === null ? [] : info.value.views
    this.encryptionLength = info === null ? 0 : info.value.encryptionLength
    this.entropy = info === null ? null : info.value.entropy

    this.indexerUpdate = false
    this._indexerMap.clear()
    this._clockUpdates.clear()
    this._length = this.core.length
    this._fork = this.core.fork
    this._changes = []

    for (const idx of this.indexers) {
      this._indexerMap.set(b4a.toString(idx.key, 'hex'), idx)
    }
  }

  // helper used by apply-state
  flushLength() {
    const header = this.core.length ? 0 : 1 // account for header

    let updates = this._changes.length + this._clockUpdates.size
    for (const { key } of this._changes) {
      const hex = b4a.toString(key, 'hex')
      if (this._clockUpdates.has(hex)) updates-- // shared are skipped
    }

    return header + this.core.length + updates + 1
  }

  async flush(views) {
    const batch = this.db.batch({ update: false })

    for (let i = 0; i < this._changes.length; i++) {
      const c = this._changes[i]
      if (this._clockUpdates.has(b4a.toString(c.key, 'hex'))) continue
      await batch.put(c.key, c.value, { valueEncoding: Member, keyEncoding: MEMBERS })
    }

    for (const [hex, length] of this._clockUpdates) {
      const isIndexer = this._indexerMap.get(hex) !== undefined
      const key = b4a.from(hex, 'hex')

      const info = await this._get(key, 0)

      const value = { isIndexer, isRemoved: info ? info.isRemoved : true, length }
      await batch.put(key, value, { valueEncoding: Member, keyEncoding: MEMBERS })
    }

    if (this.indexerUpdate) {
      this._refreshEntropy(this.core.length + batch.length + 1)
    }

    this._clockUpdates.clear()

    let maxIndex = -1
    for (const view of views) {
      if (view.mappedIndex > maxIndex) maxIndex = view.mappedIndex
    }

    while (this.views.length > maxIndex + 1) this.views.pop()

    for (const view of views) {
      const length = view.core ? view.core.length : view.length
      if (!length) continue

      const v = { key: view.key, length }

      if (view.mappedIndex !== -1) {
        this.views[view.mappedIndex] = v
      } else {
        view.mappedIndex = this.views.push(v) - 1
      }
    }

    const info = {
      version: this.version,
      members: this.members,
      pendingIndexers: this.pendingIndexers,
      indexers: this.indexers,
      heads: this.heads,
      views: this.views,
      encryptionLength: this.encryptionLength,
      entropy: this.entropy
    }

    await batch.put('info', info, { valueEncoding: Info, keyEncoding: DIGEST })
    await batch.flush()

    this._length = this.core.length // should be ok
    this._changes = []

    if (this.indexerUpdate) this.indexerUpdate = false
  }

  getEntropy(indexers, length) {
    return generateEntropy(indexers, length, this.entropy)
  }

  _refreshEntropy(length) {
    if (this.version < 2) return
    this.entropy = this.getEntropy(this.indexers, length)
  }

  checkpoint() {
    return {
      version: this.version,
      members: this.members,
      pendingIndexers: this.pendingIndexers.slice(0),
      indexers: cloneNodes(this.indexers),
      heads: cloneNodes(this.heads),
      views: cloneNodes(this.views),
      indexerUpdate: this.indexerUpdate,
      encryptionLength: this.encryptionLength,
      entropy: this.entropy ? b4a.from(this.entropy) : null,
      changes: this._changes.slice(0),
      clockUpdates: new Map([...this._clockUpdates])
    }
  }

  applyCheckpoint(checkpoint) {
    this.version = checkpoint.version
    this.members = checkpoint.members
    this.pendingIndexers = checkpoint.pendingIndexers
    this.indexers = checkpoint.indexers
    this.heads = checkpoint.heads
    this.views = checkpoint.views
    this.indexerUpdate = checkpoint.indexerUpdate
    this.encryptionLength = checkpoint.encryptionLength
    this.entropy = checkpoint.entropy

    this._changes = checkpoint.changes
    this._clockUpdates = checkpoint.clockUpdates

    this._indexerMap = new Map()
    for (const idx of this.indexers) {
      this._indexerMap.set(b4a.toString(idx.key, 'hex'), idx)
    }
  }

  addHead(node) {
    const h = { key: node.writer.core.key, length: node.length }

    for (let i = 0; i < this.heads.length; i++) {
      const head = this.heads[i]

      if (!hasDependency(node, head)) {
        if (!b4a.equals(node.writer.core.key, head.key)) continue

        // todo: remove in next major because bug was fixed here:
        // https://github.com/holepunchto/autobase-next/pull/237

        // filter out any bad heads introduced by a bug to
        // prevent inconsistencies being written to the oplog
        if (head.length > h.length) return false
      }

      const popped = this.heads.pop()
      if (popped !== head) this.heads[i--] = popped
    }

    this.heads.push(h)

    const hex = b4a.toString(h.key, 'hex')

    this._clockUpdates.set(hex, h.length)

    if (this.pendingIndexers.length > 0) {
      for (let i = 0; i < this.pendingIndexers.length; i++) {
        if (!b4a.equals(this.pendingIndexers[i], h.key)) continue
        this._updateIndexer(h.key, h.length, true, i)
        return true
      }
    }

    const idx = this._indexerMap.get(hex)
    if (idx !== undefined) {
      idx.length = h.length
    }

    return false
  }

  _updateIndexer(key, length, isIndexer, i) {
    const hex = b4a.toString(key, 'hex')

    if (!isIndexer) {
      const existing = this._indexerMap.get(hex)
      if (existing) {
        this.indexerUpdate = true
        this.indexers.splice(this.indexers.indexOf(existing), 1)
        this._indexerMap.delete(hex)
      }
      return
    }

    for (; i < this.pendingIndexers.length; i++) {
      if (b4a.equals(this.pendingIndexers[i], key)) {
        break
      }
    }

    if (length === 0) {
      if (i >= this.pendingIndexers.length) this.pendingIndexers.push(key)
      return
    }

    if (i < this.pendingIndexers.length) {
      const top = this.pendingIndexers.pop()
      if (i < this.pendingIndexers.length) this.pendingIndexers[i] = top
    }

    const idx = this._indexerMap.get(hex)

    if (idx === undefined) {
      const newIdx = { key, length }
      this._indexerMap.set(hex, newIdx)
      this.indexers.push(newIdx)

      // bootstrap is "silently" added so that initial views have no prologue
      if (!this.bootstrapping) this.indexerUpdate = true
    } else {
      idx.length = length
    }
  }

  _seenLength(key) {
    return this._clockUpdates.get(b4a.toString(key, 'hex')) || 0
  }

  async ack(key) {
    const value = await this._get(key, 0)
    const length = this._seenLength(key)
    if (value && value.length === length) return

    const isIndexer = value ? value.isIndexer : false
    const isRemoved = value ? value.isRemoved : true

    this._changes.push({ key, value: { isIndexer, isRemoved, length } })
  }

  async add(key, { isIndexer = false, length = this._seenLength(key) } = {}) {
    let value = null
    let found = false
    let changed = true

    for (let i = this._changes.length - 1; i >= 0; i--) {
      const c = this._changes[i]
      if (b4a.equals(key, c.key)) {
        value = c.value
        found = true
        break
      }
    }

    if (!found) {
      const node = await this.db.get(key, { valueEncoding: Member, keyEncoding: MEMBERS })

      if (node) {
        value = node.value
        found = true
      }
    }

    let wasTracked = false
    let wasIndexer = false

    if (found) {
      if (!value.isRemoved) wasTracked = true
      if (value.isIndexer) wasIndexer = true
      if (length < value.length) length = value.length
      if (value.length === length && value.isIndexer === isIndexer && value.isRemoved === false)
        changed = false
    }

    if (changed) {
      this._changes.push({ key, value: { isIndexer, isRemoved: false, length } })
    }

    if (!wasTracked) this.members++
    if (wasIndexer || isIndexer) this._updateIndexer(key, length, isIndexer, 0)
  }

  async remove(key) {
    let isIndexer = false

    for (const idx of this.indexers) {
      isIndexer = b4a.equals(idx.key, key)
      if (isIndexer) break
    }

    if (isIndexer) this._updateIndexer(key, null, false, 0)

    let value = null
    let found = false

    for (let i = this._changes.length - 1; i >= 0; i--) {
      const c = this._changes[i]
      if (b4a.equals(key, c.key)) {
        value = c.value
        found = true
        break
      }
    }

    if (!found) {
      const node = await this.db.get(key, { valueEncoding: Member, keyEncoding: MEMBERS })
      if (node) {
        value = node.value
        found = true
      }
    }

    const changed = !found || !value.isRemoved
    const wasTracked = found && !value.isRemoved
    const length = found ? value.length : 0

    if (changed) {
      this._changes.push({ key, value: { isIndexer: false, isRemoved: true, length } })
    }

    if (wasTracked) this.members--

    return isIndexer
  }

  async linkable(key, length) {
    const len = this._seenLength(key)
    if (len > 0) return length > len

    const info = await this._get(key, 0)
    const prevLength = info ? info.length : 0

    return length > prevLength
  }

  async has(key, opts) {
    // could be optimised...
    return (await this.get(key, opts)) !== null
  }

  async _get(key, timeout) {
    let value = null
    let found = false

    for (let i = this._changes.length - 1; i >= 0; i--) {
      const c = this._changes[i]
      if (b4a.equals(key, c.key)) {
        value = c.value
        found = true
        break
      }
    }

    if (!found) {
      const node = await this.db.get(key, { timeout, valueEncoding: Member, keyEncoding: MEMBERS })
      if (node) {
        value = node.value
        found = true
      }
    }

    return found ? value : null
  }

  async get(key, opts = {}) {
    if (this._empty) return null

    let value = await this._get(key, opts.timeout || 0)

    if (opts.unflushed) {
      const hex = b4a.toString(key, 'hex')

      for (let i = this._changes.length - 1; i >= 0; i--) {
        const c = this._changes[i]
        if (b4a.equals(c.key, key)) value = c.value
      }

      if (this._clockUpdates.has(hex)) value.length = this._clockUpdates.get(hex)
    }

    return opts.onlyActive !== false || !value.isRemoved ? value : null
  }

  async hasLocal(key) {
    if (this._empty) return false
    try {
      const node = await this.db.get(key, {
        valueEncoding: Member,
        keyEncoding: MEMBERS,
        update: false,
        wait: false
      })
      return node !== null
    } catch {
      return false
    }
  }

  async getLocalLength(key) {
    if (this._empty) return 0
    try {
      const node = await this.db.get(key, {
        valueEncoding: Member,
        keyEncoding: MEMBERS,
        update: false,
        wait: false
      })
      return node === null ? 0 : node.value.length
    } catch {
      return 0
    }
  }

  list() {
    // note, NOT safe to use during apply atm
    return this.db.createReadStream({
      valueEncoding: Member,
      keyEncoding: MEMBERS
    })
  }

  async isIndexed(key, length) {
    const co = this.db.checkout(this.core.indexedLength)
    try {
      const node = await co.get(key, { valueEncoding: Member, keyEncoding: MEMBERS })
      return node !== null && node.value.length >= length
    } finally {
      await co.close()
    }
  }

  async fork(indexers, manifests, encryptionLength, views) {
    this.indexers = []
    this.pendingIndexers = []
    this.views = views

    if (this.version < 2) this.version = 2 // bump to enable entropy

    for (const { key } of indexers) {
      // todo: probably need to check isRemoved etc
      const { length } = await this.get(key)
      this.indexers.push({ key, length })
    }

    this._refreshEntropy(this.core.length)

    const blocks = []

    // truncate needs to respect hypercore batch
    let truncate = this.core.length - 1
    for await (const data of this.db.createHistoryStream({ lt: truncate, reverse: true })) {
      if (b4a.equals(data.key, INFO_KEY)) {
        truncate = data.seq + 1
        break
      }

      blocks.push(data)
    }

    await this.core.truncate(truncate, this.core.fork)

    const batch = this.db.batch({ update: false })
    await batch.ready()

    for (const { key, value } of blocks) {
      await batch.put(key, value)
    }

    const info = {
      version: this.version,
      members: this.members,
      pendingIndexers: this.pendingIndexers,
      indexers: this.indexers,
      heads: this.heads,
      views: this.views,
      encryptionLength,
      entropy: this.entropy
    }

    await batch.put('info', info, { valueEncoding: Info, keyEncoding: DIGEST })
    await batch.flush()

    await this._reset(await this.db.get('info', { valueEncoding: Info, keyEncoding: DIGEST }))

    return truncate
  }
}

function hasDependency(node, dep) {
  for (const h of node.actualHeads) {
    if (sameNode(h, dep)) return true
  }
  return false
}

function sameNode(a, b) {
  return b4a.equals(a.key, b.key) && a.length === b.length
}

function sameIndexers(a, b) {
  if (a.views.length > 0 && b.views.length > 0) return b4a.equals(a.views[0].key, b.views[0].key)
  if (a.indexers.length !== b.indexers.length) return false

  for (let i = 0; i < a.indexers.length; i++) {
    if (!b4a.equals(a.indexers[i].key, b.indexers[i].key)) return false
  }

  return true
}

function cloneNodes(arr) {
  const c = []
  for (let i = 0; i < arr.length; i++) {
    c.push({ key: arr[i].key, length: arr[i].length })
  }
  return c
}

function generateEntropy(indexers, length, entropy) {
  const buffer = b4a.alloc(8 + 32 + indexers.length * 32)
  const state = { start: 0, end: buffer.byteLength, buffer }

  c.uint64.encode(state, length)
  c.fixed32.encode(state, entropy || GENESIS_ENTROPY)
  for (const { key } of indexers) c.fixed32.encode(state, key)

  return crypto.hash([NS_ENTROPY, buffer])
}
const safetyCatch = require('safety-catch')

const MAX_WAIT = 2 * 60 * 1000
const DEFAULT_INTERVAL = 10 * 1000

module.exports = class Timer {
  constructor(handler, interval, opts = {}) {
    this.handler = handler || noop
    this.interval = interval || DEFAULT_INTERVAL
    this.limit = opts.limit || MAX_WAIT

    this._executing = null

    this._limit = random2over1(this.limit)
    this._timer = null
    this._resolve = null
    this._start = 0
    this._stopped = false
    this._asap = false
    this._standalone = new Set()

    this._unref = opts.unref !== false
    this._timerCallback = this._executeBackground.bind(this)
  }

  _executeBackground() {
    this._executing = this._execute()
    this._executing.catch(safetyCatch) // make sure it doesnt crash in the bg
  }

  async _execute() {
    this._asap = false
    await this.handler()
    this._start = 0
    this._executing = null
    this.bump()
  }

  bump() {
    if (this._stopped || this._executing || this._asap) return

    if (!this._start) this._start = Date.now()
    else if (Date.now() - this._start > this._limit) return

    const interval = random2over1(this.interval)

    clearTimeout(this._timer)
    this._timer = setTimeout(this._timerCallback, interval)
    if (this._unref && this._timer.unref) this._timer.unref()
  }

  async trigger() {
    if (this._stopped) return
    if (this._executing) await this._executing
    if (this._stopped) return

    clearTimeout(this._timer)
    this._timer = null

    this._executeBackground()
    await this._executing
  }

  async flush() {
    if (this._executing) await this._executing
  }

  // business-as-usual
  bau() {
    if (!this._asap) return
    this._asap = false
    this.bump()
  }

  asap() {
    if (this._asap) return
    this._asap = true

    const interval = Math.floor((Math.random() * this.interval) / 3)
    clearTimeout(this._timer)
    this._timer = setTimeout(this._timerCallback, interval)
    if (this._unref && this._timer.unref) this._timer.unref()
  }

  stop() {
    if (this._timer) clearTimeout(this._timer)
    this._timer = null
    this._start = 0
    this._asap = false
    this._stopped = true

    for (const { timer, resolve } of this._standalone) {
      clearTimeout(timer)
      resolve()
    }

    this._standalone.clear()
  }

  asapStandalone() {
    const interval = Math.floor((Math.random() * this.interval) / 3)
    return new Promise((resolve) => {
      const ref = { timer: null, resolve }
      ref.timer = setTimeout(resolveStandalone, interval, ref, this._standalone)
      if (ref.timer.unref) ref.timer.unref()
      this._standalone.add(ref)
    })
  }

  unref() {
    if (this._timer && this._timer.unref) this._timer.unref()
  }
}

function resolveStandalone(ref, set) {
  set.delete(ref)
  ref.resolve()
}

// random value x between n <= x < 2n
function random2over1(n) {
  return Math.floor(n + Math.random() * n)
}

function noop() {}
const b4a = require('b4a')
const assert = require('nanoassert')

module.exports = class TopoList {
  constructor() {
    this.tip = []
    this.undo = 0
    this.shared = 0
  }

  static compare(a, b) {
    return cmp(a, b)
  }

  static add(node, indexed, offset) {
    addSorted(node, indexed, offset)
  }

  mark() {
    this.shared = this.tip.length
    this.undo = 0
  }

  // todo: bump to new api that just tracks undo
  flush(indexed = []) {
    if (indexed.length) this._applyIndexed(indexed)

    const u = {
      shared: this.shared,
      undo: this.undo,
      length: indexed.length + this.tip.length,
      indexed,
      tip: this.tip
    }

    this.mark()

    return u
  }

  print() {
    return this.tip.map((n) => n.writer.core.key.toString() + n.length)
  }

  _applyIndexed(nodes) {
    assert(nodes.length <= this.tip.length, 'Indexed batch cannot exceed tip')

    let shared = 0

    for (; shared < nodes.length; shared++) {
      if (this.tip[shared] !== nodes[shared]) break
    }

    // reordering
    if (shared < nodes.length) this._track(shared)

    const tip = []

    for (let i = shared; i < this.tip.length; i++) {
      const node = this.tip[i]
      if (node.yielded) continue
      const s = addSorted(node, tip, 0)
      if (s === tip.length - 1) continue
      this._track(shared + s)
    }

    this.tip = tip
  }

  add(node) {
    const shared = addSorted(node, this.tip, 0)
    this._track(shared)
  }

  _track(shared) {
    if (shared < this.shared) {
      this.undo += this.shared - shared
      this.shared = shared
    }
  }
}

function hasOptimisticDeps(node) {
  for (const d of node.dependencies) {
    if (d.optimistic) return true
  }
  return false
}

// this can be optimised to only return optimistic nodes that we will "touch" when moving down the node
function getOptimisticDeps(node, list) {
  const deps = new Set()
  const stack = [node]

  while (stack.length > 0) {
    const next = stack.pop()
    if (deps.has(next)) continue
    if (next.optimistic && next !== node) deps.add(next)
    for (const d of next.dependencies) {
      if (d.optimistic) stack.push(d)
    }
  }

  const result = []

  for (let i = list.length - 1; deps.size !== result.length && i >= 0; i--) {
    const n = list[i]
    if (deps.has(n)) result.push(n)
  }

  return result.reverse()
}

function addSortedOptimistic(node, list, offset) {
  const opt = getOptimisticDeps(node, list)
  const pos = new Uint32Array(opt.length)

  for (let i = 0; i < opt.length; i++) {
    const n = opt[i]
    pos[i] = n.index
    moveDown(n, list, offset)
  }

  moveDownAndUp(node, list, offset)

  for (let i = opt.length - 1; i >= 0; i--) {
    moveOptimisticUp(opt[i], list, offset)
  }

  let shared = node.index

  for (let i = 0; i < opt.length; i++) {
    const idx = pos[i]
    const n = opt[i]

    if (idx === n.index) continue

    if (idx < shared) shared = idx
    if (n.index < shared) shared = n.index
  }

  return shared
}

function addSorted(node, list, offset) {
  list.push(node)
  node.index = list.length - 1

  // "slow" path, 1% of nodes
  if (hasOptimisticDeps(node)) return addSortedOptimistic(node, list, offset)

  moveDownAndUp(node, list, offset)
  return node.index
}

function moveDown(node, list, offset) {
  while (node.index > offset) {
    const prev = list[node.index - 1]
    if (links(node, prev)) break
    list[(prev.index = node.index)] = prev
    list[--node.index] = node
  }
}

function moveOptimisticUp(node, list, offset) {
  // if optimistic move all the way up
  while (node.index < list.length - 1) {
    const next = list[node.index + 1]
    if (links(next, node)) break
    list[(next.index = node.index)] = next
    list[++node.index] = node
  }

  // stable sort in case multiple children
  while (node.index > offset) {
    const prev = list[node.index - 1]
    if (!prev.optimistic) break
    const c = cmp(prev, node)
    if (c <= 0) break
    list[(prev.index = node.index)] = prev
    list[--node.index] = node
  }
}

function moveNonOptimisticUp(node, list, offset) {
  // stable sort against next non optimistic node
  while (node.index < list.length - 1) {
    const next = list[node.index + 1]
    const c = cmpNonOptimistic(node, next, list)
    if (c <= 0) break
    list[(next.index = node.index)] = next
    list[++node.index] = node
  }
}

function moveDownAndUp(node, list, offset) {
  moveDown(node, list, offset)

  if (node.optimistic) moveOptimisticUp(node, list, offset)
  else moveNonOptimisticUp(node, list, offset)
}

function links(a, b) {
  if (b.dependents.has(a)) return true
  return a.length > 0 && b.length === a.length - 1 && a.writer === b.writer
}

function cmpNonOptimistic(a, b, list) {
  if (!b.optimistic) return cmp(a, b)

  for (let i = b.index + 1; i < list.length; i++) {
    const node = list[i]
    if (!node.optimistic) return cmp(a, node)
  }

  return -1
}

function cmp(a, b) {
  return links(b, a) ? -1 : cmpUnlinked(a, b)
}

function cmpUnlinked(a, b) {
  const c = b4a.compare(a.writer.core.key, b.writer.core.key)
  return c === 0 ? (a.length < b.length ? -1 : 1) : c
}
module.exports = class UpdateChanges {
  constructor(base) {
    this.base = base
    this.byName = new Map()
    this.tracking = null
  }

  get discoveryKey() {
    return this.base.discoveryKey
  }

  get key() {
    return this.base.key
  }

  get id() {
    return this.base.id
  }

  get system() {
    return this.base.system
  }

  track(state) {
    this.tracking = []
    this.byName.clear()

    if (!state) return

    for (const v of state.views) {
      if (v.ref) this._add(v.ref)
    }

    this._add(state.systemView.ref)
    this._add(state.encryptionView.ref)
  }

  _add(ref) {
    this.tracking.push({ ref, from: ref.atomicBatch ? ref.atomicBatch.length : 0 })
  }

  finalise() {
    if (this.tracking === null) return

    for (const { ref, from } of this.tracking) {
      const core = ref.atomicBatch || ref.batch
      const trunc = ref.atomicBatch ? ref.atomicBatch.state.lastTruncation : null

      this.byName.set(ref.name, {
        from,
        to: core ? core.length : from,
        shared: trunc ? trunc.to : from
      })
    }

    this.tracking = null
  }

  get(name) {
    return this.byName.get(name)
  }
}
const crypto = require('hypercore-crypto')
const c = require('compact-encoding')
const b4a = require('b4a')

const { OPLOG_VERSION } = require('./caps.js')
const { OplogMessage } = require('./messages.js')
const { EncryptionView } = require('./encryption.js')

module.exports = {
  encodeValue,
  decodeValue
}

function encodeValue(value, opts = {}) {
  const state = { start: 0, end: 0, buffer: null }

  const message = {
    version: opts.version || OPLOG_VERSION,
    digest: null,
    checkpoint: null,
    optimistic: !!opts.optimistic,
    node: {
      heads: opts.heads || [],
      batch: 1,
      value
    }
  }

  OplogMessage.preencode(state, message)

  if (opts.padding) {
    state.start = opts.padding
    state.end += opts.padding
  }

  state.buffer = b4a.alloc(state.end)

  OplogMessage.encode(state, message)

  if (!opts.encrypted) return state.buffer

  if (!opts.optimistic) {
    throw new Error('Encoding an encrypted value is not supported')
  }

  const padding = b4a.alloc(16) // min hash length is 16
  crypto.hash(state.buffer, padding)
  padding[0] = 0

  return b4a.concat([padding.subarray(0, 8), state.buffer])
}

async function decodeValue(value, { autobase, encryptionKey, key, manifest, index = 0 } = {}) {
  if (encryptionKey) {
    const e = new EncryptionView({ encryptionKey, bootstrap: autobase })
    const w = e.getWriterEncryption()

    await w.decrypt(index, value, { key, manifest })
    value = value.subarray(8)
  }

  const op = c.decode(OplogMessage, value)
  return op.node.value
}
const b4a = require('b4a')
const c = require('compact-encoding')
const safetyCatch = require('safety-catch')
const ReadyResource = require('ready-resource')

const WakeupEntry = {
  preencode(state, m) {
    c.fixed32.preencode(state, m.key)
    c.uint.preencode(state, m.length)
  },
  encode(state, m) {
    c.fixed32.encode(state, m.key)
    c.uint.encode(state, m.length)
  },
  decode(state) {
    return {
      key: c.fixed32.decode(state),
      length: c.uint.decode(state)
    }
  }
}

module.exports = class AutoWakeup extends ReadyResource {
  constructor(base) {
    super()

    this.base = base
    this.flushing = null

    this._rootStore = this.base.store
    this._addBound = this.add.bind(this)
    this._preupdateBound = this._preupdate.bind(this)
    this._needsFlush = false
    this._map = new Map()
  }

  [Symbol.iterator]() {
    return this._map.values()
  }

  async _preupdate(batch, key) {
    this.queue(key, batch.length)
    await this.flush()
    this.base._onwakeup()
  }

  async _save() {
    const slab = b4a.allocUnsafe(8 + this._map.size * 40) // 32 + 8
    const state = { start: 0, end: 0, buffer: slab }

    c.uint.encode(state, this._map.size)
    for (const m of this._map.values()) WakeupEntry.encode(state, m)

    await this.base.local.setUserData('autobase/wakeup', slab.subarray(0, state.start))
  }

  async _load() {
    const buffer = await this.base.local.getUserData('autobase/wakeup')
    if (!buffer) return

    const state = { start: 0, end: buffer.byteLength, buffer }

    let len = c.uint.decode(state)
    while (len-- > 0) {
      const m = WakeupEntry.decode(state)
      this._map.set(b4a.toString(m.key, 'hex'), m)
    }
  }

  async _open() {
    await this._load()

    this._rootStore.watch(this._addBound)

    for (const core of this._rootStore.cores) {
      if (core.opened === false) await core.ready().catch(noop)
      if (!core.closing) this.add(core)
    }
  }

  async _close() {
    this._rootStore.unwatch(this._addBound)

    for (const core of this._rootStore.cores) {
      if (core.opened && !core.closing) this.remove(core)
    }

    this._map.clear()

    while (this.flushing) {
      try {
        await this.flushing
      } catch {}
    }
  }

  queue(key, length) {
    const hex = b4a.toString(key, 'hex')
    const m = this._map.get(hex)

    if (m && m.length > length) {
      return false
    }

    this._needsFlush = true
    this._map.set(hex, { key, length })

    return true
  }

  unqueue(key, length) {
    const hex = b4a.toString(key, 'hex')
    const m = this._map.get(hex)

    if (!m) return true
    if (m.length > length) return false

    this._needsFlush = true
    this._map.delete(hex)

    return true
  }

  async flush() {
    if (this.closing) throw new Error('Closing')

    // wait for someone
    if (this.flushing) await this.flushing

    // if another still active they flushed us
    if (this.flushing) return this.flushing

    if (this._needsFlush === false) return
    this._needsFlush = false

    try {
      this.flushing = this._save()
      return await this.flushing
    } finally {
      this.flushing = null
    }
  }

  add(core) {
    return this._add(core).catch(safetyCatch)
  }

  remove(core) {
    return this._remove(core)
  }

  async _add(core) {
    if (core.opened === false) await core.ready()
    if (core.closing) return false

    // local writer, no need
    if (core === this.base.local.core) return false

    const rx = core.storage.read()

    const referrerPromise = rx.getUserData('referrer')
    const viewPromise = rx.getUserData('autobase/view')

    rx.tryFlush()

    const [referrer, view] = await Promise.all([referrerPromise, viewPromise])
    if (view) return false

    if (referrer === null || !b4a.equals(referrer, this.base.key)) return false

    core.preupdate = this._preupdateBound

    return true
  }

  _remove(core) {
    if (core.preupdate !== this._preupdateBound) return false
    core.preupdate = null
    return true
  }
}

function noop() {}
const c = require('compact-encoding')
const b4a = require('b4a')
const ReadyResource = require('ready-resource')
const assert = require('nanoassert')
const SignalPromise = require('signal-promise')

const Linearizer = require('./linearizer.js')
const NodeBuffer = require('./node-buffer.js')

const MAX_PRELOAD = 4
const MAX_PREFETCH = 256

module.exports = class Writer extends ReadyResource {
  constructor(base, core, length, isRemoved) {
    super()

    this.base = base
    this.core = core
    this.isRemoved = isRemoved
    this.updated = false
    this.range = null
    this.nodes = new NodeBuffer(length)
    this.node = null
    this.isActive = false
    this.isCurrentlyCoupled = false
    this.isCoupled = false // maintained by updateBootstrapWriters
    this.isBootstrap = false // maintained by updateBootstrapWriters
    this.isActiveIndexer = false
    this.available = length
    this.length = length
    this.seenLength = 0
    this.recover = false
    this.frozen = false

    this.syncSignal = null
  }

  _pause() {
    if (!this.range) return
    this.range.destroy()
    this.range = null
  }

  _resume() {
    if (this.range) {
      // if we didnt fill up the preload buffer yet, do not commit to more work
      if (this.range.range.end === this.nodes.length + MAX_PREFETCH) return
      this.range.destroy()
    }
    this.range = this.core.download({
      start: this.nodes.length,
      end: this.nodes.length + MAX_PREFETCH,
      linear: true
    })
  }

  _updateCoupling() {
    if (!this.base._coupler || this.isCoupled === this.isCurrentlyCoupled) return

    if (this.isCurrentlyCoupled) this.base._coupler.remove(this.core)
    else this.base._coupler.add(this.core)

    this.isCurrentlyCoupled = this.isCoupled
  }

  _removeCouple() {
    if (this.base._coupler && !this.isCoupled && this.isCurrentlyCoupled) {
      this.isCurrentlyCoupled = false
      this.base._coupler.remove(this.core)
    }
  }

  updateActivity() {
    if (!this.core.opened) return

    if (this.seenLength > this.core.length || this.length < this.core.length || this.isBootstrap) {
      // if we have seen a later core, or if we are behind, or if bootstrap
      this.isActive = true
      this.core.setActive(true)
    } else if (this.length === this.core.length) {
      // things look steady
      this.isActive = false
      this.core.setActive(false)
    }

    this._updateCoupling()

    if (this.core.writable) return

    if (this.base.isFastForwarding() || !this.isActive) {
      this._pause()
    } else {
      this._resume()
    }
  }

  setBootstrap(bool) {
    this.isBootstrap = bool
    this.updateActivity()
  }

  seen(length) {
    if (length > this.seenLength) this.seenLength = length
    this.updateActivity()
  }

  waitForSynced() {
    if (this.core.length === this.length) return Promise.resolve()
    if (this.syncSignal === null) this.syncSignal = new SignalPromise()
    return this.syncSignal.wait()
  }

  async _open() {
    await this.core.ready()
    await this.core.setUserData('referrer', this.base.key)

    // base might be closing here
    if (this.base.closing) return

    // remove later
    this.recover = autoRecover(this.core)

    // add it again incase it wasn't readied before, only needed if this is the first time we set the referrer...
    await this.base._wakeup.add(this.core.core)

    this.updateActivity()

    if (this.core.length > this.length) this.base._queueBump()

    this.base.emit('writer', this)
  }

  _close() {
    if (this.syncSignal !== null) this.syncSignal.notify()
    return this.core.close()
  }

  get indexed() {
    return this.nodes.offset
  }

  idle() {
    return this.length === this.available && this.length === this.core.length && this.core.opened
  }

  flushed() {
    // TODO: prop a cleaner way to express this...
    return (
      this.seenLength <= this.length &&
      this.length === this.available &&
      this.length === this.core.length &&
      this.shiftable() === false &&
      !this.core.core.upgrading &&
      this.core.opened
    )
  }

  compare(writer) {
    return b4a.compare(this.core.key, writer.core.key)
  }

  head() {
    return this.nodes.get(this.length - 1)
  }

  advance() {
    if (this.syncSignal !== null && this.length + 1 === this.core.length) this.syncSignal.notify()
    return this.length < this.available ? this.nodes.get(this.length++) : null
  }

  shiftable() {
    return this.length > this.nodes.offset
  }

  shift() {
    if (this.shiftable() === false) return false

    let node = this._shiftAndClear()
    while (node.batch > 1) node = this._shiftAndClear()

    return true
  }

  get(seq) {
    return seq < this.length ? this.nodes.get(seq) : null
  }

  append(value, heads, batch, dependencies, optimistic) {
    const node = Linearizer.createNode(
      this,
      this.nodes.length + 1,
      value,
      heads,
      batch,
      dependencies,
      optimistic
    )

    node.actualHeads = node.heads.slice(0)

    this.nodes.push(node)
    this.available++
    this.length++

    return node
  }

  async update(boot) {
    if (this.opened === false) await this.ready()
    if (this.frozen) return false

    // if this is a boot node, DO NOT, preload as that will make it load
    // and reject nodes based on the tmp state of the system.
    // prop needs a better solution but this works for now
    const preload = boot ? 1 : MAX_PRELOAD

    while (this.available - this.length < preload) {
      // quick sanity check
      if (this.nodes.length === this.core.length || this.core.length === 0) break

      // load next node
      if (this.node === null && !(await this._loadNextNode())) break
      if (!(await this._ensureNodeDependencies(boot))) break

      if (this.recover) this.node.value = null

      this.nodes.push(this.node)
      if (this.node.batch === 1) this.available = this.nodes.length
      this.node = null
    }

    this.updateActivity()

    return this.length < this.available
  }

  _shiftAndClear() {
    const node = this.nodes.shift()
    node.clear()
    return node
  }

  async _loadNextNode() {
    const seq = this.nodes.length

    if (!(await this.core.has(seq))) return false

    try {
      const { node, optimistic, trace } = await this.core.get(seq, { wait: false })

      const value = node.value == null ? null : c.decode(this.base.valueEncoding, node.value)
      this.node = Linearizer.createNode(
        this,
        seq + 1,
        value,
        node.heads,
        node.batch,
        new Set(),
        optimistic,
        trace
      )
      return true
    } catch (err) {
      this.frozen = true
      throw err
    }
  }

  async _ensureNodeDependencies(boot) {
    while (this.node.dependencies.size < this.node.heads.length) {
      const rawHead = this.node.heads[this.node.dependencies.size]

      const headWriter = await this.base._getWriterByKey(
        rawHead.key,
        -1,
        rawHead.length,
        true,
        false,
        boot
      )

      if (headWriter !== this && (headWriter === null || headWriter.length < rawHead.length)) {
        if (!boot) this.base._ensureWakeup(headWriter)
        return false
      }

      let headNode = headWriter.nodes.get(rawHead.length - 1)

      // could be a stub node
      if (!headNode) {
        for (const node of this.base.linearizer.heads) {
          if (!compareHead(node, rawHead)) continue
          headNode = node
          break
        }
      }

      // TODO: generalise DAG validation and freeze the writer
      assert(!this.node.dependencies.has(headNode), 'Corrupted DAG')

      // TODO: better way to solve the stub check is to never mutate heads below
      if (headNode === null) {
        // already yielded
        this.node.heads.splice(this.node.dependencies.size, 1)
        continue
      }

      this.node.dependencies.add(headNode)
    }

    // always link previous node if it's not indexed
    const offset = this.node.length - 1
    if (offset > this.indexed) {
      this.node.dependencies.add(this.nodes.get(offset - 1))
    }

    return true
  }
}

function compareHead(node, head) {
  if (node.length !== head.length) return false
  return b4a.equals(node.writer.core.key, head.key)
}

// this is a list of peers we bugged in the btc and planb room.
// adding them here so migration can run, can be removed in a month or so from time of commit
// note, no security implications of this, we just null them out.

function autoRecover(core) {
  assert(core.opened)

  switch (core.id) {
    case 'ghrpexaboutdm46ombqho7mroxknassnntrxx3cubfux4qi6w6hy':
    case 'qoaanao71s4he1rcd197d336qepykk4467geo1uq8cwnzmpb786o':
    case 'fomhdxgn4j4tzjqy6y7iskhffimzokt7kraddyd8orcht3r8q61o':
    case 'd8f5taxxrit51apftoi38e5b86hb98cgfd7dfp3uo1uoh95qt49o':
    case 'objyf75uggsqpjcut69xdgj46ks8r71jjrq7oxdfsz95sstchkno':
      return true
  }

  return false
}
{
  "name": "autobase",
  "version": "7.24.0",
  "description": "A multiwriter data structure for Hypercore",
  "main": "index.js",
  "scripts": {
    "format": "prettier . --write",
    "test": "prettier . --check && brittle test/all.js",
    "test:encrypted": "prettier . --check && brittle test/all.js --encrypt-all",
    "test:fixtures": "prettier . --check && brittle test/fixtures/tests/*.js",
    "test:bare": "bare test/all.js",
    "fuzz:generated": "brittle test/reference/fuzz/generated/*.js",
    "fuzz:main": "node test/fuzz/index.js",
    "fuzz": "node test/reference/fuzz/fuzz.js",
    "generate-fixtures": "node test/fixtures/generate/all.js"
  },
  "files": [
    "index.js",
    "lib/**",
    "encoding/**"
  ],
  "imports": {
    "events": {
      "bare": "bare-events",
      "default": "events"
    }
  },
  "repository": {
    "type": "git",
    "url": "git+https://github.com/holepunchto/autobase.git"
  },
  "author": "Holepunch",
  "license": "Apache-2.0",
  "bugs": {
    "url": "https://github.com/holepunchto/autobase/issues"
  },
  "homepage": "https://github.com/holepunchto/autobase#readme",
  "dependencies": {
    "b4a": "^1.6.1",
    "bare-events": "^2.2.0",
    "compact-encoding": "^2.16.0",
    "core-coupler": "^2.0.0",
    "debounceify": "^1.0.0",
    "hyperbee": "^2.22.0",
    "hypercore": "^11.4.0",
    "hypercore-crypto": "^3.4.0",
    "hypercore-id-encoding": "^1.2.0",
    "hyperschema": "^1.12.1",
    "index-encoder": "^3.3.2",
    "nanoassert": "^2.0.0",
    "protomux-wakeup": "^2.0.0",
    "ready-resource": "^1.0.0",
    "resolve-reject-promise": "^1.1.0",
    "safety-catch": "^1.0.2",
    "scope-lock": "^1.2.4",
    "signal-promise": "^1.0.3",
    "sodium-universal": "^5.0.1",
    "sub-encoder": "^2.1.1",
    "tiny-buffer-map": "^1.1.1"
  },
  "devDependencies": {
    "autobase-test-helpers": "^3.0.0",
    "brittle": "^3.1.1",
    "corestore": "^7.0.15",
    "prettier": "^3.6.2",
    "prettier-config-holepunch": "^2.0.0",
    "rache": "^1.0.0",
    "same-data": "^1.0.0",
    "task-backoff": "^1.0.0",
    "test-tmp": "^1.2.0",
    "uncaughts": "^1.1.0"
  },
  "standard": {
    "ignore": [
      "**/test/fuzz/generated/**",
      "**/test/reference/**",
      "example.mjs"
    ]
  }
}
function isBuffer(value) {
  return Buffer.isBuffer(value) || value instanceof Uint8Array
}

function isEncoding(encoding) {
  return Buffer.isEncoding(encoding)
}

function alloc(size, fill, encoding) {
  return Buffer.alloc(size, fill, encoding)
}

function allocUnsafe(size) {
  return Buffer.allocUnsafe(size)
}

function allocUnsafeSlow(size) {
  return Buffer.allocUnsafeSlow(size)
}

function byteLength(string, encoding) {
  return Buffer.byteLength(string, encoding)
}

function compare(a, b) {
  return Buffer.compare(a, b)
}

function concat(buffers, totalLength) {
  return Buffer.concat(buffers, totalLength)
}

function copy(source, target, targetStart, start, end) {
  return toBuffer(source).copy(target, targetStart, start, end)
}

function equals(a, b) {
  return toBuffer(a).equals(b)
}

function fill(buffer, value, offset, end, encoding) {
  return toBuffer(buffer).fill(value, offset, end, encoding)
}

function from(value, encodingOrOffset, length) {
  return Buffer.from(value, encodingOrOffset, length)
}

function includes(buffer, value, byteOffset, encoding) {
  return toBuffer(buffer).includes(value, byteOffset, encoding)
}

function indexOf(buffer, value, byfeOffset, encoding) {
  return toBuffer(buffer).indexOf(value, byfeOffset, encoding)
}

function lastIndexOf(buffer, value, byteOffset, encoding) {
  return toBuffer(buffer).lastIndexOf(value, byteOffset, encoding)
}

function swap16(buffer) {
  return toBuffer(buffer).swap16()
}

function swap32(buffer) {
  return toBuffer(buffer).swap32()
}

function swap64(buffer) {
  return toBuffer(buffer).swap64()
}

function toBuffer(buffer) {
  if (Buffer.isBuffer(buffer)) return buffer
  return Buffer.from(buffer.buffer, buffer.byteOffset, buffer.byteLength)
}

function toString(buffer, encoding, start, end) {
  return toBuffer(buffer).toString(encoding, start, end)
}

function write(buffer, string, offset, length, encoding) {
  return toBuffer(buffer).write(string, offset, length, encoding)
}

function readDoubleBE(buffer, offset) {
  return toBuffer(buffer).readDoubleBE(offset)
}

function readDoubleLE(buffer, offset) {
  return toBuffer(buffer).readDoubleLE(offset)
}

function readFloatBE(buffer, offset) {
  return toBuffer(buffer).readFloatBE(offset)
}

function readFloatLE(buffer, offset) {
  return toBuffer(buffer).readFloatLE(offset)
}

function readInt32BE(buffer, offset) {
  return toBuffer(buffer).readInt32BE(offset)
}

function readInt32LE(buffer, offset) {
  return toBuffer(buffer).readInt32LE(offset)
}

function readUInt32BE(buffer, offset) {
  return toBuffer(buffer).readUInt32BE(offset)
}

function readUInt32LE(buffer, offset) {
  return toBuffer(buffer).readUInt32LE(offset)
}

function writeDoubleBE(buffer, value, offset) {
  return toBuffer(buffer).writeDoubleBE(value, offset)
}

function writeDoubleLE(buffer, value, offset) {
  return toBuffer(buffer).writeDoubleLE(value, offset)
}

function writeFloatBE(buffer, value, offset) {
  return toBuffer(buffer).writeFloatBE(value, offset)
}

function writeFloatLE(buffer, value, offset) {
  return toBuffer(buffer).writeFloatLE(value, offset)
}

function writeInt32BE(buffer, value, offset) {
  return toBuffer(buffer).writeInt32BE(value, offset)
}

function writeInt32LE(buffer, value, offset) {
  return toBuffer(buffer).writeInt32LE(value, offset)
}

function writeUInt32BE(buffer, value, offset) {
  return toBuffer(buffer).writeUInt32BE(value, offset)
}

function writeUInt32LE(buffer, value, offset) {
  return toBuffer(buffer).writeUInt32LE(value, offset)
}

module.exports = {
  isBuffer,
  isEncoding,
  alloc,
  allocUnsafe,
  allocUnsafeSlow,
  byteLength,
  compare,
  concat,
  copy,
  equals,
  fill,
  from,
  includes,
  indexOf,
  lastIndexOf,
  swap16,
  swap32,
  swap64,
  toBuffer,
  toString,
  write,
  readDoubleBE,
  readDoubleLE,
  readFloatBE,
  readFloatLE,
  readInt32BE,
  readInt32LE,
  readUInt32BE,
  readUInt32LE,
  writeDoubleBE,
  writeDoubleLE,
  writeFloatBE,
  writeFloatLE,
  writeInt32BE,
  writeInt32LE,
  writeUInt32BE,
  writeUInt32LE
}
{
  "name": "b4a",
  "version": "1.7.3",
  "description": "Bridging the gap between buffers and typed arrays",
  "exports": {
    "./package": "./package.json",
    ".": {
      "react-native": "./react-native.js",
      "browser": "./browser.js",
      "default": "./index.js"
    }
  },
  "files": [
    "browser.js",
    "index.js",
    "react-native.js",
    "lib"
  ],
  "scripts": {
    "test": "npm run lint && npm run test:bare && npm run test:node",
    "test:bare": "bare test.mjs",
    "test:node": "node test.mjs",
    "lint": "prettier . --check"
  },
  "repository": {
    "type": "git",
    "url": "git+https://github.com/holepunchto/b4a.git"
  },
  "author": "Holepunch",
  "license": "Apache-2.0",
  "bugs": {
    "url": "https://github.com/holepunchto/b4a/issues"
  },
  "homepage": "https://github.com/holepunchto/b4a#readme",
  "devDependencies": {
    "brittle": "^3.5.2",
    "nanobench": "^3.0.0",
    "prettier": "^3.6.2",
    "prettier-config-holepunch": "^1.0.0"
  },
  "peerDependencies": {
    "react-native-b4a": "*"
  },
  "peerDependenciesMeta": {
    "react-native-b4a": {
      "optional": true
    }
  }
}
module.exports = require.addon()
const constants = require('./lib/constants')
const Hash = require('./lib/hash')
const Hmac = require('./lib/hmac')
const { Cipheriv, Decipheriv } = require('./lib/cipher')
const { randomBytes, randomFill, randomUUID } = require('./lib/random')
const pbkdf2 = require('./lib/pbkdf2')
const { generateKeyPair } = require('./lib/key')
const { sign, verify } = require('./lib/signature')

exports.constants = constants

exports.Hash = Hash

exports.createHash = function createHash(algorithm, opts) {
  return new Hash(algorithm, opts)
}

exports.Hmac = Hmac

exports.createHmac = function createHmac(algorithm, key, opts) {
  return new Hmac(algorithm, key, opts)
}

exports.Cipheriv = Cipheriv

exports.createCipheriv = function createCipheriv(algorithm, key, iv, opts) {
  return new Cipheriv(algorithm, key, iv, opts)
}

exports.Decipheriv = Decipheriv

exports.createDecipheriv = function createDecipheriv(algorithm, key, iv, opts) {
  return new Decipheriv(algorithm, key, iv, opts)
}

exports.randomBytes = randomBytes

exports.randomFill = randomFill

// For Node.js compatibility
exports.randomFillSync = function randomFillSync(buffer, offset, size) {
  return exports.randomFill(buffer, offset, size)
}

exports.randomUUID = randomUUID

exports.pbkdf2 = pbkdf2

// For Node.js compatibility
exports.pbkdf2Sync = function pbkdf2Sync(password, salt, iterations, keylen, digest) {
  return exports.pbkdf2(password, salt, iterations, keylen, digest)
}

exports.generateKeyPair = generateKeyPair

exports.sign = sign

exports.verify = verify

// For Node.js compatibility
exports.webcrypto = require('./web')
const { Transform } = require('bare-stream')
const binding = require('../binding')
const constants = require('./constants')

const {
  cipher: {
    AES128ECB,
    AES128CBC,
    AES128CTR,
    AES128OFB,
    AES256ECB,
    AES256CBC,
    AES256CTR,
    AES256OFB,
    AES128GCM,
    AES256GCM,
    CHACHA20POLY1305,
    XCHACHA20POLY1305
  }
} = constants

class CryptoCipher {
  constructor(algorithm, key, iv, encrypt, opts = {}) {
    const { encoding = 'utf8' } = opts

    if (typeof key === 'string') key = Buffer.from(key, encoding)
    if (typeof iv === 'string') iv = Buffer.from(iv, encoding)

    if (key.byteLength !== binding.cipherKeyLength(algorithm)) {
      throw new RangeError('Invalid key length')
    }

    if (iv.byteLength < binding.cipherIVLength(algorithm)) {
      throw new RangeError('Invalid iv length')
    }

    this._handle = binding.cipherInit(
      algorithm,
      key.buffer,
      key.byteOffset,
      key.byteLength,
      iv.buffer,
      iv.byteOffset,
      iv.byteLength,
      encrypt
    )
  }

  update(data, inputEncoding = 'utf8', outputEncoding) {
    if (typeof data === 'string') data = Buffer.from(data, inputEncoding)

    const out = new ArrayBuffer(binding.cipherBlockSize(this._handle))

    const written = binding.cipherUpdate(
      this._handle,
      data.buffer,
      data.byteOffset,
      data.byteLength,
      out
    )

    const result = Buffer.from(out, 0, written)

    return outputEncoding ? result.toString(outputEncoding) : result
  }

  final(outputEncoding) {
    const out = new ArrayBuffer(binding.cipherBlockSize(this._handle))

    const written = binding.cipherFinal(this._handle, out)

    const result = Buffer.from(out, 0, written)

    return outputEncoding ? result.toString(outputEncoding) : result
  }

  setAutoPadding(pad) {
    binding.cipherSetPadding(pad)
  }
}

class CryptoAuthenticatedCipher {
  constructor(algorithm, key, nonce, opts = {}) {
    const { encoding = 'utf8', authTagLength = 16 } = opts

    if (typeof key === 'string') key = Buffer.from(key, encoding)
    if (typeof nonce === 'string') nonce = Buffer.from(nonce, encoding)

    if (key.byteLength !== binding.aeadKeyLength(algorithm)) {
      throw new RangeError('Invalid key length')
    }

    if (nonce.byteLength < binding.aeadNonceLength(algorithm)) {
      throw new RangeError('Invalid nonce length')
    }

    this._buffer = []
    this._nonce = nonce
    this._authTag = null
    this._authTagLength = authTagLength
    this._additionalData = null

    this._handle = binding.aeadInit(
      algorithm,
      key.buffer,
      key.byteOffset,
      key.byteLength,
      authTagLength
    )
  }

  update(data, inputEncoding = 'utf8', outputEncoding) {
    if (typeof data === 'string') data = Buffer.from(data, inputEncoding)

    this._buffer.push(data)

    return outputEncoding ? '' : Buffer.alloc(0)
  }

  setAAD(buffer, opts = {}) {
    const { encoding = 'utf8' } = opts

    if (typeof buffer === 'string') buffer = Buffer.from(buffer, encoding)

    this._additionalData = buffer
  }

  getAuthTag() {
    return this._authTag
  }

  setAuthTag(authTag, encoding) {
    if (typeof authTag === 'string') authTag = Buffer.from(authTag, encoding)

    this._authTag = authTag
  }
}

class CryptoAuthenticatedSeal extends CryptoAuthenticatedCipher {
  final(outputEncoding) {
    const data = this._buffer.length === 1 ? this._buffer[0] : Buffer.concat(this._buffer)

    const nonce = this._nonce
    const ad = this._additionalData || Buffer.alloc(0)

    const out = new ArrayBuffer(data.byteLength + binding.aeadMaxOverhead(this._handle))

    binding.aeadSeal(
      this._handle,
      data.buffer,
      data.byteOffset,
      data.byteLength,
      nonce.buffer,
      nonce.byteOffset,
      nonce.byteLength,
      ad.buffer,
      ad.byteOffset,
      ad.byteLength,
      out
    )

    const written = out.byteLength - this._authTagLength

    this._authTag = Buffer.from(out, written)

    const result = Buffer.from(out, 0, written)

    return outputEncoding ? result.toString(outputEncoding) : result
  }
}

class CryptoAuthenticatedOpen extends CryptoAuthenticatedCipher {
  final(outputEncoding) {
    this._buffer.push(this._authTag)

    const data = Buffer.concat(this._buffer)

    const nonce = this._nonce
    const ad = this._additionalData || Buffer.alloc(0)

    const out = new ArrayBuffer(data.byteLength)

    binding.aeadOpen(
      this._handle,
      data.buffer,
      data.byteOffset,
      data.byteLength,
      nonce.buffer,
      nonce.byteOffset,
      nonce.byteLength,
      ad.buffer,
      ad.byteOffset,
      ad.byteLength,
      out
    )

    const written = out.byteLength - this._authTagLength

    const result = Buffer.from(out, 0, written)

    return outputEncoding ? result.toString(outputEncoding) : result
  }
}

exports.Cipheriv = class CryptoCipheriv extends Transform {
  constructor(algorithm, key, iv, opts = {}) {
    super(opts)

    algorithm = constants.toCipher(algorithm)

    switch (algorithm) {
      case AES128ECB:
      case AES128CBC:
      case AES128CTR:
      case AES128OFB:
      case AES256ECB:
      case AES256CBC:
      case AES256CTR:
      case AES256OFB:
        this._cipher = new CryptoCipher(algorithm, key, iv, true, opts)
        break

      case AES128GCM:
      case AES256GCM:
      case CHACHA20POLY1305:
      case XCHACHA20POLY1305:
        this._cipher = new CryptoAuthenticatedSeal(algorithm, key, iv, opts)
        break
    }
  }

  update(data, inputEncoding, outputEncoding) {
    return this._cipher.update(data, inputEncoding, outputEncoding)
  }

  final(outputEncoding) {
    return this._cipher.final(outputEncoding)
  }

  setAutoPadding(pad) {
    this._cipher.setAutoPadding(pad)

    return this
  }

  setAAD(buffer, opts) {
    this._cipher.setAAD(buffer, opts)

    return this
  }

  getAuthTag() {
    return this._cipher.getAuthTag()
  }

  _transform(data, encoding, cb) {
    this.update(data)

    cb(null)
  }

  _flush(cb) {
    this.push(this.digest())

    cb(null)
  }
}

exports.Decipheriv = class CryptoDeipheriv extends Transform {
  constructor(algorithm, key, iv, opts = {}) {
    super(opts)

    algorithm = constants.toCipher(algorithm)

    switch (algorithm) {
      case AES128ECB:
      case AES128CBC:
      case AES128CTR:
      case AES128OFB:
      case AES256ECB:
      case AES256CBC:
      case AES256CTR:
      case AES256OFB:
        this._cipher = new CryptoCipher(algorithm, key, iv, false, opts)
        break

      case AES128GCM:
      case AES256GCM:
      case CHACHA20POLY1305:
      case XCHACHA20POLY1305:
        this._cipher = new CryptoAuthenticatedOpen(algorithm, key, iv, opts)
        break
    }
  }

  update(data, inputEncoding, outputEncoding) {
    return this._cipher.update(data, inputEncoding, outputEncoding)
  }

  final(outputEncoding) {
    return this._cipher.final(outputEncoding)
  }

  setAutoPadding(pad) {
    this._cipher.setAutoPadding(pad)

    return this
  }

  setAAD(buffer, opts) {
    this._cipher.setAAD(buffer, opts)

    return this
  }

  setAuthTag(authTag, encoding) {
    this._cipher.setAuthTag(authTag, encoding)

    return this
  }

  _transform(data, encoding, cb) {
    this.update(data)

    cb(null)
  }

  _flush(cb) {
    this.push(this.digest())

    cb(null)
  }
}
const binding = require('../binding')
const errors = require('./errors')

module.exports = exports = {
  hash: {
    MD5: binding.MD5,
    SHA1: binding.SHA1,
    SHA256: binding.SHA256,
    SHA512: binding.SHA512,
    BLAKE2B256: binding.BLAKE2B256
  },
  signature: {
    ED25519: binding.ED25519
  },
  cipher: {
    AES128ECB: binding.AES128ECB,
    AES128CBC: binding.AES128CBC,
    AES128CTR: binding.AES128CTR,
    AES128OFB: binding.AES128OFB,
    AES256ECB: binding.AES256ECB,
    AES256CBC: binding.AES256CBC,
    AES256CTR: binding.AES256CTR,
    AES256OFB: binding.AES256OFB,
    AES128GCM: binding.AES128GCM,
    AES256GCM: binding.AES256GCM,
    CHACHA20POLY1305: binding.CHACHA20POLY1305,
    XCHACHA20POLY1305: binding.XCHACHA20POLY1305
  },
  keyType: {
    ED25519: binding.ED25519
  }
}

exports.toHash = function toHash(hash) {
  if (typeof hash === 'number') return hash

  if (typeof hash === 'string') {
    hash = hash.replace(/-/g, '')

    if (hash in exports.hash === false) {
      hash = hash.toUpperCase()

      if (hash in exports.hash === false) {
        throw errors.UNKNOWN_HASH(`Unknown hash '${hash}'`)
      }
    }

    return exports.hash[hash]
  }

  throw new TypeError(`Hash must be a number or string. Received ${typeof hash} (${hash})`)
}

exports.toCipher = function toCiper(cipher) {
  if (typeof cipher === 'number') return cipher

  if (typeof cipher === 'string') {
    cipher = cipher.replace(/-/g, '')

    if (cipher in exports.cipher === false) {
      cipher = cipher.toUpperCase()

      if (cipher in exports.cipher === false) {
        throw errors.UNKNOWN_CIPHER(`Unknown cipher '${cipher}'`)
      }
    }

    return exports.cipher[cipher]
  }

  throw new TypeError(`Cipher must be a number or string. Received ${typeof cipher} (${cipher})`)
}

exports.toKeyType = function toKeyType(type) {
  if (typeof type === 'number') return type

  if (typeof type === 'string') {
    type = type.replace(/-/g, '')

    if (type in exports.keyType === false) {
      type = type.toUpperCase()

      if (type in exports.keyType === false) {
        throw errors.UNKNOWN_KEY_TYPE(`Unknown key type '${type}'`)
      }
    }

    return exports.keyType[type]
  }

  throw new TypeError(`Key type must be a number or string. Received ${typeof type} (${type})`)
}
module.exports = class CryptoError extends Error {
  constructor(msg, fn = CryptoError, code = fn.name) {
    super(`${code}: ${msg}`)
    this.code = code

    if (Error.captureStackTrace) {
      Error.captureStackTrace(this, fn)
    }
  }

  get name() {
    return 'CryptoError'
  }

  static UNKNOWN_HASH(msg) {
    return new CryptoError(msg, CryptoError.UNKNOWN_HASH)
  }

  static UNKNOWN_CIPHER(msg) {
    return new CryptoError(msg, CryptoError.UNKNOWN_CIPHER)
  }

  static UNKNOWN_KEY_TYPE(msg) {
    return new CryptoError(msg, CryptoError.UNKNOWN_KEY_TYPE)
  }

  static INVALID_ACCESS(msg) {
    return new CryptoError(msg, CryptoError.INVALID_ACCESS)
  }

  static INVALID_DATA(msg) {
    return new CryptoError(msg, CryptoError.INVALID_DATA)
  }

  static OPERATION_ERROR(msg) {
    return new CryptoError(msg, CryptoError.OPERATION_ERROR)
  }

  static NOT_SUPPORTED(msg) {
    return new CryptoError(msg, CryptoError.NOT_SUPPORTED)
  }
}
const { Transform } = require('bare-stream')
const binding = require('../binding')
const constants = require('./constants')

module.exports = class CryptoHash extends Transform {
  constructor(algorithm, opts = {}) {
    super(opts)

    this._handle = binding.hashInit(constants.toHash(algorithm))
  }

  update(data, encoding = 'utf8') {
    if (typeof data === 'string') data = Buffer.from(data, encoding)

    binding.hashUpdate(this._handle, data.buffer, data.byteOffset, data.byteLength)

    return this
  }

  digest(encoding) {
    const digest = Buffer.from(binding.hashFinal(this._handle))

    return encoding && encoding !== 'buffer' ? digest.toString(encoding) : digest
  }

  _transform(data, encoding, cb) {
    this.update(data)

    cb(null)
  }

  _flush(cb) {
    this.push(this.digest())

    cb(null)
  }
}
const { Transform } = require('bare-stream')
const binding = require('../binding')
const constants = require('./constants')

module.exports = class CryptoHmac extends Transform {
  constructor(algorithm, key, opts = {}) {
    super(opts)

    const { encoding = 'utf8' } = opts

    if (typeof key === 'string') key = Buffer.from(key, encoding)

    this._handle = binding.hmacInit(
      constants.toHash(algorithm),
      key.buffer,
      key.byteOffset,
      key.byteLength
    )
  }

  update(data, encoding = 'utf8') {
    if (typeof data === 'string') data = Buffer.from(data, encoding)

    binding.hmacUpdate(this._handle, data.buffer, data.byteOffset, data.byteLength)

    return this
  }

  digest(encoding) {
    const digest = Buffer.from(binding.hmacFinal(this._handle))

    return encoding && encoding !== 'buffer' ? digest.toString(encoding) : digest
  }

  _transform(data, encoding, cb) {
    this.update(data)

    cb(null)
  }

  _flush(cb) {
    this.push(this.digest())

    cb(null)
  }
}
const binding = require('../binding')
const constants = require('./constants')

const {
  keyType: { ED25519 }
} = constants

class CryptoKey {
  constructor(keyType) {
    this._keyType = keyType
  }
}

exports.Key = CryptoKey

class CryptoEd25519Key extends CryptoKey {
  constructor(key) {
    super(ED25519)

    this._key = key
  }

  get asymmetricKeyType() {
    return 'ed25519'
  }
}

class CryptoEd25519PublicKey extends CryptoEd25519Key {
  get type() {
    return 'public'
  }
}

exports.Ed25519PublicKey = CryptoEd25519PublicKey

class CryptoEd25519PrivateKey extends CryptoEd25519Key {
  get type() {
    return 'private'
  }
}

exports.Ed25519PrivateKey = CryptoEd25519PrivateKey

exports.generateKeyPair = function generateKeyPair(type, opts = {}) {
  type = constants.toKeyType(type)

  switch (type) {
    case ED25519: {
      const { publicKey, privateKey } = binding.ed25519GenerateKeypair()

      return {
        publicKey: new CryptoEd25519PublicKey(publicKey),
        privateKey: new CryptoEd25519PrivateKey(privateKey)
      }
    }
  }
}
const binding = require('../binding')
const constants = require('./constants')

module.exports = function pbkdf2(password, salt, iterations, keylen, digest, cb) {
  if (iterations <= 0) {
    throw new RangeError('iterations is out of range')
  }

  if (typeof password === 'string') password = Buffer.from(password)
  if (typeof salt === 'string') salt = Buffer.from(salt)

  const buffer = Buffer.from(
    binding.pbkdf2(
      password.buffer,
      password.byteOffset,
      password.byteLength,
      salt.buffer,
      salt.byteOffset,
      salt.byteLength,
      iterations,
      constants.toHash(digest),
      keylen
    )
  )

  if (cb) queueMicrotask(() => cb(null, buffer))
  else return buffer
}
const binding = require('../binding')

exports.randomBytes = function randomBytes(size, cb) {
  const buffer = Buffer.allocUnsafe(size)
  exports.randomFill(buffer)
  if (cb) queueMicrotask(() => cb(null, buffer))
  else return buffer
}

exports.randomFill = function randomFill(buffer, offset, size, cb) {
  if (typeof offset === 'function') {
    cb = offset
    offset = undefined
  } else if (typeof size === 'function') {
    cb = size
    size = undefined
  }

  const elementSize = buffer.BYTES_PER_ELEMENT || 1

  if (offset === undefined) offset = 0
  else offset *= elementSize

  if (size === undefined) size = buffer.byteLength - offset
  else size *= elementSize

  if (offset < 0 || offset > buffer.byteLength) {
    throw new RangeError('offset is out of range')
  }

  if (size < 0 || size > buffer.byteLength) {
    throw new RangeError('size is out of range')
  }

  if (offset + size > buffer.byteLength) {
    throw new RangeError('offset + size is out of range')
  }

  let arraybuffer

  if (ArrayBuffer.isView(buffer)) {
    offset += buffer.byteOffset
    arraybuffer = buffer.buffer
  } else {
    arraybuffer = buffer
  }

  binding.randomFill(arraybuffer, offset, size)

  if (cb) queueMicrotask(() => cb(null, buffer))
  else return buffer
}

exports.randomUUID = function randomUUID() {
  const uuid = exports.randomBytes(16)

  uuid[6] = (uuid[6] >>> 4) | 0b01000000
  uuid[8] = (uuid[8] >>> 2) | 0b10000000

  return (
    uuid.subarray(0, 4).toString('hex') +
    '-' +
    uuid.subarray(4, 6).toString('hex') +
    '-' +
    uuid.subarray(6, 8).toString('hex') +
    '-' +
    uuid.subarray(8, 10).toString('hex') +
    '-' +
    uuid.subarray(10, 16).toString('hex')
  )
}
const binding = require('../binding')
const constants = require('./constants')

const {
  keyType: { ED25519 }
} = constants

exports.sign = function sign(algorithm, data, key) {
  if (ArrayBuffer.isView(data)) {
    data = Buffer.coerce(data)
  } else {
    data = Buffer.from(data)
  }

  switch (key._keyType) {
    case ED25519:
      return Buffer.from(
        binding.ed25519Sign(data.buffer, data.byteOffset, data.byteLength, key._key)
      )
  }
}

exports.verify = function verify(algorithm, data, key, signature) {
  if (ArrayBuffer.isView(data)) {
    data = Buffer.coerce(data)
  } else {
    data = Buffer.from(data)
  }

  if (ArrayBuffer.isView(signature)) {
    signature = Buffer.coerce(signature)
  } else {
    signature = Buffer.from(signature)
  }

  switch (key._keyType) {
    case ED25519:
      return binding.ed25519Verify(
        data.buffer,
        data.byteOffset,
        data.byteLength,
        signature.buffer,
        signature.byteOffset,
        key._key
      )
  }
}
const crypto = require('../../..')
const binding = require('../../../binding')
const errors = require('../../errors')
const { Ed25519PublicKey, Ed25519PrivateKey } = require('../../key')
const CryptoKey = require('../crypto-key')

// https://w3c.github.io/webcrypto/#ed25519

// https://w3c.github.io/webcrypto/#ed25519-operations-sign
exports.sign = function sign(algorithm, key, data) {
  if (key.type !== 'private') {
    throw errors.INVALID_ACCESS('Must pass private key for Ed25519 signing')
  }

  const signature = crypto.sign(null, data, key._handle)

  return signature.buffer.slice(0, signature.byteLength)
}

// https://w3c.github.io/webcrypto/#ed25519-operations-verify
exports.verify = function verify(algorithm, key, signature, data) {
  if (key.type !== 'public') {
    throw errors.INVALID_ACCESS('Must pass public key for Ed25519 verification')
  }

  return crypto.verify(null, data, key._handle, signature)
}

// https://w3c.github.io/webcrypto/#ed25519-operations-generate-key
exports.generateKey = function generateKey(algorithm, extractable, usages) {
  for (const usage of usages) {
    if (usage !== 'sign' && usage !== 'verify') {
      throw new SyntaxError(
        `Usage '${usage}' cannot be used for the Ed25519 generateKey() operation`
      )
    }
  }

  const keys = crypto.generateKeyPair('ed25519')

  algorithm = { name: 'Ed25519' }

  return {
    publicKey: new CryptoKey('public', true, algorithm, ['verify'], keys.publicKey),
    privateKey: new CryptoKey('private', extractable, algorithm, ['sign'], keys.privateKey)
  }
}

// https://w3c.github.io/webcrypto/#ed25519-operations-import-key
exports.importKey = function importKey(format, keyData, algorithm, extractable, usages) {
  switch (format) {
    case 'spki':
      for (const usage of usages) {
        if (usage !== 'verify') {
          throw new SyntaxError(
            `Usage '${usage}' cannot be used for the Ed25519 importKey() operation`
          )
        }
      }

      keyData = binding.ed25519FromSPKI(keyData.buffer, keyData.byteOffset, keyData.byteLength)

      return new CryptoKey(
        'public',
        extractable,
        {
          name: 'Ed25519'
        },
        usages,
        new Ed25519PublicKey(keyData)
      )
    case 'pkcs8':
      for (const usage of usages) {
        if (usage !== 'sign') {
          throw new SyntaxError(
            `Usage '${usage}' cannot be used for the Ed25519 importKey() operation`
          )
        }
      }

      keyData = binding.ed25519FromPKCS8(keyData.buffer, keyData.byteOffset, keyData.byteLength)

      return new CryptoKey(
        'private',
        extractable,
        {
          name: 'Ed25519'
        },
        usages,
        new Ed25519PrivateKey(keyData)
      )
    case 'raw':
      for (const usage of usages) {
        if (usage !== 'verify') {
          throw new SyntaxError(
            `Usage '${usage}' cannot be used for the Ed25519 importKey() operation`
          )
        }
      }

      if (keyData.byteLength * 8 !== 256) {
        throw errors.INVALID_DATA('Key must be 256 bits')
      }

      return new CryptoKey(
        'public',
        extractable,
        {
          name: 'Ed25519'
        },
        usages,
        new Ed25519PublicKey(keyData.buffer)
      )
    case 'jwk':
      const jwk = keyData

      if ('d' in jwk) {
        if (usages.some((usage) => usage !== 'sign')) {
          throw new SyntaxError('JWK must be valid for signing')
        }
      } else {
        if (usages.some((usage) => usage !== 'verify')) {
          throw new SyntaxError('JWK must be valid for verification')
        }
      }

      if (jwk.kty !== 'OKP') {
        throw errors.INVALID_DATA('JWK key must be an octet key-pair')
      }

      if (jwk.crv !== 'Ed25519') {
        throw errors.INVALID_DATA('JWK must use the Ed25519 curve')
      }

      if ('alg' in jwk && jwk.alg !== 'Ed25519' && jwk.alg !== 'EdDSA') {
        throw errors.INVALID_DATA('JWK must use the Ed25519 curve')
      }

      if (usages.length && 'use' in jwk && jwk.use !== 'sig') {
        throw errors.INVALID_DATA('JWK cannot be used for signatures')
      }

      if ('ext' in jwk && jwk.ext !== extractable && extractable) {
        throw errors.INVALID_DATA('JWK is not extractable')
      }

      if ('d' in jwk) {
        const key = Buffer.concat([
          Buffer.from(jwk.d, 'base64url'),
          Buffer.from(jwk.x, 'base64url')
        ])

        if (key.byteLength * 8 !== 512) {
          throw errors.INVALID_DATA('Key must be 512 bits')
        }

        return new CryptoKey(
          'private',
          extractable,
          {
            name: 'Ed25519'
          },
          usages,
          new Ed25519PrivateKey(key.buffer)
        )
      }

      const key = Buffer.from(jwk.x, 'base64url')

      if (key.byteLength * 8 !== 256) {
        throw errors.INVALID_DATA('Key must be 256 bits')
      }

      return new CryptoKey(
        'public',
        extractable,
        {
          name: 'Ed25519'
        },
        usages,
        new Ed25519PublicKey(key.buffer)
      )
    default:
      throw errors.NOT_SUPPORTED(
        `Format '${format}' cannot be used for the Ed25519 importKey() operation`
      )
  }
}

// https://w3c.github.io/webcrypto/#ed25519-operations-export-key
exports.exportKey = function exportKey(format, key) {
  const data = key._handle

  switch (format) {
    case 'spki':
      if (key.type !== 'public') {
        throw errors.INVALID_ACCESS(
          `Key of type '${key.type}' cannot be used for the Ed25519 exportKey() operation`
        )
      }

      return binding.ed25519ToSPKI(data._key)
    case 'pkcs8':
      if (key.type !== 'private') {
        throw errors.INVALID_ACCESS(
          `Key of type '${key.type}' cannot be used for the Ed25519 exportKey() operation`
        )
      }

      return binding.ed25519ToPKCS8(data._key)
    case 'raw': {
      if (key.type !== 'public') {
        throw errors.INVALID_ACCESS(
          `Key of type '${key.type}' cannot be used for the Ed25519 exportKey() operation`
        )
      }

      return data._key.slice()
    }
    case 'jwk': {
      const buffer = Buffer.from(data._key)

      if (key.type === 'private') {
        const d = buffer.subarray(0, 32).toString('base64url')
        const x = buffer.subarray(32).toString('base64url')

        return {
          kty: 'OKP',
          alg: 'Ed25519',
          crv: 'Ed25519',
          x,
          d,
          key_ops: key.usages,
          ext: key.extractable
        }
      }

      return {
        kty: 'OKP',
        alg: 'Ed25519',
        crv: 'Ed25519',
        x: buffer.toString('base64url'),
        key_ops: key.usages,
        ext: key.extractable
      }
    }
    default:
      throw errors.NOT_SUPPORTED(
        `Format '${format}' cannot be used for the HMAC exportKey() operation`
      )
  }
}
const crypto = require('../../..')
const errors = require('../../errors')
const CryptoKey = require('../crypto-key')

// https://w3c.github.io/webcrypto/#hmac

// https://w3c.github.io/webcrypto/#hmac-operations-sign
exports.sign = function sign(algorithm, key, data) {
  const digest = crypto.createHmac(key.algorithm.hash.name, key._handle).update(data).digest()

  return digest.buffer.slice(0, digest.byteLength)
}

// https://w3c.github.io/webcrypto/#hmac-operations-verify
exports.verify = function verify(algorithm, key, signature, data) {
  const digest = crypto.createHmac(key.algorithm.hash.name, key._handle).update(data).digest()

  if (ArrayBuffer.isView(signature)) {
    signature = Buffer.coerce(signature)
  } else {
    signature = Buffer.from(signature)
  }

  return signature.equals(digest)
}

// https://w3c.github.io/webcrypto/#hmac-operations-generate-key
exports.generateKey = function generateKey(algorithm, extractable, usages) {
  for (const usage of usages) {
    if (usage !== 'sign' && usage !== 'verify') {
      throw new SyntaxError(`Usage '${usage}' cannot be used for the HMAC generateKey() operation`)
    }
  }

  const { length = exports.getKeyLength(algorithm) } = algorithm.length

  let hash = algorithm.hash

  if (typeof hash === 'string') hash = { name: hash }

  const key = crypto.createHmac(hash.name, crypto.randomBytes(length)).digest()

  return new CryptoKey(
    'secret',
    extractable,
    {
      name: 'HMAC',
      length,
      hash: {
        name: hash.name.toUpperCase()
      }
    },
    usages,
    key
  )
}

// https://w3c.github.io/webcrypto/#hmac-operations-import-key
exports.importKey = function importKey(format, keyData, algorithm, extractable, usages) {
  for (const usage of usages) {
    if (usage !== 'sign' && usage !== 'verify') {
      throw new SyntaxError(`Invalid usage ${usage}`)
    }
  }

  let hash = algorithm.hash

  if (typeof hash === 'string') hash = { name: hash }

  let data

  switch (format) {
    case 'raw':
      data = keyData
      break
    case 'jwk':
      const jwk = keyData

      if (jwk.kty !== 'oct') {
        throw errors.INVALID_DATA('JWK key must be an octet sequence')
      }

      data = Buffer.from(jwk.k, 'base64url')

      switch (hash.name.toLowerCase()) {
        case 'sha-1':
          if (jwk.alg === 'HS1') break
          else throw errors.INVALID_DATA('Invalid JWK key algorithm')
        case 'sha-256':
          if (jwk.alg === 'HS256') break
          else throw errors.INVALID_DATA('Invalid JWK key algorithm')
        case 'sha-384':
          if (jwk.alg === 'HS384') break
          else throw errors.INVALID_DATA('Invalid JWK key algorithm')
        case 'sha-512':
          if (jwk.alg === 'HS512') break
          else throw errors.INVALID_DATA('Invalid JWK key algorithm')
      }

      if (usages.length && 'use' in jwk && jwk.use !== 'sign') {
        throw errors.INVALID_DATA('JWK cannot be used for signing')
      }

      if ('ext' in jwk && jwk.ext !== extractable && extractable) {
        throw errors.INVALID_DATA('JWK is not extractable')
      }
      break
    default:
      throw errors.NOT_SUPPORTED(
        `Format '${format}' cannot be used for the HMAC importKey() operation`
      )
  }

  const length = data.byteLength * 8

  if (length === 0) {
    throw errors.INVALID_DATA('Key cannot be empty')
  }

  return new CryptoKey(
    'secret',
    extractable,
    {
      name: 'HMAC',
      length,
      hash: {
        name: hash.name.toUpperCase()
      }
    },
    usages,
    data
  )
}

// https://w3c.github.io/webcrypto/#hmac-operations-export-key
exports.exportKey = function exportKey(format, key) {
  const data = key._handle

  switch (format) {
    case 'raw':
      return data.buffer.slice(0, data.byteLength)
    case 'jwk': {
      const jwk = {
        kty: 'oct',
        k: data.toString('base64url'),
        alg: null,
        key_ops: key.usages,
        ext: key.extractable
      }

      switch (key.algorithm.hash.name) {
        case 'SHA-1':
          jwk.alg = 'HS1'
          break
        case 'SHA-256':
          jwk.alg = 'HS256'
          break
        case 'SHA-384':
          jwk.alg = 'HS384'
          break
        case 'SHA-512':
          jwk.alg = 'HS512'
          break
      }

      return jwk
    }
    default:
      throw errors.NOT_SUPPORTED(
        `Format '${format}' cannot be used for the HMAC exportKey() operation`
      )
  }
}

// https://w3c.github.io/webcrypto/#hmac-operations-get-key-length
exports.getKeyLength = function getKeyLength(algorithm) {
  const { length, hash } = algorithm

  if (length === undefined) {
    if (hash === 'SHA-1' || hash === 'SHA-256') return 512
    if (hash === 'SHA-512') return 1024

    throw errors.OPERATION_ERROR(`Invalid hash '${hash}'`)
  }

  if (length === 0) {
    throw errors.OPERATION_ERROR(`Invalid length ${length}`)
  }

  return length
}
const crypto = require('../../..')
const errors = require('../../errors')
const CryptoKey = require('../crypto-key')

// https://w3c.github.io/webcrypto/#pbkdf2

// https://w3c.github.io/webcrypto/#pbkdf2-operations-derive-bits
exports.deriveBits = function deriveBits(algorithm, key, length) {
  if (length === undefined || length % 8) {
    throw errors.OPERATION_ERROR('Length must be multiple of 8')
  }

  if (algorithm.iterations === 0) {
    throw errors.OPERATION_ERROR('Iterations must be non-0')
  }

  if (length === 0) {
    return new ArrayBuffer(0)
  }

  let hash = algorithm.hash

  if (typeof hash === 'string') hash = { name: hash }

  const result = crypto.pbkdf2(
    key._handle,
    algorithm.salt,
    algorithm.iterations,
    length / 8,
    hash.name
  )

  return result.buffer
}

// https://w3c.github.io/webcrypto/#pbkdf2-operations-import-key
exports.importKey = function importKey(format, keyData, algorithm, extractable, usages) {
  if (format !== 'raw') {
    throw errors.NOT_SUPPORTED(
      `Format '${format}' cannot be used for the PBKDF2 importKey() operation`
    )
  }

  for (const usage of usages) {
    if (usage !== 'deriveKey' && usage !== 'deriveBits') {
      throw new SyntaxError(`Invalid usage ${usage}`)
    }
  }

  if (extractable) {
    throw new SyntaxError('Extractable must be false')
  }

  return new CryptoKey(
    'secret',
    extractable,
    {
      name: 'PBKDF2'
    },
    usages,
    keyData
  )
}
const crypto = require('../../..')

// https://w3c.github.io/webcrypto/#sha

// https://w3c.github.io/webcrypto/#sha-operations-digest
exports.digest = function digest(name, data) {
  const digest = crypto.createHash(name).update(data).digest()

  return digest.buffer.slice(0, digest.byteLength)
}
// https://w3c.github.io/webcrypto/#cryptokey-interface
module.exports = class CryptoKey {
  constructor(type, extractable, algorithm, usages, handle = null) {
    this._type = type
    this._extractable = extractable
    this._algorithm = algorithm
    this._usages = usages
    this._handle = handle
  }

  // https://w3c.github.io/webcrypto/#dom-cryptokey-type
  get type() {
    return this._type
  }

  // https://w3c.github.io/webcrypto/#dom-cryptokey-extractable
  get extractable() {
    return this._extractable
  }

  // https://w3c.github.io/webcrypto/#dom-cryptokey-algorithm
  get algorithm() {
    return this._algorithm
  }

  // https://w3c.github.io/webcrypto/#dom-cryptokey-usages
  get usages() {
    return this._usages
  }

  [Symbol.for('bare.inspect')]() {
    return {
      __proto__: { constructor: CryptoKey },

      type: this.type,
      extractable: this.extractable,
      algorithm: this.algorithm,
      usages: this.usages
    }
  }
}
{
  "name": "bare-crypto",
  "version": "1.12.0",
  "description": "Cryptographic primitives for JavaScript",
  "exports": {
    "./package": "./package.json",
    ".": {
      "types": "./index.d.ts",
      "default": "./index.js"
    },
    "./web": {
      "types": "./web.d.ts",
      "default": "./web.js"
    },
    "./global": {
      "types": "./global.d.ts",
      "default": "./global.js"
    },
    "./constants": "./lib/constants.js",
    "./errors": "./lib/errors.js"
  },
  "files": [
    "index.js",
    "index.d.ts",
    "web.js",
    "web.d.ts",
    "global.js",
    "global.d.ts",
    "binding.c",
    "binding.js",
    "CMakeLists.txt",
    "lib",
    "prebuilds"
  ],
  "addon": true,
  "scripts": {
    "test": "prettier . --check && bare test.js"
  },
  "repository": {
    "type": "git",
    "url": "git+https://github.com/holepunchto/bare-crypto.git"
  },
  "author": "Holepunch",
  "license": "Apache-2.0",
  "bugs": {
    "url": "https://github.com/holepunchto/bare-crypto/issues"
  },
  "homepage": "https://github.com/holepunchto/bare-crypto#readme",
  "dependencies": {
    "bare-stream": "^2.6.3"
  },
  "devDependencies": {
    "bare-buffer": "^3.0.1",
    "brittle": "^3.5.0",
    "cmake-bare": "^1.1.6",
    "cmake-fetch": "^1.0.0",
    "prettier": "^3.4.2",
    "prettier-config-holepunch": "^2.0.0"
  },
  "peerDependencies": {
    "bare-buffer": "*"
  },
  "peerDependenciesMeta": {
    "bare-buffer": {
      "optional": true
    }
  }
}
const crypto = require('.')
const errors = require('./lib/errors')
const CryptoKey = require('./lib/web/crypto-key')
const hmac = require('./lib/web/algorithm/hmac')
const pbkdf2 = require('./lib/web/algorithm/pbkdf2')
const ed25519 = require('./lib/web/algorithm/ed25519')
const sha = require('./lib/web/algorithm/sha')

exports.CryptoKey = CryptoKey

// https://w3c.github.io/webcrypto/#Crypto-method-getRandomValues
exports.getRandomValues = function getRandomValues(array) {
  return crypto.randomFillSync(array)
}

// https://w3c.github.io/webcrypto/#dfn-Crypto-method-randomUUID
exports.randomUUID = crypto.randomUUID

// https://w3c.github.io/webcrypto/#subtlecrypto-interface
exports.SubtleCrypto = class SubtleCrypto {
  // https://w3c.github.io/webcrypto/#SubtleCrypto-method-generateKey
  async generateKey(algorithm, extractable, usages) {
    if (typeof algorithm === 'string') algorithm = { name: algorithm }

    switch (algorithm.name.toLowerCase()) {
      case 'hmac':
        return hmac.generateKey(algorithm, extractable, usages)
      case 'ed25519':
        return ed25519.generateKey(algorithm, extractable, usages)
      default:
        throw errors.NOT_SUPPORTED(
          `Algorithm '${algorithm.name}' does not support the generateKey() operation`
        )
    }
  }

  // https://w3c.github.io/webcrypto/#SubtleCrypto-method-importKey
  async importKey(format, keyData, algorithm, extractable, usages) {
    if (typeof algorithm === 'string') algorithm = { name: algorithm }

    switch (format) {
      case 'raw':
      case 'pkcs8':
      case 'spki':
        if (ArrayBuffer.isView(keyData)) {
          keyData = Buffer.from(keyData)
        } else {
          keyData = Buffer.from(keyData.slice())
        }
        break
    }

    switch (algorithm.name.toLowerCase()) {
      case 'hmac':
        return hmac.importKey(format, keyData, algorithm, extractable, usages)
      case 'ed25519':
        return ed25519.importKey(format, keyData, algorithm, extractable, usages)
      case 'pbkdf2':
        return pbkdf2.importKey(format, keyData, algorithm, extractable, usages)
      default:
        throw errors.NOT_SUPPORTED(
          `Algorithm '${algorithm.name}' does not support the importKey() operation`
        )
    }
  }

  // https://w3c.github.io/webcrypto/#SubtleCrypto-method-exportKey
  async exportKey(format, key) {
    if (!key.extractable) {
      throw errors.INVALID_ACCESS('Key is not extractable')
    }

    switch (key.algorithm.name.toLowerCase()) {
      case 'hmac':
        return hmac.exportKey(format, key)
      case 'ed25519':
        return ed25519.exportKey(format, key)
      default:
        throw errors.NOT_SUPPORTED(
          `Algorithm '${key.algorithm.name}' does not support the exportKey() operation`
        )
    }
  }

  // https://w3c.github.io/webcrypto/#SubtleCrypto-method-sign
  async sign(algorithm, key, data) {
    if (typeof algorithm === 'string') algorithm = { name: algorithm }

    if (algorithm.name.toLowerCase() !== key.algorithm.name.toLowerCase()) {
      throw errors.INVALID_ACCESS(`Algorithm '${algorithm.name}' does not match key'`)
    }

    if (!key.usages.includes('sign')) {
      throw errors.INVALID_ACCESS('Key cannot be used for signing')
    }

    switch (algorithm.name.toLowerCase()) {
      case 'hmac':
        return hmac.sign(algorithm, key, data)
      case 'ed25519':
        return ed25519.sign(algorithm, key, data)
      default:
        throw errors.NOT_SUPPORTED(
          `Algorithm '${algorithm.name}' does not support the sign() operation`
        )
    }
  }

  // https://w3c.github.io/webcrypto/#SubtleCrypto-method-verify
  async verify(algorithm, key, signature, data) {
    if (typeof algorithm === 'string') algorithm = { name: algorithm }

    if (algorithm.name.toLowerCase() !== key.algorithm.name.toLowerCase()) {
      throw errors.INVALID_ACCESS(`Algorithm '${algorithm.name}' does not match key'`)
    }

    if (!key.usages.includes('verify')) {
      throw errors.INVALID_ACCESS('Key cannot be used for verification')
    }

    switch (algorithm.name.toLowerCase()) {
      case 'hmac':
        return hmac.verify(algorithm, key, signature, data)
      case 'ed25519':
        return ed25519.verify(algorithm, key, signature, data)
      default:
        throw errors.NOT_SUPPORTED(
          `Algorithm '${algorithm.name}' does not support the verify() operation`
        )
    }
  }

  // https://w3c.github.io/webcrypto/#SubtleCrypto-method-deriveBits
  async deriveBits(algorithm, key, length) {
    if (typeof algorithm === 'string') algorithm = { name: algorithm }

    if (algorithm.name.toLowerCase() !== key.algorithm.name.toLowerCase()) {
      throw errors.INVALID_ACCESS(`Algorithm '${algorithm.name}' does not match key'`)
    }

    if (!key.usages.includes('deriveBits')) {
      throw errors.INVALID_ACCESS('Key cannot be used to derive bits')
    }

    switch (algorithm.name.toLowerCase()) {
      case 'pbkdf2':
        return pbkdf2.deriveBits(algorithm, key, length)
      default:
        throw errors.NOT_SUPPORTED(
          `Algorithm '${algorithm.name}' does not support the deriveBits() operation`
        )
    }
  }

  // https://w3c.github.io/webcrypto/#SubtleCrypto-method-deriveKey
  async deriveKey(algorithm, baseKey, derivedKeyType, extractable, usages) {
    if (typeof algorithm === 'string') algorithm = { name: algorithm }

    if (typeof derivedKeyType === 'string') {
      derivedKeyType = { name: derivedKeyType }
    }

    if (algorithm.name.toLowerCase() !== baseKey.algorithm.name.toLowerCase()) {
      throw errors.INVALID_ACCESS(`Algorithm '${algorithm.name}' does not match key'`)
    }

    if (!baseKey.usages.includes('deriveKey')) {
      throw errors.INVALID_ACCESS('Key cannot be used to derive key')
    }

    let length

    switch (derivedKeyType.name.toLowerCase()) {
      case 'hmac':
        length = hmac.getKeyLength(derivedKeyType)
        break
      default:
        throw errors.NOT_SUPPORTED(
          `Algorithm '${derivedKeyType.name}' does not support the getKeyLength() operation`
        )
    }

    let secret

    switch (algorithm.name.toLowerCase()) {
      case 'pbkdf2':
        secret = pbkdf2.deriveBits(algorithm, baseKey, length)
        break
      default:
        throw errors.NOT_SUPPORTED(
          `Algorithm '${algorithm.name}' does not support the deriveBits() operation`
        )
    }

    return this.importKey('raw', secret, derivedKeyType, extractable, usages)
  }

  // https://w3c.github.io/webcrypto/#SubtleCrypto-method-digest
  async digest(algorithm, data) {
    if (typeof algorithm === 'string') algorithm = { name: algorithm }

    switch (algorithm.name.toLowerCase()) {
      case 'sha-1':
        return sha.digest('sha1', data)
      case 'sha-256':
        return sha.digest('sha256', data)
      case 'sha-512':
        return sha.digest('sha512', data)
      default:
        throw errors.NOT_SUPPORTED(
          `Algorithm '${algorithm.name}' does not support the digest() operation`
        )
    }
  }
}

exports.subtle = new exports.SubtleCrypto()

// https://w3c.github.io/webcrypto/#crypto-interface
exports.Crypto = class Crypto {
  get subtle() {
    return exports.subtle
  }

  getRandomValues(array) {
    return exports.getRandomValues(array)
  }

  randomUUID() {
    return exports.randomUUID()
  }
}
const errors = require('./lib/errors')

class EventListener {
  constructor() {
    this.list = []
    this.count = 0
  }

  append(ctx, name, fn, once) {
    this.count++
    ctx.emit('newListener', name, fn) // Emit BEFORE adding
    this.list.push([fn, once])
  }

  prepend(ctx, name, fn, once) {
    this.count++
    ctx.emit('newListener', name, fn) // Emit BEFORE adding
    this.list.unshift([fn, once])
  }

  remove(ctx, name, fn) {
    for (let i = 0, n = this.list.length; i < n; i++) {
      const l = this.list[i]

      if (l[0] === fn) {
        this.list.splice(i, 1)

        if (this.count === 1) delete ctx._events[name]

        ctx.emit('removeListener', name, fn) // Emit AFTER removing

        this.count--
        return
      }
    }
  }

  removeAll(ctx, name) {
    const list = [...this.list]
    this.list = []

    if (this.count === list.length) delete ctx._events[name]

    for (let i = list.length - 1; i >= 0; i--) {
      ctx.emit('removeListener', name, list[i][0]) // Emit AFTER removing
    }

    this.count -= list.length
  }

  emit(ctx, name, ...args) {
    const list = [...this.list]

    for (let i = 0, n = list.length; i < n; i++) {
      const l = list[i]

      if (l[1] === true) this.remove(ctx, name, l[0])

      Reflect.apply(l[0], ctx, args)
    }

    return list.length > 0
  }
}

function appendListener(ctx, name, fn, once) {
  if (ctx._events === undefined) ctx._events = Object.create(null)
  const e = ctx._events[name] || (ctx._events[name] = new EventListener())
  e.append(ctx, name, fn, once)
  return ctx
}

function prependListener(ctx, name, fn, once) {
  if (ctx._events === undefined) ctx._events = Object.create(null)
  const e = ctx._events[name] || (ctx._events[name] = new EventListener())
  e.prepend(ctx, name, fn, once)
  return ctx
}

function removeListener(ctx, name, fn) {
  if (ctx._events === undefined) return ctx
  const e = ctx._events[name]
  if (e !== undefined) e.remove(ctx, name, fn)
  return ctx
}

function throwUnhandledError(...args) {
  let err

  if (args.length > 0) err = args[0]

  if (err instanceof Error === false) err = errors.UNHANDLED_ERROR(err)

  if (Error.captureStackTrace) {
    Error.captureStackTrace(err, exports.prototype.emit)
  }

  queueMicrotask(() => {
    throw err
  })
}

module.exports = exports = class EventEmitter {
  constructor() {
    this._events = Object.create(null)
  }

  addListener(name, fn) {
    return appendListener(this, name, fn, false)
  }

  addOnceListener(name, fn) {
    return appendListener(this, name, fn, true)
  }

  prependListener(name, fn) {
    return prependListener(this, name, fn, false)
  }

  prependOnceListener(name, fn) {
    return prependListener(this, name, fn, true)
  }

  removeListener(name, fn) {
    return removeListener(this, name, fn)
  }

  on(name, fn) {
    return appendListener(this, name, fn, false)
  }

  once(name, fn) {
    return appendListener(this, name, fn, true)
  }

  off(name, fn) {
    return removeListener(this, name, fn)
  }

  emit(name, ...args) {
    if (name === 'error' && this._events !== undefined && this._events.error === undefined) {
      throwUnhandledError(...args)
    }

    if (this._events === undefined) return false
    const e = this._events[name]
    return e === undefined ? false : e.emit(this, name, ...args)
  }

  listeners(name) {
    if (this._events === undefined) return []
    const e = this._events[name]
    return e === undefined ? [] : [...e.list]
  }

  listenerCount(name) {
    if (this._events === undefined) return 0
    const e = this._events[name]
    return e === undefined ? 0 : e.list.length
  }

  getMaxListeners() {
    return EventEmitter.defaultMaxListeners
  }

  setMaxListeners(n) {}

  removeAllListeners(name) {
    if (arguments.length === 0) {
      for (const key of Reflect.ownKeys(this._events)) {
        if (key === 'removeListener') continue
        this.removeAllListeners(key)
      }
      this.removeAllListeners('removeListener')
    } else {
      const e = this._events[name]
      if (e !== undefined) e.removeAll(this, name)
    }
    return this
  }
}

exports.EventEmitter = exports

exports.errors = errors

exports.defaultMaxListeners = 10

exports.on = function on(emitter, name, opts = {}) {
  const { signal } = opts

  if (signal && signal.aborted) {
    throw errors.OPERATION_ABORTED(signal.reason)
  }

  let error = null
  let done = false

  const events = []
  const promises = []

  if (name !== 'error') emitter.on('error', onerror)

  if (signal) signal.addEventListener('abort', onabort)

  emitter.on(name, onevent)

  return {
    next() {
      if (events.length) {
        return Promise.resolve({ value: events.shift(), done: false })
      }

      if (error) {
        const err = error

        error = null

        return Promise.reject(err)
      }

      if (done) return onclose()

      return new Promise((resolve, reject) => promises.push({ resolve, reject }))
    },

    return() {
      return onclose()
    },

    throw(err) {
      return onerror(err)
    },

    [Symbol.asyncIterator]() {
      return this
    }
  }

  function onevent(...args) {
    if (promises.length) {
      promises.shift().resolve({ value: args, done: false })
    } else {
      events.push(args)
    }
  }

  function onerror(err) {
    emitter.off(name, onevent).off('error', onerror)

    if (promises.length) {
      promises.shift().reject(err)
    } else {
      error = err
    }

    return Promise.resolve({ done: true })
  }

  function onabort() {
    signal.removeEventListener('abort', onabort)

    onerror(errors.OPERATION_ABORTED(signal.reason))
  }

  function onclose() {
    emitter.off(name, onevent)

    if (name !== 'error') emitter.off('error', onerror)

    if (signal) signal.removeEventListener('abort', onabort)

    done = true

    if (promises.length) promises.shift().resolve({ done: true })

    return Promise.resolve({ done: true })
  }
}

exports.once = function once(emitter, name, opts = {}) {
  const { signal } = opts

  if (signal && signal.aborted) {
    return Promise.reject(errors.OPERATION_ABORTED(signal.reason))
  }

  return new Promise((resolve, reject) => {
    if (name !== 'error') emitter.on('error', onerror)

    if (signal) signal.addEventListener('abort', onabort)

    emitter.once(name, onevent)

    function onevent(...args) {
      if (name !== 'error') emitter.off('error', onerror)

      if (signal) signal.removeEventListener('abort', onabort)

      resolve(args)
    }

    function onerror(err) {
      emitter.off(name, onevent)

      if (name !== 'error') emitter.off('error', onerror)

      reject(err)
    }

    function onabort() {
      signal.removeEventListener('abort', onabort)

      onerror(errors.OPERATION_ABORTED(signal.reason))
    }
  })
}

exports.forward = function forward(from, to, names, opts = {}) {
  if (typeof names === 'string') names = [names]

  const { emit = to.emit.bind(to) } = opts

  const listeners = names.map(
    (name) =>
      function onevent(...args) {
        emit(name, ...args)
      }
  )

  to.on('newListener', (name) => {
    const i = names.indexOf(name)

    if (i !== -1 && to.listenerCount(name) === 0) {
      from.on(name, listeners[i])
    }
  }).on('removeListener', (name) => {
    const i = names.indexOf(name)

    if (i !== -1 && to.listenerCount(name) === 0) {
      from.off(name, listeners[i])
    }
  })
}

exports.listenerCount = function listenerCount(emitter, name) {
  return emitter.listenerCount(name)
}

exports.getMaxListeners = function getMaxListeners(emitter) {
  if (typeof emitter.getMaxListeners === 'function') {
    return emitter.getMaxListeners()
  }

  return exports.defaultMaxListeners
}

exports.setMaxListeners = function setMaxListeners(n, ...emitters) {
  if (emitters.length === 0) exports.defaultMaxListeners = n
  else {
    for (const emitter of emitters) {
      if (typeof emitter.setMaxListeners === 'function') {
        emitter.setMaxListeners(n)
      }
    }
  }
}
module.exports = class EventEmitterError extends Error {
  constructor(msg, code, fn = EventEmitterError, opts) {
    super(`${code}: ${msg}`, opts)
    this.code = code

    if (Error.captureStackTrace) {
      Error.captureStackTrace(this, fn)
    }
  }

  get name() {
    return 'EventEmitterError'
  }

  static OPERATION_ABORTED(cause, msg = 'Operation aborted') {
    return new EventEmitterError(msg, 'OPERATION_ABORTED', EventEmitterError.OPERATION_ABORTED, {
      cause
    })
  }

  static UNHANDLED_ERROR(cause, msg = 'Unhandled error') {
    return new EventEmitterError(msg, 'UNHANDLED_ERROR', EventEmitterError.UNHANDLED_ERROR, {
      cause
    })
  }
}
{
  "name": "bare-events",
  "version": "2.8.2",
  "description": "Event emitters for JavaScript",
  "exports": {
    "./package": "./package.json",
    ".": {
      "types": "./index.d.ts",
      "default": "./index.js"
    },
    "./global": {
      "types": "./global.d.ts",
      "default": "./global.js"
    },
    "./web": {
      "types": "./web.d.ts",
      "default": "./web.js"
    },
    "./errors": "./lib/errors.js"
  },
  "files": [
    "index.js",
    "index.d.ts",
    "global.js",
    "global.d.ts",
    "web.js",
    "web.d.ts",
    "lib"
  ],
  "scripts": {
    "test": "npm run lint && npm run test:bare && npm run test:node",
    "test:bare": "bare test.js",
    "test:node": "node test.js",
    "lint": "prettier . --check"
  },
  "repository": {
    "type": "git",
    "url": "git+https://github.com/holepunchto/bare-events.git"
  },
  "author": "Holepunch",
  "license": "Apache-2.0",
  "bugs": {
    "url": "https://github.com/holepunchto/bare-events/issues"
  },
  "homepage": "https://github.com/holepunchto/bare-events#readme",
  "devDependencies": {
    "bare-abort-controller": "^1.0.0",
    "brittle": "^3.3.2",
    "prettier": "^3.4.2",
    "prettier-config-holepunch": "^2.0.0",
    "uncaughts": "^1.1.1"
  },
  "peerDependencies": {
    "bare-abort-controller": "*"
  },
  "peerDependenciesMeta": {
    "bare-abort-controller": {
      "optional": true
    }
  }
}
module.exports = require.addon()
const FIFO = require('fast-fifo')
const EventEmitter = require('bare-events')
const path = require('bare-path')
const { fileURLToPath } = require('bare-url')
const { Readable, Writable } = require('bare-stream')
const binding = require('./binding')
const constants = require('./lib/constants')
const FileError = require('./lib/errors')

const isWindows = Bare.platform === 'win32'

exports.constants = constants

class FileRequest {
  static _free = []

  static borrow() {
    if (this._free.length > 0) return this._free.pop()
    return new FileRequest()
  }

  static return(req) {
    if (this._free.length < 32) this._free.push(req.reset())
    else req.destroy()
  }

  constructor() {
    this._reset()
    this._handle = binding.requestInit(this, this._onresult)
  }

  get handle() {
    return this._handle
  }

  retain(value) {
    this._retain = value // Tie the lifetime of `value` to the lifetime of `this`
  }

  reset() {
    if (this._handle === null) return this

    binding.requestReset(this._handle)

    this._reset()

    return this
  }

  destroy() {
    if (this._handle === null) return this

    binding.requestDestroy(this._handle)

    this._reset()
    this._handle = null

    return this
  }

  then(resolve, reject) {
    return this._promise.then(resolve, reject)
  }

  return() {
    if (this._handle === null) return this

    FileRequest.return(this)

    return this
  }

  [Symbol.dispose]() {
    this.return()
  }

  _reset() {
    const { promise, resolve, reject } = Promise.withResolvers()

    this._promise = promise
    this._resolve = resolve
    this._reject = reject
    this._retain = null
  }

  _onresult(err, status) {
    if (err) this._reject(err)
    else this._resolve(status)
  }
}

function ok(result, cb) {
  if (typeof result === 'function') {
    cb = result
    result = undefined
  }

  if (cb) cb(null, result)
  else return result
}

function fail(err, cb) {
  if (cb) cb(err)
  else throw err
}

function done(err, result, cb) {
  if (typeof result === 'function') {
    cb = result
    result = undefined
  }

  if (err) fail(err, cb)
  else return ok(result, cb)
}

async function open(filepath, flags = 'r', mode = 0o666, cb) {
  if (typeof flags === 'function') {
    cb = flags
    flags = 'r'
    mode = 0o666
  } else if (typeof mode === 'function') {
    cb = mode
    mode = 0o666
  }

  if (typeof flags === 'string') flags = toFlags(flags)
  if (typeof mode === 'string') mode = toMode(mode)

  filepath = toNamespacedPath(filepath)

  using req = FileRequest.borrow()

  let fd
  let err = null
  try {
    binding.open(req.handle, filepath, flags, mode)

    fd = await req
  } catch (e) {
    err = new FileError(e.message, {
      operation: 'open',
      code: e.code,
      path: filepath
    })
  }

  return done(err, fd, cb)
}

function openSync(filepath, flags = 'r', mode = 0o666) {
  if (typeof flags === 'string') flags = toFlags(flags)
  if (typeof mode === 'string') mode = toMode(mode)

  filepath = toNamespacedPath(filepath)

  using req = FileRequest.borrow()

  try {
    return binding.openSync(req.handle, filepath, flags, mode)
  } catch (e) {
    throw new FileError(e.message, {
      operation: 'open',
      code: e.code,
      path: filepath
    })
  }
}

async function close(fd, cb) {
  using req = FileRequest.borrow()

  let err = null
  try {
    binding.close(req.handle, fd)

    await req
  } catch (e) {
    err = new FileError(e.message, { operation: 'close', code: e.code, fd })
  }

  return done(err, cb)
}

function closeSync(fd) {
  using req = FileRequest.borrow()

  try {
    binding.closeSync(req.handle, fd)
  } catch (e) {
    throw new FileError(e.message, { operation: 'close', code: e.code, fd })
  }
}

async function access(filepath, mode = constants.F_OK, cb) {
  if (typeof mode === 'function') {
    cb = mode
    mode = constants.F_OK
  }

  filepath = toNamespacedPath(filepath)

  using req = FileRequest.borrow()

  let err = null
  try {
    binding.access(req.handle, filepath, mode)

    await req
  } catch (e) {
    err = new FileError(e.message, {
      operation: 'access',
      code: e.code,
      path: filepath
    })
  }

  return done(err, cb)
}

function accessSync(filepath, mode = constants.F_OK) {
  filepath = toNamespacedPath(filepath)

  using req = FileRequest.borrow()

  try {
    binding.accessSync(req.handle, filepath, mode)
  } catch (e) {
    throw new FileError(e.message, {
      operation: 'access',
      code: e.code,
      path: filepath
    })
  }
}

async function exists(filepath, cb) {
  let ok = true
  try {
    await access(filepath)
  } catch {
    ok = false
  }

  return done(null, ok, cb)
}

function existsSync(filepath) {
  try {
    accessSync(filepath)
  } catch {
    return false
  }

  return true
}

async function read(fd, buffer, offset = 0, len = buffer.byteLength - offset, pos = -1, cb) {
  if (typeof offset === 'function') {
    cb = offset
    offset = 0
    len = buffer.byteLength
    pos = -1
  } else if (typeof len === 'function') {
    cb = len
    len = buffer.byteLength - offset
    pos = -1
  } else if (typeof pos === 'function') {
    cb = pos
    pos = -1
  }

  if (typeof pos !== 'number') pos = -1

  using req = FileRequest.borrow()

  let bytes
  let err = null
  try {
    binding.read(req.handle, fd, buffer, offset, len, pos)

    bytes = await req
  } catch (e) {
    err = new FileError(e.message, { operation: 'read', code: e.code, fd })
  }

  return done(err, bytes, cb)
}

function readSync(fd, buffer, offset = 0, len = buffer.byteLength - offset, pos = -1) {
  using req = FileRequest.borrow()

  try {
    return binding.readSync(req.handle, fd, buffer, offset, len, pos)
  } catch (e) {
    throw new FileError(e.message, { operation: 'read', code: e.code, fd })
  }
}

async function readv(fd, buffers, pos = -1, cb) {
  if (typeof pos === 'function') {
    cb = pos
    pos = -1
  }

  if (typeof pos !== 'number') pos = -1

  using req = FileRequest.borrow()

  let bytes
  let err = null
  try {
    binding.readv(req.handle, fd, buffers, pos)

    bytes = await req
  } catch (e) {
    err = new FileError(e.message, { operation: 'readv', code: e.code, fd })
  }

  return done(err, bytes, cb)
}

function readvSync(fd, buffers, pos = -1) {
  if (typeof pos !== 'number') pos = -1

  using req = FileRequest.borrow()

  try {
    return binding.readvSync(req.handle, fd, buffers, pos)
  } catch (e) {
    throw new FileError(e.message, { operation: 'readv', code: e.code, fd })
  }
}

async function write(fd, data, offset = 0, len, pos = -1, cb) {
  if (typeof data === 'string') {
    let encoding = len
    cb = pos
    pos = offset

    if (typeof pos === 'function') {
      cb = pos
      pos = -1
      encoding = 'utf8'
    } else if (typeof encoding === 'function') {
      cb = encoding
      encoding = 'utf8'
    }

    if (typeof pos === 'string') {
      encoding = pos
      pos = -1
    }

    data = Buffer.from(data, encoding)
    offset = 0
    len = data.byteLength
  } else if (typeof offset === 'function') {
    cb = offset
    offset = 0
    len = data.byteLength
    pos = -1
  } else if (typeof len === 'function') {
    cb = len
    len = data.byteLength - offset
    pos = -1
  } else if (typeof pos === 'function') {
    cb = pos
    pos = -1
  }

  if (typeof len !== 'number') len = data.byteLength - offset
  if (typeof pos !== 'number') pos = -1

  using req = FileRequest.borrow()

  let bytes
  let err = null
  try {
    binding.write(req.handle, fd, data, offset, len, pos)

    bytes = await req
  } catch (e) {
    err = new FileError(e.message, { operation: 'write', code: e.code, fd })
  }

  return done(err, bytes, cb)
}

function writeSync(fd, data, offset = 0, len, pos = -1) {
  if (typeof data === 'string') {
    let encoding = len
    pos = offset

    if (typeof pos === 'string') {
      encoding = pos
      pos = -1
    }

    data = Buffer.from(data, encoding)
    offset = 0
    len = data.byteLength
  }

  if (typeof len !== 'number') len = data.byteLength - offset
  if (typeof pos !== 'number') pos = -1

  using req = FileRequest.borrow()

  try {
    return binding.writeSync(req.handle, fd, data, offset, len, pos)
  } catch (e) {
    throw new FileError(e.message, { operation: 'write', code: e.code, fd })
  }
}

async function writev(fd, buffers, pos = -1, cb) {
  if (typeof pos === 'function') {
    cb = pos
    pos = -1
  }

  if (typeof pos !== 'number') pos = -1

  using req = FileRequest.borrow()

  let bytes
  let err = null
  try {
    binding.writev(req.handle, fd, buffers, pos)

    bytes = await req
  } catch (e) {
    err = new FileError(e.message, { operation: 'writev', code: e.code, fd })
  }

  return done(err, bytes, cb)
}

function writevSync(fd, buffers, pos = -1) {
  if (typeof pos !== 'number') pos = -1

  using req = FileRequest.borrow()

  try {
    return binding.writevSync(req.handle, fd, buffers, pos)
  } catch (e) {
    throw new FileError(e.message, { operation: 'writev', code: e.code, fd })
  }
}

async function stat(filepath, cb) {
  filepath = toNamespacedPath(filepath)

  using req = FileRequest.borrow()

  let st
  let err = null
  try {
    binding.stat(req.handle, filepath)

    await req

    st = new Stats(...binding.requestResultStat(req.handle))
  } catch (e) {
    err = new FileError(e.message, {
      operation: 'stat',
      code: e.code,
      path: filepath
    })
  }

  return done(err, st, cb)
}

function statSync(filepath) {
  filepath = toNamespacedPath(filepath)

  using req = FileRequest.borrow()

  try {
    binding.statSync(req.handle, filepath)

    return new Stats(...binding.requestResultStat(req.handle))
  } catch (e) {
    throw new FileError(e.message, {
      operation: 'stat',
      code: e.code,
      path: filepath
    })
  }
}

async function lstat(filepath, cb) {
  filepath = toNamespacedPath(filepath)

  using req = FileRequest.borrow()

  let st
  let err = null
  try {
    binding.lstat(req.handle, filepath)

    await req

    st = new Stats(...binding.requestResultStat(req.handle))
  } catch (e) {
    err = new FileError(e.message, {
      operation: 'lstat',
      code: e.code,
      path: filepath
    })
  }

  return done(err, st, cb)
}

function lstatSync(filepath) {
  filepath = toNamespacedPath(filepath)

  using req = FileRequest.borrow()

  try {
    binding.lstatSync(req.handle, filepath)

    return new Stats(...binding.requestResultStat(req.handle))
  } catch (e) {
    throw new FileError(e.message, {
      operation: 'lstat',
      code: e.code,
      path: filepath
    })
  }
}

async function fstat(fd, cb) {
  using req = FileRequest.borrow()

  let st
  let err = null
  try {
    binding.fstat(req.handle, fd)

    await req

    st = new Stats(...binding.requestResultStat(req.handle))
  } catch (e) {
    err = new FileError(e.message, { operation: 'fstat', code: e.code, fd })
  }

  return done(err, st, cb)
}

function fstatSync(fd) {
  using req = FileRequest.borrow()

  try {
    binding.fstatSync(req.handle, fd)

    return new Stats(...binding.requestResultStat(req.handle))
  } catch (e) {
    throw new FileError(e.message, { operation: 'fstat', code: e.code, fd })
  }
}

async function ftruncate(fd, len = 0, cb) {
  if (typeof len === 'function') {
    cb = len
    len = 0
  }

  if (typeof len !== 'number') len = 0

  using req = FileRequest.borrow()

  let err = null
  try {
    binding.ftruncate(req.handle, fd, len)

    await req
  } catch (e) {
    err = new FileError(e.message, { operation: 'ftruncate', code: e.code, fd })
  }

  return done(err, cb)
}

function ftruncateSync(fd, len = 0) {
  if (typeof len !== 'number') len = 0

  using req = FileRequest.borrow()

  try {
    binding.ftruncateSync(req.handle, fd, len)
  } catch (e) {
    throw new FileError(e.message, { operation: 'ftruncate', code: e.code, fd })
  }
}

async function chmod(filepath, mode, cb) {
  if (typeof mode === 'string') mode = toMode(mode)

  filepath = toNamespacedPath(filepath)

  using req = FileRequest.borrow()

  let err = null
  try {
    binding.chmod(req.handle, filepath, mode)

    await req
  } catch (e) {
    err = new FileError(e.message, {
      operation: 'chmod',
      code: e.code,
      path: filepath
    })
  }

  return done(err, cb)
}

function chmodSync(filepath, mode) {
  if (typeof mode === 'string') mode = toMode(mode)

  filepath = toNamespacedPath(filepath)

  using req = FileRequest.borrow()

  try {
    binding.chmodSync(req.handle, filepath, mode)
  } catch (e) {
    throw new FileError(e.message, {
      operation: 'chmod',
      code: e.code,
      path: filepath
    })
  }
}

async function fchmod(fd, mode, cb) {
  if (typeof mode === 'string') mode = toMode(mode)

  using req = FileRequest.borrow()

  let err = null
  try {
    binding.fchmod(req.handle, fd, mode)

    await req
  } catch (e) {
    err = new FileError(e.message, { operation: 'fchmod', code: e.code, fd })
  }

  return done(err, cb)
}

function fchmodSync(fd, mode) {
  if (typeof mode === 'string') mode = toMode(mode)

  using req = FileRequest.borrow()

  try {
    binding.fchmodSync(req.handle, fd, mode)
  } catch (e) {
    throw new FileError(e.message, { operation: 'fchmod', code: e.code, fd })
  }
}

async function utimes(filepath, atime, mtime, cb) {
  if (typeof atime !== 'number') atime = atime.getTime() / 1000
  if (typeof mtime !== 'number') mtime = mtime.getTime() / 1000

  filepath = toNamespacedPath(filepath)

  using req = FileRequest.borrow()

  let err = null
  try {
    binding.utimes(req.handle, filepath, atime, mtime)

    await req
  } catch (e) {
    err = new FileError(e.message, {
      operation: 'utimes',
      code: e.code,
      path: filepath
    })
  }

  return done(err, cb)
}

function utimesSync(filepath, atime, mtime) {
  if (typeof atime !== 'number') atime = atime.getTime() / 1000
  if (typeof mtime !== 'number') mtime = mtime.getTime() / 1000

  filepath = toNamespacedPath(filepath)

  using req = FileRequest.borrow()

  try {
    binding.utimesSync(req.handle, filepath, atime, mtime)
  } catch (e) {
    throw new FileError(e.message, {
      operation: 'utimes',
      code: e.code,
      path: filepath
    })
  }
}

async function mkdir(filepath, opts, cb) {
  if (typeof opts === 'function') {
    cb = opts
    opts = { mode: 0o777 }
  }

  if (typeof opts === 'number') opts = { mode: opts }
  else if (!opts) opts = {}

  const mode = typeof opts.mode === 'number' ? opts.mode : 0o777

  filepath = toNamespacedPath(filepath)

  if (opts.recursive) {
    let err = null
    try {
      try {
        await mkdir(filepath, { mode })
      } catch (err) {
        if (err.code !== 'ENOENT') {
          if (!(await stat(filepath)).isDirectory()) throw err
        } else {
          while (filepath.endsWith(path.sep)) filepath = filepath.slice(0, -1)
          const i = filepath.lastIndexOf(path.sep)
          if (i <= 0) throw err

          await mkdir(filepath.slice(0, i), { mode, recursive: true })

          try {
            await mkdir(filepath, { mode })
          } catch (err) {
            if (!(await stat(filepath)).isDirectory()) throw err
          }
        }
      }
    } catch (e) {
      err = e
    }

    return done(err, cb)
  }

  using req = FileRequest.borrow()

  let err = null
  try {
    binding.mkdir(req.handle, filepath, mode)

    await req
  } catch (e) {
    err = new FileError(e.message, {
      operation: 'mkdir',
      code: e.code,
      path: filepath
    })
  }

  return done(err, cb)
}

function mkdirSync(filepath, opts) {
  if (typeof opts === 'number') opts = { mode: opts }
  else if (!opts) opts = {}

  const mode = typeof opts.mode === 'number' ? opts.mode : 0o777

  filepath = toNamespacedPath(filepath)

  if (opts.recursive) {
    try {
      mkdirSync(filepath, { mode })
    } catch (err) {
      if (err.code !== 'ENOENT') {
        if (!statSync(filepath).isDirectory()) throw err
      } else {
        while (filepath.endsWith(path.sep)) filepath = filepath.slice(0, -1)
        const i = filepath.lastIndexOf(path.sep)
        if (i <= 0) throw err

        mkdirSync(filepath.slice(0, i), { mode, recursive: true })

        try {
          mkdirSync(filepath, { mode })
        } catch (err) {
          if (!statSync(filepath).isDirectory()) throw err
        }
      }
    }

    return
  }

  using req = FileRequest.borrow()

  try {
    binding.mkdirSync(req.handle, filepath, mode)
  } catch (e) {
    throw new FileError(e.message, {
      operation: 'mkdir',
      code: e.code,
      path: filepath
    })
  }
}

async function rmdir(filepath, cb) {
  filepath = toNamespacedPath(filepath)

  using req = FileRequest.borrow()

  let err = null
  try {
    binding.rmdir(req.handle, filepath)

    await req
  } catch (e) {
    err = new FileError(e.message, {
      operation: 'rmdir',
      code: e.code,
      path: filepath
    })
  }

  return done(err, cb)
}

function rmdirSync(filepath) {
  filepath = toNamespacedPath(filepath)

  using req = FileRequest.borrow()

  try {
    binding.rmdirSync(req.handle, filepath)
  } catch (e) {
    throw new FileError(e.message, {
      operation: 'rmdir',
      code: e.code,
      path: filepath
    })
  }
}

async function rm(filepath, opts, cb) {
  if (typeof opts === 'function') {
    cb = opts
    opts = {}
  }

  if (!opts) opts = {}

  filepath = toNamespacedPath(filepath)

  let err = null
  try {
    const st = await lstat(filepath)

    if (st.isDirectory()) {
      if (opts.recursive) {
        try {
          await rmdir(filepath)
        } catch (err) {
          if (err.code !== 'ENOTEMPTY') throw err

          const files = await readdir(filepath)

          for (const file of files) {
            await rm(filepath + path.sep + file, opts)
          }

          await rmdir(filepath)
        }
      } else {
        throw new FileError('is a directory', {
          operation: 'rm',
          code: 'EISDIR',
          path: filepath
        })
      }
    } else {
      await unlink(filepath)
    }
  } catch (e) {
    if (e.code !== 'ENOENT' || !opts.force) err = e
  }

  return done(err, cb)
}

function rmSync(filepath, opts) {
  if (!opts) opts = {}

  filepath = toNamespacedPath(filepath)

  try {
    const st = lstatSync(filepath)

    if (st.isDirectory()) {
      if (opts.recursive) {
        try {
          rmdirSync(filepath)
        } catch (err) {
          if (err.code !== 'ENOTEMPTY') throw err

          const files = readdirSync(filepath)

          for (const file of files) {
            rmSync(filepath + path.sep + file, opts)
          }

          rmdirSync(filepath)
        }
      } else {
        throw new FileError('is a directory', {
          operation: 'rm',
          code: 'EISDIR',
          path: filepath
        })
      }
    } else {
      unlinkSync(filepath)
    }
  } catch (err) {
    if (err.code !== 'ENOENT' || !opts.force) throw err
  }
}

async function unlink(filepath, cb) {
  filepath = toNamespacedPath(filepath)

  using req = FileRequest.borrow()

  let err = null
  try {
    binding.unlink(req.handle, filepath)

    await req
  } catch (e) {
    err = new FileError(e.message, {
      operation: 'unlink',
      code: e.code,
      path: filepath
    })
  }

  return done(err, cb)
}

function unlinkSync(filepath) {
  filepath = toNamespacedPath(filepath)

  using req = FileRequest.borrow()

  try {
    binding.unlinkSync(req.handle, filepath)
  } catch (e) {
    throw new FileError(e.message, {
      operation: 'unlink',
      code: e.code,
      path: filepath
    })
  }
}

async function rename(src, dst, cb) {
  src = toNamespacedPath(src)
  dst = toNamespacedPath(dst)

  using req = FileRequest.borrow()

  let err = null
  try {
    binding.rename(req.handle, src, dst)

    await req
  } catch (e) {
    err = new FileError(e.message, {
      operation: 'rename',
      code: e.code,
      path: src,
      destination: dst
    })
  }

  return done(err, cb)
}

function renameSync(src, dst) {
  src = toNamespacedPath(src)
  dst = toNamespacedPath(dst)

  using req = FileRequest.borrow()

  try {
    binding.renameSync(req.handle, src, dst)
  } catch (e) {
    throw new FileError(e.message, {
      operation: 'rename',
      code: e.code,
      path: src,
      destination: dst
    })
  }
}

async function copyFile(src, dst, mode = 0, cb) {
  if (typeof mode === 'function') {
    cb = mode
    mode = 0
  }

  src = toNamespacedPath(src)
  dst = toNamespacedPath(dst)

  using req = FileRequest.borrow()

  let err = null
  try {
    binding.copyfile(req.handle, src, dst, mode)

    await req
  } catch (e) {
    err = new FileError(e.message, {
      operation: 'copyfile',
      code: e.code,
      path: src,
      destination: dst
    })
  }

  return done(err, cb)
}

function copyFileSync(src, dst, mode = 0) {
  src = toNamespacedPath(src)
  dst = toNamespacedPath(dst)

  using req = FileRequest.borrow()

  try {
    binding.copyfileSync(req.handle, src, dst, mode)
  } catch (e) {
    throw new FileError(e.message, {
      operation: 'copyfile',
      code: e.code,
      path: src,
      destination: dst
    })
  }
}

async function cp(src, dst, opts, cb) {
  if (typeof opts === 'function') {
    cb = opts
    opts = {}
  }

  if (!opts) opts = {}

  src = toNamespacedPath(src)
  dst = toNamespacedPath(dst)

  let err = null
  try {
    const st = await lstat(src)

    if (st.isDirectory()) {
      if (opts.recursive !== true) {
        throw new FileError('is a directory', {
          operation: 'cp',
          code: 'EISDIR',
          path: src
        })
      }

      try {
        await lstat(dst)
      } catch (e) {
        if (e.code === 'ENOENT') {
          await mkdir(dst, { mode: st.mode, recursive: true })
        } else {
          throw e
        }
      }

      const dir = await opendir(src)
      for await (const { name } of dir) {
        await cp(path.join(src, name), path.join(dst, name), opts)
      }
    } else if (st.isFile()) {
      await copyFile(src, dst)
      await chmod(dst, st.mode)
    }
  } catch (e) {
    err = e
  }

  return done(err, cb)
}

async function realpath(filepath, opts, cb) {
  if (typeof opts === 'function') {
    cb = opts
    opts = {}
  }

  if (typeof opts === 'string') opts = { encoding: opts }
  else if (!opts) opts = {}

  const { encoding = 'utf8' } = opts

  filepath = toNamespacedPath(filepath)

  using req = FileRequest.borrow()

  let res
  let err = null
  try {
    binding.realpath(req.handle, filepath)

    await req

    res = Buffer.from(binding.requestResultString(req.handle))

    if (encoding !== 'buffer') res = res.toString(encoding)
  } catch (e) {
    err = new FileError(e.message, {
      operation: 'realpath',
      code: e.code,
      path: filepath
    })
  }

  return done(err, res, cb)
}

function realpathSync(filepath, opts) {
  if (typeof opts === 'string') opts = { encoding: opts }
  else if (!opts) opts = {}

  const { encoding = 'utf8' } = opts

  filepath = toNamespacedPath(filepath)

  using req = FileRequest.borrow()

  try {
    binding.realpathSync(req.handle, filepath)

    let res = Buffer.from(binding.requestResultString(req.handle))

    if (encoding !== 'buffer') res = res.toString(encoding)

    return res
  } catch (e) {
    throw new FileError(e.message, {
      operation: 'realpath',
      code: e.code,
      path: filepath
    })
  }
}

async function readlink(filepath, opts, cb) {
  if (typeof opts === 'function') {
    cb = opts
    opts = {}
  }

  if (typeof opts === 'string') opts = { encoding: opts }
  else if (!opts) opts = {}

  const { encoding = 'utf8' } = opts

  filepath = toNamespacedPath(filepath)

  using req = FileRequest.borrow()

  let res
  let err = null
  try {
    binding.readlink(req.handle, filepath)

    await req

    res = Buffer.from(binding.requestResultString(req.handle))

    if (encoding !== 'buffer') res = res.toString(encoding)
  } catch (e) {
    err = new FileError(e.message, {
      operation: 'readlink',
      code: e.code,
      path: filepath
    })
  }

  return done(err, res, cb)
}

function readlinkSync(filepath, opts) {
  if (typeof opts === 'string') opts = { encoding: opts }
  else if (!opts) opts = {}

  const { encoding = 'utf8' } = opts

  filepath = toNamespacedPath(filepath)

  using req = FileRequest.borrow()

  try {
    binding.readlinkSync(req.handle, filepath)

    let res = Buffer.from(binding.requestResultString(req.handle))

    if (encoding !== 'buffer') res = res.toString(encoding)

    return res
  } catch (e) {
    throw new FileError(e.message, {
      operation: 'readlink',
      code: e.code,
      path: filepath
    })
  }
}

function normalizeSymlinkTarget(target, type, filepath) {
  if (isWindows) {
    if (type === 'junction') target = path.resolve(filepath, '..', target)

    if (path.isAbsolute(target)) return path.toNamespacedPath(target)

    return target.replace(/\//g, path.sep)
  }

  return target
}

async function symlink(target, filepath, type, cb) {
  if (typeof type === 'function') {
    cb = type
    type = null
  }

  filepath = toNamespacedPath(filepath)

  if (typeof type === 'string') {
    switch (type) {
      case 'file':
      default:
        type = 0
        break
      case 'dir':
        type = constants.UV_FS_SYMLINK_DIR
        break
      case 'junction':
        type = constants.UV_FS_SYMLINK_JUNCTION
        break
    }
  } else if (typeof type !== 'number') {
    if (isWindows) {
      target = path.resolve(filepath, '..', target)

      try {
        type = (await stat(target)).isDirectory()
          ? constants.UV_FS_SYMLINK_DIR
          : constants.UV_FS_SYMLINK_JUNCTION
      } catch {
        type = 0
      }
    } else {
      type = 0
    }
  }

  target = normalizeSymlinkTarget(target)

  using req = FileRequest.borrow()

  let err = null
  try {
    binding.symlink(req.handle, target, filepath, type)

    await req
  } catch (e) {
    err = new FileError(e.message, {
      operation: 'symlink',
      code: e.code,
      path: target,
      destination: filepath
    })
  }

  return done(err, cb)
}

function symlinkSync(target, filepath, type) {
  filepath = toNamespacedPath(filepath)

  if (typeof type === 'string') {
    switch (type) {
      case 'file':
      default:
        type = 0
        break
      case 'dir':
        type = constants.UV_FS_SYMLINK_DIR
        break
      case 'junction':
        type = constants.UV_FS_SYMLINK_JUNCTION
        break
    }
  } else if (typeof type !== 'number') {
    if (isWindows) {
      target = path.resolve(filepath, '..', target)

      try {
        type = statSync(target).isDirectory()
          ? constants.UV_FS_SYMLINK_DIR
          : constants.UV_FS_SYMLINK_JUNCTION
      } catch {
        type = 0
      }
    } else {
      type = 0
    }
  }

  target = normalizeSymlinkTarget(target)

  using req = FileRequest.borrow()

  try {
    binding.symlinkSync(req.handle, target, filepath, type)
  } catch (e) {
    throw new FileError(e.message, {
      operation: 'symlink',
      code: e.code,
      path: target,
      destination: filepath
    })
  }
}

async function opendir(filepath, opts, cb) {
  if (typeof opts === 'function') {
    cb = opts
    opts = {}
  }

  if (typeof opts === 'string') opts = { encoding: opts }
  else if (!opts) opts = {}

  filepath = toNamespacedPath(filepath)

  using req = FileRequest.borrow()

  let dir
  let err = null
  try {
    binding.opendir(req.handle, filepath)

    await req

    dir = new Dir(filepath, binding.requestResultDir(req.handle), opts)
  } catch (e) {
    err = new FileError(e.message, {
      operation: 'opendir',
      code: e.code,
      path: filepath
    })
  }

  return done(err, dir, cb)
}

function opendirSync(filepath, opts) {
  if (typeof opts === 'string') opts = { encoding: opts }
  else if (!opts) opts = {}

  filepath = toNamespacedPath(filepath)

  using req = FileRequest.borrow()

  try {
    binding.opendirSync(req.handle, filepath)

    return new Dir(filepath, binding.requestResultDir(req.handle), opts)
  } catch (e) {
    throw new FileError(e.message, {
      operation: 'opendir',
      code: e.code,
      path: filepath
    })
  }
}

async function readdir(filepath, opts, cb) {
  if (typeof opts === 'function') {
    cb = opts
    opts = {}
  }

  if (typeof opts === 'string') opts = { encoding: opts }
  else if (!opts) opts = {}

  const { withFileTypes = false } = opts

  filepath = toNamespacedPath(filepath)

  let result = []
  let err = null
  try {
    const dir = await opendir(filepath)

    for await (const entry of dir) {
      result.push(withFileTypes ? entry : entry.name)
    }
  } catch (e) {
    result = []
    err = e
  }

  return done(err, result, cb)
}

function readdirSync(filepath, opts) {
  if (typeof opts === 'string') opts = { encoding: opts }
  else if (!opts) opts = {}

  const { withFileTypes = false } = opts

  filepath = toNamespacedPath(filepath)

  const dir = opendirSync(filepath, opts)
  const result = []

  for (const entry of dir) {
    result.push(withFileTypes ? entry : entry.name)
  }

  return result
}

async function readFile(filepath, opts, cb) {
  if (typeof opts === 'function') {
    cb = opts
    opts = {}
  }

  if (typeof opts === 'string') opts = { encoding: opts }
  else if (!opts) opts = {}

  const { encoding = 'buffer' } = opts

  let fd = -1
  let buffer = null
  let err = null
  try {
    fd = await open(filepath, opts.flag || 'r')

    const st = await fstat(fd)

    let len = 0

    if (st.size === 0) {
      const buffers = []

      while (true) {
        buffer = Buffer.allocUnsafe(8192)
        const r = await read(fd, buffer)
        len += r
        if (r === 0) break
        buffers.push(buffer.subarray(0, r))
      }

      buffer = Buffer.concat(buffers)
    } else {
      buffer = Buffer.allocUnsafe(st.size)

      while (true) {
        const r = await read(fd, len ? buffer.subarray(len) : buffer)
        len += r
        if (r === 0 || len === buffer.byteLength) break
      }

      if (len !== buffer.byteLength) buffer = buffer.subarray(0, len)
    }

    if (encoding !== 'buffer') buffer = buffer.toString(encoding)
  } catch (e) {
    err = e
  } finally {
    if (fd !== -1) await close(fd)
  }

  return done(err, buffer, cb)
}

function readFileSync(filepath, opts) {
  if (typeof opts === 'string') opts = { encoding: opts }
  else if (!opts) opts = {}

  const { encoding = 'buffer' } = opts

  let fd = -1
  try {
    fd = openSync(filepath, opts.flag || 'r')

    const st = fstatSync(fd)

    let buffer
    let len = 0

    if (st.size === 0) {
      const buffers = []

      while (true) {
        buffer = Buffer.allocUnsafe(8192)
        const r = readSync(fd, buffer)
        len += r
        if (r === 0) break
        buffers.push(buffer.subarray(0, r))
      }

      buffer = Buffer.concat(buffers)
    } else {
      buffer = Buffer.allocUnsafe(st.size)

      while (true) {
        const r = readSync(fd, len ? buffer.subarray(len) : buffer)
        len += r
        if (r === 0 || len === buffer.byteLength) break
      }

      if (len !== buffer.byteLength) buffer = buffer.subarray(0, len)
    }

    if (encoding !== 'buffer') buffer = buffer.toString(encoding)

    return buffer
  } finally {
    if (fd !== -1) closeSync(fd)
  }
}

async function writeFile(filepath, data, opts, cb) {
  if (typeof opts === 'function') {
    cb = opts
    opts = {}
  }

  if (typeof opts === 'string') opts = { encoding: opts }
  else if (!opts) opts = {}

  if (typeof data === 'string') data = Buffer.from(data, opts.encoding)

  let fd = -1
  let len = 0
  let err = null
  try {
    fd = await open(filepath, opts.flag || 'w', opts.mode || 0o666)

    while (true) {
      len += await write(fd, len ? data.subarray(len) : data)
      if (len === data.byteLength) break
    }
  } catch (e) {
    err = e
  } finally {
    if (fd !== -1) await close(fd)
  }

  return done(err, len, cb)
}

function writeFileSync(filepath, data, opts) {
  if (typeof opts === 'string') opts = { encoding: opts }
  else if (!opts) opts = {}

  if (typeof data === 'string') data = Buffer.from(data, opts.encoding)

  let fd = -1
  try {
    fd = openSync(filepath, opts.flag || 'w', opts.mode || 0o666)

    let len = 0

    while (true) {
      len += writeSync(fd, len ? data.subarray(len) : data)
      if (len === data.byteLength) break
    }
  } finally {
    if (fd !== -1) closeSync(fd)
  }
}

function appendFile(filepath, data, opts, cb) {
  if (typeof opts === 'function') {
    cb = opts
    opts = {}
  }

  if (typeof opts === 'string') opts = { encoding: opts }
  else if (!opts) opts = {}

  if (!opts.flag) opts = { ...opts, flag: 'a' }

  return writeFile(filepath, data, opts, cb)
}

function appendFileSync(filepath, data, opts) {
  if (typeof opts === 'string') opts = { encoding: opts }
  else if (!opts) opts = {}

  if (!opts.flag) opts = { ...opts, flag: 'a' }

  return writeFileSync(filepath, data, opts)
}

function watch(filepath, opts, cb) {
  if (typeof opts === 'function') {
    cb = opts
    opts = {}
  }

  if (typeof opts === 'string') opts = { encoding: opts }
  else if (!opts) opts = {}

  filepath = toNamespacedPath(filepath)

  return new Watcher(filepath, opts, cb)
}

class Stats {
  constructor(
    dev,
    mode,
    nlink,
    uid,
    gid,
    rdev,
    blksize,
    ino,
    size,
    blocks,
    atimeMs,
    mtimeMs,
    ctimeMs,
    birthtimeMs
  ) {
    this.dev = dev
    this.mode = mode
    this.nlink = nlink
    this.uid = uid
    this.gid = gid
    this.rdev = rdev
    this.blksize = blksize
    this.ino = ino
    this.size = size
    this.blocks = blocks
    this.atimeMs = atimeMs
    this.mtimeMs = mtimeMs
    this.ctimeMs = ctimeMs
    this.birthtimeMs = birthtimeMs
    this.atime = new Date(atimeMs)
    this.mtime = new Date(mtimeMs)
    this.ctime = new Date(ctimeMs)
    this.birthtime = new Date(birthtimeMs)
  }

  isDirectory() {
    return (this.mode & constants.S_IFMT) === constants.S_IFDIR
  }

  isFile() {
    return (this.mode & constants.S_IFMT) === constants.S_IFREG
  }

  isBlockDevice() {
    return (this.mode & constants.S_IFMT) === constants.S_IFBLK
  }

  isCharacterDevice() {
    return (this.mode & constants.S_IFCHR) === constants.S_IFCHR
  }

  isFIFO() {
    return (this.mode & constants.S_IFMT) === constants.S_IFIFO
  }

  isSymbolicLink() {
    return (this.mode & constants.S_IFMT) === constants.S_IFLNK
  }

  isSocket() {
    return (this.mode & constants.S_IFMT) === constants.S_IFSOCK
  }
}

class Dir {
  constructor(path, handle, opts = {}) {
    const { encoding = 'utf8', bufferSize = 32 } = opts

    this.path = path

    this._encoding = encoding
    this._capacity = bufferSize
    this._buffer = new FIFO()
    this._ended = false
    this._handle = handle
  }

  async read(cb) {
    if (this._buffer.length) return ok(this._buffer.shift(), cb)
    if (this._ended) return ok(null, cb)

    using req = FileRequest.borrow()

    let entries
    let err = null
    try {
      req.retain(binding.readdir(req.handle, this._handle, this._capacity))

      await req

      entries = binding.requestResultDirents(req.handle)
    } catch (e) {
      err = new FileError(e.message, {
        operation: 'readdir',
        code: e.code,
        path: this.path
      })
    }

    if (err) return fail(err, cb)

    if (entries.length === 0) {
      this._ended = true

      return ok(null, cb)
    }

    for (const entry of entries) {
      let name = Buffer.from(entry.name)

      if (this._encoding !== 'buffer') name = name.toString(this._encoding)

      this._buffer.push(new Dirent(this.path, name, entry.type))
    }

    return ok(this._buffer.shift(), cb)
  }

  readSync() {
    if (this._buffer.length) return this._buffer.shift()
    if (this._ended) return null

    using req = FileRequest.borrow()

    let entries
    try {
      req.retain(binding.readdirSync(req.handle, this._handle, this._capacity))

      entries = binding.requestResultDirents(req.handle)
    } catch (e) {
      throw new FileError(e.message, {
        operation: 'readdir',
        code: e.code,
        path: this.path
      })
    }

    if (entries.length === 0) {
      this._ended = true

      return null
    }

    for (const entry of entries) {
      let name = Buffer.from(entry.name)

      if (this._encoding !== 'buffer') name = name.toString(this._encoding)

      this._buffer.push(new Dirent(this.path, name, entry.type))
    }

    return this._buffer.shift()
  }

  async close(cb) {
    using req = FileRequest.borrow()

    let err = null
    try {
      binding.closedir(req.handle, this._handle)

      await req
    } catch (e) {
      err = new FileError(e.message, {
        operation: 'closedir',
        code: e.code,
        path: this.path
      })
    }

    this._handle = null

    return done(err, cb)
  }

  closeSync() {
    using req = FileRequest.borrow()

    try {
      binding.closedirSync(req.handle, this._handle)
    } catch (e) {
      throw new FileError(e.message, {
        operation: 'closedir',
        code: e.code,
        path: this.path
      })
    }

    this._handle = null
  }

  [Symbol.dispose]() {
    this.closeSync()
  }

  async [Symbol.asyncDispose]() {
    await this.close()
  }

  *[Symbol.iterator]() {
    while (true) {
      const entry = this.readSync()
      if (entry === null) break
      yield entry
    }

    this.closeSync()
  }

  async *[Symbol.asyncIterator]() {
    while (true) {
      const entry = await this.read()
      if (entry === null) break
      yield entry
    }

    await this.close()
  }
}

class Dirent {
  constructor(parentPath, name, type) {
    this.parentPath = parentPath
    this.name = name
    this.type = type
  }

  isFile() {
    return this.type === constants.UV_DIRENT_FILE
  }

  isDirectory() {
    return this.type === constants.UV_DIRENT_DIR
  }

  isSymbolicLink() {
    return this.type === constants.UV_DIRENT_LINK
  }

  isFIFO() {
    return this.type === constants.UV_DIRENT_FIFO
  }

  isSocket() {
    return this.type === constants.UV_DIRENT_SOCKET
  }

  isCharacterDevice() {
    return this.type === constants.UV_DIRENT_CHAR
  }

  isBlockDevice() {
    return this.type === constants.UV_DIRENT_BLOCK
  }
}

class FileReadStream extends Readable {
  constructor(path, opts = {}) {
    const { eagerOpen = true } = opts

    super({ eagerOpen, ...opts })

    this.path = path
    this.fd = typeof opts.fd === 'number' ? opts.fd : -1
    this.flags = opts.flags || 'r'
    this.mode = opts.mode || 0o666

    this._offset = opts.start || 0
    this._missing = 0

    if (opts.length) {
      this._missing = opts.length
    } else if (typeof opts.end === 'number') {
      this._missing = opts.end - this._offset + 1
    } else {
      this._missing = -1
    }
  }

  async _open(cb) {
    let err

    if (this.fd === -1) {
      err = null
      try {
        this.fd = await open(this.path, this.flags, this.mode)
      } catch (e) {
        err = e
      }

      if (err) return cb(err)
    }

    let st
    err = null
    try {
      st = await fstat(this.fd)
    } catch (e) {
      err = e
    }

    if (err) return cb(err)

    if (this._missing === -1) this._missing = st.size

    if (st.size < this._offset) {
      this._offset = st.size
      this._missing = 0
    } else if (st.size < this._offset + this._missing) {
      this._missing = st.size - this._offset
    }

    cb(null)
  }

  async _read(size) {
    if (this._missing <= 0) return this.push(null)

    const data = Buffer.allocUnsafe(Math.min(this._missing, size))

    let len
    let err = null
    try {
      len = await read(this.fd, data, 0, data.byteLength, this._offset)
    } catch (e) {
      err = e
    }

    if (err) return this.destroy(err)

    if (len === 0) return this.push(null)

    if (this._missing < len) len = this._missing

    this._missing -= len
    this._offset += len

    this.push(data.subarray(0, len))
  }

  async _destroy(err, cb) {
    if (this.fd === -1) return cb(err)

    err = null
    try {
      await close(this.fd)
    } catch (e) {
      err = e
    }

    cb(err)
  }
}

class FileWriteStream extends Writable {
  constructor(path, opts = {}) {
    const { eagerOpen = true } = opts

    super({ eagerOpen, ...opts })

    this.path = path
    this.fd = typeof opts.fd === 'number' ? opts.fd : -1
    this.flags = opts.flags || 'w'
    this.mode = opts.mode || 0o666
  }

  async _open(cb) {
    if (this.fd !== -1) return cb(null)

    let err = null
    try {
      this.fd = await open(this.path, this.flags, this.mode)
    } catch (e) {
      err = e
    }

    cb(err)
  }

  async _writev(batch, cb) {
    let err = null
    try {
      await writev(
        this.fd,
        batch.map(({ chunk }) => chunk)
      )
    } catch (e) {
      err = e
    }

    cb(err)
  }

  async _destroy(err, cb) {
    if (this.fd === -1) return cb(err)

    err = null
    try {
      await close(this.fd)
    } catch (e) {
      err = e
    }

    cb(err)
  }
}

class Watcher extends EventEmitter {
  constructor(path, opts, onchange) {
    if (typeof opts === 'function') {
      onchange = opts
      opts = {}
    }

    if (!opts) opts = {}

    const { persistent = true, recursive = false, encoding = 'utf8' } = opts

    super()

    this._closed = false
    this._encoding = encoding
    this._handle = binding.watcherInit(path, recursive, this, this._onevent, this._onclose)

    if (!persistent) this.unref()

    if (onchange) this.on('change', onchange)
  }

  close() {
    if (this._closed) return
    this._closed = true

    binding.watcherClose(this._handle)
  }

  ref() {
    if (this._handle) binding.watcherRef(this._handle)
    return this
  }

  unref() {
    if (this._handle) binding.watcherUnref(this._handle)
    return this
  }

  [Symbol.asyncIterator]() {
    const buffer = []
    let done = false
    let error = null
    let next = null

    this.on('change', (eventType, filename) => {
      if (next) {
        next.resolve({ done: false, value: { eventType, filename } })
        next = null
      } else {
        buffer.push({ eventType, filename })
      }
    })
      .on('error', (err) => {
        done = true
        error = err

        if (next) {
          next.reject(error)
          next = null
        }
      })
      .on('close', () => {
        done = true

        if (next) {
          next.resolve({ done })
          next = null
        }
      })

    return {
      next: () =>
        new Promise((resolve, reject) => {
          if (error) return reject(error)

          if (buffer.length) return resolve({ done: false, value: buffer.shift() })

          if (done) return resolve({ done })

          next = { resolve, reject }
        })
    }
  }

  _onevent(err, events, filename) {
    if (err) {
      this.close()
      this.emit('error', err)
    } else {
      const path =
        this._encoding === 'buffer'
          ? Buffer.from(filename)
          : Buffer.from(filename).toString(this._encoding)

      if (events & binding.UV_RENAME) {
        this.emit('change', 'rename', path)
      }

      if (events & binding.UV_CHANGE) {
        this.emit('change', 'change', path)
      }
    }
  }

  _onclose() {
    this._handle = null

    this.emit('close')
  }
}

exports.access = access
exports.appendFile = appendFile
exports.chmod = chmod
// exports.chown = chown TODO
exports.close = close
exports.copyFile = copyFile
exports.cp = cp
exports.exists = exists
exports.fchmod = fchmod
// exports.fchown = fchown TODO
// exports.fdatasync = fdatasync TODO
exports.fstat = fstat
// exports.fsync = fsync TODO
exports.ftruncate = ftruncate
// exports.futimes = futimes TODO
// exports.lchmod = lchmod TODO
// exports.lchown = lchown TODO
// exports.lutimes = lutimes TODO
// exports.link = link TODO
exports.lstat = lstat
exports.mkdir = mkdir
// exports.mkdtemp = mkdtemp TODO
exports.open = open
exports.opendir = opendir
exports.read = read
exports.readFile = readFile
exports.readdir = readdir
exports.readlink = readlink
exports.readv = readv
exports.realpath = realpath
exports.rename = rename
exports.rm = rm
exports.rmdir = rmdir
exports.stat = stat
// exports.statfs = statfs TODO
exports.symlink = symlink
// exports.truncate = truncate TODO
exports.unlink = unlink
exports.utimes = utimes
exports.watch = watch
exports.write = write
exports.writeFile = writeFile
exports.writev = writev

exports.accessSync = accessSync
exports.appendFileSync = appendFileSync
exports.chmodSync = chmodSync
// exports.chownSync = chownSync TODO
exports.closeSync = closeSync
exports.copyFileSync = copyFileSync
exports.existsSync = existsSync
exports.fchmodSync = fchmodSync
// exports.fchownSync = fchownSync TODO
// exports.fdatasyncSync = fdatasyncSync TODO
exports.fstatSync = fstatSync
// exports.fsyncSync = fsyncSync TODO
exports.ftruncateSync = ftruncateSync
// exports.futimesSync = futimesSync TODO
// exports.lchmodSync = lchmodSync TODO
// exports.lchownSync = lchownSync TODO
// exports.lutimesSync = lutimesSync TODO
// exports.linkSync = linkSync TODO
exports.lstatSync = lstatSync
exports.mkdirSync = mkdirSync
// exports.mkdtempSync = mkdtempSync TODO
exports.openSync = openSync
exports.opendirSync = opendirSync
exports.readFileSync = readFileSync
exports.readSync = readSync
exports.readdirSync = readdirSync
exports.readlinkSync = readlinkSync
exports.readvSync = readvSync
exports.realpathSync = realpathSync
exports.renameSync = renameSync
exports.rmSync = rmSync
exports.rmdirSync = rmdirSync
exports.statSync = statSync
// exports.statfsSync = statfsSync TODO
exports.symlinkSync = symlinkSync
// exports.truncateSync = truncateSync TODO
exports.unlinkSync = unlinkSync
exports.utimesSync = utimesSync
exports.writeFileSync = writeFileSync
exports.writeSync = writeSync
exports.writevSync = writevSync

exports.promises = require('./promises')

exports.Stats = Stats
exports.Dir = Dir
exports.Dirent = Dirent
exports.Watcher = Watcher

exports.ReadStream = FileReadStream

exports.createReadStream = function createReadStream(path, opts) {
  return new FileReadStream(path, opts)
}

exports.WriteStream = FileWriteStream

exports.createWriteStream = function createWriteStream(path, opts) {
  return new FileWriteStream(path, opts)
}

function toNamespacedPath(filepath) {
  if (typeof filepath !== 'string') {
    if (URL.isURL(filepath)) filepath = fileURLToPath(filepath)
    else filepath = filepath.toString()
  }

  return path.toNamespacedPath(filepath)
}

function toFlags(flags) {
  switch (flags) {
    case 'r':
      return constants.O_RDONLY
    case 'rs': // Fall through.
    case 'sr':
      return constants.O_RDONLY | constants.O_SYNC
    case 'r+':
      return constants.O_RDWR
    case 'rs+': // Fall through.
    case 'sr+':
      return constants.O_RDWR | constants.O_SYNC

    case 'w':
      return constants.O_TRUNC | constants.O_CREAT | constants.O_WRONLY
    case 'wx': // Fall through.
    case 'xw':
      return constants.O_TRUNC | constants.O_CREAT | constants.O_WRONLY | constants.O_EXCL

    case 'w+':
      return constants.O_TRUNC | constants.O_CREAT | constants.O_RDWR
    case 'wx+': // Fall through.
    case 'xw+':
      return constants.O_TRUNC | constants.O_CREAT | constants.O_RDWR | constants.O_EXCL

    case 'a':
      return constants.O_APPEND | constants.O_CREAT | constants.O_WRONLY
    case 'ax': // Fall through.
    case 'xa':
      return constants.O_APPEND | constants.O_CREAT | constants.O_WRONLY | constants.O_EXCL
    case 'as': // Fall through.
    case 'sa':
      return constants.O_APPEND | constants.O_CREAT | constants.O_WRONLY | constants.O_SYNC

    case 'a+':
      return constants.O_APPEND | constants.O_CREAT | constants.O_RDWR
    case 'ax+': // Fall through.
    case 'xa+':
      return constants.O_APPEND | constants.O_CREAT | constants.O_RDWR | constants.O_EXCL
    case 'as+': // Fall through.
    case 'sa+':
      return constants.O_APPEND | constants.O_CREAT | constants.O_RDWR | constants.O_SYNC
    default:
      return 0
  }
}

function toMode(mode) {
  return parseInt(mode, 8)
}
const binding = require('../binding')

module.exports = {
  O_RDWR: binding.O_RDWR,
  O_RDONLY: binding.O_RDONLY,
  O_WRONLY: binding.O_WRONLY,
  O_CREAT: binding.O_CREAT,
  O_TRUNC: binding.O_TRUNC,
  O_APPEND: binding.O_APPEND,

  F_OK: binding.F_OK || 0,
  R_OK: binding.R_OK || 0,
  W_OK: binding.W_OK || 0,
  X_OK: binding.X_OK || 0,

  S_IFMT: binding.S_IFMT,
  S_IFREG: binding.S_IFREG,
  S_IFDIR: binding.S_IFDIR,
  S_IFCHR: binding.S_IFCHR,
  S_IFLNK: binding.S_IFLNK,
  S_IFBLK: binding.S_IFBLK || 0,
  S_IFIFO: binding.S_IFIFO || 0,
  S_IFSOCK: binding.S_IFSOCK || 0,

  S_IRUSR: binding.S_IRUSR || 0,
  S_IWUSR: binding.S_IWUSR || 0,
  S_IXUSR: binding.S_IXUSR || 0,
  S_IRGRP: binding.S_IRGRP || 0,
  S_IWGRP: binding.S_IWGRP || 0,
  S_IXGRP: binding.S_IXGRP || 0,
  S_IROTH: binding.S_IROTH || 0,
  S_IWOTH: binding.S_IWOTH || 0,
  S_IXOTH: binding.S_IXOTH || 0,

  UV_DIRENT_UNKNOWN: binding.UV_DIRENT_UNKNOWN,
  UV_DIRENT_FILE: binding.UV_DIRENT_FILE,
  UV_DIRENT_DIR: binding.UV_DIRENT_DIR,
  UV_DIRENT_LINK: binding.UV_DIRENT_LINK,
  UV_DIRENT_FIFO: binding.UV_DIRENT_FIFO,
  UV_DIRENT_SOCKET: binding.UV_DIRENT_SOCKET,
  UV_DIRENT_CHAR: binding.UV_DIRENT_CHAR,
  UV_DIRENT_BLOCK: binding.UV_DIRENT_BLOCK,

  COPYFILE_EXCL: binding.UV_FS_COPYFILE_EXCL,
  COPYFILE_FICLONE: binding.UV_FS_COPYFILE_FICLONE,
  COPYFILE_FICLONE_FORCE: binding.UV_FS_COPYFILE_FICLONE_FORCE,
  UV_FS_SYMLINK_DIR: binding.UV_FS_SYMLINK_DIR,
  UV_FS_SYMLINK_JUNCTION: binding.UV_FS_SYMLINK_JUNCTION
}
const os = require('bare-os')

module.exports = class FileError extends Error {
  constructor(msg, opts = {}) {
    const { code, operation = null, path = null, destination = null, fd = -1 } = opts

    if (operation !== null) msg += describe(operation, opts)

    super(`${code}: ${msg}`)

    this.code = code

    if (operation !== null) this.operation = operation
    if (path !== null) this.path = path
    if (destination !== null) this.destination = destination
    if (fd !== -1) this.fd = fd
  }

  get name() {
    return 'FileError'
  }

  // For Node.js compatibility
  get errno() {
    return os.constants.errnos[this.code]
  }

  // For Node.js compatibility
  get syscall() {
    return this.operation
  }

  // For Node.js compatibility
  get dest() {
    return this.destination
  }
}

function describe(operation, opts) {
  const { path = null, destination = null, fd = -1 } = opts

  let result = `, ${operation}`

  if (path !== null) {
    result += ` ${JSON.stringify(path)}`

    if (destination !== null) {
      result += ` -> ${JSON.stringify(destination)}`
    }
  } else if (fd !== -1) {
    result += ` ${fd}`
  }

  return result
}
{
  "name": "bare-fs",
  "version": "4.5.2",
  "description": "Native file system operations for Javascript",
  "exports": {
    "./package": "./package.json",
    ".": {
      "types": "./index.d.ts",
      "default": "./index.js"
    },
    "./promises": {
      "types": "./promises.d.ts",
      "default": "./promises.js"
    },
    "./constants": {
      "types": "./lib/constants.d.ts",
      "default": "./lib/constants.js"
    }
  },
  "files": [
    "index.js",
    "index.d.ts",
    "promises.js",
    "promises.d.ts",
    "binding.c",
    "binding.js",
    "CMakeLists.txt",
    "lib",
    "prebuilds"
  ],
  "addon": true,
  "scripts": {
    "test": "prettier . --check && bare test.js"
  },
  "repository": {
    "type": "git",
    "url": "git+https://github.com/holepunchto/bare-fs.git"
  },
  "author": "Holepunch",
  "license": "Apache-2.0",
  "bugs": {
    "url": "https://github.com/holepunchto/bare-fs/issues"
  },
  "homepage": "https://github.com/holepunchto/bare-fs#readme",
  "engines": {
    "bare": ">=1.16.0"
  },
  "dependencies": {
    "bare-events": "^2.5.4",
    "bare-path": "^3.0.0",
    "bare-stream": "^2.6.4",
    "bare-url": "^2.2.2",
    "fast-fifo": "^1.3.2"
  },
  "devDependencies": {
    "bare-buffer": "^3.0.2",
    "bare-crypto": "^1.11.2",
    "brittle": "^3.1.1",
    "cmake-bare": "^1.1.7",
    "prettier": "^3.4.1",
    "prettier-config-holepunch": "^2.0.0"
  },
  "peerDependencies": {
    "bare-buffer": "*"
  },
  "peerDependenciesMeta": {
    "bare-buffer": {
      "optional": true
    }
  }
}
const EventEmitter = require('bare-events')
const fs = require('.')

class FileHandle extends EventEmitter {
  constructor(fd) {
    this.fd = fd
  }

  async close() {
    await fs.close(fd)

    this.fd = -1
    this.emit('close')
  }

  async read(buffer, ...args) {
    return {
      bytesRead: await fs.read(this.fd, buffer, ...args),
      buffer
    }
  }

  async readv(buffers, ...args) {
    return {
      bytesRead: await fs.readv(this.fd, buffers, ...args),
      buffers
    }
  }

  async write(buffer, ...args) {
    return {
      bytesWritten: await fs.write(this.fd, buffer, ...args),
      buffer
    }
  }

  async writev(buffers, ...args) {
    return {
      bytesWritten: await fs.writev(this.fd, buffers, ...args),
      buffers
    }
  }

  async stat() {
    return fs.fstat(this.fd)
  }

  async chmod(mode) {
    await fs.fchmod(this.fd, mode)
  }

  createReadStream(opts) {
    return fs.createReadStream(null, { ...opts, fd: this.fd })
  }

  createWriteStream(opts) {
    return fs.createWriteStream(null, { ...opts, fd: this.fd })
  }

  async [Symbol.asyncDispose]() {
    await this.close()
  }
}

exports.open = async function open(filepath, flags, mode) {
  return new FileHandle(await fs.open(filepath, flags, mode))
}

exports.access = fs.access
exports.appendFile = fs.appendFile
exports.chmod = fs.chmod
exports.constants = fs.constants
exports.copyFile = fs.copyFile
exports.cp = fs.cp
exports.lstat = fs.lstat
exports.mkdir = fs.mkdir
exports.opendir = fs.opendir
exports.readFile = fs.readFile
exports.readdir = fs.readdir
exports.readlink = fs.readlink
exports.realpath = fs.realpath
exports.rename = fs.rename
exports.rm = fs.rm
exports.rmdir = fs.rmdir
exports.stat = fs.stat
exports.symlink = fs.symlink
exports.unlink = fs.unlink
exports.utimes = fs.utimes
exports.watch = fs.watch
exports.writeFile = fs.writeFile
module.exports = require.addon()
const binding = require('./binding')
const errors = require('./lib/errors')
const constants = require('./lib/constants')

exports.constants = constants

exports.EOL = binding.platform === 'win32' ? '\r\n' : '\n'

exports.platform = function platform() {
  return binding.platform
}

exports.arch = function arch() {
  return binding.arch
}

exports.type = binding.type
exports.version = binding.version
exports.release = binding.release
exports.machine = binding.machine
exports.execPath = binding.execPath
exports.pid = binding.pid
exports.ppid = binding.ppid
exports.cwd = binding.cwd
exports.chdir = binding.chdir
exports.tmpdir = binding.tmpdir
exports.homedir = binding.homedir
exports.hostname = binding.hostname
exports.userInfo = binding.userInfo

exports.kill = function kill(pid, signal = constants.signals.SIGTERM) {
  if (typeof signal === 'string') {
    if (signal in constants.signals === false) {
      throw errors.UNKNOWN_SIGNAL('Unknown signal: ' + signal)
    }

    signal = constants.signals[signal]
  }

  binding.kill(pid, signal)
}

exports.endianness = function endianness() {
  return binding.isLittleEndian ? 'LE' : 'BE'
}

exports.availableParallelism = binding.availableParallelism

exports.cpuUsage = function cpuUsage(previous) {
  const current = binding.cpuUsage()

  if (previous) {
    return {
      user: current.user - previous.user,
      system: current.system - previous.system
    }
  }

  return current
}

exports.threadCpuUsage = function threadCpuUsage(previous) {
  const current = binding.threadCpuUsage()

  if (previous) {
    return {
      user: current.user - previous.user,
      system: current.system - previous.system
    }
  }

  return current
}

exports.resourceUsage = binding.resourceUsage
exports.memoryUsage = binding.memoryUsage
exports.freemem = binding.freemem
exports.totalmem = binding.totalmem
exports.uptime = binding.uptime
exports.loadavg = binding.loadavg
exports.cpus = binding.cpus

exports.getProcessTitle = binding.getProcessTitle

exports.setProcessTitle = function setProcessTitle(title) {
  if (typeof title !== 'string') title = title.toString()

  if (title.length >= 256) {
    throw errors.TITLE_OVERFLOW('Process title is too long')
  }

  binding.setProcessTitle(title)
}

exports.getEnvKeys = binding.getEnvKeys
exports.getEnv = binding.getEnv
exports.hasEnv = binding.hasEnv
exports.setEnv = binding.setEnv
exports.unsetEnv = binding.unsetEnv
const binding = require('../binding')

module.exports = {
  signals: binding.signals,
  errnos: binding.errnos
}
module.exports = class OSError extends Error {
  constructor(msg, code, fn = OSError) {
    super(`${code}: ${msg}`)
    this.code = code

    if (Error.captureStackTrace) {
      Error.captureStackTrace(this, fn)
    }
  }

  get name() {
    return 'OSError'
  }

  static UNKNOWN_SIGNAL(msg) {
    return new OSError(msg, 'UNKNOWN_SIGNAL', OSError.UNKNOWN_SIGNAL)
  }

  static TITLE_OVERFLOW(msg) {
    return new OSError(msg, 'TITLE_OVERFLOW', OSError.TITLE_OVERFLOW)
  }
}
{
  "name": "bare-os",
  "version": "3.6.2",
  "description": "Operating system utilities for Javascript",
  "exports": {
    ".": {
      "types": "./index.d.ts",
      "default": "./index.js"
    },
    "./package": "./package.json",
    "./constants": "./lib/constants.js",
    "./errors": "./lib/errors.js"
  },
  "files": [
    "index.js",
    "index.d.ts",
    "binding.c",
    "binding.js",
    "CMakeLists.txt",
    "lib",
    "prebuilds"
  ],
  "addon": true,
  "scripts": {
    "test": "prettier . --check && bare test.js"
  },
  "repository": {
    "type": "git",
    "url": "git+https://github.com/holepunchto/bare-os.git"
  },
  "author": "Holepunch",
  "license": "Apache-2.0",
  "bugs": {
    "url": "https://github.com/holepunchto/bare-os/issues"
  },
  "homepage": "https://github.com/holepunchto/bare-os#readme",
  "engines": {
    "bare": ">=1.14.0"
  },
  "devDependencies": {
    "brittle": "^3.1.1",
    "cmake-bare": "^1.1.6",
    "prettier": "^3.4.2",
    "prettier-config-standard": "^7.0.0"
  }
}
/* global Bare */

// This export SHOULD NOT be shortened in any way as having the full
// `module.exports = require(...)` statement is crucial for synthesizing
// ESM exports.

if (Bare.platform === 'win32') {
  module.exports = require('./lib/win32')
} else {
  module.exports = require('./lib/posix')
}
module.exports = {
  CHAR_UPPERCASE_A: 0x41,
  CHAR_LOWERCASE_A: 0x61,
  CHAR_UPPERCASE_Z: 0x5a,
  CHAR_LOWERCASE_Z: 0x7a,
  CHAR_DOT: 0x2e,
  CHAR_FORWARD_SLASH: 0x2f,
  CHAR_BACKWARD_SLASH: 0x5c,
  CHAR_COLON: 0x3a,
  CHAR_QUESTION_MARK: 0x3f
}
const os = require('bare-os')

const { normalizeString } = require('./shared')
const {
  CHAR_DOT,
  CHAR_FORWARD_SLASH
} = require('./constants')

function isPosixPathSeparator (code) {
  return code === CHAR_FORWARD_SLASH
}

exports.win32 = require('./win32')
exports.posix = exports

exports.sep = '/'
exports.delimiter = ':'

exports.resolve = function resolve (...args) {
  let resolvedPath = ''
  let resolvedAbsolute = false

  for (let i = args.length - 1; i >= -1 && !resolvedAbsolute; i--) {
    const path = i >= 0 ? args[i] : os.cwd()

    if (path.length === 0) {
      continue
    }

    resolvedPath = `${path}/${resolvedPath}`
    resolvedAbsolute = path.charCodeAt(0) === CHAR_FORWARD_SLASH
  }

  resolvedPath = normalizeString(resolvedPath, !resolvedAbsolute, '/', isPosixPathSeparator)

  if (resolvedAbsolute) {
    return `/${resolvedPath}`
  }

  return resolvedPath.length > 0 ? resolvedPath : '.'
}

exports.normalize = function normalize (path) {
  if (path.length === 0) return '.'

  const isAbsolute = path.charCodeAt(0) === CHAR_FORWARD_SLASH
  const trailingSeparator = path.charCodeAt(path.length - 1) === CHAR_FORWARD_SLASH

  path = normalizeString(path, !isAbsolute, '/', isPosixPathSeparator)

  if (path.length === 0) {
    if (isAbsolute) return '/'
    return trailingSeparator ? './' : '.'
  }

  if (trailingSeparator) path += '/'

  return isAbsolute ? `/${path}` : path
}

exports.isAbsolute = function isAbsolute (path) {
  return path.length > 0 && path.charCodeAt(0) === CHAR_FORWARD_SLASH
}

exports.join = function join (...args) {
  if (args.length === 0) return '.'
  let joined
  for (let i = 0; i < args.length; ++i) {
    const arg = args[i]
    if (arg.length > 0) {
      if (joined === undefined) joined = arg
      else joined += `/${arg}`
    }
  }
  if (joined === undefined) return '.'
  return exports.normalize(joined)
}

exports.relative = function relative (from, to) {
  if (from === to) return ''

  from = exports.resolve(from)
  to = exports.resolve(to)

  if (from === to) return ''

  const fromStart = 1
  const fromEnd = from.length
  const fromLen = fromEnd - fromStart
  const toStart = 1
  const toLen = to.length - toStart

  const length = (fromLen < toLen ? fromLen : toLen)
  let lastCommonSep = -1
  let i = 0
  for (; i < length; i++) {
    const fromCode = from.charCodeAt(fromStart + i)
    if (fromCode !== to.charCodeAt(toStart + i)) {
      break
    } else if (fromCode === CHAR_FORWARD_SLASH) {
      lastCommonSep = i
    }
  }
  if (i === length) {
    if (toLen > length) {
      if (to.charCodeAt(toStart + i) === CHAR_FORWARD_SLASH) {
        return to.substring(toStart + i + 1)
      }
      if (i === 0) {
        return to.substring(toStart + i)
      }
    } else if (fromLen > length) {
      if (from.charCodeAt(fromStart + i) === CHAR_FORWARD_SLASH) {
        lastCommonSep = i
      } else if (i === 0) {
        lastCommonSep = 0
      }
    }
  }

  let out = ''
  for (i = fromStart + lastCommonSep + 1; i <= fromEnd; ++i) {
    if (i === fromEnd || from.charCodeAt(i) === CHAR_FORWARD_SLASH) {
      out += out.length === 0 ? '..' : '/..'
    }
  }

  return `${out}${to.substring(toStart + lastCommonSep)}`
}

exports.toNamespacedPath = function toNamespacedPath (path) {
  return path
}

exports.dirname = function dirname (path) {
  if (path.length === 0) return '.'
  const hasRoot = path.charCodeAt(0) === CHAR_FORWARD_SLASH
  let end = -1
  let matchedSlash = true
  for (let i = path.length - 1; i >= 1; --i) {
    if (path.charCodeAt(i) === CHAR_FORWARD_SLASH) {
      if (!matchedSlash) {
        end = i
        break
      }
    } else {
      matchedSlash = false
    }
  }

  if (end === -1) return hasRoot ? '/' : '.'
  if (hasRoot && end === 1) return '//'
  return path.substring(0, end)
}

exports.basename = function basename (path, suffix) {
  let start = 0
  let end = -1
  let matchedSlash = true

  if (suffix !== undefined && suffix.length > 0 && suffix.length <= path.length) {
    if (suffix === path) { return '' }
    let extIdx = suffix.length - 1
    let firstNonSlashEnd = -1
    for (let i = path.length - 1; i >= 0; --i) {
      const code = path.charCodeAt(i)
      if (code === CHAR_FORWARD_SLASH) {
        if (!matchedSlash) {
          start = i + 1
          break
        }
      } else {
        if (firstNonSlashEnd === -1) {
          matchedSlash = false
          firstNonSlashEnd = i + 1
        }
        if (extIdx >= 0) {
          if (code === suffix.charCodeAt(extIdx)) {
            if (--extIdx === -1) {
              end = i
            }
          } else {
            extIdx = -1
            end = firstNonSlashEnd
          }
        }
      }
    }

    if (start === end) end = firstNonSlashEnd
    else if (end === -1) end = path.length
    return path.substring(start, end)
  }

  for (let i = path.length - 1; i >= 0; --i) {
    if (path.charCodeAt(i) === CHAR_FORWARD_SLASH) {
      if (!matchedSlash) {
        start = i + 1
        break
      }
    } else if (end === -1) {
      matchedSlash = false
      end = i + 1
    }
  }

  if (end === -1) return ''
  return path.substring(start, end)
}

exports.extname = function extname (path) {
  let startDot = -1
  let startPart = 0
  let end = -1
  let matchedSlash = true
  let preDotState = 0
  for (let i = path.length - 1; i >= 0; --i) {
    const code = path.charCodeAt(i)
    if (code === CHAR_FORWARD_SLASH) {
      if (!matchedSlash) {
        startPart = i + 1
        break
      }
      continue
    }
    if (end === -1) {
      matchedSlash = false
      end = i + 1
    }
    if (code === CHAR_DOT) {
      if (startDot === -1) startDot = i
      else if (preDotState !== 1) preDotState = 1
    } else if (startDot !== -1) {
      preDotState = -1
    }
  }

  if (startDot === -1 || end === -1 || preDotState === 0 || (preDotState === 1 && startDot === end - 1 && startDot === startPart + 1)) {
    return ''
  }
  return path.substring(startDot, end)
}
const {
  CHAR_DOT,
  CHAR_FORWARD_SLASH
} = require('./constants')

exports.normalizeString = function normalizeString (path, allowAboveRoot, separator, isPathSeparator) {
  let res = ''
  let lastSegmentLength = 0
  let lastSlash = -1
  let dots = 0
  let code = 0
  for (let i = 0; i <= path.length; ++i) {
    if (i < path.length) {
      code = path.charCodeAt(i)
    } else if (isPathSeparator(code)) {
      break
    } else {
      code = CHAR_FORWARD_SLASH
    }

    if (isPathSeparator(code)) {
      if (lastSlash === i - 1 || dots === 1) ;
      else if (dots === 2) {
        if (res.length < 2 || lastSegmentLength !== 2 || res.charCodeAt(res.length - 1) !== CHAR_DOT || res.charCodeAt(res.length - 2) !== CHAR_DOT) {
          if (res.length > 2) {
            const lastSlashIndex = res.lastIndexOf(separator)
            if (lastSlashIndex === -1) {
              res = ''
              lastSegmentLength = 0
            } else {
              res = res.substring(0, lastSlashIndex)
              lastSegmentLength =
                res.length - 1 - res.lastIndexOf(separator)
            }
            lastSlash = i
            dots = 0
            continue
          } else if (res.length !== 0) {
            res = ''
            lastSegmentLength = 0
            lastSlash = i
            dots = 0
            continue
          }
        }
        if (allowAboveRoot) {
          res += res.length > 0 ? `${separator}..` : '..'
          lastSegmentLength = 2
        }
      } else {
        if (res.length > 0) {
          res += `${separator}${path.substring(lastSlash + 1, i)}`
        } else {
          res = path.substring(lastSlash + 1, i)
        }
        lastSegmentLength = i - lastSlash - 1
      }
      lastSlash = i
      dots = 0
    } else if (code === CHAR_DOT && dots !== -1) {
      ++dots
    } else {
      dots = -1
    }
  }
  return res
}
const os = require('bare-os')

const { normalizeString } = require('./shared')
const {
  CHAR_UPPERCASE_A,
  CHAR_LOWERCASE_A,
  CHAR_UPPERCASE_Z,
  CHAR_LOWERCASE_Z,
  CHAR_DOT,
  CHAR_FORWARD_SLASH,
  CHAR_BACKWARD_SLASH,
  CHAR_COLON,
  CHAR_QUESTION_MARK
} = require('./constants')

function isWindowsPathSeparator (code) {
  return code === CHAR_FORWARD_SLASH || code === CHAR_BACKWARD_SLASH
}

function isWindowsDeviceRoot (code) {
  return (code >= CHAR_UPPERCASE_A && code <= CHAR_UPPERCASE_Z) ||
         (code >= CHAR_LOWERCASE_A && code <= CHAR_LOWERCASE_Z)
}

exports.posix = require('./posix')
exports.win32 = exports

exports.sep = '\\'
exports.delimiter = ';'

exports.resolve = function resolve (...args) {
  let resolvedDevice = ''
  let resolvedTail = ''
  let resolvedAbsolute = false

  for (let i = args.length - 1; i >= -1; i--) {
    let path
    if (i >= 0) {
      path = args[i]

      if (path.length === 0) continue
    } else if (resolvedDevice.length === 0) {
      path = os.cwd()
    } else {
      path = os.getEnv(`=${resolvedDevice}`) || os.cwd()

      if (path === undefined || (path.substring(0, 2).toLowerCase() !== resolvedDevice.toLowerCase() && path.charCodeAt(2) === CHAR_BACKWARD_SLASH)) {
        path = `${resolvedDevice}\\`
      }
    }

    const len = path.length
    let rootEnd = 0
    let device = ''
    let isAbsolute = false
    const code = path.charCodeAt(0)

    if (len === 1) {
      if (isWindowsPathSeparator(code)) {
        rootEnd = 1
        isAbsolute = true
      }
    } else if (isWindowsPathSeparator(code)) {
      isAbsolute = true

      if (isWindowsPathSeparator(path.charCodeAt(1))) {
        let j = 2
        let last = j
        while (j < len && !isWindowsPathSeparator(path.charCodeAt(j))) {
          j++
        }
        if (j < len && j !== last) {
          const firstPart = path.substring(last, j)
          last = j
          while (j < len && isWindowsPathSeparator(path.charCodeAt(j))) {
            j++
          }
          if (j < len && j !== last) {
            last = j
            while (j < len && !isWindowsPathSeparator(path.charCodeAt(j))) {
              j++
            }
            if (j === len || j !== last) {
              device = `\\\\${firstPart}\\${path.substring(last, j)}`
              rootEnd = j
            }
          }
        }
      } else {
        rootEnd = 1
      }
    } else if (isWindowsDeviceRoot(code) && path.charCodeAt(1) === CHAR_COLON) {
      device = path.substring(0, 2)
      rootEnd = 2
      if (len > 2 && isWindowsPathSeparator(path.charCodeAt(2))) {
        isAbsolute = true
        rootEnd = 3
      }
    }

    if (device.length > 0) {
      if (resolvedDevice.length > 0) {
        if (device.toLowerCase() !== resolvedDevice.toLowerCase()) { continue }
      } else {
        resolvedDevice = device
      }
    }

    if (resolvedAbsolute) {
      if (resolvedDevice.length > 0) { break }
    } else {
      resolvedTail = `${path.substring(rootEnd)}\\${resolvedTail}`
      resolvedAbsolute = isAbsolute
      if (isAbsolute && resolvedDevice.length > 0) {
        break
      }
    }
  }

  resolvedTail = normalizeString(resolvedTail, !resolvedAbsolute, '\\', isWindowsPathSeparator)

  return resolvedAbsolute ? `${resolvedDevice}\\${resolvedTail}` : `${resolvedDevice}${resolvedTail}` || '.'
}

exports.normalize = function normalize (path) {
  const len = path.length
  if (len === 0) return '.'
  let rootEnd = 0
  let device
  let isAbsolute = false
  const code = path.charCodeAt(0)

  if (len === 1) {
    return code === CHAR_FORWARD_SLASH ? '\\' : path
  }

  if (isWindowsPathSeparator(code)) {
    isAbsolute = true

    if (isWindowsPathSeparator(path.charCodeAt(1))) {
      let j = 2
      let last = j
      while (j < len && !isWindowsPathSeparator(path.charCodeAt(j))) {
        j++
      }
      if (j < len && j !== last) {
        const firstPart = path.substring(last, j)
        last = j
        while (j < len && isWindowsPathSeparator(path.charCodeAt(j))) {
          j++
        }
        if (j < len && j !== last) {
          last = j
          while (j < len && !isWindowsPathSeparator(path.charCodeAt(j))) {
            j++
          }
          if (j === len) {
            return `\\\\${firstPart}\\${path.substring(last)}\\`
          }
          if (j !== last) {
            device = `\\\\${firstPart}\\${path.substring(last, j)}`
            rootEnd = j
          }
        }
      }
    } else {
      rootEnd = 1
    }
  } else if (isWindowsDeviceRoot(code) && path.charCodeAt(1) === CHAR_COLON) {
    device = path.substring(0, 2)
    rootEnd = 2
    if (len > 2 && isWindowsPathSeparator(path.charCodeAt(2))) {
      isAbsolute = true
      rootEnd = 3
    }
  }

  let tail = rootEnd < len ? normalizeString(path.substring(rootEnd), !isAbsolute, '\\', isWindowsPathSeparator) : ''
  if (tail.length === 0 && !isAbsolute) {
    tail = '.'
  }
  if (tail.length > 0 && isWindowsPathSeparator(path.charCodeAt(len - 1))) {
    tail += '\\'
  }
  if (device === undefined) {
    return isAbsolute ? `\\${tail}` : tail
  }
  return isAbsolute ? `${device}\\${tail}` : `${device}${tail}`
}

exports.isAbsolute = function isAbsolute (path) {
  const len = path.length
  if (len === 0) return false

  const code = path.charCodeAt(0)

  return isWindowsPathSeparator(code) || (len > 2 && isWindowsDeviceRoot(code) && path.charCodeAt(1) === CHAR_COLON && isWindowsPathSeparator(path.charCodeAt(2)))
}

exports.join = function join (...args) {
  if (args.length === 0) return '.'

  let joined
  let firstPart
  for (let i = 0; i < args.length; ++i) {
    const arg = args[i]
    if (arg.length > 0) {
      if (joined === undefined) joined = firstPart = arg
      else joined += `\\${arg}`
    }
  }

  if (joined === undefined) return '.'

  let needsReplace = true
  let slashCount = 0
  if (isWindowsPathSeparator(firstPart.charCodeAt(0))) {
    ++slashCount
    const firstLen = firstPart.length
    if (firstLen > 1 && isWindowsPathSeparator(firstPart.charCodeAt(1))) {
      ++slashCount
      if (firstLen > 2) {
        if (isWindowsPathSeparator(firstPart.charCodeAt(2))) {
          ++slashCount
        } else {
          needsReplace = false
        }
      }
    }
  }
  if (needsReplace) {
    while (slashCount < joined.length && isWindowsPathSeparator(joined.charCodeAt(slashCount))) {
      slashCount++
    }

    if (slashCount >= 2) {
      joined = `\\${joined.substring(slashCount)}`
    }
  }

  return exports.normalize(joined)
}

exports.relative = function relative (from, to) {
  if (from === to) return ''

  const fromOrig = exports.resolve(from)
  const toOrig = exports.resolve(to)

  if (fromOrig === toOrig) return ''

  from = fromOrig.toLowerCase()
  to = toOrig.toLowerCase()

  if (from === to) return ''

  let fromStart = 0
  while (fromStart < from.length && from.charCodeAt(fromStart) === CHAR_BACKWARD_SLASH) {
    fromStart++
  }
  let fromEnd = from.length
  while (fromEnd - 1 > fromStart && from.charCodeAt(fromEnd - 1) === CHAR_BACKWARD_SLASH) {
    fromEnd--
  }
  const fromLen = fromEnd - fromStart

  let toStart = 0
  while (toStart < to.length && to.charCodeAt(toStart) === CHAR_BACKWARD_SLASH) {
    toStart++
  }
  let toEnd = to.length
  while (toEnd - 1 > toStart && to.charCodeAt(toEnd - 1) === CHAR_BACKWARD_SLASH) {
    toEnd--
  }
  const toLen = toEnd - toStart

  const length = fromLen < toLen ? fromLen : toLen
  let lastCommonSep = -1
  let i = 0
  for (; i < length; i++) {
    const fromCode = from.charCodeAt(fromStart + i)
    if (fromCode !== to.charCodeAt(toStart + i)) {
      break
    } else if (fromCode === CHAR_BACKWARD_SLASH) {
      lastCommonSep = i
    }
  }

  if (i !== length) {
    if (lastCommonSep === -1) return toOrig
  } else {
    if (toLen > length) {
      if (to.charCodeAt(toStart + i) === CHAR_BACKWARD_SLASH) {
        return toOrig.substring(toStart + i + 1)
      }
      if (i === 2) {
        return toOrig.substring(toStart + i)
      }
    }
    if (fromLen > length) {
      if (from.charCodeAt(fromStart + i) === CHAR_BACKWARD_SLASH) {
        lastCommonSep = i
      } else if (i === 2) {
        lastCommonSep = 3
      }
    }
    if (lastCommonSep === -1) lastCommonSep = 0
  }

  let out = ''
  for (i = fromStart + lastCommonSep + 1; i <= fromEnd; ++i) {
    if (i === fromEnd || from.charCodeAt(i) === CHAR_BACKWARD_SLASH) {
      out += out.length === 0 ? '..' : '\\..'
    }
  }

  toStart += lastCommonSep

  if (out.length > 0) {
    return `${out}${toOrig.substring(toStart, toEnd)}`
  }
  if (toOrig.charCodeAt(toStart) === CHAR_BACKWARD_SLASH) {
    ++toStart
  }
  return toOrig.substring(toStart, toEnd)
}

exports.toNamespacedPath = function toNamespacedPath (path) {
  if (path.length === 0) return path

  const resolvedPath = exports.resolve(path)

  if (resolvedPath.length <= 2) return path

  if (resolvedPath.charCodeAt(0) === CHAR_BACKWARD_SLASH) {
    if (resolvedPath.charCodeAt(1) === CHAR_BACKWARD_SLASH) {
      const code = resolvedPath.charCodeAt(2)
      if (code !== CHAR_QUESTION_MARK && code !== CHAR_DOT) {
        return `\\\\?\\UNC\\${resolvedPath.substring(2)}`
      }
    }
  } else if (
    isWindowsDeviceRoot(resolvedPath.charCodeAt(0)) &&
      resolvedPath.charCodeAt(1) === CHAR_COLON &&
      resolvedPath.charCodeAt(2) === CHAR_BACKWARD_SLASH
  ) {
    return `\\\\?\\${resolvedPath}`
  }

  return path
}

exports.dirname = function dirname (path) {
  const len = path.length
  if (len === 0) return '.'
  let rootEnd = -1
  let offset = 0
  const code = path.charCodeAt(0)

  if (len === 1) {
    return isWindowsPathSeparator(code) ? path : '.'
  }

  if (isWindowsPathSeparator(code)) {
    rootEnd = offset = 1

    if (isWindowsPathSeparator(path.charCodeAt(1))) {
      let j = 2
      let last = j
      while (j < len && !isWindowsPathSeparator(path.charCodeAt(j))) {
        j++
      }
      if (j < len && j !== last) {
        last = j
        while (j < len && isWindowsPathSeparator(path.charCodeAt(j))) {
          j++
        }
        if (j < len && j !== last) {
          last = j
          while (j < len && !isWindowsPathSeparator(path.charCodeAt(j))) {
            j++
          }
          if (j === len) {
            return path
          }
          if (j !== last) {
            rootEnd = offset = j + 1
          }
        }
      }
    }
  } else if (isWindowsDeviceRoot(code) && path.charCodeAt(1) === CHAR_COLON) {
    rootEnd = len > 2 && isWindowsPathSeparator(path.charCodeAt(2)) ? 3 : 2
    offset = rootEnd
  }

  let end = -1
  let matchedSlash = true
  for (let i = len - 1; i >= offset; --i) {
    if (isWindowsPathSeparator(path.charCodeAt(i))) {
      if (!matchedSlash) {
        end = i
        break
      }
    } else {
      matchedSlash = false
    }
  }

  if (end === -1) {
    if (rootEnd === -1) return '.'

    end = rootEnd
  }
  return path.substring(0, end)
}

exports.basename = function basename (path, suffix) {
  let start = 0
  let end = -1
  let matchedSlash = true

  if (path.length >= 2 && isWindowsDeviceRoot(path.charCodeAt(0)) && path.charCodeAt(1) === CHAR_COLON) {
    start = 2
  }

  if (suffix !== undefined && suffix.length > 0 && suffix.length <= path.length) {
    if (suffix === path) return ''
    let extIdx = suffix.length - 1
    let firstNonSlashEnd = -1
    for (let i = path.length - 1; i >= start; --i) {
      const code = path.charCodeAt(i)
      if (isWindowsPathSeparator(code)) {
        if (!matchedSlash) {
          start = i + 1
          break
        }
      } else {
        if (firstNonSlashEnd === -1) {
          matchedSlash = false
          firstNonSlashEnd = i + 1
        }
        if (extIdx >= 0) {
          if (code === suffix.charCodeAt(extIdx)) {
            if (--extIdx === -1) {
              end = i
            }
          } else {
            extIdx = -1
            end = firstNonSlashEnd
          }
        }
      }
    }

    if (start === end) end = firstNonSlashEnd
    else if (end === -1) end = path.length
    return path.substring(start, end)
  }
  for (let i = path.length - 1; i >= start; --i) {
    if (isWindowsPathSeparator(path.charCodeAt(i))) {
      if (!matchedSlash) {
        start = i + 1
        break
      }
    } else if (end === -1) {
      matchedSlash = false
      end = i + 1
    }
  }

  if (end === -1) return ''
  return path.substring(start, end)
}

exports.extname = function extname (path) {
  let start = 0
  let startDot = -1
  let startPart = 0
  let end = -1
  let matchedSlash = true
  let preDotState = 0

  if (path.length >= 2 && path.charCodeAt(1) === CHAR_COLON && isWindowsDeviceRoot(path.charCodeAt(0))) {
    start = startPart = 2
  }

  for (let i = path.length - 1; i >= start; --i) {
    const code = path.charCodeAt(i)
    if (isWindowsPathSeparator(code)) {
      if (!matchedSlash) {
        startPart = i + 1
        break
      }
      continue
    }
    if (end === -1) {
      matchedSlash = false
      end = i + 1
    }
    if (code === CHAR_DOT) {
      if (startDot === -1) startDot = i
      else if (preDotState !== 1) preDotState = 1
    } else if (startDot !== -1) {
      preDotState = -1
    }
  }

  if (startDot === -1 || end === -1 || preDotState === 0 || (preDotState === 1 && startDot === end - 1 && startDot === startPart + 1)) {
    return ''
  }
  return path.substring(startDot, end)
}
{
  "name": "bare-path",
  "version": "3.0.0",
  "description": "Path manipulation library for JavaScript",
  "exports": {
    ".": "./index.js",
    "./package": "./package.json",
    "./posix": "./lib/posix.js",
    "./win32": "./lib/win32.js"
  },
  "files": [
    "index.js",
    "lib",
    "NOTICE"
  ],
  "scripts": {
    "test": "standard && bare test.js"
  },
  "repository": {
    "type": "git",
    "url": "git+https://github.com/holepunchto/bare-path.git"
  },
  "author": "Holepunch",
  "license": "Apache-2.0",
  "bugs": {
    "url": "https://github.com/holepunchto/bare-path/issues"
  },
  "homepage": "https://github.com/holepunchto/bare-path#readme",
  "dependencies": {
    "bare-os": "^3.0.1"
  },
  "devDependencies": {
    "brittle": "^3.3.2",
    "standard": "^17.0.0"
  }
}
const safetyCatch = require('safety-catch')
const b4a = require('b4a')
const c = require('compact-encoding')
const m = require('./lib/messages')
const { type: t, stream: s } = require('./lib/constants')
const IncomingRequest = require('./lib/incoming-request')
const IncomingStream = require('./lib/incoming-stream')
const OutgoingRequest = require('./lib/outgoing-request')
const OutgoingStream = require('./lib/outgoing-stream')
const CommandRouter = require('./lib/command-router')

module.exports = exports = class RPC {
  constructor(stream, onrequest = noop) {
    this._stream = stream
    this._id = 0

    this._outgoingRequests = new Map()
    this._outgoingResponses = new Map()
    this._incomingRequests = new Map()
    this._incomingResponses = new Map()
    this._pendingRequests = new Set()
    this._pendingResponses = new Set()

    this._buffer = []
    this._buffered = 0
    this._frame = -1
    this._draining = []

    if (typeof onrequest === 'function') {
      onrequest = onrequest.bind(this)
    } else {
      onrequest = onrequest._onrequest.bind(onrequest)
    }

    this._onrequest = onrequest
    this._onerror = this._onerror.bind(this)
    this._ondata = this._ondata.bind(this)
    this._ondrain = this._ondrain.bind(this)

    this._stream
      .on('error', this._onerror)
      .on('data', this._ondata)
      .on('drain', this._ondrain)
  }

  request(command) {
    return new OutgoingRequest(this, ++this._id, command)
  }

  _sendMessage(message, cb) {
    const header = c.encode(m.header, message)

    let flushed = this._stream.write(header)

    if (message.data) flushed = this._stream.write(message.data)

    if (cb) {
      if (flushed) cb(null)
      else this._draining.push(cb)
    }
  }

  _sendRequest(request, data = null) {
    this._outgoingRequests.set(request.id, request)

    this._sendMessage({
      type: t.REQUEST,
      id: request.id,
      command: request.command,
      stream: 0,
      data
    })
  }

  _createRequestStream(request, isInitiator, opts) {
    if (isInitiator) {
      this._outgoingRequests.set(request.id, request)

      request._requestStream = new OutgoingStream(
        this,
        request,
        t.REQUEST,
        opts
      )
    } else {
      this._incomingRequests.set(request.id, request)

      request._requestStream = new IncomingStream(
        this,
        request,
        t.REQUEST,
        opts
      )

      request._requestStream.on('close', () =>
        this._incomingRequests.delete(request.id)
      )
    }
  }

  _sendResponse(request, data) {
    this._sendMessage({
      type: t.RESPONSE,
      id: request.id,
      stream: 0,
      error: null,
      data
    })
  }

  _createResponseStream(request, isInitiator, opts) {
    if (isInitiator) {
      this._outgoingResponses.set(request.id, request)

      request._responseStream = new OutgoingStream(
        this,
        request,
        t.RESPONSE,
        opts
      )
    } else {
      this._incomingResponses.set(request.id, request)

      request._responseStream = new IncomingStream(
        this,
        request,
        t.RESPONSE,
        opts
      )

      request._responseStream.on('close', () =>
        this._incomingResponses.delete(request.id)
      )
    }
  }

  _sendError(request, err) {
    this._sendMessage({
      type: t.RESPONSE,
      id: request.id,
      stream: 0,
      error: err,
      data: null
    })
  }

  _onerror(err) {
    this._ondrain(err)

    // TODO Destroy pending requests and responses
  }

  _ondata(data) {
    this._buffer.push(data)
    this._buffered += data.byteLength

    if (this._frame === -1) {
      this._onbeforeframe()
    } else {
      this._onafterframe()
    }
  }

  _onbeforeframe() {
    if (this._buffered < 4) return

    const buffer =
      this._buffer.length === 1 ? this._buffer[0] : b4a.concat(this._buffer)

    this._buffer = [buffer]
    this._frame = 4 + c.uint32.decode(c.state(0, 4, buffer))

    this._onafterframe()
  }

  _onafterframe() {
    if (this._buffered < this._frame) return

    const buffer =
      this._buffer.length === 1 ? this._buffer[0] : b4a.concat(this._buffer)

    const frame = this._frame

    this._buffered -= frame
    this._buffer = this._buffered > 0 ? [buffer.subarray(frame)] : []
    this._frame = -1

    this._onmessage(buffer.subarray(0, frame))
    this._onbeforeframe()
  }

  async _onmessage(buffer) {
    let message
    try {
      message = m.message.decode(c.state(0, buffer.length, buffer))
    } catch (err) {
      safetyCatch(err)

      return this._stream.destroy(err)
    }

    switch (message.type) {
      case t.REQUEST:
        const request = new IncomingRequest(
          this,
          message.id,
          message.command,
          message.data
        )

        try {
          await this._onrequest(request)
        } catch (err) {
          safetyCatch(err)

          this._sendError(request, err)
        }
        break
      case t.RESPONSE:
        try {
          this._onresponse(message)
        } catch (err) {
          safetyCatch(err)
        }
        break
      case t.STREAM:
        try {
          this._onstream(message)
        } catch (err) {
          safetyCatch(err)
        }
    }
  }

  _onresponse(message) {
    if (message.id === 0) return

    const request = this._outgoingRequests.get(message.id)
    if (request === undefined) return

    if (message.error) {
      request._reject(message.error)
    } else if (message.stream === 0) {
      request._resolve(message.data)
    }
  }

  _onstream(message) {
    if (message.id === 0) return

    if (message.stream & s.OPEN) this._onstreamopen(message)
    else if (message.stream & s.CLOSE) this._onstreamclose(message)
    else if (message.stream & s.PAUSE) this._onstreampause(message)
    else if (message.stream & s.RESUME) this._onstreamresume(message)
    else if (message.stream & s.DATA) this._onstreamdata(message)
    else if (message.stream & s.END) this._onstreamend(message)
    else if (message.stream & s.DESTROY) this._onstreamdestroy(message)
  }

  _onstreamopen(message) {
    let stream

    if (message.stream & s.REQUEST) {
      const request = this._outgoingRequests.get(message.id)
      if (request === undefined) {
        this._pendingRequests.add(message.id)
        return
      }

      stream = request._requestStream

      if (stream._pendingOpen === null) {
        this._pendingRequests.add(message.id)
        return
      }
    } else if (message.stream & s.RESPONSE) {
      const request = this._outgoingResponses.get(message.id)
      if (request === undefined) {
        this._pendingResponses.add(message.id)
        return
      }

      stream = request._responseStream

      if (stream._pendingOpen === null) {
        this._pendingResponses.add(message.id)
        return
      }
    } else {
      return
    }

    stream._continueOpen()
  }

  _onstreamclose(message) {
    let stream

    if (message.stream & s.REQUEST) {
      const request = this._incomingRequests.get(message.id)
      if (request === undefined) return

      stream = request._requestStream
    } else if (message.stream & s.RESPONSE) {
      const request = this._incomingResponses.get(message.id)
      if (request === undefined) return

      stream = request._responseStream
    } else {
      return
    }

    if (message.error) stream.destroy(message.error)
    else stream.push(null)
  }

  _onstreampause(message) {
    let stream

    if (message.stream & s.REQUEST) {
      const request = this._outgoingRequests.get(message.id)
      if (request === undefined) return

      stream = request._requestStream
    } else if (message.stream & s.RESPONSE) {
      const request = this._outgoingResponses.get(message.id)
      if (request === undefined) return

      stream = request._responseStream
    } else {
      return
    }

    stream.cork()
  }

  _onstreamresume(message) {
    let stream

    if (message.stream & s.REQUEST) {
      const request = this._outgoingRequests.get(message.id)
      if (request === undefined) return

      stream = request._requestStream
    } else if (message.stream & s.RESPONSE) {
      const request = this._outgoingResponses.get(message.id)
      if (request === undefined) return

      stream = request._responseStream
    } else {
      return
    }

    stream.uncork()
  }

  _onstreamdata(message) {
    let stream

    if (message.stream & s.REQUEST) {
      const request = this._incomingRequests.get(message.id)
      if (request === undefined) return

      stream = request._requestStream
    } else if (message.stream & s.RESPONSE) {
      const request = this._incomingResponses.get(message.id)
      if (request === undefined) return

      stream = request._responseStream
    } else {
      return
    }

    if (stream.push(message.data) === false) {
      this._sendMessage({
        type: t.STREAM,
        id: stream._request.id,
        stream: stream._mask | s.PAUSE,
        error: null,
        data: null
      })
    }
  }

  _onstreamend(message) {
    let stream

    if (message.stream & s.REQUEST) {
      const request = this._incomingRequests.get(message.id)
      if (request === undefined) return

      stream = request._requestStream
    } else if (message.stream & s.RESPONSE) {
      const request = this._incomingResponses.get(message.id)
      if (request === undefined) return

      stream = request._responseStream
    } else {
      return
    }

    stream.push(null)
  }

  _onstreamdestroy(message) {
    let stream

    if (message.stream & s.REQUEST) {
      const request = this._outgoingRequests.get(message.id)
      if (request === undefined) return

      stream = request._requestStream
    } else if (message.stream & s.RESPONSE) {
      const request = this._outgoingResponses.get(message.id)
      if (request === undefined) return

      stream = request._responseStream
    } else {
      return
    }

    stream.destroy(message.error)
  }

  _ondrain(err = null) {
    const draining = this._draining

    this._draining = []

    for (const cb of draining) cb(err)
  }
}

exports.CommandRouter = CommandRouter

function noop() {}
const c = require('compact-encoding')

module.exports = class RPCCommandRouter {
  constructor(opts = {}) {
    const { valueEncoding = c.raw } = opts

    this._responders = new Map()
    this._defaultValueEncoding = valueEncoding
  }

  respond(command, opts = {}, onrequest) {
    if (typeof opts === 'function') {
      onrequest = opts
      opts = {}
    }

    const {
      valueEncoding = this._defaultValueEncoding,
      requestEncoding = valueEncoding,
      responseEncoding = valueEncoding
    } = opts

    this._responders.set(command, {
      onrequest,
      requestEncoding,
      responseEncoding
    })
  }

  async _onrequest(req) {
    const responder = this._responders.get(req.command)

    if (responder === undefined) return

    const { onrequest, requestEncoding, responseEncoding } = responder

    let data = req.data

    if (requestEncoding) data = c.decode(requestEncoding, data)

    data = await onrequest(req, data)

    if (req.sent) return

    if (responseEncoding) data = c.encode(responseEncoding, data)

    req.reply(data)
  }
}
module.exports = {
  type: {
    REQUEST: 1,
    RESPONSE: 2,
    STREAM: 3
  },
  stream: {
    OPEN: 0x1,
    CLOSE: 0x2,
    PAUSE: 0x4,
    RESUME: 0x8,
    DATA: 0x10,
    END: 0x20,
    DESTROY: 0x40,
    ERROR: 0x80,
    REQUEST: 0x100,
    RESPONSE: 0x200
  }
}
module.exports = class RPCError extends Error {
  constructor(msg, code, fn = RPCError) {
    super(`${code}: ${msg}`)
    this.code = code

    if (Error.captureStackTrace) {
      Error.captureStackTrace(this, fn)
    }
  }

  get name() {
    return 'RPCError'
  }

  static UNKNOWN_MESSAGE(msg) {
    return new RPCError(msg, 'UNKNOWN_MESSAGE', RPCError.UNKNOWN_MESSAGE)
  }

  static ALREADY_SENT(msg) {
    return new RPCError(msg, 'ALREADY_SENT', RPCError.ALREADY_SENT)
  }

  static ALREADY_RECEIVED(msg) {
    return new RPCError(msg, 'ALREADY_RECEIVED', RPCError.ALREADY_RECEIVED)
  }
}
const c = require('compact-encoding')
const errors = require('./errors')

module.exports = class RPCIncomingRequest {
  constructor(rpc, id, command, data) {
    this.rpc = rpc
    this.id = id
    this.command = command
    this.data = data
    this.sent = false
    this.received = false

    this._requestStream = null
    this._responseStream = null
  }

  reply(data, encoding) {
    if (this.sent) {
      throw errors.ALREADY_SENT('Response has already been sent')
    }

    encoding =
      encoding && encoding !== 'buffer'
        ? c.from(encoding)
        : typeof data === 'string'
          ? c.raw.utf8
          : null

    this.sent = true

    this.rpc._sendResponse(this, encoding ? c.encode(encoding, data) : data)
  }

  createResponseStream(opts = {}) {
    if (this.sent) {
      throw errors.ALREADY_SENT('Response has already been sent')
    }

    this.sent = true

    this.rpc._createResponseStream(this, true, opts)

    return this._responseStream
  }

  createRequestStream(opts = {}) {
    if (this.received) {
      throw errors.ALREADY_RECEIVED('Request has already been received')
    }

    this.received = true

    this.rpc._createRequestStream(this, false, opts)

    return this._requestStream
  }
}
const { Readable } = require('bare-stream')
const { type: t, stream: s } = require('./constants')

module.exports = class RPCIncomingStream extends Readable {
  constructor(rpc, request, type, opts) {
    super({ ...opts, eagerOpen: true })

    this._rpc = rpc
    this._request = request
    this._type = type
    this._mask = type === t.REQUEST ? s.REQUEST : s.RESPONSE
  }

  _open(cb) {
    this._rpc._sendMessage(
      {
        type: t.STREAM,
        id: this._request.id,
        stream: this._mask | s.OPEN,
        error: null,
        data: null
      },
      cb
    )
  }

  _read() {
    this._rpc._sendMessage({
      type: t.STREAM,
      id: this._request.id,
      stream: this._mask | s.RESUME,
      error: null,
      data: null
    })
  }

  _destroy(err, cb) {
    if (err) {
      this._rpc._sendMessage(
        {
          type: t.STREAM,
          id: this._request.id,
          stream: this._mask | s.DESTROY | s.ERROR,
          error: err,
          data: null
        },
        cb
      )
    } else {
      this._rpc._sendMessage(
        {
          type: t.STREAM,
          id: this._request.id,
          stream: this._mask | s.DESTROY,
          error: null,
          data: null
        },
        cb
      )
    }
  }
}
const c = require('compact-encoding')
const { type: t, stream: s } = require('./constants')
const errors = require('./errors')

const error = {
  preencode(state, m) {
    c.utf8.preencode(state, m.message)
    c.utf8.preencode(state, String(m.code) || '')
    c.int.preencode(state, Number(m.errno) || 0)
  },
  encode(state, m) {
    c.utf8.encode(state, m.message)
    c.utf8.encode(state, String(m.code) || '')
    c.int.encode(state, Number(m.errno) || 0)
  },
  decode(state) {
    const err = new Error(`${c.utf8.decode(state)}`)
    err.code = c.utf8.decode(state)
    err.errno = c.int.decode(state)
    return err
  }
}

exports.header = {
  preencode(state, m) {
    c.uint32.preencode(state, 0) // Frame
    c.uint.preencode(state, m.type)
    c.uint.preencode(state, m.id)

    let hasData = false

    switch (m.type) {
      case t.REQUEST:
        c.uint.preencode(state, m.command)
        c.uint.preencode(state, m.stream)
        if (m.stream === 0) hasData = true
        break

      case t.RESPONSE:
        c.bool.preencode(state, !!m.error)
        c.uint.preencode(state, m.stream)

        if (m.error) error.preencode(state, m.error)
        else if (m.stream === 0) hasData = true
        break

      case t.STREAM:
        c.uint.preencode(state, m.stream)

        if (m.stream & s.ERROR) error.preencode(state, m.error)
        else if (m.stream & s.DATA) hasData = true
        break
    }

    if (hasData) c.uint.preencode(state, m.data ? m.data.byteLength : 0)
  },
  encode(state, m) {
    const frame = state.start

    c.uint32.encode(state, 0) // Frame

    const start = state.start

    c.uint.encode(state, m.type)
    c.uint.encode(state, m.id)

    let hasData = false

    switch (m.type) {
      case t.REQUEST:
        c.uint.encode(state, m.command)
        c.uint.encode(state, m.stream)
        if (m.stream === 0) hasData = true
        break

      case t.RESPONSE:
        c.bool.encode(state, !!m.error)
        c.uint.encode(state, m.stream)

        if (m.error) error.encode(state, m.error)
        else if (m.stream === 0) hasData = true
        break

      case t.STREAM:
        c.uint.encode(state, m.stream)

        if (m.stream & s.ERROR) error.encode(state, m.error)
        else if (m.stream & s.DATA) hasData = true
        break
    }

    if (hasData) c.uint.encode(state, m.data ? m.data.byteLength : 0)

    const end = state.start

    state.start = frame

    c.uint32.encode(
      state,
      end - start + (hasData && m.data ? m.data.byteLength : 0)
    )

    state.start = end
  }
}

exports.message = {
  decode(state) {
    const frame = c.uint32.decode(state)

    if (state.end - state.start < frame) throw new RangeError('Out of bounds')

    const type = c.uint.decode(state)
    const id = c.uint.decode(state)

    switch (type) {
      case t.REQUEST: {
        const command = c.uint.decode(state)
        const stream = c.uint.decode(state)
        const data = stream === 0 ? c.buffer.decode(state) : null

        return { type, id, command, stream, data }
      }

      case t.RESPONSE: {
        const err = c.bool.decode(state)
        const stream = c.uint.decode(state)

        if (err) {
          return { type, id, stream, error: error.decode(state), data: null }
        }

        if (stream === 0) {
          return { type, id, stream, error: null, data: c.buffer.decode(state) }
        }

        return { type, id, stream, error: null, data: null }
      }

      case t.STREAM:
        const stream = c.uint.decode(state)

        if (stream & s.ERROR) {
          return { type, id, stream, error: error.decode(state), data: null }
        }

        if (stream & s.DATA) {
          return { type, id, stream, error: null, data: c.buffer.decode(state) }
        }

        return { type, id, stream, error: null, data: null }

      default:
        throw errors.UNKNOWN_MESSAGE(`Unknown message '${type}'`)
    }
  }
}
const c = require('compact-encoding')
const errors = require('./errors')

module.exports = class RPCOutgoingRequest {
  constructor(rpc, id, command) {
    this.rpc = rpc
    this.id = id
    this.command = command
    this.sent = false
    this.received = false

    this._promise = new Promise((resolve, reject) => {
      this._resolve = resolve
      this._reject = reject
    })

    this._requestStream = null
    this._responseStream = null
  }

  send(data, encoding) {
    if (this.sent) {
      throw errors.ALREADY_SENT('Request has already been sent')
    }

    encoding =
      encoding && encoding !== 'buffer'
        ? c.from(encoding)
        : typeof data === 'string'
          ? c.raw.utf8
          : null

    this.sent = true

    this.rpc._sendRequest(this, encoding ? c.encode(encoding, data) : data)
  }

  reply(encoding) {
    if (this.received) {
      throw errors.ALREADY_RECEIVED('Response is already being received')
    }

    encoding = encoding && encoding !== 'buffer' ? c.from(encoding) : null

    this.received = true

    return encoding
      ? this._promise.then((data) => c.decode(encoding, data))
      : this._promise
  }

  createRequestStream(opts = {}) {
    if (this.sent) {
      throw errors.ALREADY_SENT('Request has already been sent')
    }

    this.sent = true

    this.rpc._createRequestStream(this, true, opts)

    return this._requestStream
  }

  createResponseStream(opts = {}) {
    if (this.received) {
      throw errors.ALREADY_RECEIVED('Response has already been received')
    }

    this.received = true

    this.rpc._createResponseStream(this, false, opts)

    return this._responseStream
  }
}
const { Writable } = require('bare-stream')
const { type: t, stream: s } = require('./constants')

module.exports = class RPCOutgoingStream extends Writable {
  constructor(rpc, request, type, opts) {
    super({ ...opts, eagerOpen: true })

    this._rpc = rpc
    this._request = request
    this._type = type
    this._mask = type === t.REQUEST ? s.REQUEST : s.RESPONSE

    this._pendingOpen = null
  }

  _open(cb) {
    let pending

    const onflushed = () => {
      if (pending.has(this._request.id)) {
        pending.delete(this._request.id)

        cb(null)
      } else {
        this._pendingOpen = cb
      }
    }

    switch (this._type) {
      case t.REQUEST:
        pending = this._rpc._pendingRequests

        this._rpc._sendMessage(
          {
            type: t.REQUEST,
            id: this._request.id,
            command: this._request.command,
            stream: s.OPEN,
            data: null
          },
          onflushed
        )
        break

      case t.RESPONSE:
        pending = this._rpc._pendingResponses

        this._rpc._sendMessage(
          {
            type: t.RESPONSE,
            id: this._request.id,
            error: false,
            stream: s.OPEN,
            data: null
          },
          onflushed
        )
        break
    }
  }

  _continueOpen() {
    if (this._pendingOpen === null) return
    const cb = this._pendingOpen
    this._pendingOpen = null
    cb()
  }

  _write(data, encoding, cb) {
    this._rpc._sendMessage(
      {
        type: t.STREAM,
        id: this._request.id,
        stream: this._mask | s.DATA,
        error: null,
        data
      },
      cb
    )
  }

  _final(cb) {
    this._rpc._sendMessage(
      {
        type: t.STREAM,
        id: this._request.id,
        stream: this._mask | s.END,
        error: null,
        data: null
      },
      cb
    )
  }

  _destroy(err, cb) {
    if (err) {
      this._rpc._sendMessage(
        {
          type: t.STREAM,
          id: this._request.id,
          stream: this._mask | s.CLOSE | s.ERROR,
          error: err,
          data: null
        },
        cb
      )
    } else {
      this._rpc._sendMessage(
        {
          type: t.STREAM,
          id: this._request.id,
          stream: this._mask | s.CLOSE,
          error: null,
          data: null
        },
        cb
      )
    }
  }
}
{
  "name": "bare-rpc",
  "version": "0.2.13",
  "description": "librpc ABI compatible RPC for Bare",
  "exports": {
    ".": {
      "types": "./index.d.ts",
      "default": "./index.js"
    },
    "./package": "./package.json",
    "./errors": {
      "types": "./lib/errors.d.ts",
      "default": "./lib/errors.js"
    }
  },
  "files": [
    "index.js",
    "index.d.ts",
    "lib"
  ],
  "scripts": {
    "test": "prettier . --check && bare test.js"
  },
  "repository": {
    "type": "git",
    "url": "git+https://github.com/holepunchto/bare-rpc.git"
  },
  "author": "Holepunch",
  "license": "Apache-2.0",
  "bugs": {
    "url": "https://github.com/holepunchto/bare-rpc/issues"
  },
  "homepage": "https://github.com/holepunchto/bare-rpc#readme",
  "dependencies": {
    "b4a": "^1.6.6",
    "bare-stream": "^2.1.3",
    "compact-encoding": "^2.15.0",
    "safety-catch": "^1.0.2"
  },
  "devDependencies": {
    "bare-buffer": "^3.0.1",
    "bare-ipc": "^1.1.0",
    "brittle": "^3.2.1",
    "prettier": "^3.4.2",
    "prettier-config-standard": "^7.0.0"
  },
  "peerDependencies": {
    "bare-buffer": "*"
  },
  "peerDependenciesMeta": {
    "bare-buffer": {
      "optional": true
    }
  }
}
const stream = require('streamx')

const defaultEncoding = 'utf8'

module.exports = exports = stream.Stream

exports.pipeline = stream.pipeline

exports.isStream = stream.isStream
exports.isEnded = stream.isEnded
exports.isFinished = stream.isFinished
exports.isDisturbed = stream.isDisturbed

exports.getStreamError = stream.getStreamError

exports.Stream = exports

exports.Readable = class Readable extends stream.Readable {
  constructor(opts = {}) {
    super({
      ...opts,
      byteLength: null,
      byteLengthReadable: null,
      map: null,
      mapReadable: null
    })

    if (this._construct) this._open = this._construct

    if (this._read !== stream.Readable.prototype._read) {
      this._read = read.bind(this, this._read)
    }

    if (this._destroy !== stream.Stream.prototype._destroy) {
      this._destroy = destroy.bind(this, this._destroy)
    }
  }

  push(chunk, encoding) {
    if (typeof chunk === 'string') {
      chunk = Buffer.from(chunk, encoding || defaultEncoding)
    }

    return super.push(chunk)
  }

  unshift(chunk, encoding) {
    if (typeof chunk === 'string') {
      chunk = Buffer.from(chunk, encoding || defaultEncoding)
    }

    super.unshift(chunk)
  }
}

exports.Writable = class Writable extends stream.Writable {
  constructor(opts = {}) {
    super({
      ...opts,
      byteLength: null,
      byteLengthWritable,
      map: null,
      mapWritable: null
    })

    if (this._construct) this._open = this._construct

    if (this._write !== stream.Writable.prototype._write) {
      this._write = write.bind(this, this._write)
    }

    if (this._destroy !== stream.Stream.prototype._destroy) {
      this._destroy = destroy.bind(this, this._destroy)
    }
  }

  write(chunk, encoding, cb) {
    if (typeof encoding === 'function') {
      cb = encoding
      encoding = null
    }

    if (typeof chunk === 'string') {
      encoding = encoding || defaultEncoding
      chunk = Buffer.from(chunk, encoding)
    } else {
      encoding = 'buffer'
    }

    const result = super.write({ chunk, encoding })

    if (cb) stream.Writable.drained(this).then(() => cb(null), cb)

    return result
  }

  end(chunk, encoding, cb) {
    if (typeof chunk === 'function') {
      cb = chunk
      chunk = null
    } else if (typeof encoding === 'function') {
      cb = encoding
      encoding = null
    }

    if (typeof chunk === 'string') {
      encoding = encoding || defaultEncoding
      chunk = Buffer.from(chunk, encoding || defaultEncoding)
    } else {
      encoding = 'buffer'
    }

    const result =
      chunk !== undefined && chunk !== null
        ? super.end({ chunk, encoding })
        : super.end()

    if (cb) this.once('finish', () => cb(null))

    return result
  }
}

exports.Duplex = class Duplex extends stream.Duplex {
  constructor(opts = {}) {
    super({
      ...opts,
      byteLength: null,
      byteLengthReadable: null,
      byteLengthWritable,
      map: null,
      mapReadable: null,
      mapWritable: null
    })

    if (this._construct) this._open = this._construct

    if (this._read !== stream.Readable.prototype._read) {
      this._read = read.bind(this, this._read)
    }

    if (this._write !== stream.Duplex.prototype._write) {
      this._write = write.bind(this, this._write)
    }

    if (this._destroy !== stream.Stream.prototype._destroy) {
      this._destroy = destroy.bind(this, this._destroy)
    }
  }

  push(chunk, encoding) {
    if (typeof chunk === 'string') {
      chunk = Buffer.from(chunk, encoding || defaultEncoding)
    }

    return super.push(chunk)
  }

  unshift(chunk, encoding) {
    if (typeof chunk === 'string') {
      chunk = Buffer.from(chunk, encoding || defaultEncoding)
    }

    super.unshift(chunk)
  }

  write(chunk, encoding, cb) {
    if (typeof encoding === 'function') {
      cb = encoding
      encoding = null
    }

    if (typeof chunk === 'string') {
      encoding = encoding || defaultEncoding
      chunk = Buffer.from(chunk, encoding)
    } else {
      encoding = 'buffer'
    }

    const result = super.write({ chunk, encoding })

    if (cb) stream.Writable.drained(this).then(() => cb(null), cb)

    return result
  }

  end(chunk, encoding, cb) {
    if (typeof chunk === 'function') {
      cb = chunk
      chunk = null
    } else if (typeof encoding === 'function') {
      cb = encoding
      encoding = null
    }

    if (typeof chunk === 'string') {
      encoding = encoding || defaultEncoding
      chunk = Buffer.from(chunk, encoding)
    } else {
      encoding = 'buffer'
    }

    const result =
      chunk !== undefined && chunk !== null
        ? super.end({ chunk, encoding })
        : super.end()

    if (cb) this.once('finish', () => cb(null))

    return result
  }
}

exports.Transform = class Transform extends stream.Transform {
  constructor(opts = {}) {
    super({
      ...opts,
      byteLength: null,
      byteLengthReadable: null,
      byteLengthWritable,
      map: null,
      mapReadable: null,
      mapWritable: null
    })

    if (this._transform !== stream.Transform.prototype._transform) {
      this._transform = transform.bind(this, this._transform)
    } else {
      this._transform = passthrough
    }
  }

  push(chunk, encoding) {
    if (typeof chunk === 'string') {
      chunk = Buffer.from(chunk, encoding || defaultEncoding)
    }

    return super.push(chunk)
  }

  unshift(chunk, encoding) {
    if (typeof chunk === 'string') {
      chunk = Buffer.from(chunk, encoding || defaultEncoding)
    }

    super.unshift(chunk)
  }

  write(chunk, encoding, cb) {
    if (typeof encoding === 'function') {
      cb = encoding
      encoding = null
    }

    if (typeof chunk === 'string') {
      encoding = encoding || defaultEncoding
      chunk = Buffer.from(chunk, encoding)
    } else {
      encoding = 'buffer'
    }

    const result = super.write({ chunk, encoding })

    if (cb) stream.Writable.drained(this).then(() => cb(null), cb)

    return result
  }

  end(chunk, encoding, cb) {
    if (typeof chunk === 'function') {
      cb = chunk
      chunk = null
    } else if (typeof encoding === 'function') {
      cb = encoding
      encoding = null
    }

    if (typeof chunk === 'string') {
      encoding = encoding || defaultEncoding
      chunk = Buffer.from(chunk, encoding)
    } else {
      encoding = 'buffer'
    }

    const result =
      chunk !== undefined && chunk !== null
        ? super.end({ chunk, encoding })
        : super.end()

    if (cb) this.once('finish', () => cb(null))

    return result
  }
}

exports.PassThrough = class PassThrough extends exports.Transform {}

exports.finished = function finished(stream, opts, cb) {
  if (typeof opts === 'function') {
    cb = opts
    opts = {}
  }

  if (!opts) opts = {}

  const { cleanup = false } = opts

  const done = () => {
    cb(exports.getStreamError(stream, { all: true }))

    if (cleanup) detach()
  }

  const detach = () => {
    stream.off('close', done)
    stream.off('error', noop)
  }

  if (stream.destroyed) {
    done()
  } else {
    stream.on('close', done)
    stream.on('error', noop)
  }

  return detach
}

function read(read, cb) {
  read.call(this, 65536)

  cb(null)
}

function write(write, data, cb) {
  write.call(this, data.chunk, data.encoding, cb)
}

function transform(transform, data, cb) {
  transform.call(this, data.chunk, data.encoding, cb)
}

function destroy(destroy, cb) {
  destroy.call(this, exports.getStreamError(this), cb)
}

function passthrough(data, cb) {
  cb(null, data.chunk)
}

function byteLengthWritable(data) {
  return data.chunk.byteLength
}

function noop() {}
{
  "name": "bare-stream",
  "version": "2.7.0",
  "description": "Streaming data for JavaScript",
  "exports": {
    ".": {
      "types": "./index.d.ts",
      "default": "./index.js"
    },
    "./package": "./package.json",
    "./promises": "./promises.js",
    "./web": {
      "types": "./web.d.ts",
      "default": "./web.js"
    },
    "./global": "./global.js"
  },
  "files": [
    "index.js",
    "index.d.ts",
    "promises.js",
    "web.js",
    "web.d.ts",
    "global.js"
  ],
  "scripts": {
    "test": "prettier . --check && bare test.js"
  },
  "repository": {
    "type": "git",
    "url": "git+https://github.com/holepunchto/bare-stream.git"
  },
  "author": "Holepunch",
  "license": "Apache-2.0",
  "bugs": {
    "url": "https://github.com/holepunchto/bare-stream/issues"
  },
  "homepage": "https://github.com/holepunchto/bare-stream#readme",
  "dependencies": {
    "streamx": "^2.21.0"
  },
  "devDependencies": {
    "bare-buffer": "^3.0.0",
    "bare-events": "^2.5.4",
    "brittle": "^3.5.2",
    "prettier": "^3.3.3",
    "prettier-config-standard": "^7.0.0"
  },
  "peerDependencies": {
    "bare-buffer": "*",
    "bare-events": "*"
  },
  "peerDependenciesMeta": {
    "bare-buffer": {
      "optional": true
    },
    "bare-events": {
      "optional": true
    }
  }
}
module.exports = require.addon()
const path = require('bare-path')
const binding = require('./binding')
const errors = require('./lib/errors')
const URLSearchParams = require('./lib/url-search-params')

const kind = Symbol.for('bare.url.kind')

const isWindows = Bare.platform === 'win32'

module.exports = exports = class URL {
  static get [kind]() {
    return 0 // Compatibility version
  }

  constructor(input, base, opts = {}) {
    if (arguments.length === 0) throw errors.INVALID_URL()

    input = String(input)

    if (base !== undefined) base = String(base)

    this._components = new Uint32Array(8)

    this._parse(input, base, opts.throw !== false)

    if (this._href) this._params = new URLSearchParams(this.search, this)
  }

  get [kind]() {
    return URL[kind]
  }

  // https://url.spec.whatwg.org/#dom-url-href

  get href() {
    return this._href
  }

  set href(value) {
    this._update(value)

    this._params._parse(this.search)
  }

  // https://url.spec.whatwg.org/#dom-url-protocol

  get protocol() {
    return this._slice(0, this._components[0]) + ':'
  }

  set protocol(value) {
    this._update(this._replace(value.replace(/:+$/, ''), 0, this._components[0]))
  }

  // https://url.spec.whatwg.org/#dom-url-username

  get username() {
    return this._slice(this._components[0] + 3 /* :// */, this._components[1])
  }

  set username(value) {
    if (cannotHaveCredentialsOrPort(this)) {
      return
    }

    if (this.username === '') value += '@'

    this._update(this._replace(value, this._components[0] + 3 /* :// */, this._components[1]))
  }

  // https://url.spec.whatwg.org/#dom-url-password

  get password() {
    return this._href.slice(this._components[1] + 1 /* : */, this._components[2] - 1 /* @ */)
  }

  set password(value) {
    if (cannotHaveCredentialsOrPort(this)) {
      return
    }

    let start = this._components[1] + 1 /* : */
    let end = this._components[2] - 1 /* @ */

    if (this.password === '') {
      value = ':' + value
      start--
    }

    if (this.username === '') {
      value += '@'
      end++
    }

    this._update(this._replace(value, start, end))
  }

  // https://url.spec.whatwg.org/#dom-url-host

  get host() {
    return this._slice(this._components[2], this._components[5])
  }

  set host(value) {
    if (hasOpaquePath(this)) {
      return
    }

    this._update(
      this._replace(value, this._components[2], this._components[value.includes(':') ? 5 : 3])
    )
  }

  // https://url.spec.whatwg.org/#dom-url-hostname

  get hostname() {
    return this._slice(this._components[2], this._components[3])
  }

  set hostname(value) {
    if (hasOpaquePath(this)) {
      return
    }

    this._update(this._replace(value, this._components[2], this._components[3]))
  }

  // https://url.spec.whatwg.org/#dom-url-port

  get port() {
    return this._slice(this._components[3] + 1 /* : */, this._components[5])
  }

  set port(value) {
    if (cannotHaveCredentialsOrPort(this)) {
      return
    }

    let start = this._components[3] + 1 /* : */

    if (this.port === '') {
      value = ':' + value
      start--
    }

    this._update(this._replace(value, start, this._components[5]))
  }

  // https://url.spec.whatwg.org/#dom-url-pathname

  get pathname() {
    return this._slice(this._components[5], this._components[6] - 1 /* ? */)
  }

  set pathname(value) {
    if (hasOpaquePath(this)) {
      return
    }

    if (value[0] !== '/' && value[0] !== '\\') {
      value = '/' + value
    }

    this._update(this._replace(value, this._components[5], this._components[6] - 1 /* ? */))
  }

  // https://url.spec.whatwg.org/#dom-url-search

  get search() {
    return this._slice(this._components[6] - 1 /* ? */, this._components[7] - 1 /* # */)
  }

  set search(value) {
    if (value && value[0] !== '?') value = '?' + value

    this._update(
      this._replace(value, this._components[6] - 1 /* ? */, this._components[7] - 1 /* # */)
    )

    this._params._parse(this.search)
  }

  // https://url.spec.whatwg.org/#dom-url-searchparams

  get searchParams() {
    return this._params
  }

  // https://url.spec.whatwg.org/#dom-url-hash

  get hash() {
    return this._slice(this._components[7] - 1 /* # */)
  }

  set hash(value) {
    if (value && value[0] !== '#') value = '#' + value

    this._update(this._replace(value, this._components[7] - 1 /* # */))
  }

  toString() {
    return this._href
  }

  toJSON() {
    return this._href
  }

  [Symbol.for('bare.inspect')]() {
    return {
      __proto__: { constructor: URL },

      href: this.href,
      protocol: this.protocol,
      username: this.username,
      password: this.password,
      host: this.host,
      hostname: this.hostname,
      port: this.port,
      pathname: this.pathname,
      search: this.search,
      searchParams: this.searchParams,
      hash: this.hash
    }
  }

  _slice(start, end = this._href.length) {
    return this._href.slice(start, end)
  }

  _replace(replacement, start, end = this._href.length) {
    return this._slice(0, start) + replacement + this._slice(end)
  }

  _parse(input, base, shouldThrow) {
    try {
      this._href = binding.parse(
        String(input),
        base ? String(base) : null,
        this._components,
        shouldThrow
      )
    } catch (err) {
      if (err instanceof TypeError) throw err

      throw errors.INVALID_URL(`Invalid URL '${input}'`, input)
    }
  }

  _update(input) {
    try {
      this._parse(input, null, true)
    } catch (err) {
      if (err instanceof TypeError) throw err
    }
  }
}

// https://url.spec.whatwg.org/#url-opaque-path
function hasOpaquePath(url) {
  return url.pathname[0] !== '/'
}

// https://url.spec.whatwg.org/#cannot-have-a-username-password-port
function cannotHaveCredentialsOrPort(url) {
  return url.hostname === '' || url.protocol === 'file:'
}

const URL = exports

exports.URL = URL
exports.URLSearchParams = URLSearchParams

exports.errors = errors

exports.isURL = function isURL(value) {
  if (value instanceof URL) return true

  return typeof value === 'object' && value !== null && value[kind] === URL[kind]
}

// https://url.spec.whatwg.org/#dom-url-parse
exports.parse = function parse(input, base) {
  const url = new URL(input, base, { throw: false })
  return url._href ? url : null
}

// https://url.spec.whatwg.org/#dom-url-canparse
exports.canParse = function canParse(input, base) {
  return binding.canParse(String(input), base ? String(base) : null)
}

exports.fileURLToPath = function fileURLToPath(url) {
  if (typeof url === 'string') {
    url = new URL(url)
  }

  if (url.protocol !== 'file:') {
    throw errors.INVALID_URL_SCHEME('The URL must use the file: protocol')
  }

  if (isWindows) {
    if (/%2f|%5c/i.test(url.pathname)) {
      throw errors.INVALID_FILE_URL_PATH(
        'The file: URL path must not include encoded \\ or / characters'
      )
    }
  } else {
    if (url.hostname) {
      throw errors.INVALID_FILE_URL_HOST("The file: URL host must be 'localhost' or empty")
    }

    if (/%2f/i.test(url.pathname)) {
      throw errors.INVALID_FILE_URL_PATH('The file: URL path must not include encoded / characters')
    }
  }

  const pathname = path.normalize(decodeURIComponent(url.pathname))

  if (isWindows) {
    if (url.hostname) return '\\\\' + url.hostname + pathname

    const letter = pathname.charCodeAt(1) | 0x20

    if (letter < 0x61 /* a */ || letter > 0x7a /* z */ || pathname.charCodeAt(2) !== 0x3a /* : */) {
      throw errors.INVALID_FILE_URL_PATH('The file: URL path must be absolute')
    }

    return pathname.slice(1)
  }

  return pathname
}

exports.pathToFileURL = function pathToFileURL(pathname) {
  let resolved = path.resolve(pathname)

  if (pathname[pathname.length - 1] === '/') {
    resolved += '/'
  } else if (isWindows && pathname[pathname.length - 1] === '\\') {
    resolved += '\\'
  }

  resolved = resolved
    .replaceAll('%', '%25') // Must be first
    .replaceAll('#', '%23')
    .replaceAll('?', '%3f')
    .replaceAll('\n', '%0a')
    .replaceAll('\r', '%0d')
    .replaceAll('\t', '%09')

  if (!isWindows) {
    resolved = resolved.replaceAll('\\', '%5c')
  }

  return new URL('file:' + resolved)
}

exports.format = function format(parts) {
  const { protocol, auth, host, hostname, port, pathname, search, query, hash, slashes } = parts

  let result = ''

  if (typeof protocol === 'string') {
    result += protocol

    if (protocol[protocol.length - 1] !== ':') {
      result += ':'
    }

    if (slashes === true || /https?|ftp|gopher|file/.test(protocol)) {
      result += '//'
    }
  }

  if (typeof auth === 'string') {
    if (host || hostname) result += auth + '@'
  }

  if (typeof host === 'string') result += host
  else {
    result += hostname

    if (port) result += ':' + port
  }

  if (typeof pathname === 'string' && pathname !== '') {
    if (pathname[0] !== '/') result += '/'
    result += pathname
  }

  if (typeof search === 'string') {
    if (search[0] !== '?') result += '?'
    result += search
  } else if (typeof query === 'object' && query !== null) {
    result += '?' + new URLSearchParams(query)
  }

  if (typeof hash === 'string') {
    if (hash[0] !== '#') result += '#'
    result += hash
  }

  return result
}
module.exports = class URLError extends Error {
  constructor(msg, fn = URLError, code = fn.name) {
    super(`${code}: ${msg}`)

    this.code = code

    if (Error.captureStackTrace) Error.captureStackTrace(this, fn)
  }

  get name() {
    return 'URLError'
  }

  static INVALID_URL(msg, input) {
    const err = new URLError(msg, URLError.INVALID_URL)

    err.input = input

    return err
  }

  static INVALID_URL_SCHEME(msg = 'Invalid URL') {
    return new URLError(msg, URLError.INVALID_URL_SCHEME)
  }

  static INVALID_FILE_URL_HOST(msg = 'Invalid file: URL host') {
    return new URLError(msg, URLError.INVALID_FILE_URL_HOST)
  }

  static INVALID_FILE_URL_PATH(msg = 'Invalid file: URL path') {
    return new URLError(msg, URLError.INVALID_FILE_URL_PATH)
  }
}
module.exports = class URLSearchParams {
  static _urls = new WeakMap()

  // https://url.spec.whatwg.org/#dom-urlsearchparams-urlsearchparams
  constructor(init, url = null) {
    this._params = new Map()

    if (url) URLSearchParams._urls.set(this, url)

    if (typeof init === 'string') {
      this._parse(init)
    } else if (init) {
      for (const [name, value] of typeof init[Symbol.iterator] === 'function'
        ? init
        : Object.entries(init)) {
        this.append(name, value)
      }
    }
  }

  // https://url.spec.whatwg.org/#dom-urlsearchparams-size
  get size() {
    return this._params.length
  }

  // https://url.spec.whatwg.org/#dom-urlsearchparams-append
  append(name, value = null) {
    if (value === null) return

    let list = this._params.get(name)

    if (list === undefined) {
      list = []
      this._params.set(name, list)
    }

    list.push(value)

    this._update()
  }

  // https://url.spec.whatwg.org/#dom-urlsearchparams-delete
  delete(name, value = null) {
    if (value === null) this._params.delete(name)
    else {
      let list = this._params.get(name)

      if (list === undefined) return

      list = list.filter((found) => found !== value)

      if (list.length === 0) this._params.delete(name)
      else this._params.set(name, list)
    }

    this._update()
  }

  // https://url.spec.whatwg.org/#dom-urlsearchparams-get
  get(name) {
    const list = this._params.get(name)

    if (list === undefined) return null

    return list[0]
  }

  // https://url.spec.whatwg.org/#dom-urlsearchparams-getall
  getAll(name) {
    const list = this._params.get(name)

    if (list === undefined) return []

    return Array.from(list)
  }

  // https://url.spec.whatwg.org/#dom-urlsearchparams-has
  has(name, value = null) {
    const list = this._params.get(name)

    if (list === undefined) return false

    if (value === null) return true

    return list.includes(value)
  }

  // https://url.spec.whatwg.org/#dom-urlsearchparams-set
  set(name, value = null) {
    if (value === null) this._params.delete(name)
    else this._params.set(name, [value])

    this._update()
  }

  toString() {
    return this._serialize()
  }

  toJSON() {
    return [...this]
  }

  *[Symbol.iterator]() {
    for (const [name, values] of this._params) {
      for (const value of values) yield [name, value]
    }
  }

  [Symbol.for('bare.inspect')]() {
    const object = {
      __proto__: { constructor: URLSearchParams }
    }

    for (const [name, values] of this._params) {
      if (values.length === 1) object[name] = values[0]
      else object[name] = values
    }

    return object
  }

  // https://url.spec.whatwg.org/#concept-urlsearchparams-update
  _update() {
    const url = URLSearchParams._urls.get(this)

    if (url === undefined) return

    url.search = this._serialize()
  }

  // https://url.spec.whatwg.org/#concept-urlencoded-parser
  _parse(input) {
    if (input[0] === '?') input = input.substring(1)

    this._params = new Map()

    for (const sequence of input.split('&')) {
      if (sequence.length === 0) continue

      let i = sequence.indexOf('=')
      if (i === -1) i = sequence.length

      const name = decodeURIComponent(sequence.substring(0, i))
      const value = decodeURIComponent(sequence.substring(i + 1, sequence.length))

      let list = this._params.get(name)

      if (list === undefined) {
        list = []
        this._params.set(name, list)
      }

      list.push(value)
    }
  }

  // https://url.spec.whatwg.org/#concept-urlencoded-serializer
  _serialize() {
    let output = ''

    for (let [name, values] of this._params) {
      name = encodeURIComponent(name)

      for (const value of values) {
        if (output) output += '&'

        output += name + '=' + encodeURIComponent(value)
      }
    }

    return output
  }
}
{
  "name": "bare-url",
  "version": "2.3.2",
  "description": "WHATWG URL implementation for JavaScript",
  "exports": {
    "./package": "./package.json",
    ".": {
      "types": "./index.d.ts",
      "default": "./index.js"
    },
    "./global": {
      "types": "./global.d.ts",
      "default": "./global.js"
    }
  },
  "files": [
    "index.js",
    "index.d.ts",
    "global.js",
    "global.d.ts",
    "binding.c",
    "binding.js",
    "CMakeLists.txt",
    "lib",
    "prebuilds"
  ],
  "addon": true,
  "scripts": {
    "test": "prettier . --check && bare test.js"
  },
  "repository": {
    "type": "git",
    "url": "git+https://github.com/holepunchto/bare-url.git"
  },
  "author": "Holepunch",
  "license": "Apache-2.0",
  "bugs": {
    "url": "https://github.com/holepunchto/bare-url/issues"
  },
  "homepage": "https://github.com/holepunchto/bare-url",
  "dependencies": {
    "bare-path": "^3.0.0"
  },
  "devDependencies": {
    "brittle": "^3.3.2",
    "cmake-bare": "^1.1.6",
    "cmake-fetch": "^1.0.0",
    "prettier": "^3.3.3",
    "prettier-config-holepunch": "^2.0.0"
  }
}
const FACTOR = new Uint16Array(8)

function factor4096 (i, n) {
  while (n > 0) {
    const f = i & 4095
    FACTOR[--n] = f
    i = (i - f) / 4096
  }
  return FACTOR
}

module.exports = class BigSparseArray {
  constructor () {
    this.tiny = new TinyArray()
    this.maxLength = 4096
    this.factor = 1
  }

  set (index, val) {
    if (val !== undefined) {
      while (index >= this.maxLength) {
        this.maxLength *= 4096
        this.factor++
        if (!this.tiny.isEmptyish()) {
          const t = new TinyArray()
          t.set(0, this.tiny)
          this.tiny = t
        }
      }
    }

    const f = factor4096(index, this.factor)
    const last = this.factor - 1

    let tiny = this.tiny
    for (let i = 0; i < last; i++) {
      const next = tiny.get(f[i])
      if (next === undefined) {
        if (val === undefined) return
        tiny = tiny.set(f[i], new TinyArray())
      } else {
        tiny = next
      }
    }

    return tiny.set(f[last], val)
  }

  get (index) {
    if (index >= this.maxLength) return

    const f = factor4096(index, this.factor)
    const last = this.factor - 1

    let tiny = this.tiny
    for (let i = 0; i < last; i++) {
      tiny = tiny.get(f[i])
      if (tiny === undefined) return
    }

    return tiny.get(f[last])
  }
}

class TinyArray {
  constructor () {
    this.s = 0
    this.b = new Array(1)
    this.f = new Uint16Array(1)
  }

  isEmptyish () {
    return this.b.length === 1 && this.b[0] === undefined
  }

  get (i) {
    if (this.s === 12) return this.b[i]
    const f = i >>> this.s
    const r = i & (this.b.length - 1)
    return this.f[r] === f ? this.b[r] : undefined
  }

  set (i, v) {
    while (this.s !== 12) {
      const f = i >>> this.s
      const r = i & (this.b.length - 1)
      const o = this.b[r]

      if (o === undefined || f === this.f[r]) {
        this.b[r] = v
        this.f[r] = f
        return v
      }

      this.grow()
    }

    this.b[i] = v
    return v
  }

  grow () {
    const os = this.s
    const ob = this.b
    const of = this.f

    this.s += 4
    this.b = new Array(this.b.length << 4)
    this.f = this.s === 12 ? null : new Uint8Array(this.b.length)

    const m = this.b.length - 1

    for (let or = 0; or < ob.length; or++) {
      if (ob[or] === undefined) continue

      const i = of[or] << os | or
      const f = i >>> this.s
      const r = i & m

      this.b[r] = ob[or]
      if (this.s !== 12) this.f[r] = f
    }
  }
}
{
  "name": "big-sparse-array",
  "version": "1.0.3",
  "description": "A sparse array optimised for low memory whilst still being fast",
  "main": "index.js",
  "dependencies": {},
  "devDependencies": {
    "brittle": "^3.1.1",
    "standard": "^16.0.3"
  },
  "scripts": {
    "test": "standard && brittle test.js"
  },
  "repository": {
    "type": "git",
    "url": "https://github.com/mafintosh/big-sparse-array.git"
  },
  "author": "Mathias Buus (@mafintosh)",
  "license": "MIT",
  "bugs": {
    "url": "https://github.com/mafintosh/big-sparse-array/issues"
  },
  "homepage": "https://github.com/mafintosh/big-sparse-array"
}
const b4a = require('b4a')

function byteLength (size) {
  return Math.ceil(size / 8)
}

function get (buffer, bit) {
  const n = buffer.BYTES_PER_ELEMENT * 8

  const offset = bit & (n - 1)
  const i = (bit - offset) / n

  return (buffer[i] & (1 << offset)) !== 0
}

function set (buffer, bit, value = true) {
  const n = buffer.BYTES_PER_ELEMENT * 8

  const offset = bit & (n - 1)
  const i = (bit - offset) / n
  const mask = 1 << offset

  if (value) {
    if ((buffer[i] & mask) !== 0) return false
  } else {
    if ((buffer[i] & mask) === 0) return false
  }

  buffer[i] ^= mask
  return true
}

function setRange (buffer, start, end, value = true) {
  const n = buffer.BYTES_PER_ELEMENT * 8

  let remaining = end - start
  let offset = start & (n - 1)
  let i = (start - offset) / n

  let changed = false

  while (remaining > 0) {
    const mask = (2 ** Math.min(remaining, n - offset) - 1) << offset

    if (value) {
      if ((buffer[i] & mask) !== mask) {
        buffer[i] |= mask
        changed = true
      }
    } else {
      if ((buffer[i] & mask) !== 0) {
        buffer[i] &= ~mask
        changed = true
      }
    }

    remaining -= n - offset
    offset = 0
    i++
  }

  return changed
}

function fill (buffer, value, start = 0, end = buffer.byteLength * 8) {
  const n = buffer.BYTES_PER_ELEMENT * 8
  let i, j

  {
    const offset = start & (n - 1)
    i = (start - offset) / n

    if (offset !== 0) {
      const mask = (2 ** Math.min(n - offset, end - start) - 1) << offset

      if (value) buffer[i] |= mask
      else buffer[i] &= ~mask

      i++
    }
  }

  {
    const offset = end & (n - 1)
    j = (end - offset) / n

    if (offset !== 0 && j >= i) {
      const mask = (2 ** offset) - 1

      if (value) buffer[j] |= mask
      else buffer[j] &= ~mask
    }
  }

  return buffer.fill(value ? (2 ** n) - 1 : 0, i, j)
}

function toggle (buffer, bit) {
  const n = buffer.BYTES_PER_ELEMENT * 8

  const offset = bit & (n - 1)
  const i = (bit - offset) / n
  const mask = 1 << offset

  buffer[i] ^= mask
  return (buffer[i] & mask) !== 0
}

function remove (buffer, bit) {
  return set(buffer, bit, false)
}

function removeRange (buffer, start, end) {
  return setRange(buffer, start, end, false)
}

function indexOf (buffer, value, position = 0) {
  for (let i = position, n = buffer.byteLength * 8; i < n; i++) {
    if (get(buffer, i) === value) return i
  }

  return -1
}

function lastIndexOf (buffer, value, position = buffer.byteLength * 8 - 1) {
  for (let i = position; i >= 0; i--) {
    if (get(buffer, i) === value) return i
  }

  return -1
}

function of (...bits) {
  return from(bits)
}

function from (bits) {
  const buffer = b4a.alloc(byteLength(bits.length))
  for (let i = 0; i < bits.length; i++) set(buffer, i, bits[i])
  return buffer
}

function * iterator (buffer) {
  for (let i = 0, n = buffer.byteLength * 8; i < n; i++) yield get(buffer, i)
}

module.exports = {
  byteLength,
  get,
  set,
  setRange,
  fill,
  toggle,
  remove,
  removeRange,
  indexOf,
  lastIndexOf,
  of,
  from,
  iterator
}
{
  "name": "bits-to-bytes",
  "version": "1.3.0",
  "description": "Functions for doing bit manipulation of typed arrays",
  "main": "index.js",
  "files": [
    "index.js"
  ],
  "scripts": {
    "test": "standard && brittle test.mjs"
  },
  "repository": {
    "type": "git",
    "url": "git+https://github.com/holepunchto/bits-to-bytes.git"
  },
  "author": "Kasper Isager Dalsgar <kasper@funktionel.co>",
  "license": "ISC",
  "bugs": {
    "url": "https://github.com/holepunchto/bits-to-bytes/issues"
  },
  "homepage": "https://github.com/holepunchto/bits-to-bytes#readme",
  "dependencies": {
    "b4a": "^1.5.0"
  },
  "devDependencies": {
    "brittle": "^2.3.1",
    "standard": "^17.0.0"
  }
}
const EventEmitter = require('events')
const Protomux = require('protomux')
const { Readable } = require('streamx')
const sodium = require('sodium-universal')
const b4a = require('b4a')
const c = require('compact-encoding')
const bitfield = require('compact-encoding-bitfield')
const bits = require('bits-to-bytes')
const errors = require('./lib/errors')

exports.Server = class BlindRelayServer extends EventEmitter {
  constructor (opts = {}) {
    super()

    const {
      createStream
    } = opts

    this._createStream = createStream
    this._pairing = new Map()
    this._sessions = new Set()
  }

  get sessions () {
    return this._sessions[Symbol.iterator]()
  }

  accept (stream, opts) {
    const session = new BlindRelaySession(this, stream, opts)

    this._sessions.add(session)

    return session
  }

  async close () {
    const ending = []

    for (const session of this._sessions) {
      ending.push(session.end())
    }

    await Promise.all(ending)

    this._pairing.clear()
  }
}

class BlindRelaySession extends EventEmitter {
  constructor (server, stream, opts = {}) {
    super()

    const {
      id,
      handshake,
      handshakeEncoding
    } = opts

    this._server = server
    this._mux = Protomux.from(stream)

    this._channel = this._mux.createChannel({
      protocol: 'blind-relay',
      id,
      handshake: handshake ? handshakeEncoding || c.raw : null,
      onopen: this._onopen.bind(this),
      onclose: this._onclose.bind(this),
      ondestroy: this._ondestroy.bind(this)
    })

    this._pair = this._channel.addMessage({
      encoding: m.pair,
      onmessage: this._onpair.bind(this)
    })

    this._unpair = this._channel.addMessage({
      encoding: m.unpair,
      onmessage: this._onunpair.bind(this)
    })

    this._ending = null
    this._destroyed = false
    this._error = null
    this._pairing = new Set()
    this._streams = new Map()

    this._onerror = (err) => this.emit('error', err)

    this._channel.open(handshake)
  }

  get closed () {
    return this._channel.closed
  }

  get mux () {
    return this._mux
  }

  get stream () {
    return this._mux.stream
  }

  _onopen () {
    this.emit('open')
  }

  _onclose () {
    this._ending = Promise.resolve()

    const err = this._error || errors.CHANNEL_CLOSED()

    for (const token of this._pairing) {
      this._server._pairing.delete(token.toString('hex'))
    }

    for (const stream of this._streams.values()) {
      stream
        .off('error', this._onerror)
        .on('error', noop)
        .destroy(err)
    }

    this._pairing.clear()
    this._streams.clear()

    this._server._sessions.delete(this)

    this.emit('close')
  }

  _ondestroy () {
    this._destroyed = true
    this.emit('destroy')
  }

  _onpair ({ isInitiator, token, id: remoteId }) {
    const keyString = token.toString('hex')

    let pair = this._server._pairing.get(keyString)

    if (pair === undefined) {
      pair = new BlindRelayPair(token)
      this._server._pairing.set(keyString, pair)
    } else if (pair.links[+isInitiator]) return

    this._pairing.add(keyString)

    pair.links[+isInitiator] = new BlindRelayLink(this, isInitiator, remoteId)

    if (!pair.paired) return

    this._server._pairing.delete(keyString)

    // 1st pass: Create the raw streams needed for each end of the link.
    for (const link of pair.links) {
      link.createStream()
    }

    // 2nd pass: Connect the raw streams and set up handlers.
    for (const { isInitiator, session, stream } of pair.links) {
      const remote = pair.remote(isInitiator)

      stream
        .on('error', session._onerror)
        .on('close', () => session._streams.delete(keyString))
        .relayTo(remote.stream)

      session._pairing.delete(keyString)
      session._streams.set(keyString, stream)
    }

    // 3rd pass: Let either end of the link know the streams were set up.
    for (const { isInitiator, session, remoteId, stream } of pair.links) {
      session._pair.send({
        isInitiator,
        token,
        id: stream.id,
        seq: 0
      })

      session._endMaybe()

      session.emit('pair', isInitiator, token, stream, remoteId)
    }
  }

  _onunpair ({ token }) {
    const keyString = token.toString('hex')

    const pair = this._server._pairing.get(keyString)

    if (pair) {
      for (const link of pair.links) {
        if (link) link.session._pairing.delete(keyString)
      }

      return this._server._pairing.delete(keyString)
    }

    const stream = this._streams.get(keyString)

    if (stream) {
      stream
        .off('error', this._onerror)
        .on('error', noop)
        .destroy(errors.PAIRING_CANCELLED())

      this._streams.delete(keyString)
    }
  }

  cork () {
    this._channel.cork()
  }

  uncork () {
    this._channel.uncork()
  }

  async end () {
    if (this._ending) return this._ending

    this._ending = EventEmitter.once(this, 'close')
    this._endMaybe()

    return this._ending
  }

  _endMaybe () {
    if (this._ending && this._pairing.size === 0) {
      this._channel.close()
    }
  }

  destroy (err) {
    if (this._destroyed) return
    this._destroyed = true

    this._error = err || errors.CHANNEL_DESTROYED()
    this._channel.close()
  }
}

class BlindRelayPair {
  constructor (token) {
    this.token = token
    this.links = [null, null]
  }

  get paired () {
    return this.links[0] !== null && this.links[1] !== null
  }

  remote (isInitiator) {
    return this.links[isInitiator ? 0 : 1]
  }
}

class BlindRelayLink {
  constructor (session, isInitiator, remoteId) {
    this.session = session
    this.isInitiator = isInitiator
    this.remoteId = remoteId
    this.stream = null
  }

  createStream () {
    if (this.stream) return

    this.stream = this.session._server._createStream({
      firewall: this._onfirewall.bind(this)
    })
  }

  _onfirewall (socket, port, host) {
    this.stream.connect(socket, this.remoteId, port, host)

    return false
  }
}

exports.Client = class BlindRelayClient extends EventEmitter {
  static _clients = new WeakMap()

  static from (stream, opts) {
    let client = this._clients.get(stream)
    if (client) return client
    client = new this(stream, opts)
    this._clients.set(stream, client)
    return client
  }

  constructor (stream, opts = {}) {
    super()

    const {
      id,
      handshake,
      handshakeEncoding
    } = opts

    this._mux = Protomux.from(stream)

    this._channel = this._mux.createChannel({
      protocol: 'blind-relay',
      id,
      handshake: handshake ? handshakeEncoding || c.raw : null,
      onopen: this._onopen.bind(this),
      onclose: this._onclose.bind(this),
      ondestroy: this._ondestroy.bind(this)
    })

    this._pair = this._channel.addMessage({
      encoding: m.pair,
      onmessage: this._onpair.bind(this)
    })

    this._unpair = this._channel.addMessage({
      encoding: m.unpair
    })

    this._ending = false
    this._destroyed = false
    this._error = null
    this._requests = new Map()

    this._channel.open(handshake)
  }

  get closed () {
    return this._channel.closed
  }

  get mux () {
    return this._mux
  }

  get stream () {
    return this._mux.stream
  }

  get requests () {
    return this._requests.values()
  }

  _onopen () {
    this.emit('open')
  }

  _onclose () {
    this._ending = Promise.resolve()

    const err = this._error || errors.CHANNEL_CLOSED()

    for (const request of this._requests.values()) {
      request.destroy(err)
    }

    this._requests.clear()

    this.constructor._clients.delete(this.stream)

    this.emit('close')
  }

  _ondestroy () {
    this._destroyed = true
    this.emit('destroy')
  }

  _onpair ({ isInitiator, token, id: remoteId }) {
    const request = this._requests.get(token.toString('hex'))

    if (request === undefined || request.isInitiator !== isInitiator) return

    request.push(remoteId)
    request.push(null)

    this.emit('pair', request.isInitiator, request.token, request.stream, remoteId)
  }

  pair (isInitiator, token, stream) {
    if (this._destroyed) throw errors.CHANNEL_DESTROYED()

    const keyString = token.toString('hex')

    if (this._requests.has(keyString)) throw errors.ALREADY_PAIRING()

    const request = new BlindRelayRequest(this, isInitiator, token, stream)

    this._requests.set(keyString, request)

    return request
  }

  unpair (token) {
    if (this._destroyed) throw errors.CHANNEL_DESTROYED()

    const request = this._requests.get(token.toString('hex'))

    if (request) request.destroy(errors.PAIRING_CANCELLED())

    this._unpair.send({ token })
  }

  cork () {
    this._channel.cork()
  }

  uncork () {
    this._channel.uncork()
  }

  async end () {
    if (this._ending) return this._ending

    this._ending = EventEmitter.once(this, 'close')
    this._endMaybe()

    return this._ending
  }

  _endMaybe () {
    if (this._ending && this._requests.size === 0) {
      this._channel.close()
    }
  }

  destroy (err) {
    if (this._destroyed) return
    this._destroyed = true

    this._error = err || errors.CHANNEL_DESTROYED()
    this._channel.close()
  }
}

class BlindRelayRequest extends Readable {
  constructor (client, isInitiator, token, stream) {
    super()

    this.client = client
    this.isInitiator = isInitiator
    this.token = token
    this.stream = stream
  }

  _open (cb) {
    if (this.client._destroyed) return cb(errors.CHANNEL_DESTROYED())

    this.client._pair.send({
      isInitiator: this.isInitiator,
      token: this.token,
      id: this.stream.id,
      seq: 0
    })

    cb(null)
  }

  _destroy (cb) {
    this.client._requests.delete(this.token.toString('hex'))

    cb(null)

    this.client._endMaybe()
  }
}

exports.token = function token (buf = b4a.allocUnsafe(32)) {
  sodium.randombytes_buf(buf)
  return buf
}

function noop () {}

const m = exports.messages = {}

const flags = bitfield(7)

m.pair = {
  preencode (state, m) {
    flags.preencode(state)
    c.fixed32.preencode(state, m.token)
    c.uint.preencode(state, m.id)
    c.uint.preencode(state, m.seq)
  },
  encode (state, m) {
    flags.encode(state, bits.of(m.isInitiator))
    c.fixed32.encode(state, m.token)
    c.uint.encode(state, m.id)
    c.uint.encode(state, m.seq)
  },
  decode (state) {
    const [isInitiator] = bits.iterator(flags.decode(state))

    return {
      isInitiator,
      token: c.fixed32.decode(state),
      id: c.uint.decode(state),
      seq: c.uint.decode(state)
    }
  }
}

m.unpair = {
  preencode (state, m) {
    flags.preencode(state)
    c.fixed32.preencode(state, m.token)
  },
  encode (state, m) {
    flags.encode(state, bits.of())
    c.fixed32.encode(state, m.token)
  },
  decode (state) {
    flags.decode(state)

    return {
      token: c.fixed32.decode(state)
    }
  }
}
module.exports = class BlindRelayError extends Error {
  constructor (msg, code, fn = BlindRelayError) {
    super(`${code}: ${msg}`)
    this.code = code

    if (Error.captureStackTrace) {
      Error.captureStackTrace(this, fn)
    }
  }

  get name () {
    return 'BlindRelayError'
  }

  static DUPLICATE_CHANNEL (msg = 'Duplicate channel') {
    return new BlindRelayError(msg, 'DUPLICATE_CHANNEL', BlindRelayError.DUPLICATE_CHANNEL)
  }

  static CHANNEL_CLOSED (msg = 'Channel closed') {
    return new BlindRelayError(msg, 'CHANNEL_CLOSED', BlindRelayError.CHANNEL_CLOSED)
  }

  static CHANNEL_DESTROYED (msg = 'Channel destroyed') {
    return new BlindRelayError(msg, 'CHANNEL_DESTROYED', BlindRelayError.CHANNEL_DESTROYED)
  }

  static ALREADY_PAIRING (msg = 'Already pairing') {
    return new BlindRelayError(msg, 'ALREADY_PAIRING', BlindRelayError.ALREADY_PAIRING)
  }

  static PAIRING_CANCELLED (msg = 'Pairing cancelled') {
    return new BlindRelayError(msg, 'PAIRING_CANCELLED', BlindRelayError.PAIRING_CANCELLED)
  }
}
{
  "name": "blind-relay",
  "version": "1.4.0",
  "description": "Blind relay for UDX over Protomux channels",
  "main": "index.js",
  "files": [
    "index.js",
    "lib"
  ],
  "scripts": {
    "test": "standard && brittle test.mjs"
  },
  "repository": {
    "type": "git",
    "url": "git+https://github.com/holepunchto/blind-relay.git"
  },
  "author": "Holepunch",
  "license": "Apache-2.0",
  "bugs": {
    "url": "https://github.com/holepunchto/blind-relay/issues"
  },
  "homepage": "https://github.com/holepunchto/blind-relay#readme",
  "imports": {
    "events": {
      "bare": "bare-events",
      "default": "events"
    }
  },
  "dependencies": {
    "b4a": "^1.6.4",
    "bare-events": "^2.2.0",
    "bits-to-bytes": "^1.3.0",
    "compact-encoding": "^2.12.0",
    "compact-encoding-bitfield": "^1.0.0",
    "protomux": "^3.5.1",
    "sodium-universal": "^5.0.0",
    "streamx": "^2.15.1"
  },
  "devDependencies": {
    "brittle": "^3.2.1",
    "hyperdht": "^6.6.1",
    "standard": "^17.0.0",
    "udx-native": "^1.6.1"
  }
}
// https://ipinfo.io/bogon

const b4a = require('b4a')
const c = require('compact-encoding')
const net = require('compact-encoding-net')

module.exports = exports = function isBogon (ip) {
  return isBogonIP(ensureBuffer(ip))
}

exports.isBogon = exports

exports.isPrivate = function isPrivate (ip) {
  return isPrivateIP(ensureBuffer(ip))
}

function isBogonIP (ip) {
  return isPrivateIP(ip) || isReservedIP(ip)
}

function isPrivateIP (ip) {
  return ip.byteLength === 4 ? isPrivateIPv4(ip) : false // IPv6 has no private IPs
}

function isPrivateIPv4 (ip) {
  return (
    // 10.0.0.0/8  Private-use networks
    (ip[0] === 10) ||
    // 100.64.0.0/10 Carrier-grade NAT
    (ip[0] === 100 && ip[1] >= 64 && ip[1] <= 127) ||
    // 127.0.0.0/8 Loopback + Name collision occurrence (127.0.53.53)
    (ip[0] === 127) ||
    // 169.254.0.0/16  Link local
    (ip[0] === 169 && ip[1] === 254) ||
    // 172.16.0.0/12 Private-use networks
    (ip[0] === 172 && ip[1] >= 16 && ip[1] <= 31) ||
    // 192.168.0.0/16  Private-use networks
    (ip[0] === 192 && ip[1] === 168)
  )
}

function isReservedIP (ip) {
  return ip.byteLength === 4 ? isReservedIPv4(ip) : isReservedIPv6(ip)
}

function isReservedIPv4 (ip) {
  return (
    // 0.0.0.0/8 "This" network
    (ip[0] === 0) ||
    // 192.0.0.0/24  IETF protocol assignments
    (ip[0] === 192 && ip[1] === 0 && ip[2] === 0) ||
    // 192.0.2.0/24  TEST-NET-1
    (ip[0] === 192 && ip[1] === 0 && ip[2] === 2) ||
    // 198.18.0.0/15 Network interconnect device benchmark testing
    (ip[0] === 198 && ip[1] >= 18 && ip[1] <= 19) ||
    // 198.51.100.0/24 TEST-NET-2
    (ip[0] === 198 && ip[1] === 51 && ip[2] === 100) ||
    // 203.0.113.0/24  TEST-NET-3
    (ip[0] === 203 && ip[1] === 0 && ip[2] === 113) ||
    // 224.0.0.0/4 Multicast
    (ip[0] >= 224 && ip[0] <= 239) ||
    // 240.0.0.0/4 Reserved for future use
    (ip[0] >= 240) ||
    // 255.255.255.255/32
    (ip[0] === 255 && ip[1] === 255 && ip[2] === 255 && ip[3] === 255)
  )
}

function isReservedIPv6 (ip) {
  return (
    // ::/128 Node-scope unicast unspecified address
    // ::1/128 Node-scope unicast loopback address
    (
      ip[0] === 0 && ip[1] === 0 && ip[2] === 0 && ip[3] === 0 && ip[4] === 0 &&
      ip[5] === 0 && ip[6] === 0 && ip[7] === 0 && ip[8] === 0 && ip[9] === 0 &&
      ip[10] === 0 && ip[11] === 0 && ip[12] === 0 && ip[13] === 0 && ip[14] === 0 &&
      ip[15] <= 1
    ) ||
    // ::ffff:0:0/96 IPv4-mapped addresses
    // ::/96 IPv4-compatible addresses
    (
      ip[0] === 0 && ip[1] === 0 && ip[2] === 0 && ip[3] === 0 && ip[4] === 0 &&
      ip[5] === 0 && ip[6] === 0 && ip[7] === 0 && ip[8] === 0 && ip[9] === 0 &&
      (ip[10] === 0 || ip[10] === 0xff) &&
      (ip[11] === 0 || ip[11] === 0xff)
    ) ||
    // 100::/64 Remotely triggered black hole addresses
    (ip[0] === 0x01 && ip[1] === 0 && ip[2] === 0 && ip[3] === 0 && ip[4] === 0 && ip[5] === 0 && ip[6] === 0 && ip[7] === 0) ||
    // 2001:10::/28 Overlay routable cryptographic hash identifiers (ORCHID)
    (ip[0] === 0x20 && ip[1] === 0x01 && ip[2] === 0 && ip[3] >= 0x10 && ip[3] <= 0x1f) ||
    // 2001:20::/28 Overlay routable cryptographic hash identifiers version 2 (ORCHIDv2)
    (ip[0] === 0x20 && ip[1] === 0x01 && ip[2] === 0 && ip[3] >= 0x20 && ip[3] <= 0x2f) ||
    // 2001:db8::/32 Documentation prefix
    (ip[0] === 0x20 && ip[1] === 0x01 && ip[2] === 0x0d && ip[3] === 0xb8) ||
    // fc00::/7 Unique local addresses (ULA)
    (ip[0] >= 0xfc && ip[0] <= 0xfd) ||
    // fe80::/10 Link-local unicast
    (ip[0] === 0xfe && ip[1] >= 0x80 && ip[1] <= 0xbf) ||
    // ff00::/8 Multicast
    (ip[0] === 0xff)
  )
}

const state = c.state(0, 0, b4a.allocUnsafe(1 /* family */ + 16))

function ensureBuffer (ip) {
  if (b4a.isBuffer(ip)) return ip

  net.ip.preencode(state, ip)
  net.ip.encode(state, ip)

  const buffer = state.buffer.subarray(1 /* family */, state.end)

  state.start = 0
  state.end = 0

  return buffer
}
{
  "name": "bogon",
  "version": "1.1.0",
  "description": "Check if an IP is a bogon",
  "main": "index.js",
  "scripts": {
    "test": "standard && brittle test.mjs"
  },
  "dependencies": {
    "compact-encoding": "^2.11.0",
    "compact-encoding-net": "^1.2.0"
  },
  "devDependencies": {
    "brittle": "^3.0.4",
    "nanobench": "^2.1.1",
    "standard": "^17.0.0"
  },
  "repository": {
    "type": "git",
    "url": "https://github.com/mafintosh/bogon.git"
  },
  "author": "Mathias Buus (@mafintosh)",
  "license": "MIT",
  "bugs": {
    "url": "https://github.com/mafintosh/bogon/issues"
  },
  "homepage": "https://github.com/mafintosh/bogon"
}
const b4a = require('b4a')

module.exports = codecs

codecs.ascii = createString('ascii')
codecs.utf8 = createString('utf-8')
codecs.hex = createString('hex')
codecs.base64 = createString('base64')
codecs.ucs2 = createString('ucs2')
codecs.utf16le = createString('utf16le')
codecs.ndjson = createJSON(true)
codecs.json = createJSON(false)
codecs.binary = {
  name: 'binary',
  encode: function encodeBinary (obj) {
    return typeof obj === 'string'
      ? b4a.from(obj, 'utf-8')
      : b4a.toBuffer(obj)
  },
  decode: function decodeBinary (buf) {
    return b4a.toBuffer(buf)
  }
}

function isCompactEncoding (c) {
  return !!(c.encode && c.decode && c.preencode)
}

function fromCompactEncoding (c) {
  return {
    name: 'compact-encoding',
    encode: function encodeWithCompact (value) {
      const state = { start: 0, end: 0, buffer: null, cache: null }
      c.preencode(state, value)
      state.buffer = b4a.allocUnsafe(state.end)
      c.encode(state, value)
      return state.buffer
    },
    decode: function decodeWithCompact (buffer) {
      return c.decode({ start: 0, end: buffer.byteLength, buffer, cache: null })
    }
  }
}

function codecs (fmt, fallback) {
  if (typeof fmt === 'object' && fmt) {
    return isCompactEncoding(fmt) ? fromCompactEncoding(fmt) : fmt
  }

  switch (fmt) {
    case 'ndjson': return codecs.ndjson
    case 'json': return codecs.json
    case 'ascii': return codecs.ascii
    case 'utf-8':
    case 'utf8': return codecs.utf8
    case 'hex': return codecs.hex
    case 'base64': return codecs.base64
    case 'ucs-2':
    case 'ucs2': return codecs.ucs2
    case 'utf16-le':
    case 'utf16le': return codecs.utf16le
  }

  return fallback !== undefined ? fallback : codecs.binary
}

function createJSON (newline) {
  return {
    name: newline ? 'ndjson' : 'json',
    encode: newline ? encodeNDJSON : encodeJSON,
    decode: function decodeJSON (buf) {
      return JSON.parse(b4a.toString(buf))
    }
  }

  function encodeJSON (val) {
    return b4a.from(JSON.stringify(val))
  }

  function encodeNDJSON (val) {
    return b4a.from(JSON.stringify(val) + '\n')
  }
}

function createString (type) {
  return {
    name: type,
    encode: function encodeString (val) {
      if (typeof val !== 'string') val = val.toString()
      return b4a.from(val, type)
    },
    decode: function decodeString (buf) {
      return b4a.toString(buf, type)
    }
  }
}
{
  "name": "codecs",
  "version": "3.1.0",
  "description": "Create an binary encoder/decoder for json, utf-8 or custom types",
  "main": "index.js",
  "dependencies": {
    "b4a": "^1.6.3"
  },
  "devDependencies": {
    "brittle": "^3.2.1",
    "compact-encoding": "^2.11.0",
    "standard": "^17.0.0"
  },
  "scripts": {
    "test": "standard && brittle test.js"
  },
  "repository": {
    "type": "git",
    "url": "https://github.com/mafintosh/codecs.git"
  },
  "author": "Mathias Buus (@mafintosh)",
  "license": "MIT",
  "bugs": {
    "url": "https://github.com/mafintosh/codecs/issues"
  },
  "homepage": "https://github.com/mafintosh/codecs"
}
const c = require('compact-encoding')

module.exports = function bitfield (length) {
  if (length > 64) throw new RangeError('Bitfield cannot be larger than 64 bits')

  let byteLength
  if (length < 8) byteLength = 1
  else if (length <= 16) byteLength = 2
  else if (length <= 32) byteLength = 4
  else byteLength = 8

  return {
    preencode (state) {
      state.end++ // Length byte, used for data when byteLength === 1

      if (byteLength === 1) ;
      else if (byteLength === 2) c.uint16.preencode(state)
      else if (byteLength === 4) c.uint32.preencode(state)
      else c.uint64.preencode(state)
    },

    encode (state, b) {
      if (byteLength === 1) ;
      else if (byteLength === 2) c.uint8.encode(state, 0xfd)
      else if (byteLength === 4) c.uint8.encode(state, 0xfe)
      else c.uint8.encode(state, 0xff)

      if (typeof b === 'number') {
        if (byteLength === 1) c.uint8.encode(state, b)
        else if (byteLength === 2) c.uint16.encode(state, b)
        else if (byteLength === 4) c.uint32.encode(state, b)
        else c.uint64.encode(state, b)
      } else {
        state.buffer.set(b, state.start)

        if (b.byteLength < byteLength) {
          // Zero-fill the rest of the byte length.
          state.buffer.fill(
            0,
            state.start + b.byteLength,
            state.start + byteLength
          )
        }

        state.start += byteLength
      }
    },

    decode (state) {
      const byte = state.buffer[state.start]

      let byteLength
      if (byte <= 0xfc) byteLength = 1
      else if (byte === 0xfd) byteLength = 2
      else if (byte === 0xfe) byteLength = 4
      else byteLength = 8

      if (byteLength > 1) state.start++ // Skip the length byte

      if (state.end - state.start < byteLength) throw new Error('Out of bounds')

      const b = state.buffer.subarray(state.start, (state.start += byteLength))

      return length <= 8 ? b.subarray(0, 1) : b
    }
  }
}
{
  "name": "compact-encoding-bitfield",
  "version": "1.0.0",
  "description": "Compact codec for bitfields",
  "main": "index.js",
  "files": [
    "index.js"
  ],
  "scripts": {
    "test": "standard && brittle test.mjs"
  },
  "repository": {
    "type": "git",
    "url": "git+https://github.com/compact-encoding/compact-encoding-bitfield.git"
  },
  "author": "Kasper Isager Dalsgar <kasper@funktionel.co>",
  "license": "ISC",
  "bugs": {
    "url": "https://github.com/compact-encoding/compact-encoding-bitfield/issues"
  },
  "homepage": "https://github.com/compact-encoding/compact-encoding-bitfield#readme",
  "dependencies": {
    "compact-encoding": "^2.4.1"
  },
  "devDependencies": {
    "brittle": "^1.3.5",
    "standard": "^16.0.3"
  },
  "standard": {
    "ignore": [
      "__snapshots__/**"
    ]
  }
}
const c = require('compact-encoding')

const port = c.uint16

const address = (host, family) => {
  return {
    preencode (state, m) {
      host.preencode(state, m.host)
      port.preencode(state, m.port)
    },
    encode (state, m) {
      host.encode(state, m.host)
      port.encode(state, m.port)
    },
    decode (state) {
      return {
        host: host.decode(state),
        family,
        port: port.decode(state)
      }
    }
  }
}

const ipv4 = {
  preencode (state) {
    state.end += 4
  },
  encode (state, string) {
    const start = state.start
    const end = start + 4

    let i = 0

    while (i < string.length) {
      let n = 0
      let c

      while (i < string.length && (c = string.charCodeAt(i++)) !== /* . */ 0x2e) {
        n = n * 10 + (c - /* 0 */ 0x30)
      }

      state.buffer[state.start++] = n
    }

    state.start = end
  },
  decode (state) {
    if (state.end - state.start < 4) throw new Error('Out of bounds')
    return (
      state.buffer[state.start++] + '.' +
      state.buffer[state.start++] + '.' +
      state.buffer[state.start++] + '.' +
      state.buffer[state.start++]
    )
  }
}

const ipv4Address = address(ipv4, 4)

const ipv6 = {
  preencode (state) {
    state.end += 16
  },
  encode (state, string) {
    const start = state.start
    const end = start + 16

    let i = 0
    let split = null

    while (i < string.length) {
      let n = 0
      let c

      while (i < string.length && (c = string.charCodeAt(i++)) !== /* : */ 0x3a) {
        if (c >= 0x30 && c <= 0x39) n = n * 0x10 + (c - /* 0 */ 0x30)
        else if (c >= 0x41 && c <= 0x46) n = n * 0x10 + (c - /* A */ 0x41 + 10)
        else if (c >= 0x61 && c <= 0x66) n = n * 0x10 + (c - /* a */ 0x61 + 10)
      }

      state.buffer[state.start++] = n >>> 8
      state.buffer[state.start++] = n

      if (i < string.length && string.charCodeAt(i) === /* : */ 0x3a) {
        i++
        split = state.start
      }
    }

    if (split !== null) {
      const offset = end - state.start
      state.buffer
        .copyWithin(split + offset, split)
        .fill(0, split, split + offset)
    }

    state.start = end
  },
  decode (state) {
    if (state.end - state.start < 16) throw new Error('Out of bounds')
    return (
      (state.buffer[state.start++] * 256 + state.buffer[state.start++]).toString(16) + ':' +
      (state.buffer[state.start++] * 256 + state.buffer[state.start++]).toString(16) + ':' +
      (state.buffer[state.start++] * 256 + state.buffer[state.start++]).toString(16) + ':' +
      (state.buffer[state.start++] * 256 + state.buffer[state.start++]).toString(16) + ':' +
      (state.buffer[state.start++] * 256 + state.buffer[state.start++]).toString(16) + ':' +
      (state.buffer[state.start++] * 256 + state.buffer[state.start++]).toString(16) + ':' +
      (state.buffer[state.start++] * 256 + state.buffer[state.start++]).toString(16) + ':' +
      (state.buffer[state.start++] * 256 + state.buffer[state.start++]).toString(16)
    )
  }
}

const ipv6Address = address(ipv6, 6)

const ip = {
  preencode (state, string) {
    const family = string.includes(':') ? 6 : 4
    c.uint8.preencode(state, family)
    if (family === 4) ipv4.preencode(state)
    else ipv6.preencode(state)
  },
  encode (state, string) {
    const family = string.includes(':') ? 6 : 4
    c.uint8.encode(state, family)
    if (family === 4) ipv4.encode(state, string)
    else ipv6.encode(state, string)
  },
  decode (state) {
    const family = c.uint8.decode(state)
    if (family === 4) return ipv4.decode(state)
    else return ipv6.decode(state)
  }
}

const ipAddress = {
  preencode (state, m) {
    ip.preencode(state, m.host)
    port.preencode(state, m.port)
  },
  encode (state, m) {
    ip.encode(state, m.host)
    port.encode(state, m.port)
  },
  decode (state) {
    const family = c.uint8.decode(state)
    return {
      host: family === 4 ? ipv4.decode(state) : ipv6.decode(state),
      family,
      port: port.decode(state)
    }
  }
}

module.exports = {
  port,
  ipv4,
  ipv4Address,
  ipv6,
  ipv6Address,
  ip,
  ipAddress
}
{
  "name": "compact-encoding-net",
  "version": "1.2.0",
  "description": "Compact codecs for net types",
  "main": "index.js",
  "scripts": {
    "test": "standard && brittle test.mjs"
  },
  "repository": {
    "type": "git",
    "url": "git+https://github.com/compact-encoding/compact-encoding-net.git"
  },
  "author": "Kasper Isager Dalsgar <kasper@funktionel.co>",
  "license": "ISC",
  "bugs": {
    "url": "https://github.com/compact-encoding/compact-encoding-net/issues"
  },
  "homepage": "https://github.com/compact-encoding/compact-encoding-net#readme",
  "dependencies": {
    "compact-encoding": "^2.4.1"
  },
  "devDependencies": {
    "brittle": "^1.3.5",
    "nanobench": "^2.1.1",
    "standard": "^16.0.3"
  }
}
const LE = (exports.LE =
  new Uint8Array(new Uint16Array([0xff]).buffer)[0] === 0xff)

exports.BE = !LE
const b4a = require('b4a')

const { BE } = require('./endian')

exports.state = function (start = 0, end = 0, buffer = null) {
  return { start, end, buffer }
}

const raw = (exports.raw = require('./raw'))

const uint = (exports.uint = {
  preencode(state, n) {
    state.end += n <= 0xfc ? 1 : n <= 0xffff ? 3 : n <= 0xffffffff ? 5 : 9
  },
  encode(state, n) {
    if (n <= 0xfc) uint8.encode(state, n)
    else if (n <= 0xffff) {
      state.buffer[state.start++] = 0xfd
      uint16.encode(state, n)
    } else if (n <= 0xffffffff) {
      state.buffer[state.start++] = 0xfe
      uint32.encode(state, n)
    } else {
      state.buffer[state.start++] = 0xff
      uint64.encode(state, n)
    }
  },
  decode(state) {
    const a = uint8.decode(state)
    if (a <= 0xfc) return a
    if (a === 0xfd) return uint16.decode(state)
    if (a === 0xfe) return uint32.decode(state)
    return uint64.decode(state)
  }
})

const uint8 = (exports.uint8 = {
  preencode(state, n) {
    state.end += 1
  },
  encode(state, n) {
    validateUint(n)
    state.buffer[state.start++] = n
  },
  decode(state) {
    if (state.start >= state.end) throw new Error('Out of bounds')
    return state.buffer[state.start++]
  }
})

const uint16 = (exports.uint16 = {
  preencode(state, n) {
    state.end += 2
  },
  encode(state, n) {
    validateUint(n)
    state.buffer[state.start++] = n
    state.buffer[state.start++] = n >>> 8
  },
  decode(state) {
    if (state.end - state.start < 2) throw new Error('Out of bounds')
    return state.buffer[state.start++] + state.buffer[state.start++] * 0x100
  }
})

const uint24 = (exports.uint24 = {
  preencode(state, n) {
    state.end += 3
  },
  encode(state, n) {
    validateUint(n)
    state.buffer[state.start++] = n
    state.buffer[state.start++] = n >>> 8
    state.buffer[state.start++] = n >>> 16
  },
  decode(state) {
    if (state.end - state.start < 3) throw new Error('Out of bounds')
    return (
      state.buffer[state.start++] +
      state.buffer[state.start++] * 0x100 +
      state.buffer[state.start++] * 0x10000
    )
  }
})

const uint32 = (exports.uint32 = {
  preencode(state, n) {
    state.end += 4
  },
  encode(state, n) {
    validateUint(n)
    state.buffer[state.start++] = n
    state.buffer[state.start++] = n >>> 8
    state.buffer[state.start++] = n >>> 16
    state.buffer[state.start++] = n >>> 24
  },
  decode(state) {
    if (state.end - state.start < 4) throw new Error('Out of bounds')
    return (
      state.buffer[state.start++] +
      state.buffer[state.start++] * 0x100 +
      state.buffer[state.start++] * 0x10000 +
      state.buffer[state.start++] * 0x1000000
    )
  }
})

const uint40 = (exports.uint40 = {
  preencode(state, n) {
    state.end += 5
  },
  encode(state, n) {
    validateUint(n)
    const r = Math.floor(n / 0x100)
    uint8.encode(state, n)
    uint32.encode(state, r)
  },
  decode(state) {
    if (state.end - state.start < 5) throw new Error('Out of bounds')
    return uint8.decode(state) + 0x100 * uint32.decode(state)
  }
})

const uint48 = (exports.uint48 = {
  preencode(state, n) {
    state.end += 6
  },
  encode(state, n) {
    validateUint(n)
    const r = Math.floor(n / 0x10000)
    uint16.encode(state, n)
    uint32.encode(state, r)
  },
  decode(state) {
    if (state.end - state.start < 6) throw new Error('Out of bounds')
    return uint16.decode(state) + 0x10000 * uint32.decode(state)
  }
})

const uint56 = (exports.uint56 = {
  preencode(state, n) {
    state.end += 7
  },
  encode(state, n) {
    validateUint(n)
    const r = Math.floor(n / 0x1000000)
    uint24.encode(state, n)
    uint32.encode(state, r)
  },
  decode(state) {
    if (state.end - state.start < 7) throw new Error('Out of bounds')
    return uint24.decode(state) + 0x1000000 * uint32.decode(state)
  }
})

const uint64 = (exports.uint64 = {
  preencode(state, n) {
    state.end += 8
  },
  encode(state, n) {
    validateUint(n)
    const r = Math.floor(n / 0x100000000)
    uint32.encode(state, n)
    uint32.encode(state, r)
  },
  decode(state) {
    if (state.end - state.start < 8) throw new Error('Out of bounds')
    return uint32.decode(state) + 0x100000000 * uint32.decode(state)
  }
})

const int = (exports.int = zigZagInt(uint))
exports.int8 = zigZagInt(uint8)
exports.int16 = zigZagInt(uint16)
exports.int24 = zigZagInt(uint24)
exports.int32 = zigZagInt(uint32)
exports.int40 = zigZagInt(uint40)
exports.int48 = zigZagInt(uint48)
exports.int56 = zigZagInt(uint56)
exports.int64 = zigZagInt(uint64)

const biguint64 = (exports.biguint64 = {
  preencode(state, n) {
    state.end += 8
  },
  encode(state, n) {
    const view = new DataView(
      state.buffer.buffer,
      state.start + state.buffer.byteOffset,
      8
    )
    view.setBigUint64(0, n, true) // little endian
    state.start += 8
  },
  decode(state) {
    if (state.end - state.start < 8) throw new Error('Out of bounds')
    const view = new DataView(
      state.buffer.buffer,
      state.start + state.buffer.byteOffset,
      8
    )
    const n = view.getBigUint64(0, true) // little endian
    state.start += 8
    return n
  }
})

exports.bigint64 = zigZagBigInt(biguint64)

const biguint = (exports.biguint = {
  preencode(state, n) {
    let len = 0
    for (let m = n; m; m = m >> 64n) len++
    uint.preencode(state, len)
    state.end += 8 * len
  },
  encode(state, n) {
    let len = 0
    for (let m = n; m; m = m >> 64n) len++
    uint.encode(state, len)
    const view = new DataView(
      state.buffer.buffer,
      state.start + state.buffer.byteOffset,
      8 * len
    )
    for (let m = n, i = 0; m; m = m >> 64n, i += 8) {
      view.setBigUint64(i, BigInt.asUintN(64, m), true) // little endian
    }
    state.start += 8 * len
  },
  decode(state) {
    const len = uint.decode(state)
    if (state.end - state.start < 8 * len) throw new Error('Out of bounds')
    const view = new DataView(
      state.buffer.buffer,
      state.start + state.buffer.byteOffset,
      8 * len
    )
    let n = 0n
    for (let i = len - 1; i >= 0; i--)
      n = (n << 64n) + view.getBigUint64(i * 8, true) // little endian
    state.start += 8 * len
    return n
  }
})

exports.bigint = zigZagBigInt(biguint)

exports.lexint = require('./lexint')

exports.float32 = {
  preencode(state, n) {
    state.end += 4
  },
  encode(state, n) {
    const view = new DataView(
      state.buffer.buffer,
      state.start + state.buffer.byteOffset,
      4
    )
    view.setFloat32(0, n, true) // little endian
    state.start += 4
  },
  decode(state) {
    if (state.end - state.start < 4) throw new Error('Out of bounds')
    const view = new DataView(
      state.buffer.buffer,
      state.start + state.buffer.byteOffset,
      4
    )
    const float = view.getFloat32(0, true) // little endian
    state.start += 4
    return float
  }
}

exports.float64 = {
  preencode(state, n) {
    state.end += 8
  },
  encode(state, n) {
    const view = new DataView(
      state.buffer.buffer,
      state.start + state.buffer.byteOffset,
      8
    )
    view.setFloat64(0, n, true) // little endian
    state.start += 8
  },
  decode(state) {
    if (state.end - state.start < 8) throw new Error('Out of bounds')
    const view = new DataView(
      state.buffer.buffer,
      state.start + state.buffer.byteOffset,
      8
    )
    const float = view.getFloat64(0, true) // little endian
    state.start += 8
    return float
  }
}

const buffer = (exports.buffer = {
  preencode(state, b) {
    if (b) uint8array.preencode(state, b)
    else state.end++
  },
  encode(state, b) {
    if (b) uint8array.encode(state, b)
    else state.buffer[state.start++] = 0
  },
  decode(state) {
    const len = uint.decode(state)
    if (len === 0) return null
    if (state.end - state.start < len) throw new Error('Out of bounds')
    return state.buffer.subarray(state.start, (state.start += len))
  }
})

exports.binary = {
  ...buffer,
  preencode(state, b) {
    if (typeof b === 'string') utf8.preencode(state, b)
    else buffer.preencode(state, b)
  },
  encode(state, b) {
    if (typeof b === 'string') utf8.encode(state, b)
    else buffer.encode(state, b)
  }
}

exports.arraybuffer = {
  preencode(state, b) {
    uint.preencode(state, b.byteLength)
    state.end += b.byteLength
  },
  encode(state, b) {
    uint.encode(state, b.byteLength)

    const view = new Uint8Array(b)

    state.buffer.set(view, state.start)
    state.start += b.byteLength
  },
  decode(state) {
    const len = uint.decode(state)

    const b = new ArrayBuffer(len)
    const view = new Uint8Array(b)

    view.set(state.buffer.subarray(state.start, (state.start += len)))

    return b
  }
}

function typedarray(TypedArray, swap) {
  const n = TypedArray.BYTES_PER_ELEMENT

  return {
    preencode(state, b) {
      uint.preencode(state, b.length)
      state.end += b.byteLength
    },
    encode(state, b) {
      uint.encode(state, b.length)

      const view = new Uint8Array(b.buffer, b.byteOffset, b.byteLength)

      if (BE && swap) swap(view)

      state.buffer.set(view, state.start)
      state.start += b.byteLength
    },
    decode(state) {
      const len = uint.decode(state)

      let b = state.buffer.subarray(state.start, (state.start += len * n))
      if (b.byteLength !== len * n) throw new Error('Out of bounds')
      if (b.byteOffset % n !== 0) b = new Uint8Array(b)

      if (BE && swap) swap(b)

      return new TypedArray(b.buffer, b.byteOffset, b.byteLength / n)
    }
  }
}

const uint8array = (exports.uint8array = typedarray(Uint8Array))
exports.uint16array = typedarray(Uint16Array, b4a.swap16)
exports.uint32array = typedarray(Uint32Array, b4a.swap32)

exports.int8array = typedarray(Int8Array)
exports.int16array = typedarray(Int16Array, b4a.swap16)
exports.int32array = typedarray(Int32Array, b4a.swap32)

exports.biguint64array = typedarray(BigUint64Array, b4a.swap64)
exports.bigint64array = typedarray(BigInt64Array, b4a.swap64)

exports.float32array = typedarray(Float32Array, b4a.swap32)
exports.float64array = typedarray(Float64Array, b4a.swap64)

function string(encoding) {
  return {
    preencode(state, s) {
      const len = b4a.byteLength(s, encoding)
      uint.preencode(state, len)
      state.end += len
    },
    encode(state, s) {
      const len = b4a.byteLength(s, encoding)
      uint.encode(state, len)
      b4a.write(state.buffer, s, state.start, encoding)
      state.start += len
    },
    decode(state) {
      const len = uint.decode(state)
      if (state.end - state.start < len) throw new Error('Out of bounds')
      return b4a.toString(
        state.buffer,
        encoding,
        state.start,
        (state.start += len)
      )
    },
    fixed(n) {
      return {
        preencode(state) {
          state.end += n
        },
        encode(state, s) {
          b4a.write(state.buffer, s, state.start, n, encoding)
          state.start += n
        },
        decode(state) {
          if (state.end - state.start < n) throw new Error('Out of bounds')
          return b4a.toString(
            state.buffer,
            encoding,
            state.start,
            (state.start += n)
          )
        }
      }
    }
  }
}

const utf8 = (exports.string = exports.utf8 = string('utf-8'))
exports.ascii = string('ascii')
exports.hex = string('hex')
exports.base64 = string('base64')
exports.ucs2 = exports.utf16le = string('utf16le')

exports.bool = {
  preencode(state, b) {
    state.end++
  },
  encode(state, b) {
    state.buffer[state.start++] = b ? 1 : 0
  },
  decode(state) {
    if (state.start >= state.end) throw Error('Out of bounds')
    return state.buffer[state.start++] === 1
  }
}

const fixed = (exports.fixed = function fixed(n) {
  return {
    preencode(state, s) {
      if (s.byteLength !== n) throw new Error('Incorrect buffer size')
      state.end += n
    },
    encode(state, s) {
      state.buffer.set(s, state.start)
      state.start += n
    },
    decode(state) {
      if (state.end - state.start < n) throw new Error('Out of bounds')
      return state.buffer.subarray(state.start, (state.start += n))
    }
  }
})

exports.fixed32 = fixed(32)
exports.fixed64 = fixed(64)

exports.array = function array(enc) {
  return {
    preencode(state, list) {
      uint.preencode(state, list.length)
      for (let i = 0; i < list.length; i++) enc.preencode(state, list[i])
    },
    encode(state, list) {
      uint.encode(state, list.length)
      for (let i = 0; i < list.length; i++) enc.encode(state, list[i])
    },
    decode(state) {
      const len = uint.decode(state)
      if (len > 0x100000) throw new Error('Array is too big')
      const arr = new Array(len)
      for (let i = 0; i < len; i++) arr[i] = enc.decode(state)
      return arr
    }
  }
}

exports.frame = function frame(enc) {
  const dummy = exports.state()

  return {
    preencode(state, m) {
      const end = state.end
      enc.preencode(state, m)
      uint.preencode(state, state.end - end)
    },
    encode(state, m) {
      dummy.end = 0
      enc.preencode(dummy, m)
      uint.encode(state, dummy.end)
      enc.encode(state, m)
    },
    decode(state) {
      const end = state.end
      const len = uint.decode(state)
      state.end = state.start + len
      const m = enc.decode(state)
      state.start = state.end
      state.end = end
      return m
    }
  }
}

exports.date = {
  preencode(state, d) {
    int.preencode(state, d.getTime())
  },
  encode(state, d) {
    int.encode(state, d.getTime())
  },
  decode(state, d) {
    return new Date(int.decode(state))
  }
}

exports.json = {
  preencode(state, v) {
    utf8.preencode(state, JSON.stringify(v))
  },
  encode(state, v) {
    utf8.encode(state, JSON.stringify(v))
  },
  decode(state) {
    return JSON.parse(utf8.decode(state))
  }
}

exports.ndjson = {
  preencode(state, v) {
    utf8.preencode(state, JSON.stringify(v) + '\n')
  },
  encode(state, v) {
    utf8.encode(state, JSON.stringify(v) + '\n')
  },
  decode(state) {
    return JSON.parse(utf8.decode(state))
  }
}

// simple helper for when you want to just express nothing
exports.none = {
  preencode(state, n) {
    // do nothing
  },
  encode(state, n) {
    // do nothing
  },
  decode(state) {
    return null
  }
}

// "any" encoders here for helping just structure any object without schematising it

const anyArray = {
  preencode(state, arr) {
    uint.preencode(state, arr.length)
    for (let i = 0; i < arr.length; i++) {
      any.preencode(state, arr[i])
    }
  },
  encode(state, arr) {
    uint.encode(state, arr.length)
    for (let i = 0; i < arr.length; i++) {
      any.encode(state, arr[i])
    }
  },
  decode(state) {
    const arr = []
    let len = uint.decode(state)
    while (len-- > 0) {
      arr.push(any.decode(state))
    }
    return arr
  }
}

const anyObject = {
  preencode(state, o) {
    const keys = Object.keys(o)
    uint.preencode(state, keys.length)
    for (const key of keys) {
      utf8.preencode(state, key)
      any.preencode(state, o[key])
    }
  },
  encode(state, o) {
    const keys = Object.keys(o)
    uint.encode(state, keys.length)
    for (const key of keys) {
      utf8.encode(state, key)
      any.encode(state, o[key])
    }
  },
  decode(state) {
    let len = uint.decode(state)
    const o = {}
    while (len-- > 0) {
      const key = utf8.decode(state)
      o[key] = any.decode(state)
    }
    return o
  }
}

const anyTypes = [
  exports.none,
  exports.bool,
  exports.string,
  exports.buffer,
  exports.uint,
  exports.int,
  exports.float64,
  anyArray,
  anyObject,
  exports.date
]

const any = (exports.any = {
  preencode(state, o) {
    const t = getType(o)
    uint.preencode(state, t)
    anyTypes[t].preencode(state, o)
  },
  encode(state, o) {
    const t = getType(o)
    uint.encode(state, t)
    anyTypes[t].encode(state, o)
  },
  decode(state) {
    const t = uint.decode(state)
    if (t >= anyTypes.length) throw new Error('Unknown type: ' + t)
    return anyTypes[t].decode(state)
  }
})

const port = (exports.port = uint16)

const address = (host, family) => {
  return {
    preencode(state, m) {
      host.preencode(state, m.host)
      port.preencode(state, m.port)
    },
    encode(state, m) {
      host.encode(state, m.host)
      port.encode(state, m.port)
    },
    decode(state) {
      return {
        host: host.decode(state),
        family,
        port: port.decode(state)
      }
    }
  }
}

const ipv4 = (exports.ipv4 = {
  preencode(state) {
    state.end += 4
  },
  encode(state, string) {
    const start = state.start
    const end = start + 4

    let i = 0

    while (i < string.length) {
      let n = 0
      let c

      while (
        i < string.length &&
        (c = string.charCodeAt(i++)) !== /* . */ 0x2e
      ) {
        n = n * 10 + (c - /* 0 */ 0x30)
      }

      state.buffer[state.start++] = n
    }

    state.start = end
  },
  decode(state) {
    if (state.end - state.start < 4) throw new Error('Out of bounds')
    return (
      state.buffer[state.start++] +
      '.' +
      state.buffer[state.start++] +
      '.' +
      state.buffer[state.start++] +
      '.' +
      state.buffer[state.start++]
    )
  }
})

exports.ipv4Address = address(ipv4, 4)

const ipv6 = (exports.ipv6 = {
  preencode(state) {
    state.end += 16
  },
  encode(state, string) {
    const start = state.start
    const end = start + 16

    let i = 0
    let split = null

    while (i < string.length) {
      let n = 0
      let c

      while (
        i < string.length &&
        (c = string.charCodeAt(i++)) !== /* : */ 0x3a
      ) {
        if (c >= 0x30 && c <= 0x39) n = n * 0x10 + (c - /* 0 */ 0x30)
        else if (c >= 0x41 && c <= 0x46) n = n * 0x10 + (c - /* A */ 0x41 + 10)
        else if (c >= 0x61 && c <= 0x66) n = n * 0x10 + (c - /* a */ 0x61 + 10)
      }

      state.buffer[state.start++] = n >>> 8
      state.buffer[state.start++] = n

      if (i < string.length && string.charCodeAt(i) === /* : */ 0x3a) {
        i++
        split = state.start
      }
    }

    if (split !== null) {
      const offset = end - state.start
      state.buffer
        .copyWithin(split + offset, split)
        .fill(0, split, split + offset)
    }

    state.start = end
  },
  decode(state) {
    if (state.end - state.start < 16) throw new Error('Out of bounds')
    return (
      (
        state.buffer[state.start++] * 256 +
        state.buffer[state.start++]
      ).toString(16) +
      ':' +
      (
        state.buffer[state.start++] * 256 +
        state.buffer[state.start++]
      ).toString(16) +
      ':' +
      (
        state.buffer[state.start++] * 256 +
        state.buffer[state.start++]
      ).toString(16) +
      ':' +
      (
        state.buffer[state.start++] * 256 +
        state.buffer[state.start++]
      ).toString(16) +
      ':' +
      (
        state.buffer[state.start++] * 256 +
        state.buffer[state.start++]
      ).toString(16) +
      ':' +
      (
        state.buffer[state.start++] * 256 +
        state.buffer[state.start++]
      ).toString(16) +
      ':' +
      (
        state.buffer[state.start++] * 256 +
        state.buffer[state.start++]
      ).toString(16) +
      ':' +
      (
        state.buffer[state.start++] * 256 +
        state.buffer[state.start++]
      ).toString(16)
    )
  }
})

exports.ipv6Address = address(ipv6, 6)

const ip = (exports.ip = {
  preencode(state, string) {
    const family = string.includes(':') ? 6 : 4
    uint8.preencode(state, family)
    if (family === 4) ipv4.preencode(state)
    else ipv6.preencode(state)
  },
  encode(state, string) {
    const family = string.includes(':') ? 6 : 4
    uint8.encode(state, family)
    if (family === 4) ipv4.encode(state, string)
    else ipv6.encode(state, string)
  },
  decode(state) {
    const family = uint8.decode(state)
    if (family === 4) return ipv4.decode(state)
    else return ipv6.decode(state)
  }
})

exports.ipAddress = {
  preencode(state, m) {
    ip.preencode(state, m.host)
    port.preencode(state, m.port)
  },
  encode(state, m) {
    ip.encode(state, m.host)
    port.encode(state, m.port)
  },
  decode(state) {
    const family = uint8.decode(state)
    return {
      host: family === 4 ? ipv4.decode(state) : ipv6.decode(state),
      family,
      port: port.decode(state)
    }
  }
}

function getType(o) {
  if (o === null || o === undefined) return 0
  if (typeof o === 'boolean') return 1
  if (typeof o === 'string') return 2
  if (b4a.isBuffer(o)) return 3
  if (typeof o === 'number') {
    if (Number.isInteger(o)) return o >= 0 ? 4 : 5
    return 6
  }
  if (Array.isArray(o)) return 7
  if (o instanceof Date) return 9
  if (typeof o === 'object') return 8

  throw new Error('Unsupported type for ' + o)
}

exports.from = function from(enc) {
  if (typeof enc === 'string') return fromNamed(enc)
  if (enc.preencode) return enc
  if (enc.encodingLength) return fromAbstractEncoder(enc)
  return fromCodec(enc)
}

function fromNamed(enc) {
  switch (enc) {
    case 'ascii':
      return raw.ascii
    case 'utf-8':
    case 'utf8':
      return raw.utf8
    case 'hex':
      return raw.hex
    case 'base64':
      return raw.base64
    case 'utf16-le':
    case 'utf16le':
    case 'ucs-2':
    case 'ucs2':
      return raw.ucs2
    case 'ndjson':
      return raw.ndjson
    case 'json':
      return raw.json
    case 'binary':
    default:
      return raw.binary
  }
}

function fromCodec(enc) {
  let tmpM = null
  let tmpBuf = null

  return {
    preencode(state, m) {
      tmpM = m
      tmpBuf = enc.encode(m)
      state.end += tmpBuf.byteLength
    },
    encode(state, m) {
      raw.encode(state, m === tmpM ? tmpBuf : enc.encode(m))
      tmpM = tmpBuf = null
    },
    decode(state) {
      return enc.decode(raw.decode(state))
    }
  }
}

function fromAbstractEncoder(enc) {
  return {
    preencode(state, m) {
      state.end += enc.encodingLength(m)
    },
    encode(state, m) {
      enc.encode(m, state.buffer, state.start)
      state.start += enc.encode.bytes
    },
    decode(state) {
      const m = enc.decode(state.buffer, state.start, state.end)
      state.start += enc.decode.bytes
      return m
    }
  }
}

exports.encode = function encode(enc, m) {
  const state = exports.state()
  enc.preencode(state, m)
  state.buffer = b4a.allocUnsafe(state.end)
  enc.encode(state, m)
  return state.buffer
}

exports.decode = function decode(enc, buffer) {
  return enc.decode(exports.state(0, buffer.byteLength, buffer))
}

function zigZagInt(enc) {
  return {
    preencode(state, n) {
      enc.preencode(state, zigZagEncodeInt(n))
    },
    encode(state, n) {
      enc.encode(state, zigZagEncodeInt(n))
    },
    decode(state) {
      return zigZagDecodeInt(enc.decode(state))
    }
  }
}

function zigZagDecodeInt(n) {
  return n === 0 ? n : (n & 1) === 0 ? n / 2 : -(n + 1) / 2
}

function zigZagEncodeInt(n) {
  // 0, -1, 1, -2, 2, ...
  return n < 0 ? 2 * -n - 1 : n === 0 ? 0 : 2 * n
}

function zigZagBigInt(enc) {
  return {
    preencode(state, n) {
      enc.preencode(state, zigZagEncodeBigInt(n))
    },
    encode(state, n) {
      enc.encode(state, zigZagEncodeBigInt(n))
    },
    decode(state) {
      return zigZagDecodeBigInt(enc.decode(state))
    }
  }
}

function zigZagDecodeBigInt(n) {
  return n === 0n ? n : (n & 1n) === 0n ? n / 2n : -(n + 1n) / 2n
}

function zigZagEncodeBigInt(n) {
  // 0, -1, 1, -2, 2, ...
  return n < 0n ? 2n * -n - 1n : n === 0n ? 0n : 2n * n
}

function validateUint(n) {
  if (n >= 0 === false /* Handles NaN as well */)
    throw new Error('uint must be positive')
}
module.exports = {
  preencode,
  encode,
  decode
}

function preencode(state, num) {
  if (num < 251) {
    state.end++
  } else if (num < 256) {
    state.end += 2
  } else if (num < 0x10000) {
    state.end += 3
  } else if (num < 0x1000000) {
    state.end += 4
  } else if (num < 0x100000000) {
    state.end += 5
  } else {
    state.end++
    const exp = Math.floor(Math.log(num) / Math.log(2)) - 32
    preencode(state, exp)
    state.end += 6
  }
}

function encode(state, num) {
  const max = 251
  const x = num - max

  if (num < max) {
    state.buffer[state.start++] = num
  } else if (num < 256) {
    state.buffer[state.start++] = max
    state.buffer[state.start++] = x
  } else if (num < 0x10000) {
    state.buffer[state.start++] = max + 1
    state.buffer[state.start++] = (x >> 8) & 0xff
    state.buffer[state.start++] = x & 0xff
  } else if (num < 0x1000000) {
    state.buffer[state.start++] = max + 2
    state.buffer[state.start++] = x >> 16
    state.buffer[state.start++] = (x >> 8) & 0xff
    state.buffer[state.start++] = x & 0xff
  } else if (num < 0x100000000) {
    state.buffer[state.start++] = max + 3
    state.buffer[state.start++] = x >> 24
    state.buffer[state.start++] = (x >> 16) & 0xff
    state.buffer[state.start++] = (x >> 8) & 0xff
    state.buffer[state.start++] = x & 0xff
  } else {
    // need to use Math here as bitwise ops are 32 bit
    const exp = Math.floor(Math.log(x) / Math.log(2)) - 32
    state.buffer[state.start++] = 0xff

    encode(state, exp)
    const rem = x / Math.pow(2, exp - 11)

    for (let i = 5; i >= 0; i--) {
      state.buffer[state.start++] = (rem / Math.pow(2, 8 * i)) & 0xff
    }
  }
}

function decode(state) {
  const max = 251

  if (state.end - state.start < 1) throw new Error('Out of bounds')

  const flag = state.buffer[state.start++]

  if (flag < max) return flag

  if (state.end - state.start < flag - max + 1) {
    throw new Error('Out of bounds.')
  }

  if (flag < 252) {
    return state.buffer[state.start++] + max
  }

  if (flag < 253) {
    return (
      (state.buffer[state.start++] << 8) + state.buffer[state.start++] + max
    )
  }

  if (flag < 254) {
    return (
      (state.buffer[state.start++] << 16) +
      (state.buffer[state.start++] << 8) +
      state.buffer[state.start++] +
      max
    )
  }

  // << 24 result may be interpreted as negative
  if (flag < 255) {
    return (
      state.buffer[state.start++] * 0x1000000 +
      (state.buffer[state.start++] << 16) +
      (state.buffer[state.start++] << 8) +
      state.buffer[state.start++] +
      max
    )
  }

  const exp = decode(state)

  if (state.end - state.start < 6) throw new Error('Out of bounds')

  let rem = 0
  for (let i = 5; i >= 0; i--) {
    rem += state.buffer[state.start++] * Math.pow(2, 8 * i)
  }

  return rem * Math.pow(2, exp - 11) + max
}
{
  "name": "compact-encoding",
  "version": "2.18.0",
  "description": "A series of compact encoding schemes for building small and fast parsers and serializers",
  "main": "index.js",
  "dependencies": {
    "b4a": "^1.3.0"
  },
  "devDependencies": {
    "brittle": "^3.0.0",
    "prettier": "^3.6.2",
    "prettier-config-holepunch": "^1.0.0"
  },
  "scripts": {
    "format": "prettier . --write",
    "test": "prettier . --check && brittle test.js"
  },
  "repository": {
    "type": "git",
    "url": "https://github.com/compact-encoding/compact-encoding.git"
  },
  "author": "Mathias Buus (@mafintosh)",
  "license": "Apache-2.0",
  "bugs": {
    "url": "https://github.com/compact-encoding/compact-encoding/issues"
  },
  "homepage": "https://github.com/compact-encoding/compact-encoding"
}
const b4a = require('b4a')

const { BE } = require('./endian')

exports = module.exports = {
  preencode(state, b) {
    state.end += b.byteLength
  },
  encode(state, b) {
    state.buffer.set(b, state.start)
    state.start += b.byteLength
  },
  decode(state) {
    const b = state.buffer.subarray(state.start, state.end)
    state.start = state.end
    return b
  }
}

const buffer = (exports.buffer = {
  preencode(state, b) {
    if (b) uint8array.preencode(state, b)
    else state.end++
  },
  encode(state, b) {
    if (b) uint8array.encode(state, b)
    else state.buffer[state.start++] = 0
  },
  decode(state) {
    const b = state.buffer.subarray(state.start)
    if (b.byteLength === 0) return null
    state.start = state.end
    return b
  }
})

exports.binary = {
  ...buffer,
  preencode(state, b) {
    if (typeof b === 'string') utf8.preencode(state, b)
    else buffer.preencode(state, b)
  },
  encode(state, b) {
    if (typeof b === 'string') utf8.encode(state, b)
    else buffer.encode(state, b)
  }
}

exports.arraybuffer = {
  preencode(state, b) {
    state.end += b.byteLength
  },
  encode(state, b) {
    const view = new Uint8Array(b)

    state.buffer.set(view, state.start)
    state.start += b.byteLength
  },
  decode(state) {
    const b = new ArrayBuffer(state.end - state.start)
    const view = new Uint8Array(b)

    view.set(state.buffer.subarray(state.start))

    state.start = state.end

    return b
  }
}

function typedarray(TypedArray, swap) {
  const n = TypedArray.BYTES_PER_ELEMENT

  return {
    preencode(state, b) {
      state.end += b.byteLength
    },
    encode(state, b) {
      const view = new Uint8Array(b.buffer, b.byteOffset, b.byteLength)

      if (BE && swap) swap(view)

      state.buffer.set(view, state.start)
      state.start += b.byteLength
    },
    decode(state) {
      let b = state.buffer.subarray(state.start)
      if (b.byteOffset % n !== 0) b = new Uint8Array(b)

      if (BE && swap) swap(b)

      state.start = state.end

      return new TypedArray(b.buffer, b.byteOffset, b.byteLength / n)
    }
  }
}

const uint8array = (exports.uint8array = typedarray(Uint8Array))
exports.uint16array = typedarray(Uint16Array, b4a.swap16)
exports.uint32array = typedarray(Uint32Array, b4a.swap32)

exports.int8array = typedarray(Int8Array)
exports.int16array = typedarray(Int16Array, b4a.swap16)
exports.int32array = typedarray(Int32Array, b4a.swap32)

exports.biguint64array = typedarray(BigUint64Array, b4a.swap64)
exports.bigint64array = typedarray(BigInt64Array, b4a.swap64)

exports.float32array = typedarray(Float32Array, b4a.swap32)
exports.float64array = typedarray(Float64Array, b4a.swap64)

function string(encoding) {
  return {
    preencode(state, s) {
      state.end += b4a.byteLength(s, encoding)
    },
    encode(state, s) {
      state.start += b4a.write(state.buffer, s, state.start, encoding)
    },
    decode(state) {
      const s = b4a.toString(state.buffer, encoding, state.start)
      state.start = state.end
      return s
    }
  }
}

const utf8 = (exports.string = exports.utf8 = string('utf-8'))
exports.ascii = string('ascii')
exports.hex = string('hex')
exports.base64 = string('base64')
exports.ucs2 = exports.utf16le = string('utf16le')

exports.array = function array(enc) {
  return {
    preencode(state, list) {
      for (const value of list) enc.preencode(state, value)
    },
    encode(state, list) {
      for (const value of list) enc.encode(state, value)
    },
    decode(state) {
      const arr = []
      while (state.start < state.end) arr.push(enc.decode(state))
      return arr
    }
  }
}

exports.json = {
  preencode(state, v) {
    utf8.preencode(state, JSON.stringify(v))
  },
  encode(state, v) {
    utf8.encode(state, JSON.stringify(v))
  },
  decode(state) {
    return JSON.parse(utf8.decode(state))
  }
}

exports.ndjson = {
  preencode(state, v) {
    utf8.preencode(state, JSON.stringify(v) + '\n')
  },
  encode(state, v) {
    utf8.encode(state, JSON.stringify(v) + '\n')
  },
  decode(state) {
    return JSON.parse(utf8.decode(state))
  }
}
const safetyCatch = require('safety-catch')

module.exports = class CoreCoupler {
  constructor (target, wakeup) {
    this.target = target
    this.wakeup = wakeup
    this.coupled = new Set()

    this._onpeeraddBound = this._onpeeradd.bind(this)
    this.target.on('peer-add', this._onpeeraddBound)
  }

  add (core) {
    const added = this.coupled.size
    this.coupled.add(core)
    if (added !== this.coupled.size) this._couple(core)
  }

  remove (core) {
    this.coupled.delete(core)
  }

  destroy () {
    this.target.off('peer-add', this._onpeeraddBound)
  }

  async update (stream) {
    const muxer = stream.userData
    if (!muxer) return

    try {
      if (!(await this._hasMuxer(this.target, muxer))) return

      let wakeup = null

      for (const core of this.coupled) {
        if (await this._hasMuxer(core, muxer)) continue
        if (wakeup === null) wakeup = []
        wakeup.push(core)
      }

      if (wakeup !== null) {
        this.wakeup(stream, wakeup)
      }
    } catch (err) {
      safetyCatch(err)
    }
  }

  async _couple (core) {
    try {
      let wakeup = null

      for (const peer of this.target.peers) {
        if (await this._hasPeer(core, peer)) continue
        if (wakeup === null) wakeup = []
        wakeup.push(peer)
      }

      if (wakeup !== null && this.coupled.has(core)) {
        for (const peer of wakeup) this.wakeup(peer.stream, [core])
      }
    } catch (err) {
      safetyCatch(err)
    }
  }

  async _onpeeradd (peer) {
    try {
      let wakeup = null

      for (const core of this.coupled) {
        if (await this._hasPeer(core, peer)) continue
        if (wakeup === null) wakeup = []
        wakeup.push(core)
      }

      if (wakeup !== null) {
        this.wakeup(peer.stream, wakeup)
      }
    } catch (err) {
      safetyCatch(err)
    }
  }

  _hasMuxer (core, muxer) {
    const ch = muxer.getLastChannel({ protocol: 'hypercore', id: core.discoveryKey })
    if (ch) return ch.fullyOpened()

    const cha = muxer.getLastChannel({ protocol: 'hypercore/alpha', id: core.discoveryKey })
    if (cha) return cha.fullyOpened()

    return Promise.resolve(false)
  }

  _hasPeer (core, peer) {
    return this._hasMuxer(core, peer.protomux)
  }
}
{
  "name": "core-coupler",
  "version": "2.0.0",
  "description": "Couple the peers of cores",
  "main": "index.js",
  "scripts": {
    "test": "standard"
  },
  "dependencies": {
    "safety-catch": "^1.0.2"
  },
  "devDependencies": {
    "corestore": "^6.18.2",
    "standard": "^17.1.0"
  },
  "repository": {
    "type": "git",
    "url": "https://github.com/holepunchto/core-coupler.git"
  },
  "author": "Holepunch Inc",
  "license": "Apache-2.0",
  "bugs": {
    "url": "https://github.com/holepunchto/core-coupler/issues"
  },
  "homepage": "https://github.com/holepunchto/core-coupler"
}
const b4a = require('b4a')
const Hypercore = require('hypercore')
const ReadyResource = require('ready-resource')
const sodium = require('sodium-universal')
const crypto = require('hypercore-crypto')
const ID = require('hypercore-id-encoding')
const { isAndroid } = require('which-runtime')
const { STORAGE_EMPTY, ASSERTION } = require('hypercore-errors')

const auditStore = require('./lib/audit.js')

const [NS] = crypto.namespace('corestore', 1)
const DEFAULT_NAMESPACE = b4a.alloc(32) // This is meant to be 32 0-bytes

class StreamTracker {
  constructor() {
    this.records = []
  }

  add(stream, isExternal) {
    const record = { index: 0, stream, isExternal }
    record.index = this.records.push(record) - 1
    return record
  }

  remove(record) {
    const popped = this.records.pop()
    if (popped === record) return
    this.records[(popped.index = record.index)] = popped
  }

  attachAll(core) {
    for (let i = 0; i < this.records.length; i++) {
      const record = this.records[i]
      const muxer = record.stream.noiseStream.userData
      if (!core.replicator.attached(muxer)) core.replicator.attachTo(muxer)
    }
  }

  destroy() {
    // reverse is safer cause we delete mb
    for (let i = this.records.length - 1; i >= 0; i--) {
      const record = this.records[i]
      if (!record.isExternal) record.stream.destroy()
    }
  }
}

class SessionTracker {
  constructor() {
    this.map = new Map()
  }

  get size() {
    return this.map.size
  }

  get(id) {
    const existing = this.map.get(id)
    if (existing !== undefined) return existing
    const fresh = []
    this.map.set(id, fresh)
    return fresh
  }

  gc(id) {
    this.map.delete(id)
  }

  list(id) {
    return id ? this.map.get(id) || [] : [...this]
  }

  *[Symbol.iterator]() {
    for (const sessions of this.map.values()) {
      yield* sessions[Symbol.iterator]()
    }
  }
}

class CoreTracker {
  constructor() {
    this.map = new Map()
    this.watching = []

    this._gcing = new Set()
    this._gcInterval = null
    this._gcCycleBound = this._gcCycle.bind(this)
  }

  get size() {
    return this.map.size
  }

  watch(store) {
    if (store.watchIndex !== -1) return
    store.watchIndex = this.watching.push(store) - 1
  }

  unwatch(store) {
    if (store.watchIndex === -1) return
    const head = this.watching.pop()
    if (head !== store) this.watching[(head.watchIndex = store.watchIndex)] = head
    store.watchIndex = -1
  }

  resume(id) {
    const core = this.map.get(id)

    if (!core) return null

    // signal back that we have a closing one stored
    if (core.closing) return core

    if (core.gc) {
      this._gcing.delete(core)
      if (this._gcing.size === 0) this._stopGC()
      core.gc = 0
    }

    return core
  }

  opened(id) {
    const core = this.map.get(id)
    return !!(core && core.opened && !core.closing)
  }

  get(id) {
    // we allow you do call this from the outside, so support normal buffers also
    if (b4a.isBuffer(id)) id = b4a.toString(id, 'hex')
    const core = this.map.get(id)
    if (!core || core.closing) return null
    return core
  }

  set(id, core) {
    this.map.set(id, core)
    if (this.watching.length > 0) this._emit(core)
  }

  _emit(core) {
    for (let i = this.watching.length - 1; i >= 0; i--) {
      const store = this.watching[i]
      for (const fn of store.watchers) fn(core)
    }
  }

  _gc(core) {
    const id = toHex(core.discoveryKey)
    if (this.map.get(id) === core) this.map.delete(id)
  }

  _gcCycle() {
    for (const core of this._gcing) {
      if (++core.gc < 4) continue
      const gc = this._gc.bind(this, core)
      core.close().then(gc, gc)
      this._gcing.delete(core)
    }

    if (this._gcing.size === 0) this._stopGC()
  }

  gc(core) {
    core.gc = 1 // first strike
    this._gcing.add(core)
    if (this._gcing.size === 1) this._startGC()
  }

  _stopGC() {
    clearInterval(this._gcInterval)
    this._gcInterval = null
  }

  _startGC() {
    if (this._gcInterval) return
    this._gcInterval = setInterval(this._gcCycleBound, 2000)
    if (this._gcInterval.unref) this._gcInterval.unref()
  }

  close() {
    this._stopGC()
    this._gcing.clear()

    const all = []
    for (const core of this.map.values()) {
      core.onidle = noop // no reentry
      all.push(core.close())
    }
    this.map.clear()

    return Promise.all(all)
  }

  *[Symbol.iterator]() {
    for (const core of this.map.values()) {
      if (!core.closing) yield core
    }
  }
}

class FindingPeers {
  constructor() {
    this.count = 0
    this.pending = []
  }

  add(core) {
    if (this.count === 0) return
    this.pending.push(core.findingPeers())
  }

  inc(sessions) {
    if (++this.count !== 1) return

    for (const core of sessions) {
      this.pending.push(core.findingPeers())
    }
  }

  dec(sessions) {
    if (--this.count !== 0) return
    while (this.pending.length > 0) this.pending.pop()()
  }
}

class Corestore extends ReadyResource {
  constructor(storage, opts = {}) {
    super()

    this.root = opts.root || null
    this.storage = this.root
      ? this.root.storage
      : Hypercore.defaultStorage(storage, {
          id: opts.id,
          allowBackup: opts.allowBackup,
          readOnly: opts.readOnly,
          wait: opts.wait
        })
    this.streamTracker = this.root ? this.root.streamTracker : new StreamTracker()
    this.cores = this.root ? this.root.cores : new CoreTracker()
    this.sessions = new SessionTracker()
    this.corestores = this.root ? this.root.corestores : new Set()
    this.readOnly = opts.writable === false || !!opts.readOnly
    this.globalCache = this.root ? this.root.globalCache : opts.globalCache || null
    this.primaryKey = this.root ? this.root.primaryKey : opts.primaryKey || null
    this.ns = opts.namespace || DEFAULT_NAMESPACE
    this.manifestVersion = opts.manifestVersion || 1
    this.shouldSuspend = isAndroid ? !!opts.suspend : opts.suspend !== false
    this.active = opts.active !== false

    this.watchers = null
    this.watchIndex = -1

    this._findingPeers = null // here for legacy
    this._ongcBound = this._ongc.bind(this)

    if (opts.primaryKey && !this.root && !opts.unsafe) {
      throw ASSERTION(
        'Passing the primary key is unsafe unless you know what you are doing. Set unsafe: true to acknowledge that'
      )
    }

    if (this.root) this.corestores.add(this)

    this.ready().catch(noop)
  }

  watch(fn) {
    if (this.watchers === null) {
      this.watchers = new Set()
      this.cores.watch(this)
    }

    this.watchers.add(fn)
  }

  unwatch(fn) {
    if (this.watchers === null) return

    this.watchers.delete(fn)

    if (this.watchers.size === 0) {
      this.watchers = null
      this.cores.unwatch(this)
    }
  }

  findingPeers() {
    if (this._findingPeers === null) this._findingPeers = new FindingPeers()
    this._findingPeers.inc(this.sessions)
    let done = false
    return () => {
      if (done) return
      done = true
      this._findingPeers.dec(this.sessions)
    }
  }

  audit(opts = {}) {
    return auditStore(this, opts)
  }

  async suspend({ log = noop } = {}) {
    await log('Flushing db...')
    // If readOnly we don't need to flush
    if (!this.storage.readOnly) await this.storage.db.flush()
    if (!this.shouldSuspend) return
    await log('Suspending db...')
    await this.storage.suspend()
  }

  resume() {
    return this.storage.resume()
  }

  session(opts) {
    this._maybeClosed()
    const root = this.root || this
    return new Corestore(null, {
      manifestVersion: this.manifestVersion,
      ...opts,
      root
    })
  }

  namespace(name, opts) {
    return this.session({
      ...opts,
      namespace: generateNamespace(this.ns, name)
    })
  }

  list(namespace) {
    return this.storage.createDiscoveryKeyStream(namespace)
  }

  getAuth(discoveryKey) {
    return this.storage.getAuth(discoveryKey)
  }

  _ongc(session) {
    if (session.sessions.length === 0) this.sessions.gc(session.id)
  }

  async _getOrSetSeed() {
    const seed = await this.storage.getSeed()
    if (seed !== null) return seed
    return await this.storage.setSeed(this.primaryKey || crypto.randomBytes(32))
  }

  async _open() {
    if (this.root !== null) {
      if (this.root.opened === false) await this.root.ready()
      this.primaryKey = this.root.primaryKey
      return
    }

    const primaryKey = await this._getOrSetSeed()

    if (this.primaryKey === null) {
      this.primaryKey = primaryKey
      return
    }

    if (!b4a.equals(primaryKey, this.primaryKey)) {
      throw new Error('Another corestore is stored here')
    }
  }

  async _close() {
    const closing = []
    const hanging = [...this.sessions]
    for (const sess of hanging) closing.push(sess.close())

    if (this.watchers !== null) this.cores.unwatch(this)

    if (this.root !== null) {
      await Promise.all(closing)
      return
    }

    for (const store of this.corestores) {
      closing.push(store.close())
    }

    await Promise.all(closing)

    await this.cores.close()
    await this.storage.close()
  }

  async _attachMaybe(muxer, discoveryKey) {
    if (this.opened === false) await this.ready()
    if (
      !this.cores.opened(toHex(discoveryKey)) &&
      !(await this.storage.hasCore(discoveryKey, { ifMigrated: true }))
    )
      return
    if (this.closing) return

    const core = this._openCore(discoveryKey, { createIfMissing: false })

    if (!core) return
    if (!core.opened) await core.ready()

    if (!core.replicator.attached(muxer)) {
      core.replicator.attachTo(muxer)
    }

    core.checkIfIdle()
  }

  _shouldReplicate(core, muxer) {
    return (
      core.replicator.downloading && !core.replicator.attached(muxer) && core.opened && this.active
    )
  }

  replicate(isInitiator, opts) {
    this._maybeClosed()

    const isExternal = isStream(isInitiator)
    const stream = Hypercore.createProtocolStream(isInitiator, {
      ...opts,
      ondiscoverykey: (discoveryKey) => {
        if (this.closing) return
        const muxer = stream.noiseStream.userData
        return this._attachMaybe(muxer, discoveryKey)
      }
    })

    if (this.cores.size > 0) {
      const muxer = stream.noiseStream.userData
      const uncork = muxer.uncork.bind(muxer)
      muxer.cork()

      for (const core of this.cores) {
        if (!this._shouldReplicate(core, muxer)) {
          continue
        }
        core.replicator.attachTo(muxer)
      }

      stream.noiseStream.opened.then(uncork)
    }

    const record = this.streamTracker.add(stream, isExternal)
    stream.once('close', () => this.streamTracker.remove(record))
    return stream
  }

  _maybeClosed() {
    if (this.closing || (this.root !== null && this.root.closing)) {
      throw new Error('Corestore is closed')
    }
  }

  async staticify(core, opts) {
    if (!this.opened) await this.ready()
    if (!core.opened) await core.ready()

    const rx = core.state.storage.read()

    const headPromise = rx.getHead()
    const authPromise = rx.getAuth()

    rx.tryFlush()

    const [head, auth] = await Promise.all([headPromise, authPromise])
    if (!head || head.length === 0) throw new Error('Core must have data')

    const prologue = {
      length: head.length,
      hash: head.rootHash
    }

    const manifest = {
      version: 1,
      hash: auth.manifest.hash,
      quorum: 0,
      signers: [],
      prologue
    }

    const c = {
      key: null,
      discoveryKey: null,
      manifest,
      core: core.state.storage.core
    }

    c.key = Hypercore.key(c.manifest)
    c.discoveryKey = Hypercore.discoveryKey(c.key)

    await this.storage.createCore(c)

    const staticCore = this.get({ ...opts, key: c.key })
    await staticCore.ready()
    return staticCore
  }

  get(opts) {
    this._maybeClosed()

    if (b4a.isBuffer(opts) || typeof opts === 'string') opts = { key: opts }
    if (!opts) opts = {}

    const conf = {
      preload: null,
      sessions: null,
      ongc: null,
      core: null,
      active: opts.active !== false,
      encryption: opts.encryption || null,
      encryptionKey: opts.encryptionKey || null, // back compat, should remove
      isBlockKey: !!opts.isBlockKey, // back compat, should remove
      valueEncoding: opts.valueEncoding || null,
      exclusive: !!opts.exclusive,
      manifest: opts.manifest || null,
      keyPair: opts.keyPair || null,
      onwait: opts.onwait || null,
      wait: opts.wait !== false,
      timeout: opts.timeout || 0,
      draft: !!opts.draft,
      writable: opts.writable === undefined && this.readOnly ? false : opts.writable
    }

    // name requires us to rt to storage + ready, so needs preload
    // same goes if user has defined async preload obvs
    if (opts.name || opts.preload) {
      conf.preload = this._preload(opts)
      return this._makeSession(conf)
    }

    if (opts.discoveryKey && !opts.key && !opts.manifest) {
      conf.preload = this._preloadCheckIfExists(opts)
      return this._makeSession(conf)
    }

    // if not not we can sync create it, which just is easier for the
    // upstream user in terms of guarantees (key is there etc etc)
    const core = this._openCore(null, opts)

    conf.core = core
    conf.sessions = this.sessions.get(core.id)
    conf.ongc = this._ongcBound

    return this._makeSession(conf)
  }

  _makeSession(conf) {
    const session = new Hypercore(null, null, conf)
    if (this._findingPeers !== null) this._findingPeers.add(session)
    return session
  }

  async createKeyPair(name, ns = this.ns) {
    if (this.opened === false) await this.ready()
    return createKeyPair(this.primaryKey, ns, name)
  }

  async _preloadCheckIfExists(opts) {
    const has = await this.storage.hasCore(opts.discoveryKey)
    if (!has) throw STORAGE_EMPTY('No Hypercore is stored here')
    return this._preload(opts)
  }

  async _preload(opts) {
    if (opts.preload) opts = { ...opts, ...(await opts.preload) }
    if (this.opened === false) await this.ready()

    const discoveryKey = opts.name
      ? await this.storage.getAlias({ name: opts.name, namespace: this.ns })
      : null
    this._maybeClosed()

    const core = this._openCore(discoveryKey, opts)

    return {
      core,
      sessions: this.sessions.get(core.id),
      ongc: this._ongcBound,
      encryption: opts.encryption || null,
      encryptionKey: opts.encryptionKey || null, // back compat, should remove
      isBlockKey: !!opts.isBlockKey // back compat, should remove
    }
  }

  _auth(discoveryKey, opts) {
    const result = {
      keyPair: null,
      key: null,
      discoveryKey,
      manifest: null
    }

    if (opts.name) {
      result.keyPair = createKeyPair(this.primaryKey, this.ns, opts.name)
    } else if (opts.keyPair) {
      result.keyPair = opts.keyPair
    }

    if (opts.manifest) {
      result.manifest = opts.manifest
    } else if (result.keyPair && !result.discoveryKey) {
      result.manifest = {
        version: this.manifestVersion,
        signers: [{ publicKey: result.keyPair.publicKey }]
      }
    }

    if (opts.key) result.key = ID.decode(opts.key)
    else if (result.manifest) result.key = Hypercore.key(result.manifest)

    if (result.discoveryKey) return result

    if (opts.discoveryKey) result.discoveryKey = ID.decode(opts.discoveryKey)
    else if (result.key) result.discoveryKey = crypto.discoveryKey(result.key)
    else throw new Error('Could not derive discovery from input')

    return result
  }

  _openCore(discoveryKey, opts) {
    const auth = this._auth(discoveryKey, opts)

    const id = toHex(auth.discoveryKey)
    const existing = this.cores.resume(id)
    if (existing && !existing.closing) return existing

    const core = Hypercore.createCore(this.storage, {
      preopen: existing && existing.opened ? existing.closing : null, // always wait for the prev one to close first in any case...
      eagerUpgrade: true,
      notDownloadingLinger: opts.notDownloadingLinger,
      allowFork: opts.allowFork !== false,
      inflightRange: opts.inflightRange,
      compat: false, // no compat for now :)
      force: opts.force,
      createIfMissing: opts.createIfMissing,
      discoveryKey: auth.discoveryKey,
      overwrite: opts.overwrite,
      key: auth.key,
      keyPair: auth.keyPair,
      legacy: opts.legacy,
      manifest: auth.manifest,
      globalCache: opts.globalCache || this.globalCache || null,
      alias: opts.name ? { name: opts.name, namespace: this.ns } : null
    })

    core.onidle = () => {
      this.cores.gc(core)
    }

    core.replicator.ondownloading = () => {
      if (this.active) this.streamTracker.attachAll(core)
    }

    this.cores.set(id, core)
    return core
  }
}

module.exports = Corestore

function isStream(s) {
  return typeof s === 'object' && s && typeof s.pipe === 'function'
}

function generateNamespace(namespace, name) {
  if (!b4a.isBuffer(name)) name = b4a.from(name)
  const out = b4a.allocUnsafeSlow(32)
  sodium.crypto_generichash_batch(out, [namespace, name])
  return out
}

function deriveSeed(primaryKey, namespace, name) {
  if (!b4a.isBuffer(name)) name = b4a.from(name)
  const out = b4a.alloc(32)
  sodium.crypto_generichash_batch(out, [NS, namespace, name], primaryKey)
  return out
}

function createKeyPair(primaryKey, namespace, name) {
  const seed = deriveSeed(primaryKey, namespace, name)
  const buf = b4a.alloc(sodium.crypto_sign_PUBLICKEYBYTES + sodium.crypto_sign_SECRETKEYBYTES)
  const keyPair = {
    publicKey: buf.subarray(0, sodium.crypto_sign_PUBLICKEYBYTES),
    secretKey: buf.subarray(sodium.crypto_sign_PUBLICKEYBYTES)
  }
  sodium.crypto_sign_seed_keypair(keyPair.publicKey, keyPair.secretKey, seed)
  return keyPair
}

function noop() {}

function toHex(discoveryKey) {
  return b4a.toString(discoveryKey, 'hex')
}
module.exports = async function* audit(store, { dryRun = false } = {}) {
  for await (const { discoveryKey, core } of store.storage.createCoreStream()) {
    if (core.version < 1) continue // not migrated, ignore

    const c = store.get({ discoveryKey, active: false })
    await c.ready()

    yield { discoveryKey, key: c.key, audit: await c.core.audit({ dryRun }) }

    try {
      await c.close()
    } catch {
      // ignore if failed, we are auditing...
    }
  }
}
{
  "name": "corestore",
  "version": "7.7.0",
  "description": "A Hypercore factory that simplifies managing collections of cores.",
  "main": "index.js",
  "files": [
    "index.js",
    "lib/*"
  ],
  "dependencies": {
    "b4a": "^1.6.7",
    "hypercore": "^11.19.0",
    "hypercore-crypto": "^3.4.2",
    "hypercore-errors": "^1.4.0",
    "hypercore-id-encoding": "^1.3.0",
    "ready-resource": "^1.1.1",
    "sodium-universal": "^5.0.1",
    "which-runtime": "^1.2.1"
  },
  "devDependencies": {
    "brittle": "^3.7.0",
    "prettier": "^3.6.2",
    "prettier-config-holepunch": "^2.0.0",
    "rache": "^1.0.0",
    "test-tmp": "^1.3.0"
  },
  "scripts": {
    "format": "prettier --write .",
    "test": "prettier --check . && brittle test/*.js",
    "test:bare": "bare test/basic.js"
  },
  "repository": {
    "type": "git",
    "url": "https://github.com/holepunchto/corestore.git"
  },
  "author": "Holepunch Inc",
  "license": "MIT",
  "bugs": {
    "url": "https://github.com/holepunchto/corestore/issues"
  },
  "homepage": "https://github.com/holepunchto/corestore"
}
module.exports = function debounce (worker, context = null) {
  debounced.running = null
  return debounced

  async function debounced () {
    if (debounced.running !== null) {
      try {
        await debounced.running
      } catch (_) {
        // ignore - do not fail on old errors
      }
    }

    // another "thread" beat us to it, just piggy pack on that one
    if (debounced.running !== null) return debounced.running

    debounced.running = worker.call(context)

    try {
      return await debounced.running
    } finally {
      debounced.running = null
    }
  }
}
{
  "name": "debounceify",
  "version": "1.1.0",
  "description": "Tiny async debouncer",
  "main": "index.js",
  "dependencies": {},
  "devDependencies": {
    "standard": "^16.0.3",
    "tape": "^5.1.1"
  },
  "repository": {
    "type": "git",
    "url": "https://github.com/mafintosh/debounceify.git"
  },
  "scripts": {
    "test": "standard && tape test.js"
  },
  "author": "Mathias Buus (@mafintosh)",
  "license": "MIT",
  "bugs": {
    "url": "https://github.com/mafintosh/debounceify/issues"
  },
  "homepage": "https://github.com/mafintosh/debounceify"
}
const fs = require('fs')
const path = require('path')
const fsx = require('fs-native-extensions')
const b4a = require('b4a')
const ReadyResource = require('ready-resource')
const FDLock = require('fd-lock')

const PLATFORM = global.Bare ? global.Bare.platform : global.process.platform
const IS_WIN = PLATFORM === 'win32'
const IS_LINUX = PLATFORM === 'linux'
const MODIFIED_SLACK = 5000
const EMPTY = b4a.alloc(0)
const ATTR = IS_LINUX ? 'user.device-file' : 'device-file'

const nl = IS_WIN ? '\r\n' : '\n'

module.exports = class DeviceFile extends ReadyResource {
  constructor(filename, { create = true, wait = false, lock = wait, data = {} } = {}) {
    super()

    this.filename = filename
    this.data = data
    this.lock = null

    this._create = create
    this._wait = wait
    this._lock = lock
  }

  async _open() {
    if (await verifyDeviceFile(this)) return
    if (!this._create) throw new Error('No device file present')
    await writeDeviceFile(this)
  }

  transfer() {
    return this.lock ? this.lock.transfer() : -1
  }

  async _close() {
    if (this.lock) await this.lock.close()
  }

  async suspend() {
    if (!this.opened) await this.ready()
    if (this.lock) await this.lock.suspend()
  }

  async resume() {
    if (!this.opened) await this.ready()
    if (this.lock) await this.lock.resume()
  }
}

async function writeDeviceFile(device) {
  let s = ''

  for (const [key, value] of Object.entries(device.data)) {
    if (value === null) continue
    s += key + '=' + value + nl
  }

  await fs.promises.mkdir(path.dirname(device.filename), { recursive: true })

  const fd = await open(device.filename, 'w')

  device.lock = device._lock ? new FDLock(fd, { wait: device._wait }) : null

  if (device.lock) {
    try {
      await device.lock.ready()
    } catch (err) {
      await device.lock.close()
      throw err
    }
  }

  const st = await fstat(fd)

  const created = Date.now()

  s += 'device/platform=' + PLATFORM + nl
  s += 'device/inode=' + st.ino + nl
  s += 'device/created=' + created + nl

  if (await setAttr(fd, ATTR, b4a.from('original'))) {
    s += 'device/attribute=original' + nl
  }

  await write(fd, b4a.from(s))

  if (!device.lock) await close(fd)
}

async function verifyDeviceFile(device) {
  let fd = 0

  try {
    fd = await open(device.filename, 'r+')
  } catch (e) {
    fd = 0
  }

  if (fd === 0) return false

  device.lock = device._lock ? new FDLock(fd, { wait: device._wait }) : null

  if (device.lock) {
    try {
      await device.lock.ready()
    } catch (err) {
      await device.lock.close()
      throw err
    }
  }

  const buf = await read(fd)
  const result = {}

  const s = b4a.toString(buf).trim().split('\n')

  let inode = 0
  let created = 0
  let attr = ''
  let platform = ''

  for (const ln of s) {
    const i = ln.indexOf('=')
    if (i === -1) continue

    const k = ln.slice(0, i).trim()
    const v = ln.slice(i + 1).trim()

    switch (k) {
      case 'device/platform':
        platform = v
        break
      case 'device/inode':
        inode = Number(v)
        break
      case 'device/created':
        created = Number(v)
        break
      case 'device/attribute':
        attr = v
        break
      default:
        result[k] = v
        break
    }
  }

  for (const [k, v] of Object.entries(device.data)) {
    if (v === null) continue
    if (result[k] === undefined) continue // allow upserts
    if (result[k] !== '' + v) {
      await teardown()
      throw new Error('Invalid device file, ' + k + ' has changed. Was ' + result[k] + ', is ' + v)
    }
  }

  const st = await fstat(fd)
  const at = await getAttr(fd, ATTR)

  const sameAttr = b4a.toString(at || EMPTY) === attr
  const modified = Math.max(st.mtime.getTime(), st.birthtime.getTime())

  if (platform && platform !== PLATFORM) {
    await teardown()
    throw new Error('Invalid device file, was made on different platform')
  }

  if (!sameAttr) {
    await teardown()
    throw new Error('Invalid device file, was moved unsafely')
  }

  if (st.ino !== inode || (created && Math.abs(modified - created) >= MODIFIED_SLACK)) {
    await teardown()
    throw new Error('Invalid device file, was modified')
  }

  if (!device.lock) await close(fd)

  device.data = result
  return true

  async function teardown () {
    if (device.lock) await device.lock.close()
    else await close(fd)
  }
}

async function getAttr(fd, name) {
  try {
    return await fsx.getAttr(fd, name)
  } catch {
    return null
  }
}

async function setAttr(fd, name, value) {
  try {
    await fsx.setAttr(fd, name, value)
    return true
  } catch {
    return false
  }
}

function fstat(fd) {
  return new Promise((resolve, reject) => {
    fs.fstat(fd, (err, st) => {
      if (err) reject(err)
      resolve(st)
    })
  })
}

function close(fd) {
  return new Promise((resolve, reject) => {
    fs.close(fd, (err, st) => {
      if (err) reject(err)
      resolve(st)
    })
  })
}

function write(fd, buf) {
  return new Promise((resolve, reject) => {
    let offset = 0

    onwrite(null, 0)

    function onwrite(err, wrote) {
      if (err) return reject(err)
      if (offset === buf.byteLength) return resolve()
      offset += wrote
      fs.write(fd, buf, offset, buf.byteLength - offset, offset, onwrite)
    }
  })
}

function read(fd) {
  const buf = b4a.allocUnsafe(4096)

  return new Promise((resolve, reject) => {
    let offset = 0

    fs.read(fd, buf, 0, buf.byteLength, 0, onread)

    function onread(err, read) {
      if (err) return reject(err)
      if (read === 0) return resolve(buf.subarray(0, offset))
      offset += read
      fs.read(fd, buf, offset, buf.byteLength - offset, offset, onread)
    }
  })
}

function open(filename, flags) {
  return new Promise((resolve, reject) => {
    fs.open(filename, flags, (err, fd) => {
      if (err) reject(err)
      resolve(fd)
    })
  })
}
{
  "name": "device-file",
  "version": "2.1.3",
  "description": "Device only file",
  "main": "index.js",
  "files": [
    "index.js"
  ],
  "dependencies": {
    "b4a": "^1.6.7",
    "bare-fs": "^4.0.1",
    "bare-path": "^3.0.0",
    "fd-lock": "^2.1.0",
    "fs-native-extensions": "^1.4.0",
    "ready-resource": "^1.2.0"
  },
  "devDependencies": {
    "brittle": "^3.13.1",
    "lunte": "^1.0.2",
    "prettier": "^3.6.2",
    "prettier-config-holepunch": "^2.0.0"
  },
  "scripts": {
    "format": "prettier --write .",
    "lint": "prettier --check . && lunte",
    "test": "brittle test.js"
  },
  "imports": {
    "fs": {
      "bare": "bare-fs",
      "default": "fs"
    },
    "path": {
      "bare": "bare-path",
      "default": "path"
    }
  },
  "repository": {
    "type": "git",
    "url": "https://github.com/holepunchto/device-file.git"
  },
  "author": "Holepunch Inc",
  "license": "Apache-2.0",
  "bugs": {
    "url": "https://github.com/holepunchto/device-file/issues"
  },
  "homepage": "https://github.com/holepunchto/device-file"
}
const { EventEmitter } = require('events')
const Table = require('kademlia-routing-table')
const TOS = require('time-ordered-set')
const UDX = require('udx-native')
const sodium = require('sodium-universal')
const c = require('compact-encoding')
const NatSampler = require('nat-sampler')
const b4a = require('b4a')
const IO = require('./lib/io')
const Query = require('./lib/query')
const Session = require('./lib/session')
const peer = require('./lib/peer')
const { UNKNOWN_COMMAND, INVALID_TOKEN } = require('./lib/errors')
const { PING, PING_NAT, FIND_NODE, DOWN_HINT } = require('./lib/commands')

const TMP = b4a.allocUnsafe(32)
const TICK_INTERVAL = 5000
const SLEEPING_INTERVAL = 3 * TICK_INTERVAL
const STABLE_TICKS = 240 // if nothing major bad happens in ~20mins we can consider this node stable (if nat is friendly)
const MORE_STABLE_TICKS = 3 * STABLE_TICKS
const REFRESH_TICKS = 60 // refresh every ~5min when idle
const RECENT_NODE = 12 // we've heard from a node less than 1min ago
const OLD_NODE = 360 // if an node has been around more than 30 min we consider it old

class DHT extends EventEmitter {
  constructor(opts = {}) {
    super()

    this.bootstrapNodes = opts.bootstrap === false ? [] : (opts.bootstrap || []).map(parseNode)
    this.table = new Table(randomBytes(32))
    this.nodes = new TOS()
    this.udx = opts.udx || new UDX()
    this.io = new IO(this.table, this.udx, {
      ...opts,
      onrequest: this._onrequest.bind(this),
      onresponse: this._onresponse.bind(this),
      ontimeout: this._ontimeout.bind(this)
    })

    this.concurrency = opts.concurrency || 10
    this.bootstrapped = false
    this.ephemeral = true
    this.firewalled = this.io.firewalled
    this.adaptive = typeof opts.ephemeral !== 'boolean' && opts.adaptive !== false
    this.destroyed = false
    this.suspended = false
    this.online = true
    this.stats = {
      queries: { active: 0, total: 0 },
      requests: this.io.stats.requests,
      commands: {
        ping: this.io.stats.commands[PING],
        pingNat: this.io.stats.commands[PING_NAT],
        findNode: this.io.stats.commands[FIND_NODE],
        downHint: this.io.stats.commands[DOWN_HINT]
      }
    }

    this._nat = new NatSampler()
    this._quickFirewall = opts.quickFirewall !== false
    this._forcePersistent = opts.ephemeral === false
    this._repinging = 0
    this._checks = 0
    this._tick = randomOffset(100) // make sure to random offset all the network ticks
    this._refreshTicks = randomOffset(REFRESH_TICKS)
    this._stableTicks = this.adaptive ? STABLE_TICKS : 0
    this._tickInterval = setInterval(this._ontick.bind(this), TICK_INTERVAL)
    this._lastTick = Date.now()
    this._lastHost = null
    this._filterNode = opts.filterNode || opts.addNode || null // opts.addNode is deprecating, use opts.filterNode instead
    this._onrow = (row) => row.on('full', (node) => this._onfullrow(node, row))
    this._nonePersistentSamples = []
    this._bootstrapping = this._bootstrap()
    this._bootstrapping.catch(noop)

    this.table.on('row', this._onrow)

    this.io.networkInterfaces.on('change', (interfaces) => this._onnetworkchange(interfaces))

    if (opts.nodes) {
      for (let i = opts.nodes.length - 1; i >= 0; i--) {
        this.addNode(opts.nodes[i])
      }
    }
  }

  static bootstrapper(port, host, opts) {
    if (!port) throw new Error('Port is required')
    if (!host) throw new Error('Host is required')
    if (host === '0.0.0.0' || host === '::') throw new Error('Invalid host')
    if (!UDX.isIPv4(host)) throw new Error('Host must be a IPv4 address')

    const dht = new this({
      port,
      ephemeral: false,
      firewalled: false,
      anyPort: false,
      bootstrap: [],
      ...opts
    })
    dht._nat.add(host, port)
    return dht
  }

  get id() {
    return this.ephemeral ? null : this.table.id
  }

  get host() {
    return this._nat.host
  }

  get port() {
    return this._nat.port
  }

  get randomized() {
    return this._nat.host !== null && this._nat.port === 0
  }

  get socket() {
    return this.firewalled ? this.io.clientSocket : this.io.serverSocket
  }

  onmessage(socket, buf, rinfo) {
    if (buf.byteLength > 1) this.io.onmessage(socket, buf, rinfo)
  }

  bind() {
    return this.io.bind()
  }

  async suspend({ log = noop } = {}) {
    log('Suspending waiting for io bind...')
    await this.io.bind()
    log('Done, continuing')
    if (this.suspended || this.destroyed) return
    this.suspended = true
    clearInterval(this._tickInterval)
    log('Done, suspending io')
    await this.io.suspend({ log })
    log('Done, dht suspended')
    this.emit('suspend')
  }

  async resume({ log = noop } = {}) {
    if (!this.suspended || this.destroyed) return
    this.suspended = false
    this._tickInterval = setInterval(this._ontick.bind(this), TICK_INTERVAL)
    this._onwakeup()
    log('Resuming io')
    await this.io.resume()
    log('Done, dht resumed')
    this.io.networkInterfaces.on('change', (interfaces) => this._onnetworkchange(interfaces))
    this.refresh()
    this.emit('resume')
  }

  address() {
    const socket = this.socket
    return socket ? socket.address() : null
  }

  localAddress() {
    if (!this.io.serverSocket) return null

    return {
      host: localIP(this.udx),
      port: this.io.serverSocket.address().port
    }
  }

  remoteAddress() {
    if (!this.host) return null
    if (!this.port) return null
    if (this.firewalled) return null
    if (!this.io.serverSocket) return null

    const port = this.io.serverSocket.address().port
    if (port !== this.port) return null

    return {
      host: this.host,
      port
    }
  }

  addNode({ host, port }) {
    this._addNode({
      id: peer.id(host, port),
      port,
      host,
      token: null,
      to: null,
      sampled: 0,
      added: this._tick,
      pinged: 0,
      seen: 0,
      downHints: 0,
      prev: null,
      next: null
    })
  }

  toArray(opts) {
    const limit = opts && opts.limit
    if (limit === 0) return []
    return this.nodes.toArray({ limit, reverse: true }).map(({ host, port }) => ({ host, port }))
  }

  async fullyBootstrapped() {
    return this._bootstrapping
  }

  ready() {
    // Deprecating, use fullyBootstrapped instead (removed on next major)
    return this.fullyBootstrapped()
  }

  findNode(target, opts) {
    if (this.destroyed) throw new Error('Node destroyed')
    this._refreshTicks = REFRESH_TICKS
    return new Query(this, target, true, FIND_NODE, null, opts)
  }

  query({ target, command, value }, opts) {
    if (this.destroyed) throw new Error('Node destroyed')
    this._refreshTicks = REFRESH_TICKS
    return new Query(this, target, false, command, value || null, opts)
  }

  ping({ host, port }, opts) {
    let value = null

    if (opts && opts.size && opts.size > 0) value = b4a.alloc(opts.size)

    const req = this.io.createRequest(
      { id: null, host, port },
      null,
      true,
      PING,
      null,
      value,
      (opts && opts.session) || null,
      opts && opts.ttl
    )
    return this._requestToPromise(req, opts)
  }

  request({ token = null, command, target = null, value = null }, { host, port }, opts) {
    const req = this.io.createRequest(
      { id: null, host, port },
      token,
      false,
      command,
      target,
      value,
      (opts && opts.session) || null,
      opts && opts.ttl
    )
    return this._requestToPromise(req, opts)
  }

  session() {
    return new Session(this)
  }

  _requestToPromise(req, opts) {
    if (req === null) return Promise.reject(new Error('Node destroyed'))

    if (opts && opts.socket) req.socket = opts.socket
    if (opts && opts.retry === false) req.retries = 0

    return new Promise((resolve, reject) => {
      req.onresponse = resolve
      req.onerror = reject
      req.send()
    })
  }

  async _bootstrap() {
    const self = this

    await Promise.resolve() // wait a tick, so apis can be used from the outside
    await this.io.bind()

    this.emit('listening')

    // TODO: some papers describe more advanced ways of bootstrapping - we should prob look into that

    let first = this.firewalled && this._quickFirewall && !this._forcePersistent
    let testNat = false

    const onlyFirewall = !this._forcePersistent

    for (let i = 0; i < 2; i++) {
      await this._backgroundQuery(this.table.id).on('data', ondata).finished()

      if (this.bootstrapped || (!testNat && !this._forcePersistent)) break
      if (!(await this._updateNetworkState(onlyFirewall))) break
    }

    if (this.bootstrapped) return
    this.bootstrapped = true

    this.emit('ready')

    function ondata(data) {
      // Simple QUICK nat heuristic.
      // If we get ONE positive nat ping before the bootstrap query finishes
      // then we always to a nat test, no matter if we are adaptive...
      // This should be expanded in the future to try more than one node etc, not always hit the first etc
      // If this fails, then nbd, as the onstable hook will pick it up later.

      if (!first) return
      first = false

      const value = b4a.allocUnsafe(2)
      c.uint16.encode({ start: 0, end: 2, buffer: value }, self.io.serverSocket.address().port)

      self._request(
        data.from,
        false,
        true,
        PING_NAT,
        null,
        value,
        null,
        () => {
          testNat = true
        },
        noop
      )
    }
  }

  refresh() {
    const node = this.table.random()
    this._backgroundQuery(node ? node.id : this.table.id).on('error', noop)
  }

  async destroy() {
    const emitClose = !this.destroyed
    this.destroyed = true
    clearInterval(this._tickInterval)
    await this.io.destroy()
    if (emitClose) this.emit('close')
  }

  _request(to, force, internal, command, target, value, session, onresponse, onerror) {
    const req = this.io.createRequest(to, null, internal, command, target, value, session)
    if (req === null) return null

    req.onresponse = onresponse
    req.onerror = onerror
    req.send(force)

    return req
  }

  _natAdd(host, port) {
    const prevHost = this._nat.host
    const prevPort = this._nat.port

    this._nat.add(host, port)

    if (prevHost === this._nat.host && prevPort === this._nat.port) return

    this.emit('nat-update', this._nat.host, this._nat.port)
  }

  // we don't check that this is a bootstrap node but we limit the sample size to very few nodes, so fine
  _sampleBootstrapMaybe(from, to) {
    if (this._nonePersistentSamples.length >= Math.max(1, this.bootstrapNodes.length)) return
    const id = from.host + ':' + from.port
    if (this._nonePersistentSamples.indexOf(id) > -1) return
    this._nonePersistentSamples.push(id)
    this._natAdd(to.host, to.port)
  }

  _addNodeFromNetwork(sample, from, to) {
    if (this._filterNode !== null && !this._filterNode(from)) {
      return
    }

    if (from.id === null) {
      this._sampleBootstrapMaybe(from, to)
      return
    }

    const oldNode = this.table.get(from.id)

    // refresh it, if we've seen this before
    if (oldNode) {
      if (sample && (oldNode.sampled === 0 || this._tick - oldNode.sampled >= OLD_NODE)) {
        oldNode.to = to
        oldNode.sampled = this._tick
        this._natAdd(to.host, to.port)
      }

      oldNode.pinged = oldNode.seen = this._tick
      this.nodes.add(oldNode)
      return
    }

    this._addNode({
      id: from.id,
      port: from.port,
      host: from.host,
      to,
      sampled: 0,
      added: this._tick,
      pinged: this._tick, // last time we interacted with them
      seen: this._tick, // last time we heard from them
      downHints: 0,
      prev: null,
      next: null
    })
  }

  _addNode(node) {
    if (this.nodes.has(node) || b4a.equals(node.id, this.table.id)) return

    node.added = node.pinged = node.seen = this._tick

    if (!this.table.add(node)) return
    this.nodes.add(node)

    if (node.to && node.sampled === 0) {
      node.sampled = this._tick
      this._natAdd(node.to.host, node.to.port)
    }

    this.emit('add-node', node)
  }

  _removeStaleNode(node, lastSeen) {
    if (node.seen <= lastSeen) this._removeNode(node)
  }

  _removeNode(node) {
    if (!this.nodes.has(node)) return

    this.table.remove(node.id)
    this.nodes.remove(node)

    this.emit('remove-node', node)
  }

  _onwakeup() {
    this._tick += 2 * OLD_NODE // bump the tick enough that everything appears old.
    this._tick += 8 - (this._tick & 7) - 2 // triggers a series of pings in two ticks
    this._stableTicks = MORE_STABLE_TICKS
    this._refreshTicks = 1 // triggers a refresh next tick (allow network time to wake up also)
    this._lastHost = null // clear network cache check

    if (this.adaptive) {
      // TODO: re-enable this as soon as we find out why this is over triggering in some edge cases
      // this.firewalled = true
      // this.io.firewalled = true

      if (!this.ephemeral) {
        this.ephemeral = true
        this.io.ephemeral = true
        this.emit('ephemeral')
      }
    }

    this.emit('wakeup')
  }

  _onfullrow(newNode, row) {
    if (!this.bootstrapped || this._repinging >= 3) return

    let oldest = null
    for (const node of row.nodes) {
      if (node.pinged === this._tick) continue
      if (
        oldest === null ||
        oldest.pinged > node.pinged ||
        (oldest.pinged === node.pinged && oldest.added > node.added)
      ) {
        oldest = node
      }
    }

    if (oldest === null) return
    if (this._tick - oldest.pinged < RECENT_NODE && this._tick - oldest.added > OLD_NODE) return

    this._repingAndSwap(newNode, oldest)
  }

  _onnetworkchange(interfaces) {
    this.emit('network-change', interfaces)
    this.emit('network-update')
  }

  _repingAndSwap(newNode, oldNode) {
    const self = this
    const lastSeen = oldNode.seen

    oldNode.pinged = this._tick

    this._repinging++
    this._request(
      { id: null, host: oldNode.host, port: oldNode.port },
      false,
      true,
      PING,
      null,
      null,
      null,
      onsuccess,
      onswap
    )

    function onsuccess(m) {
      if (oldNode.seen <= lastSeen) return onswap()
      self._repinging--
    }

    function onswap(e) {
      self._repinging--
      self._removeNode(oldNode)
      self._addNode(newNode)
    }
  }

  _onrequest(req, external) {
    if (req.from.id !== null) {
      this._addNodeFromNetwork(!external, req.from, req.to)
    }

    if (req.internal) {
      switch (req.command) {
        // standard keep alive call
        case PING: {
          req.sendReply(0, null, false, false)
          return
        }
        // check if the other side can receive a message to their other socket
        case PING_NAT: {
          if (req.value === null || req.value.byteLength < 2) return
          const port = c.uint16.decode({ start: 0, end: 2, buffer: req.value })
          if (port === 0) return
          req.from.port = port
          req.sendReply(0, null, false, false)
          return
        }
        // empty dht reply back
        case FIND_NODE: {
          if (!req.target) return
          req.sendReply(0, null, false, true)
          return
        }
        // "this is node you sent me is down" - let's try to ping it
        case DOWN_HINT: {
          if (req.value === null || req.value.byteLength < 6) return
          if (this._checks < 10) {
            sodium.crypto_generichash(TMP, req.value.subarray(0, 6))
            const node = this.table.get(TMP)
            if (node && (node.pinged < this._tick || node.downHints === 0)) {
              node.downHints++
              this._check(node)
            }
          }
          req.sendReply(0, null, false, false)
          return
        }
      }

      req.sendReply(UNKNOWN_COMMAND, null, false, req.target !== null)
      return
    }

    // ask the user to handle it or reply back with a bad command
    if (this.onrequest(req) === false) {
      req.sendReply(UNKNOWN_COMMAND, null, false, req.target !== null)
    }
  }

  onrequest(req) {
    return this.emit('request', req)
  }

  _onresponse(res, external) {
    this._addNodeFromNetwork(!external, res.from, res.to)
  }

  _ontimeout(req) {
    if (!req.to.id) return
    const node = this.table.get(req.to.id)
    if (node) this._removeNode(node)
  }

  _pingSome() {
    let cnt = this.io.inflight.length > 2 ? 3 : 5
    let oldest = this.nodes.oldest

    // tiny dht, pinged the bootstrap again
    if (!oldest) {
      this.refresh()
      return
    }

    // we've recently pinged the oldest one, so only trigger a couple of repings
    if (this._tick - oldest.pinged < RECENT_NODE) {
      cnt = 2
    }

    while (cnt--) {
      if (!oldest || this._tick === oldest.pinged) continue
      this._check(oldest)
      oldest = oldest.next
    }
  }

  _check(node) {
    node.pinged = this._tick

    const lastSeen = node.seen
    const onresponse = () => {
      this._checks--
      this._removeStaleNode(node, lastSeen)
    }
    const onerror = () => {
      this._checks--
      this._removeNode(node)
    }

    this._checks++
    this._request(
      { id: null, host: node.host, port: node.port },
      false,
      true,
      PING,
      null,
      null,
      null,
      onresponse,
      onerror
    )
  }

  _ontick() {
    const time = Date.now()

    if (time - this._lastTick > SLEEPING_INTERVAL && this.suspended === false) {
      this._onwakeup()
    } else {
      this._tick++
    }

    this._lastTick = time

    if (!this.bootstrapped || this.suspended) return

    if (this.adaptive && this.ephemeral && --this._stableTicks <= 0) {
      if (this._lastHost === this._nat.host) {
        // do not recheck the same network...
        this._stableTicks = MORE_STABLE_TICKS
      } else {
        this._updateNetworkState() // the promise returned here never fails so just ignore it
      }
    }

    if ((this._tick & 7) === 0) {
      this._pingSome()
    }

    if (
      ((this._tick & 63) === 0 && this.nodes.length < this.table.k) ||
      --this._refreshTicks <= 0
    ) {
      this.refresh()
    }
  }

  async _updateNetworkState(onlyFirewall = false) {
    if (!this.ephemeral) return false
    if (onlyFirewall && !this.firewalled) return false

    const { host, port } = this._nat

    if (!onlyFirewall) {
      // remember what host we checked and reset the counter
      this._stableTicks = MORE_STABLE_TICKS
      this._lastHost = host
    }

    // check if we have a consistent host and port
    if (host === null || port === 0) {
      return false
    }

    const natSampler = this.firewalled ? new NatSampler() : this._nat

    // ask remote nodes to ping us on our server socket to see if we have the port open
    const firewalled = this.firewalled && (await this._checkIfFirewalled(natSampler))
    if (firewalled) return false

    this.firewalled = this.io.firewalled = false

    // incase it's called in parallel for some reason, or if our nat status somehow changed
    if (!this.ephemeral || host !== this._nat.host || port !== this._nat.port) return false
    // if the firewall probe returned a different host / non consistent port, bail as well
    if (natSampler.host !== host || natSampler.port === 0) return false

    const id = peer.id(natSampler.host, natSampler.port)

    if (!onlyFirewall) {
      this.ephemeral = this.io.ephemeral = false
    }

    if (natSampler !== this._nat) {
      const prevHost = this._nat.host
      const prevPort = this._nat.port

      this._nonePersistentSamples = []
      this._nat = natSampler

      if (prevHost !== this._nat.host || prevPort !== this._nat.port) {
        this.emit('nat-update', this._nat.host, this._nat.port)
      }
    }

    // TODO: we should make this a bit more defensive in terms of using more
    // resources to make sure that the new routing table contains as many alive nodes
    // as possible, vs blindly copying them over...

    // all good! copy over the old routing table to the new one
    if (!b4a.equals(this.table.id, id)) {
      const nodes = this.table.toArray()

      this.table = this.io.table = new Table(id)

      for (const node of nodes) {
        if (b4a.equals(node.id, id)) continue
        if (!this.table.add(node)) this.nodes.remove(node)
      }

      this.table.on('row', this._onrow)

      // we need to rebootstrap/refresh since we updated our id
      if (this.bootstrapped) this.refresh()
    }

    if (!this.ephemeral) {
      this.emit('persistent')
    }

    return true
  }

  async *_resolveBootstrapNodes() {
    for (let { host, port } of this.bootstrapNodes) {
      let doLookup = false

      if (host.indexOf('@') === -1) {
        doLookup = true
      } else {
        const [suggestedIP, fallbackHost] = host.split('@')
        try {
          await this.ping({ host: suggestedIP, port })
          host = suggestedIP
        } catch {
          host = fallbackHost
          doLookup = true
        }
      }

      if (doLookup) {
        try {
          host = UDX.isIPv4(host) ? host : (await this.udx.lookup(host, { family: 4 })).host
        } catch {
          continue
        }
      }

      yield {
        id: peer.id(host, port),
        host,
        port
      }
    }
  }

  async _addBootstrapNodes(nodes) {
    for await (const node of this._resolveBootstrapNodes()) {
      nodes.push(node)
    }
  }

  async _checkIfFirewalled(natSampler = new NatSampler()) {
    const nodes = []
    for (let node = this.nodes.latest; node && nodes.length < 5; node = node.prev) {
      nodes.push(node)
    }

    if (nodes.length < 5) await this._addBootstrapNodes(nodes)
    // if no nodes are available, including bootstrappers - bail
    if (nodes.length === 0) return true

    const hosts = new Set()
    const value = b4a.allocUnsafe(2)

    c.uint16.encode({ start: 0, end: 2, buffer: value }, this.io.serverSocket.address().port)

    // double check they actually came on the server socket...
    this.io.serverSocket.on('message', onmessage)

    const pongs = await requestAll(this, true, PING_NAT, value, nodes)

    let count = 0
    for (const res of pongs) {
      if (hosts.has(res.from.host)) {
        count++
        natSampler.add(res.to.host, res.to.port)
      }
    }

    this.io.serverSocket.removeListener('message', onmessage)

    // if we got no or very few replies, consider it a fluke
    if (count < (nodes.length >= 5 ? 3 : 1)) return true

    // check that the server socket has the same ip as the client socket
    if (natSampler.host === null || this._nat.host !== natSampler.host) return true

    // check that the local port of the server socket is the same as the remote port
    // TODO: we might want a flag to opt out of this heuristic for specific remapped port servers
    if (natSampler.port === 0 || natSampler.port !== this.io.serverSocket.address().port) {
      return true
    }

    return false

    function onmessage(_, { host }) {
      hosts.add(host)
    }
  }

  _backgroundQuery(target) {
    this._refreshTicks = REFRESH_TICKS

    const backgroundCon = Math.min(this.concurrency, Math.max(2, (this.concurrency / 8) | 0))
    const q = new Query(this, target, true, FIND_NODE, null, {
      concurrency: backgroundCon,
      maxSlow: 0
    })

    q.on('data', () => {
      // yield to other traffic
      q.concurrency = this.io.inflight.length < 3 ? this.concurrency : backgroundCon
    })

    return q
  }

  // called by the query
  _online() {
    if (this.online) return
    this.online = true
    this.emit('network-update')
  }

  // called by the query
  _offline() {
    if (!this.online) return
    this.online = false
    this.emit('network-update')
  }
}

DHT.OK = 0
DHT.ERROR_UNKNOWN_COMMAND = UNKNOWN_COMMAND
DHT.ERROR_INVALID_TOKEN = INVALID_TOKEN

module.exports = DHT

function localIP(udx, family = 4) {
  let host = null

  for (const n of udx.networkInterfaces()) {
    if (n.family !== family || n.internal) continue

    // mac really likes en0, mb a better way but this shouldnt be bad anywhere so return now
    if (n.name === 'en0') return n.host

    // otherwise pick the first non internal host (let the loop continue in case we see en0)
    if (host === null) host = n.host
  }

  return host || (family === 4 ? '127.0.0.1' : '::1')
}

function parseNode(s) {
  if (typeof s === 'object') return s
  if (typeof s === 'number') return { host: '127.0.0.1', port: s }
  const [host, port] = s.split(':')
  if (!port) throw new Error('Bootstrap node format is host:port')

  return {
    host,
    port: Number(port)
  }
}

function randomBytes(n) {
  const b = b4a.alloc(n)
  sodium.randombytes_buf(b)
  return b
}

function randomOffset(n) {
  return n - ((Math.random() * 0.5 * n) | 0)
}

function requestAll(dht, internal, command, value, nodes) {
  let missing = nodes.length
  const replies = []

  return new Promise((resolve) => {
    for (const node of nodes) {
      const req = dht._request(
        node,
        false,
        internal,
        command,
        null,
        value,
        null,
        onsuccess,
        onerror
      )
      if (!req) return resolve(replies)
    }

    function onsuccess(res) {
      replies.push(res)
      if (--missing === 0) resolve(replies)
    }

    function onerror() {
      if (--missing === 0) resolve(replies)
    }
  })
}

function noop() {}
exports.PING = 0
exports.PING_NAT = 1
exports.FIND_NODE = 2
exports.DOWN_HINT = 3
module.exports = class DHTError extends Error {
  constructor(msg, code, fn = DHTError) {
    super(`${code}: ${msg}`)
    this.code = code

    if (Error.captureStackTrace) {
      Error.captureStackTrace(this, fn)
    }
  }

  get name() {
    return 'DHTError'
  }

  static UNKNOWN_COMMAND = 1
  static INVALID_TOKEN = 2

  static REQUEST_TIMEOUT(msg = 'Request timed out') {
    return new DHTError(msg, 'REQUEST_TIMEOUT', DHTError.REQUEST_TIMEOUT)
  }

  static REQUEST_DESTROYED(msg = 'Request destroyed') {
    return new DHTError(msg, 'REQUEST_DESTROYED', DHTError.REQUEST_DESTROYED)
  }

  static IO_SUSPENDED(msg = 'I/O suspended') {
    return new DHTError(msg, 'IO_SUSPENDED', DHTError.IO_SUSPENDED)
  }
}
const FIFO = require('fast-fifo')
const sodium = require('sodium-universal')
const c = require('compact-encoding')
const b4a = require('b4a')
const peer = require('./peer')
const { INVALID_TOKEN, REQUEST_TIMEOUT, REQUEST_DESTROYED, IO_SUSPENDED } = require('./errors')

const VERSION = 0b11
const RESPONSE_ID = (0b0001 << 4) | VERSION
const REQUEST_ID = (0b0000 << 4) | VERSION
const EMPTY_ARRAY = []

module.exports = class IO {
  constructor(
    table,
    udx,
    {
      maxWindow = 80,
      port = 0,
      host = '0.0.0.0',
      anyPort = true,
      firewalled = true,
      onrequest,
      onresponse = noop,
      ontimeout = noop
    } = {}
  ) {
    this.table = table
    this.udx = udx
    this.inflight = []
    this.clientSocket = null
    this.serverSocket = null
    this.firewalled = firewalled !== false
    this.ephemeral = true
    this.congestion = new CongestionWindow(maxWindow)
    this.networkInterfaces = udx.watchNetworkInterfaces()
    this.suspended = false

    this.stats = {
      requests: { active: 0, total: 0 },
      commands: [
        { tx: 0, rx: 0 }, // tx = transmitted, rx = received
        { tx: 0, rx: 0 },
        { tx: 0, rx: 0 },
        { tx: 0, rx: 0 }
      ]
    }

    this.onrequest = onrequest
    this.onresponse = onresponse
    this.ontimeout = ontimeout

    this._pending = new FIFO()
    this._rotateSecrets = 10
    this._tid = (Math.random() * 65536) | 0
    this._secrets = null
    this._drainInterval = null
    this._destroying = null
    this._binding = null

    // port can be a number or a range [start, to]
    this.portRange = port.length ? port : port === 0 ? [0, 0] : [port, port + 5]

    this._host = host
    this._anyPort = anyPort !== false
    this._boundServerPort = 0
    this._boundClientPort = 0
  }

  onmessage(socket, buffer, { host, port }) {
    if (buffer.byteLength < 2 || !(port > 0 && port < 65536) || this.suspended === true) return

    const from = { id: null, host, port }
    const state = { start: 1, end: buffer.byteLength, buffer }
    const expectedSocket = this.firewalled ? this.clientSocket : this.serverSocket
    const external = socket !== expectedSocket

    if (buffer[0] === REQUEST_ID) {
      const req = Request.decode(this, socket, from, state)
      if (req === null) return
      if (
        req.token !== null &&
        !b4a.equals(req.token, this.token(req.from, 1)) &&
        !b4a.equals(req.token, this.token(req.from, 0))
      ) {
        req.error(INVALID_TOKEN, { token: true })
        return
      }
      this.onrequest(req, external)
      return
    }

    if (buffer[0] === RESPONSE_ID) {
      const res = decodeReply(from, state)
      if (res === null) return

      for (let i = 0; i < this.inflight.length; i++) {
        const req = this.inflight[i]
        if (req.tid !== res.tid) continue

        res.rtt = Date.now() - req._timestamp

        if (i === this.inflight.length - 1) this.inflight.pop()
        else this.inflight[i] = this.inflight.pop()

        if (req.session) req.session._detach(req)

        // TODO: Auto retry here if errors.INVALID_TOKEN is returned?

        if (req._timeout) {
          clearTimeout(req._timeout)
          req._timeout = null
        }

        this.congestion.recv()

        if (req.internal && req.command < this.stats.commands.length) {
          this.stats.commands[req.command].rx++
        }

        this.stats.requests.active--

        this.onresponse(res, external)
        req.onresponse(res, req)
        break
      }
    }
  }

  token(addr, i) {
    if (this._secrets === null) {
      const buf = b4a.alloc(64)
      this._secrets = [buf.subarray(0, 32), buf.subarray(32, 64)]
      sodium.randombytes_buf(this._secrets[0])
      sodium.randombytes_buf(this._secrets[1])
    }

    const token = b4a.allocUnsafe(32)
    sodium.crypto_generichash(token, b4a.from(addr.host), this._secrets[i])
    return token
  }

  async destroy() {
    if (this._destroying) return this._destroying
    this._destroying = this._destroy()
    return this._destroying
  }

  async _destroy() {
    // simplifies timing to await the bind here also, although it might be unneeded
    await this.bind()
    await this._clear(false)
  }

  async _clear(suspended) {
    if (this._drainInterval) {
      clearInterval(this._drainInterval)
      this._drainInterval = null
    }

    while (this.inflight.length) {
      const req = this.inflight.pop()
      if (req._timeout) clearTimeout(req._timeout)
      req._timeout = null
      req.destroyed = true

      if (req.session) req.session._detach(req)

      this.congestion.recv()
      this.stats.requests.active--

      req.onerror(suspended ? IO_SUSPENDED() : REQUEST_DESTROYED(), req)
    }

    await Promise.allSettled([this.serverSocket.close(), this.clientSocket.close()])

    this.networkInterfaces.destroy()
  }

  async suspend() {
    this.suspended = true
    await this._clear(true)

    this.congestion.clear()

    if (this._drainInterval) {
      clearInterval(this._drainInterval)
      this._drainInterval = null
    }
  }

  async _rebind(binding) {
    if (binding) await binding
    if (this._destroying) return this._destroying
    await this._bindSockets()
    this.networkInterfaces = this.udx.watchNetworkInterfaces()
  }

  resume() {
    this.suspended = false
    const binding = this._binding
    this._binding = this._rebind(binding)
    return this._binding
  }

  bind() {
    if (this._binding) return this._binding
    this._binding = this._bindSockets()
    return this._binding
  }

  async _bindSockets() {
    const serverSocket = this.udx.createSocket()

    const candidatePorts = []

    // Retrying previous port always has precedence
    if (this._boundServerPort) candidatePorts.push(this._boundServerPort)

    for (let i = this.portRange[0]; i < this.portRange[1]; i++) candidatePorts.push(i)

    for (const port of candidatePorts) {
      if (serverSocket.bound) break

      try {
        serverSocket.bind(port, this._host)
      } catch (err) {
        if (!this._anyPort) {
          await serverSocket.close()
          throw err
        }
      }
    }

    if (!serverSocket.bound) {
      try {
        serverSocket.bind(0, this._host)
      } catch (err) {
        await serverSocket.close()
        throw err
      }
    }

    const clientSocket = this.udx.createSocket()

    try {
      clientSocket.bind(this._boundClientPort || 0, this._host)
    } catch {
      try {
        clientSocket.bind(0, this._host)
      } catch (err) {
        await serverSocket.close()
        await clientSocket.close()
        throw err
      }
    }

    this._boundServerPort = serverSocket.address().port
    this._boundClientPort = clientSocket.address().port

    this.clientSocket = clientSocket
    this.serverSocket = serverSocket

    this.serverSocket.on('message', this.onmessage.bind(this, this.serverSocket))
    this.clientSocket.on('message', this.onmessage.bind(this, this.clientSocket))

    if (this._drainInterval === null) {
      this._drainInterval = setInterval(this._drain.bind(this), 750)
      if (this._drainInterval.unref) this._drainInterval.unref()
    }

    for (const req of this.inflight) {
      if (!req.socket) req.socket = this.firewalled ? this.clientSocket : this.serverSocket
      req.sent = 0
      req.send(false)
    }
  }

  _drain() {
    if (this._secrets !== null && --this._rotateSecrets === 0) {
      this._rotateSecrets = 10
      const tmp = this._secrets[0]
      this._secrets[0] = this._secrets[1]
      this._secrets[1] = tmp
      sodium.crypto_generichash(tmp, tmp)
    }

    this.congestion.drain()

    while (!this.congestion.isFull()) {
      const p = this._pending.shift()
      if (p === undefined) return
      p._sendNow()
    }
  }

  createRequest(to, token, internal, command, target, value, session, ttl) {
    if (this._destroying !== null) return null

    if (this._tid === 65536) this._tid = 0

    const tid = this._tid++
    const socket = this.firewalled ? this.clientSocket : this.serverSocket

    const req = new Request(
      this,
      socket,
      tid,
      null,
      to,
      token,
      internal,
      command,
      target,
      value,
      session,
      ttl || 0
    )
    this.inflight.push(req)
    if (session) session._attach(req)

    if (internal && command < this.stats.commands.length) {
      this.stats.commands[command].tx++
    }

    this.stats.requests.active++
    this.stats.requests.total++

    return req
  }
}

class Request {
  constructor(io, socket, tid, from, to, token, internal, command, target, value, session, ttl) {
    this.socket = socket
    this.tid = tid
    this.from = from
    this.to = to
    this.token = token
    this.command = command
    this.target = target
    this.value = value
    this.internal = internal
    this.session = session
    this.ttl = ttl
    this.index = -1
    this.sent = 0
    this.retries = 3
    this.destroyed = false

    this.oncycle = noop
    this.onerror = noop
    this.onresponse = noop

    this._buffer = null
    this._io = io
    this._timeout = null
    this._timestamp = Date.now()
  }

  static decode(io, socket, from, state) {
    try {
      const flags = c.uint.decode(state)
      const tid = c.uint16.decode(state)
      const to = peer.ipv4.decode(state)
      const id = flags & 1 ? c.fixed32.decode(state) : null
      const token = flags & 2 ? c.fixed32.decode(state) : null
      const internal = (flags & 4) !== 0
      const command = c.uint.decode(state)
      const target = flags & 8 ? c.fixed32.decode(state) : null
      const value = flags & 16 ? c.buffer.decode(state) : null

      if (id !== null) from.id = validateId(id, from)

      return new Request(
        io,
        socket,
        tid,
        from,
        to,
        token,
        internal,
        command,
        target,
        value,
        null,
        0
      )
    } catch {
      return null
    }
  }

  reply(value, opts = {}) {
    const socket = opts.socket || this.socket
    const to = opts.to || this.from
    this._sendReply(0, value || null, opts.token !== false, opts.closerNodes !== false, to, socket)
  }

  error(code, opts = {}) {
    const socket = opts.socket || this.socket
    const to = opts.to || this.from
    this._sendReply(code, null, opts.token === true, opts.closerNodes !== false, to, socket)
  }

  relay(value, to, opts) {
    const socket = (opts && opts.socket) || this.socket
    const buffer = this._encodeRequest(null, value, to, socket)
    socket.trySend(buffer, to.port, to.host, this.ttl)
  }

  send(force = false) {
    if (this.destroyed) return

    if (this.socket === null) return
    if (this._buffer === null) {
      this._buffer = this._encodeRequest(this.token, this.value, this.to, this.socket)
    }

    if (!force && this._io.congestion.isFull()) {
      this._io._pending.push(this)
      return
    }

    this._sendNow()
  }

  sendReply(error, value, token, hasCloserNodes) {
    this._sendReply(error, value, token, hasCloserNodes, this.from, this.socket, null)
  }

  _sendNow() {
    if (this.destroyed) return
    this.sent++
    this._io.congestion.send()
    this.socket.trySend(this._buffer, this.to.port, this.to.host, this.ttl)
    if (this._timeout) clearTimeout(this._timeout)
    this._timeout = setTimeout(oncycle, 1000, this)
  }

  destroy(err) {
    if (this.destroyed) return
    this.destroyed = true

    if (this._timeout) {
      clearTimeout(this._timeout)
      this._timeout = null
    }

    const i = this._io.inflight.indexOf(this)
    if (i === -1) return

    if (i === this._io.inflight.length - 1) this._io.inflight.pop()
    else this._io.inflight[i] = this._io.inflight.pop()

    if (this.session) this.session._detach(this)

    this._io.stats.requests.active--
    this._io.congestion.recv()

    this.onerror(err || REQUEST_DESTROYED(), this)
  }

  _sendReply(error, value, token, hasCloserNodes, from, socket) {
    if (socket === null || this.destroyed) return

    const id = this._io.ephemeral === false && socket === this._io.serverSocket
    const closerNodes =
      this.target !== null && hasCloserNodes ? this._io.table.closest(this.target) : EMPTY_ARRAY
    const state = { start: 0, end: 1 + 1 + 6 + 2, buffer: null } // (type | version) + flags + to + tid

    if (id) state.end += 32
    if (token) state.end += 32
    if (closerNodes.length > 0) peer.ipv4Array.preencode(state, closerNodes)
    if (error > 0) c.uint.preencode(state, error)
    if (value) c.buffer.preencode(state, value)

    state.buffer = b4a.allocUnsafe(state.end)
    state.buffer[state.start++] = RESPONSE_ID
    state.buffer[state.start++] =
      (id ? 1 : 0) |
      (token ? 2 : 0) |
      (closerNodes.length > 0 ? 4 : 0) |
      (error > 0 ? 8 : 0) |
      (value ? 16 : 0)

    c.uint16.encode(state, this.tid)
    peer.ipv4.encode(state, from)

    if (id) c.fixed32.encode(state, this._io.table.id)
    if (token) c.fixed32.encode(state, this._io.token(from, 1))
    if (closerNodes.length > 0) peer.ipv4Array.encode(state, closerNodes)
    if (error > 0) c.uint.encode(state, error)
    if (value) c.buffer.encode(state, value)

    socket.trySend(state.buffer, from.port, from.host, this.ttl)
  }

  _encodeRequest(token, value, to, socket) {
    const id = this._io.ephemeral === false && socket === this._io.serverSocket
    const state = { start: 0, end: 1 + 1 + 6 + 2, buffer: null } // (type | version) + flags + to + tid

    if (id) state.end += 32
    if (token) state.end += 32

    c.uint.preencode(state, this.command)

    if (this.target) state.end += 32
    if (value) c.buffer.preencode(state, value)

    state.buffer = b4a.allocUnsafe(state.end)
    state.buffer[state.start++] = REQUEST_ID
    state.buffer[state.start++] =
      (id ? 1 : 0) |
      (token ? 2 : 0) |
      (this.internal ? 4 : 0) |
      (this.target ? 8 : 0) |
      (value ? 16 : 0)

    c.uint16.encode(state, this.tid)
    peer.ipv4.encode(state, to)

    if (id) c.fixed32.encode(state, this._io.table.id)
    if (token) c.fixed32.encode(state, token)

    c.uint.encode(state, this.command)

    if (this.target) c.fixed32.encode(state, this.target)
    if (value) c.buffer.encode(state, value)

    return state.buffer
  }
}

class CongestionWindow {
  constructor(maxWindow) {
    this._i = 0
    this._total = 0
    this._window = [0, 0, 0, 0]
    this._maxWindow = maxWindow
  }

  clear() {
    this._i = 0
    this._total = 0
    this._window = [0, 0, 0, 0]
  }

  isFull() {
    return this._total >= 2 * this._maxWindow || this._window[this._i] >= this._maxWindow
  }

  recv() {
    if (this._window[this._i] > 0) {
      this._window[this._i]--
      this._total--
    }
  }

  send() {
    this._total++
    this._window[this._i]++
  }

  drain() {
    this._i = (this._i + 1) & 3
    this._total -= this._window[this._i]
    this._window[this._i] = 0 // clear oldest
  }
}

function noop() {}

function oncycle(req) {
  req._timeout = null
  req.oncycle(req)
  if (req.sent >= req.retries) {
    req.destroy(REQUEST_TIMEOUT())
    req._io.ontimeout(req)
  } else {
    req.send()
  }
}

function decodeReply(from, state) {
  try {
    const flags = c.uint.decode(state)
    const tid = c.uint16.decode(state)
    const to = peer.ipv4.decode(state)
    const id = flags & 1 ? c.fixed32.decode(state) : null
    const token = flags & 2 ? c.fixed32.decode(state) : null
    const closerNodes = flags & 4 ? peer.ipv4Array.decode(state) : null
    const error = flags & 8 ? c.uint.decode(state) : 0
    const value = flags & 16 ? c.buffer.decode(state) : null

    if (id !== null) from.id = validateId(id, from)

    return { tid, rtt: 0, from, to, token, closerNodes, error, value }
  } catch {
    return null
  }
}

function validateId(id, from) {
  const expected = peer.id(from.host, from.port)
  return b4a.equals(expected, id) ? expected : null
}
const sodium = require('sodium-universal')
const c = require('compact-encoding')
const net = require('compact-encoding-net')
const b4a = require('b4a')

const ipv4 = {
  ...net.ipv4Address,
  decode(state) {
    const ip = net.ipv4Address.decode(state)
    return {
      id: null, // populated by the callee
      host: ip.host,
      port: ip.port
    }
  }
}

module.exports = { id, ipv4, ipv4Array: c.array(ipv4) }

function id(host, port, out = b4a.allocUnsafeSlow(32)) {
  const addr = out.subarray(0, 6)
  ipv4.encode({ start: 0, end: 6, buffer: addr }, { host, port })
  sodium.crypto_generichash(out, addr)
  return out
}
const { Readable, getStreamError } = require('streamx')
const b4a = require('b4a')
const peer = require('./peer')
const { DOWN_HINT } = require('./commands')

const DONE = []
const DOWN = []

module.exports = class Query extends Readable {
  constructor(dht, target, internal, command, value, opts = {}) {
    super()

    dht.stats.queries.total++
    dht.stats.queries.active++

    this.force = !!opts.force
    this.dht = dht
    this.k = this.dht.table.k
    this.target = target
    this.internal = internal
    this.command = command
    this.value = value
    this.errors = 0
    this.successes = 0
    this.concurrency = opts.concurrency || this.dht.concurrency
    this.inflight = 0
    this.map = opts.map || defaultMap
    this.retries = opts.retries === 0 ? 0 : opts.retries || 3
    this.maxSlow = opts.maxSlow === 0 ? 0 : opts.maxSlow || 5
    this.closestReplies = []

    this._slow = 0
    this._online = false
    this._slowdown = false
    this._seen = new Map()
    this._pending = []
    this._fromTable = false
    this._commit = opts.commit === true ? autoCommit : opts.commit || null
    this._commiting = false
    this._session = opts.session || dht.session()
    this._autoDestroySession = !opts.session
    this._onlyClosestNodes = false

    this._onvisitbound = this._onvisit.bind(this)
    this._onerrorbound = this._onerror.bind(this)
    this._oncyclebound = this._oncycle.bind(this)

    const nodes = opts.nodes || opts.closestNodes
    const replies = opts.replies || opts.closestReplies

    // add them reverse as we pop below
    if (nodes) {
      for (let i = nodes.length - 1; i >= 0; i--) {
        const node = nodes[i]
        this._addPending(
          {
            id: node.id || peer.id(node.host, node.port),
            host: node.host,
            port: node.port
          },
          null
        )
      }
    } else if (replies) {
      for (let i = replies.length - 1; i >= 0; i--) {
        this._addPending(replies[i].from, null)
      }
    }

    if (opts.onlyClosestNodes) this._onlyClosestNodes = true
  }

  get closestNodes() {
    const nodes = new Array(this.closestReplies.length)

    for (let i = 0; i < nodes.length; i++) {
      nodes[i] = this.closestReplies[i].from
    }

    return nodes
  }

  finished() {
    return new Promise((resolve, reject) => {
      if (this.destroyed) {
        const error = getStreamError(this)
        if (error) reject(error)
        else resolve()
        return
      }

      const self = this
      let error = null

      this.resume()
      this.on('error', onerror)
      this.on('close', onclose)

      function onclose() {
        self.removeListener('error', onerror)
        self.removeListener('close', onclose)
        if (error) reject(error)
        else resolve()
      }

      function onerror(err) {
        error = err
      }
    })
  }

  _addFromTable() {
    if (this._pending.length >= this.k) return
    this._fromTable = true

    const closest = this.dht.table.closest(this.target, this.k - this._pending.length)

    for (const node of closest) {
      this._addPending({ id: node.id, host: node.host, port: node.port }, null)
    }
  }

  async _open(cb) {
    this._addFromTable()
    if (this._pending.length >= this.k) return cb(null)

    for await (const node of this.dht._resolveBootstrapNodes()) {
      this._addPending(node, null)
    }

    cb(null)
  }

  _isCloser(id) {
    return (
      this.closestReplies.length < this.k ||
      this._compare(id, this.closestReplies[this.closestReplies.length - 1].from.id) < 0
    )
  }

  _addPending(node, ref) {
    if (this._onlyClosestNodes) return false

    const addr = node.host + ':' + node.port
    const refs = this._seen.get(addr)
    const isCloser = this._isCloser(node.id)

    if (refs === DONE) {
      return isCloser
    }

    if (refs === DOWN) {
      if (ref) this._downHint(ref, node)
      return isCloser
    }

    if (refs) {
      if (ref !== null) refs.push(ref)
      return isCloser
    }

    if (!isCloser) {
      return false
    }

    this._seen.set(addr, ref === null ? [] : [ref])
    this._pending.push(node)

    return true
  }

  _read(cb) {
    this._readMore()
    cb(null)
  }

  _readMore() {
    if (this.destroying || this._commiting) return

    const concurrency = (this._slowdown ? 3 : this.concurrency) + this._slow

    while (this.inflight < concurrency && this._pending.length > 0) {
      const next = this._pending.pop()
      if (next && next.id && !this._isCloser(next.id)) continue
      this._visit(next)
    }

    // if reusing closest nodes, slow down after the first readMore tick to allow
    // the closest node a chance to reply before going broad to question more
    if (!this._fromTable && this.successes === 0 && this.errors === 0) {
      this._slowdown = true
    }

    if (this._pending.length > 0) return

    // if no inflight OR all the queries we are waiting on are marked as slow (within our limits) and we have a full result.
    if (
      this.inflight === 0 ||
      (this._slow <= this.maxSlow &&
        this._slow === this.inflight &&
        this.closestReplies.length >= this.k)
    ) {
      // if more than 3/4 failed and we only used cached nodes, try again from the routing table
      if (!this._fromTable && this.successes < this.k / 4) {
        this._addFromTable()
        this._readMore()
        return
      }

      this._flush()
    }
  }

  _flush() {
    if (this._commiting) return
    this._commiting = true

    if (this._commit === null) {
      this.push(null)
      return
    }

    const p = []
    for (const m of this.closestReplies) p.push(this._commit(m, this.dht, this))
    this._endAfterCommit(p)
  }

  _endAfterCommit(ps) {
    if (!ps.length) {
      this.destroy(new Error('Too few nodes responded'))
      return
    }

    const self = this

    let pending = ps.length
    let success = 0

    for (const p of ps) p.then(ondone, onerror)

    function ondone() {
      success++
      if (--pending === 0) self.push(null)
    }

    function onerror(err) {
      if (--pending > 0) return
      if (success) self.push(null)
      else self.destroy(err)
    }
  }

  _dec(req) {
    if (req.oncycle === noop) {
      this._slow--
    } else {
      req.oncycle = noop
    }
    this.inflight--
  }

  _onvisit(m, req) {
    this._dec(req)

    this._online = true
    if (!this.dht.online) this.dht._online()

    const addr = req.to.host + ':' + req.to.port
    this._seen.set(addr, DONE)

    if (this._commiting) return

    if (m.error === 0) this.successes++
    else this.errors++

    if (m.error === 0 && m.from.id !== null && this._isCloser(m.from.id)) this._pushClosest(m)

    if (m.closerNodes !== null) {
      for (const node of m.closerNodes) {
        node.id = peer.id(node.host, node.port)
        if (this.dht._filterNode !== null && !this.dht._filterNode(node)) continue
        if (b4a.equals(node.id, this.dht.table.id)) continue
        // TODO: we could continue here instead of breaking to ensure that one of the nodes in the closer list
        // is later marked as DOWN that we gossip that back
        if (!this._addPending(node, m.from)) break
      }
    }

    if (!this._fromTable && this.successes + this.errors >= this.concurrency) {
      this._slowdown = false
    }

    if (m.error !== 0) {
      this._readMore()
      return
    }

    const data = this.map(m)
    if (!data || this.push(data) !== false) {
      this._readMore()
    }
  }

  _onerror(err, req) {
    const addr = req.to.host + ':' + req.to.port
    const refs = this._seen.get(addr)

    if (err.code === 'REQUEST_TIMEOUT') {
      this._seen.set(addr, DOWN)
      for (const node of refs) this._downHint(node, req.to)
    }

    this._dec(req)
    this.errors++
    this._readMore()
  }

  _oncycle(req) {
    req.oncycle = noop
    this._slow++
    this._readMore()
  }

  _downHint(node, down) {
    const state = { start: 0, end: 6, buffer: b4a.allocUnsafe(6) }
    peer.ipv4.encode(state, down)
    this.dht._request(node, false, true, DOWN_HINT, null, state.buffer, this._session, noop, noop)
  }

  _pushClosest(m) {
    this.closestReplies.push(m)
    for (let i = this.closestReplies.length - 2; i >= 0; i--) {
      const prev = this.closestReplies[i]
      const cmp = this._compare(prev.from.id, m.from.id)
      // if sorted, done!
      if (cmp < 0) break
      // if dup, splice it out (rare)
      if (cmp === 0) {
        this.closestReplies.splice(i + 1, 1)
        break
      }
      // swap and continue down
      this.closestReplies[i + 1] = prev
      this.closestReplies[i] = m
    }
    if (this.closestReplies.length > this.k) this.closestReplies.pop()
  }

  _compare(a, b) {
    for (let i = 0; i < a.length; i++) {
      if (a[i] === b[i]) continue
      const t = this.target[i]
      return (t ^ a[i]) - (t ^ b[i])
    }
    return 0
  }

  _visit(to) {
    this.inflight++

    const req = this.dht._request(
      to,
      this.force,
      this.internal,
      this.command,
      this.target,
      this.value,
      this._session,
      this._onvisitbound,
      this._onerrorbound
    )
    if (req === null) {
      this.destroy(new Error('Node was destroyed'))
      return
    }
    req.retries = this.retries
    req.oncycle = this._oncyclebound
    if (this.force) req.retries = 0
  }

  _destroy(cb) {
    this.dht.stats.queries.active--
    if (!this._online && this.dht.online) this.dht._offline()
    if (this._autoDestroySession) this._session.destroy()
    cb(null)
  }
}

function autoCommit(reply, dht, query) {
  if (!reply.token) return Promise.reject(new Error('No token received for closest node'))
  return dht.request(
    {
      token: reply.token,
      target: query.target,
      command: query.command,
      value: query.value
    },
    reply.from
  )
}

function defaultMap(m) {
  return m
}

function noop() {}
module.exports = class Session {
  constructor(dht) {
    this.dht = dht
    this.inflight = []
  }

  _attach(req) {
    req.index = this.inflight.push(req) - 1
  }

  _detach(req) {
    const i = req.index
    if (i === -1) return
    req.index = -1

    if (i === this.inflight.length - 1) this.inflight.pop()
    else {
      const req = (this.inflight[i] = this.inflight.pop())
      req.index = i
    }
  }

  query({ target, command, value }, opts = {}) {
    return this.dht.query({ target, command, value }, { ...opts, session: this })
  }

  request({ token, command, target, value }, { host, port }, opts = {}) {
    return this.dht.request(
      { token, command, target, value },
      { host, port },
      { ...opts, session: this }
    )
  }

  ping({ host, port }, opts = {}) {
    return this.dht.ping({ host, port }, { ...opts, session: this })
  }

  destroy(err) {
    while (this.inflight.length) {
      const req = this.inflight[0]
      // prevent destroyed requests from contributing to congestion counts
      this.dht.io.congestion.recv()
      req.destroy(err)
    }
  }
}
{
  "name": "dht-rpc",
  "version": "6.20.2",
  "description": "Make RPC calls over a Kademlia based DHT",
  "main": "index.js",
  "files": [
    "index.js",
    "lib/*.js"
  ],
  "imports": {
    "events": {
      "bare": "bare-events",
      "default": "events"
    }
  },
  "dependencies": {
    "b4a": "^1.6.1",
    "bare-events": "^2.2.0",
    "compact-encoding": "^2.11.0",
    "compact-encoding-net": "^1.2.0",
    "fast-fifo": "^1.1.0",
    "kademlia-routing-table": "^1.0.1",
    "nat-sampler": "^1.0.1",
    "sodium-universal": "^5.0.0",
    "streamx": "^2.13.2",
    "time-ordered-set": "^2.0.0",
    "udx-native": "^1.5.3"
  },
  "devDependencies": {
    "brittle": "^3.0.0",
    "lunte": "^1.3.0",
    "prettier": "^3.6.2",
    "prettier-config-holepunch": "^2.0.0",
    "test-suspend": "^1.0.0"
  },
  "scripts": {
    "format": "prettier --write .",
    "test": "npm run test:node && npm run test:bare",
    "test:bare": "brittle-bare --coverage test.js",
    "lint": "prettier --check . && lunte",
    "test:node": "brittle-node --coverage test.js"
  },
  "repository": {
    "type": "git",
    "url": "https://github.com/mafintosh/dht-rpc.git"
  },
  "author": "Mathias Buus (@mafintosh)",
  "license": "MIT",
  "bugs": {
    "url": "https://github.com/mafintosh/dht-rpc/issues"
  },
  "homepage": "https://github.com/mafintosh/dht-rpc"
}
module.exports = require('bare-events')
{
  "name": "events-universal",
  "version": "1.0.1",
  "description": "Universal wrapper for the Node.js events module",
  "exports": {
    "./package": "./package.json",
    ".": {
      "bare": "./bare.js",
      "react-native": "./react-native.js",
      "default": "./default.js"
    }
  },
  "files": [
    "index.js",
    "default.js",
    "bare.js",
    "react-native.js"
  ],
  "scripts": {
    "test": "prettier . --check"
  },
  "repository": {
    "type": "git",
    "url": "git+https://github.com/holepunchto/events-universal.git"
  },
  "author": "Holepunch",
  "license": "Apache-2.0",
  "bugs": {
    "url": "https://github.com/holepunchto/events-universal/issues"
  },
  "homepage": "https://github.com/holepunchto/events-universal#readme",
  "dependencies": {
    "bare-events": "^2.7.0"
  },
  "devDependencies": {
    "prettier": "^3.4.2",
    "prettier-config-holepunch": "^1.0.0"
  }
}
module.exports = class FixedFIFO {
  constructor (hwm) {
    if (!(hwm > 0) || ((hwm - 1) & hwm) !== 0) throw new Error('Max size for a FixedFIFO should be a power of two')
    this.buffer = new Array(hwm)
    this.mask = hwm - 1
    this.top = 0
    this.btm = 0
    this.next = null
  }

  clear () {
    this.top = this.btm = 0
    this.next = null
    this.buffer.fill(undefined)
  }

  push (data) {
    if (this.buffer[this.top] !== undefined) return false
    this.buffer[this.top] = data
    this.top = (this.top + 1) & this.mask
    return true
  }

  shift () {
    const last = this.buffer[this.btm]
    if (last === undefined) return undefined
    this.buffer[this.btm] = undefined
    this.btm = (this.btm + 1) & this.mask
    return last
  }

  peek () {
    return this.buffer[this.btm]
  }

  isEmpty () {
    return this.buffer[this.btm] === undefined
  }
}
const FixedFIFO = require('./fixed-size')

module.exports = class FastFIFO {
  constructor (hwm) {
    this.hwm = hwm || 16
    this.head = new FixedFIFO(this.hwm)
    this.tail = this.head
    this.length = 0
  }

  clear () {
    this.head = this.tail
    this.head.clear()
    this.length = 0
  }

  push (val) {
    this.length++
    if (!this.head.push(val)) {
      const prev = this.head
      this.head = prev.next = new FixedFIFO(2 * this.head.buffer.length)
      this.head.push(val)
    }
  }

  shift () {
    if (this.length !== 0) this.length--
    const val = this.tail.shift()
    if (val === undefined && this.tail.next) {
      const next = this.tail.next
      this.tail.next = null
      this.tail = next
      return this.tail.shift()
    }

    return val
  }

  peek () {
    const val = this.tail.peek()
    if (val === undefined && this.tail.next) return this.tail.next.peek()
    return val
  }

  isEmpty () {
    return this.length === 0
  }
}
{
  "name": "fast-fifo",
  "version": "1.3.2",
  "description": "A fast fifo implementation similar to the one powering nextTick in Node.js core",
  "main": "index.js",
  "files": [
    "./index.js",
    "./fixed-size.js"
  ],
  "dependencies": {},
  "devDependencies": {
    "standard": "^17.1.0",
    "brittle": "^3.3.2"
  },
  "scripts": {
    "test": "standard && brittle test.js"
  },
  "repository": {
    "type": "git",
    "url": "https://github.com/mafintosh/fast-fifo.git"
  },
  "author": "Mathias Buus (@mafintosh)",
  "license": "MIT",
  "bugs": {
    "url": "https://github.com/mafintosh/fast-fifo/issues"
  },
  "homepage": "https://github.com/mafintosh/fast-fifo"
}
const fs = require('fs')
const fsx = require('fs-native-extensions')
const onexit = require('resource-on-exit')
const ReadyResource = require('ready-resource')

module.exports = class FDLock extends ReadyResource {
  constructor(fd, opts = {}) {
    const { wait = false } = opts

    super()

    this._fd = fd
    this._wait = wait
    this._locked = false

    onexit.add(this, closeSync)
  }

  async _open() {
    try {
      await this._resume()
    } catch (err) {
      onexit.remove(this)
      await close(this)
      throw err
    }
  }

  async _close() {
    onexit.remove(this)
    await close(this)
  }

  transfer() {
    const fd = this._fd
    if (fd === -1) throw new Error('Lock has already been transferred')
    this._fd = -1
    onexit.remove(this)
    return fd
  }

  async suspend() {
    if (!this.opened) await this.ready()
    return this._suspend()
  }

  async _suspend() {
    if (this._fd === -1 || this._locked === false) return
    fsx.unlock(this._fd)
    this._locked = false
  }

  async resume() {
    if (!this.opened) await this.ready()
    return this._resume()
  }

  async _resume() {
    if (this._fd === -1 || this._locked === true) return
    if (this._wait) await fsx.waitForLock(this._fd)
    else if (!fsx.tryLock(this._fd)) {
      throw new Error('File descriptor could not be locked')
    }
    this._locked = true
  }
}

function close(lock) {
  const fd = lock._fd
  if (fd === -1) return
  lock._fd = -1
  return new Promise((resolve) => fs.close(fd, () => resolve()))
}

function closeSync(lock) {
  const fd = lock._fd
  if (fd === -1) return
  lock._fd = -1
  try {
    fs.closeSync(fd)
  } catch {}
}
{
  "name": "fd-lock",
  "version": "2.1.1",
  "description": "Stateful file descriptor locks for JavaScript",
  "exports": {
    "./package": "./package.json",
    ".": "./index.js"
  },
  "imports": {
    "fs": {
      "bare": "bare-fs",
      "default": "fs"
    }
  },
  "files": [
    "index.js"
  ],
  "scripts": {
    "test": "npm run lint && npm run test:bare && npm run test:node",
    "test:bare": "bare test.js",
    "test:node": "node test.js",
    "lint": "prettier --check .",
    "format": "prettier --write ."
  },
  "repository": {
    "type": "git",
    "url": "git+https://github.com/holepunchto/fd-lock.git"
  },
  "author": "Holepunch",
  "license": "Apache-2.0",
  "bugs": {
    "url": "https://github.com/holepunchto/fd-lock/issues"
  },
  "homepage": "https://github.com/holepunchto/fd-lock#readme",
  "dependencies": {
    "bare-fs": "^4.5.0",
    "fs-native-extensions": "^1.4.4",
    "ready-resource": "^1.2.0",
    "resource-on-exit": "^1.0.0"
  },
  "devDependencies": {
    "brittle": "^3.3.2",
    "prettier": "^3.4.2",
    "prettier-config-holepunch": "^2.0.0"
  }
}
exports.fullRoots = function (index, result) {
  if (index & 1) throw new Error('You can only look up roots for depth(0) blocks')
  if (!result) result = []

  index /= 2

  let offset = 0
  let factor = 1

  while (true) {
    if (!index) return result
    while (factor * 2 <= index) factor *= 2
    result.push(offset + factor - 1)
    offset = offset + 2 * factor
    index -= factor
    factor = 1
  }
}

exports.futureRoots = function (index, result) {
  if (index & 1) throw new Error('You can only look up future roots for depth(0) blocks')
  if (!result) result = []

  let factor = 1

  // make first root
  while (factor * 2 <= index) factor *= 2

  // full factor of 2 - done
  if (factor * 2 - 2 === index) return result

  let pos = factor / 2 - 1

  // while its not a full tree
  while ((pos + factor / 2 - 1) !== index) {
    pos += factor

    // read too far, to to left child
    while ((pos + factor / 2 - 1) > index) {
      factor /= 2
      pos -= factor / 2
    }

    // the "gap" is a future root
    result.push(pos - factor / 2)
  }

  return result
}

exports.patch = function (from, to) {
  if (from === 0 || from >= to) return []

  const roots = exports.fullRoots(from)
  const target = exports.fullRoots(to)

  // first find the first root that is different

  let i = 0
  for (; i < target.length; i++) {
    if (i >= roots.length || roots[i] !== target[i]) break
  }

  const patch = []

  if (i < roots.length) {
    // now we need to grow the newest root until it hits the diff one
    let prev = roots.length - 1

    const ite = exports.iterator(roots[prev--])

    while (ite.index !== target[i]) {
      ite.sibling()

      if (prev >= 0 && ite.index === roots[prev]) {
        prev--
      } else {
        patch.push(ite.index)
      }

      patch.push(ite.parent())
    }

    i++ // patched to next root, so inc
  }

  // include the rest

  for (; i < target.length; i++) patch.push(target[i])

  return patch
}

exports.depth = function (index) {
  let depth = 0

  index += 1
  while (!(index & 1)) {
    depth++
    index = rightShift(index)
  }

  return depth
}

exports.sibling = function (index, depth) {
  if (!depth) depth = exports.depth(index)
  const offset = exports.offset(index, depth)

  return exports.index(depth, offset & 1 ? offset - 1 : offset + 1)
}

exports.parent = function (index, depth) {
  if (!depth) depth = exports.depth(index)
  const offset = exports.offset(index, depth)

  return exports.index(depth + 1, rightShift(offset))
}

exports.leftChild = function (index, depth) {
  if (!(index & 1)) return -1
  if (!depth) depth = exports.depth(index)
  return exports.index(depth - 1, exports.offset(index, depth) * 2)
}

exports.rightChild = function (index, depth) {
  if (!(index & 1)) return -1
  if (!depth) depth = exports.depth(index)
  return exports.index(depth - 1, 1 + (exports.offset(index, depth) * 2))
}

exports.children = function (index, depth) {
  if (!(index & 1)) return null

  if (!depth) depth = exports.depth(index)
  const offset = exports.offset(index, depth) * 2

  return [
    exports.index(depth - 1, offset),
    exports.index(depth - 1, offset + 1)
  ]
}

exports.leftSpan = function (index, depth) {
  if (!(index & 1)) return index
  if (!depth) depth = exports.depth(index)
  return exports.offset(index, depth) * twoPow(depth + 1)
}

exports.rightSpan = function (index, depth) {
  if (!(index & 1)) return index
  if (!depth) depth = exports.depth(index)
  return (exports.offset(index, depth) + 1) * twoPow(depth + 1) - 2
}

exports.nextLeaf = function (index) {
  let factor = 1
  let r = index

  while ((r & 1) === 1) {
    r = (r - 1) / 2
    factor *= 2
  }

  return index + factor + 1
}

exports.count = function (index, depth) {
  if (!(index & 1)) return 1
  if (!depth) depth = exports.depth(index)
  return twoPow(depth + 1) - 1
}

exports.countLeaves = function (index) {
  return (exports.count(index) + 1) / 2
}

exports.spans = function (index, depth) {
  if (!(index & 1)) return [index, index]
  if (!depth) depth = exports.depth(index)

  const offset = exports.offset(index, depth)
  const width = twoPow(depth + 1)

  return [offset * width, (offset + 1) * width - 2]
}

exports.index = function (depth, offset) {
  return (1 + 2 * offset) * twoPow(depth) - 1
}

exports.offset = function (index, depth) {
  if (!(index & 1)) return index / 2
  if (!depth) depth = exports.depth(index)

  return ((index + 1) / twoPow(depth) - 1) / 2
}

exports.iterator = function (index) {
  const ite = new Iterator()
  ite.seek(index || 0)
  return ite
}

function twoPow (n) {
  return n < 31 ? 1 << n : ((1 << 30) * (1 << (n - 30)))
}

function rightShift (n) {
  return (n - (n & 1)) / 2
}

function Iterator () {
  this.index = 0
  this.offset = 0
  this.factor = 0
}

Iterator.prototype.seek = function (index) {
  this.index = index
  if (this.index & 1) {
    this.offset = exports.offset(index)
    this.factor = twoPow(exports.depth(index) + 1)
  } else {
    this.offset = index / 2
    this.factor = 2
  }
}

Iterator.prototype.isLeft = function () {
  return (this.offset & 1) === 0
}

Iterator.prototype.isRight = function () {
  return (this.offset & 1) === 1
}

Iterator.prototype.isRoot = function (length) {
  const currentLength = 1 + (this.index + this.factor / 2 - 1) / 2
  if (length < currentLength) return false

  const factor = this.factor * 2
  const index = (this.offset & 1)
    ? this.index - this.factor / 2
    : this.index + this.factor / 2

  const parentLength = 1 + (index + factor / 2 - 1) / 2
  return parentLength > length
}

Iterator.prototype.contains = function (index) {
  return index > this.index
    ? index < (this.index + this.factor / 2)
    : index < this.index
      ? index > (this.index - this.factor / 2)
      : true
}

Iterator.prototype.prev = function () {
  if (!this.offset) return this.index
  this.offset--
  this.index -= this.factor
  return this.index
}

Iterator.prototype.next = function () {
  this.offset++
  this.index += this.factor
  return this.index
}

Iterator.prototype.count = function () {
  if (!(this.index & 1)) return 1
  return this.factor - 1
}

Iterator.prototype.countLeaves = function () {
  return (this.count() + 1) / 2
}

Iterator.prototype.sibling = function () {
  return this.isLeft() ? this.next() : this.prev()
}

Iterator.prototype.parent = function () {
  if (this.offset & 1) {
    this.index -= this.factor / 2
    this.offset = (this.offset - 1) / 2
  } else {
    this.index += this.factor / 2
    this.offset /= 2
  }
  this.factor *= 2
  return this.index
}

Iterator.prototype.leftSpan = function () {
  this.index = this.index - this.factor / 2 + 1
  this.offset = this.index / 2
  this.factor = 2
  return this.index
}

Iterator.prototype.peekLeftSpan = function () {
  return this.index - this.factor / 2 + 1
}

Iterator.prototype.rightSpan = function () {
  this.index = this.index + this.factor / 2 - 1
  this.offset = this.index / 2
  this.factor = 2
  return this.index
}

Iterator.prototype.peekRightSpan = function () {
  return this.index + this.factor / 2 - 1
}

Iterator.prototype.leftChild = function () {
  if (this.factor === 2) return this.index
  this.factor /= 2
  this.index -= this.factor / 2
  this.offset *= 2
  return this.index
}

Iterator.prototype.rightChild = function () {
  if (this.factor === 2) return this.index
  this.factor /= 2
  this.index += this.factor / 2
  this.offset = 2 * this.offset + 1
  return this.index
}

Iterator.prototype.nextTree = function () {
  this.index = this.index + this.factor / 2 + 1
  this.offset = this.index / 2
  this.factor = 2
  return this.index
}

Iterator.prototype.prevTree = function () {
  if (!this.offset) {
    this.index = 0
    this.factor = 2
  } else {
    this.index = this.index - this.factor / 2 - 1
    this.offset = this.index / 2
    this.factor = 2
  }
  return this.index
}

Iterator.prototype.fullRoot = function (index) {
  if (index <= this.index || (this.index & 1) > 0) return false
  while (index > this.index + this.factor + this.factor / 2) {
    this.index += this.factor / 2
    this.factor *= 2
    this.offset /= 2
  }
  return true
}
{
  "name": "flat-tree",
  "version": "1.13.0",
  "description": "A series of functions to map a binary tree to a list",
  "main": "index.js",
  "files": [
    "index.js"
  ],
  "dependencies": {},
  "devDependencies": {
    "brittle": "^3.3.2",
    "standard": "^17.1.0"
  },
  "scripts": {
    "test": "standard && brittle test.js"
  },
  "repository": {
    "type": "git",
    "url": "https://github.com/mafintosh/flat-tree.git"
  },
  "author": "Mathias Buus (@mafintosh)",
  "license": "MIT",
  "bugs": {
    "url": "https://github.com/mafintosh/flat-tree/issues"
  },
  "homepage": "https://github.com/mafintosh/flat-tree"
}
require.addon = require('require-addon')

module.exports = require.addon('.', __filename)
const { isWindows } = require('which-runtime')
const binding = require('./binding')

function onwork(err, result) {
  if (err) this.reject(err)
  else this.resolve(result)
}

exports.tryLock = function tryLock(fd, offset = 0, length = 0, opts = {}) {
  if (typeof offset === 'object') {
    opts = offset
    offset = 0
  }

  if (typeof length === 'object') {
    opts = length
    length = 0
  }

  if (typeof opts !== 'object' || opts === null) {
    opts = {}
  }

  try {
    binding.tryLock(fd, offset, length, opts.shared !== true)
  } catch (err) {
    if (err.code === 'EAGAIN') return false
    throw err
  }

  return true
}

exports.waitForLock = function waitForLock(
  fd,
  offset = 0,
  length = 0,
  opts = {}
) {
  if (typeof offset === 'object') {
    opts = offset
    offset = 0
  }

  if (typeof length === 'object') {
    opts = length
    length = 0
  }

  if (typeof opts !== 'object' || opts === null) {
    opts = {}
  }

  const req = {
    handle: null,
    resolve: null,
    reject: null
  }

  const promise = new Promise((resolve, reject) => {
    req.resolve = resolve
    req.reject = reject
  })

  try {
    req.handle = binding.waitForLock(
      fd,
      offset,
      length,
      opts.shared !== true,
      req,
      onwork
    )
  } catch (err) {
    return Promise.reject(err)
  }

  return promise
}

exports.tryDowngradeLock = function tryDowngradeLock(
  fd,
  offset = 0,
  length = 0
) {
  try {
    binding.tryDowngradeLock(fd, offset, length)
  } catch (err) {
    if (err.code === 'EAGAIN') return false
    throw err
  }

  return true
}

exports.waitForDowngradeLock = function downgradeLock(
  fd,
  offset = 0,
  length = 0
) {
  const req = {
    handle: null,
    resolve: null,
    reject: null
  }

  const promise = new Promise((resolve, reject) => {
    req.resolve = resolve
    req.reject = reject
  })

  try {
    req.handle = binding.waitForDowngradeLock(fd, offset, length, req, onwork)
  } catch (err) {
    return Promise.reject(err)
  }

  return promise
}

exports.tryUpgradeLock = function tryUpgradeLock(fd, offset = 0, length = 0) {
  try {
    binding.tryUpgradeLock(fd, offset, length)
  } catch (err) {
    if (err.code === 'EAGAIN') return false
    throw err
  }

  return true
}

exports.waitForUpgradeLock = function upgradeLock(fd, offset = 0, length = 0) {
  const req = {
    handle: null,
    resolve: null,
    reject: null
  }

  const promise = new Promise((resolve, reject) => {
    req.resolve = resolve
    req.reject = reject
  })

  try {
    req.handle = binding.waitForUpgradeLock(fd, offset, length, req, onwork)
  } catch (err) {
    return Promise.reject(err)
  }

  return promise
}

exports.unlock = function unlock(fd, offset = 0, length = 0) {
  binding.unlock(fd, offset, length)
}

exports.trim = function trim(fd, offset, length) {
  const req = {
    handle: null,
    resolve: null,
    reject: null
  }

  const promise = new Promise((resolve, reject) => {
    req.resolve = resolve
    req.reject = reject
  })

  try {
    req.handle = binding.trim(fd, offset, length, req, onwork)
  } catch (err) {
    return Promise.reject(err)
  }

  return promise
}

exports.sparse = function sparse(fd) {
  // Short circuit on everything but Windows
  if (!isWindows) return Promise.resolve()

  const req = {
    handle: null,
    resolve: null,
    reject: null
  }

  const promise = new Promise((resolve, reject) => {
    req.resolve = resolve
    req.reject = reject
  })

  try {
    req.handle = binding.sparse(fd, req, onwork)
  } catch (err) {
    return Promise.reject(err)
  }

  return promise
}

exports.swap = function swap(from, to) {
  const req = {
    handle: null,
    resolve: null,
    reject: null
  }

  const promise = new Promise((resolve, reject) => {
    req.resolve = resolve
    req.reject = reject
  })

  try {
    req.handle = binding.swap(from, to, req, onwork)
  } catch (err) {
    return Promise.reject(err)
  }

  return promise
}

exports.getAttr = function getAttr(fd, name) {
  const req = {
    handle: null,
    resolve: null,
    reject: null
  }

  const promise = new Promise((resolve, reject) => {
    req.resolve = resolve
    req.reject = reject
  })

  try {
    req.handle = binding.getAttr(fd, name, req, onwork)
  } catch (err) {
    return Promise.reject(err)
  }

  return promise.then((buffer) =>
    buffer === null ? null : Buffer.from(buffer)
  )
}

exports.setAttr = function setAttr(fd, name, value, encoding) {
  if (typeof value === 'string') value = Buffer.from(value, encoding)

  const req = {
    value,
    handle: null,
    resolve: null,
    reject: null
  }

  const promise = new Promise((resolve, reject) => {
    req.resolve = resolve
    req.reject = reject
  })

  try {
    req.handle = binding.setAttr(
      fd,
      name,
      value.buffer,
      value.byteOffset,
      value.byteLength,
      req,
      onwork
    )
  } catch (err) {
    return Promise.reject(err)
  }

  return promise
}

exports.removeAttr = function removeAttr(fd, name) {
  const req = {
    handle: null,
    resolve: null,
    reject: null
  }

  const promise = new Promise((resolve, reject) => {
    req.resolve = resolve
    req.reject = reject
  })

  try {
    req.handle = binding.removeAttr(fd, name, req, onwork)
  } catch (err) {
    return Promise.reject(err)
  }

  return promise
}

exports.listAttrs = function listAttrs(fd) {
  const req = {
    handle: null,
    resolve: null,
    reject: null
  }

  const promise = new Promise((resolve, reject) => {
    req.resolve = resolve
    req.reject = reject
  })

  try {
    req.handle = binding.listAttrs(fd, req, onwork)
  } catch (err) {
    return Promise.reject(err)
  }

  return promise
}
{
  "name": "fs-native-extensions",
  "version": "1.4.5",
  "description": "Native file system extensions for advanced file operations",
  "main": "index.js",
  "files": [
    "index.js",
    "macros.h",
    "binding.c",
    "binding.js",
    "CMakeLists.txt",
    "include",
    "src",
    "prebuilds"
  ],
  "imports": {
    "child_process": {
      "bare": "bare-subprocess",
      "default": "child_process"
    },
    "fs": {
      "bare": "bare-fs",
      "default": "fs"
    },
    "fs/*": {
      "bare": "bare-fs/*",
      "default": "fs/*"
    },
    "path": {
      "bare": "bare-path",
      "default": "path"
    }
  },
  "addon": true,
  "scripts": {
    "test": "npm run lint && npm run test:bare && npm run test:node",
    "test:bare": "bare test.js",
    "test:node": "node test.js",
    "lint": "prettier . --check"
  },
  "repository": {
    "type": "git",
    "url": "git+https://github.com/holepunchto/fs-native-extensions.git"
  },
  "author": "Kasper Isager Dalsgar <kasper@funktionel.co>",
  "license": "Apache-2.0",
  "bugs": {
    "url": "https://github.com/holepunchto/fs-native-extensions/issues"
  },
  "homepage": "https://github.com/holepunchto/fs-native-extensions#readme",
  "dependencies": {
    "require-addon": "^1.1.0",
    "which-runtime": "^1.2.0"
  },
  "devDependencies": {
    "bare-compat-napi": "^1.3.8",
    "bare-fs": "^4.4.4",
    "bare-path": "^3.0.0",
    "bare-subprocess": "^4.0.5",
    "brittle": "^3.1.1",
    "cmake-bare": "^1.1.10",
    "cmake-fetch": "^1.4.7",
    "cmake-napi": "^1.0.5",
    "cmake-npm": "^1.1.0",
    "minimist": "^1.2.6",
    "prettier": "^3.5.3",
    "prettier-config-standard": "^7.0.0",
    "test-tmp": "^1.2.1"
  }
}
const codecs = require('codecs')
const { Readable } = require('streamx')
const mutexify = require('mutexify/promise')
const b4a = require('b4a')
const safetyCatch = require('safety-catch')
const ReadyResource = require('ready-resource')
const debounce = require('debounceify')
const Rache = require('rache')
const rrp = require('resolve-reject-promise')

const { all: unslabAll } = require('unslab')

const RangeIterator = require('./iterators/range')
const HistoryIterator = require('./iterators/history')
const DiffIterator = require('./iterators/diff')
const LocalBlockIterator = require('./iterators/local')
const Extension = require('./lib/extension')
const { YoloIndex, Node, Header } = require('./lib/messages')
const { BLOCK_NOT_AVAILABLE, DECODING_ERROR } = require('hypercore-errors')

const T = 5
const MIN_KEYS = T - 1
const MAX_CHILDREN = MIN_KEYS * 2 + 1

const SEP = b4a.alloc(1)
const EMPTY = b4a.alloc(0)

class Key {
  constructor (seq, value) {
    this.seq = seq
    this.value = value
  }
}

class Child {
  constructor (seq, offset, value) {
    this.seq = seq
    this.offset = offset
    this.value = value
  }
}

class Cache {
  constructor (rache) {
    this.keys = rache
    this.length = 0
  }

  get (seq) {
    return this.keys.get(seq) || null
  }

  set (seq, key) {
    this.keys.set(seq, key)
    if (seq >= this.length) this.length = seq + 1
  }

  gc (length) {
    // if we need to "work" more than 128 ticks, just bust the cache...
    if (this.length - length > 128) {
      this.keys.clear()
    } else {
      for (let i = length; i < this.length; i++) {
        this.keys.delete(i)
      }
    }

    this.length = length
  }

  clear () {
    this.keys.clear()
  }
}

class Pointers {
  constructor (decoded) {
    this.levels = decoded.levels.map(l => {
      const children = []
      const keys = []

      for (let i = 0; i < l.keys.length; i++) {
        keys.push(new Key(l.keys[i], null))
      }

      for (let i = 0; i < l.children.length; i += 2) {
        children.push(new Child(l.children[i], l.children[i + 1], null))
      }

      return { keys, children }
    })
  }

  get (i) {
    return this.levels[i]
  }

  hasKey (seq) {
    for (const lvl of this.levels) {
      for (const key of lvl.keys) {
        if (key.seq === seq) return true
      }
    }
    return false
  }
}

function inflate (entry) {
  if (entry.inflated === null) {
    entry.inflated = YoloIndex.decode(entry.index)
    entry.index = null
  }
  return new Pointers(entry.inflated)
}

function deflate (index) {
  const levels = index.map(l => {
    const keys = []
    const children = []

    for (let i = 0; i < l.value.keys.length; i++) {
      keys.push(l.value.keys[i].seq)
    }

    for (let i = 0; i < l.value.children.length; i++) {
      children.push(l.value.children[i].seq, l.value.children[i].offset)
    }

    return { keys, children }
  })

  return YoloIndex.encode({ levels })
}

class TreeNode {
  constructor (block, keys, children, offset) {
    this.block = block
    this.offset = offset
    this.keys = keys
    this.children = children
    this.changed = false

    this.preload()
  }

  preload () {
    if (this.block === null) return

    const core = getBackingCore(this.block.tree.core)
    if (!core) return

    const bitfield = core.core.bitfield
    const blocks = []

    for (let i = 0; i < this.keys.length; i++) {
      const k = this.keys[i]
      if (k.value) continue
      if (k.seq >= core.signedLength || (bitfield && bitfield.get(k.seq))) continue
      blocks.push(k.seq)
    }
    for (let i = 0; i < this.children.length; i++) {
      const c = this.children[i]
      if (c.value) continue
      if (c.seq >= core.signedLength || (bitfield && bitfield.get(c.seq))) continue
      blocks.push(c.seq)
    }

    if (blocks.length) core.download({ blocks })
  }

  async insertKey (key, value, child, node, encoding, cas) {
    let s = 0
    let e = this.keys.length
    let c

    while (s < e) {
      const mid = (s + e) >> 1
      c = b4a.compare(key.value, await this.getKey(mid))

      if (c === 0) {
        if (cas) {
          const prev = await this.getKeyNode(mid)
          if (!(await cas(prev.final(encoding), node))) return true
        }
        if (!this.block.tree.tree.alwaysDuplicate) {
          const prev = await this.getKeyNode(mid)
          if (sameValue(prev.value, value)) return true
        }
        this.changed = true
        this.keys[mid] = key
        return true
      }

      if (c < 0) e = mid
      else s = mid + 1
    }

    const i = c < 0 ? e : s
    this.keys.splice(i, 0, key)
    if (child) this.children.splice(i + 1, 0, new Child(0, 0, child))
    this.changed = true

    return this.keys.length < MAX_CHILDREN
  }

  removeKey (index) {
    this.keys.splice(index, 1)
    if (this.children.length) {
      this.children[index + 1].seq = 0 // mark as freed
      this.children.splice(index + 1, 1)
    }
    this.changed = true
  }

  async siblings (parent) {
    for (let i = 0; i < parent.children.length; i++) {
      if (parent.children[i].value === this) {
        const [left, right] = await Promise.all([
          i ? parent.getChildNode(i - 1) : null,
          i < parent.children.length - 1 ? parent.getChildNode(i + 1) : null
        ])
        return { left, index: i, right }
      }
    }

    throw new Error('Bad parent')
  }

  merge (node, median) {
    this.changed = true
    this.keys.push(median)
    for (let i = 0; i < node.keys.length; i++) this.keys.push(node.keys[i])
    for (let i = 0; i < node.children.length; i++) this.children.push(node.children[i])
  }

  async split () {
    const len = this.keys.length >> 1
    const right = TreeNode.create(this.block)

    while (right.keys.length < len) right.keys.push(this.keys.pop())
    right.keys.reverse()

    await this.getKey(this.keys.length - 1) // make sure the median is loaded
    const median = this.keys.pop()

    if (this.children.length) {
      while (right.children.length < len + 1) right.children.push(this.children.pop())
      right.children.reverse()
    }

    this.changed = true

    return {
      left: this,
      median,
      right
    }
  }

  getKeyNode (index) {
    return this.block.tree.getBlock(this.keys[index].seq)
  }

  async getChildNode (index) {
    const child = this.children[index]
    if (child.value) return child.value
    const block = child.seq === this.block.seq ? this.block : await this.block.tree.getBlock(child.seq)
    return (child.value = block.getTreeNode(child.offset))
  }

  setKey (index, key) {
    this.keys[index] = key
    this.changed = true
  }

  async getKey (index) {
    const key = this.keys[index]
    if (key.value) return key.value
    const k = key.seq === this.block.seq ? this.block.key : await this.block.tree.getKey(key.seq)
    return (key.value = k)
  }

  indexChanges (index, seq) {
    const offset = index.push(null) - 1
    this.changed = false

    for (const child of this.children) {
      if (!child.value || !child.value.changed) continue
      child.seq = seq
      child.offset = child.value.indexChanges(index, seq)
      index[child.offset] = child
    }

    return offset
  }

  updateChildren (seq, block) {
    for (const child of this.children) {
      if (!child.value || child.seq !== seq) continue
      child.value.block = block
      child.value.updateChildren(seq, block)
    }
  }

  static create (block) {
    const node = new TreeNode(block, [], [], 0)
    node.changed = true
    return node
  }
}

class BlockEntry {
  constructor (seq, tree, entry) {
    this.seq = seq
    this.tree = tree
    this.index = null
    this.entry = entry
    this.key = entry.key
    this.value = entry.value
  }

  isTarget (key) {
    return b4a.equals(this.key, key)
  }

  inflate () {
    if (this.index === null) {
      this.index = inflate(this.entry)
    }
  }

  isDeletion () {
    if (this.value !== null) return false

    if (this.index === null) {
      this.index = inflate(this.entry)
    }

    return !this.index.hasKey(this.seq)
  }

  final (encoding) {
    return {
      seq: this.seq,
      key: encoding.key ? encoding.key.decode(this.key) : this.key,
      value: this.value && (encoding.value ? encoding.value.decode(this.value) : this.value)
    }
  }

  getTreeNode (offset) {
    if (this.index === null) {
      this.index = inflate(this.entry)
    }
    const entry = this.index.get(offset)
    return new TreeNode(this, entry.keys, entry.children, offset)
  }
}

class CacheLock {
  constructor () {
    this.map = new Map()
  }

  enter (seq) {
    const pending = this.map.get(seq)

    if (!pending) {
      this.map.set(seq, [])
      return Promise.resolve()
    }

    const { resolve, promise } = rrp()
    pending.push(resolve)
    return promise
  }

  exit (seq) {
    const pending = this.map.get(seq)
    if (!pending.length) {
      this.map.delete(seq)
      return
    }

    pending.pop()()
  }
}

class BatchEntry extends BlockEntry {
  constructor (seq, tree, key, value, index) {
    super(seq, tree, { key, value, index: null, inflated: null })
    this.pendingIndex = index
  }

  isTarget (key) {
    return false
  }

  getTreeNode (offset) {
    return this.pendingIndex[offset].value
  }
}

class Hyperbee extends ReadyResource {
  constructor (core, opts = {}) {
    super()
    // this.feed is now deprecated, and will be this.core going forward
    this.feed = core
    this.core = core

    this.keyEncoding = opts.keyEncoding ? codecs(opts.keyEncoding) : null
    this.valueEncoding = opts.valueEncoding ? codecs(opts.valueEncoding) : null
    this.extension = opts.extension !== false ? getExtension(this, opts) : null
    this.metadata = opts.metadata || null
    this.lock = opts.lock || mutexify()
    this.sep = opts.sep || SEP
    this.readonly = !!opts.readonly
    this.prefix = opts.prefix || null

    // In a future version, this should be false by default
    this.alwaysDuplicate = opts.alwaysDuplicate !== false

    this._unprefixedKeyEncoding = this.keyEncoding
    this._sub = !!this.prefix
    this._checkout = opts.checkout || 0
    this._view = !!opts._view

    this._onappendBound = this._view ? null : this._onappend.bind(this)
    this._ontruncateBound = this._view ? null : this._ontruncate.bind(this)
    this._watchers = this._onappendBound ? [] : null
    this._entryWatchers = this._onappendBound ? [] : null
    this._sessions = opts.sessions !== false

    this._keyCache = null
    this._nodeCache = null
    this._cacheLock = new CacheLock()

    this._batches = []

    if (this._watchers) {
      this.core.on('append', this._onappendBound)
      this.core.on('truncate', this._ontruncateBound)
    }

    if (this.prefix && opts._sub) {
      this.keyEncoding = prefixEncoding(this.prefix, this.keyEncoding)
    }

    this.ready().catch(safetyCatch)
  }

  async _open () {
    if (this.core.opened === false) await this.core.ready()

    // snapshot
    if (this._checkout === -1) this._checkout = Math.max(1, this.core.length)

    const baseCache = Rache.from(this.core.globalCache)
    this._keyCache = new Cache(baseCache)
    this._nodeCache = new Cache(Rache.from(baseCache))
  }

  get version () {
    return Math.max(1, this._checkout || this.core.length)
  }

  get id () {
    return this.core.id
  }

  get key () {
    return this.core.key
  }

  get discoveryKey () {
    return this.core.discoveryKey
  }

  get writable () {
    return this.core.writable
  }

  get readable () {
    return this.core.readable
  }

  replicate (isInitiator, opts) {
    return this.core.replicate(isInitiator, opts)
  }

  update (opts) {
    return this.core.update(opts)
  }

  peek (range, opts) {
    return iteratorPeek(this.createRangeIterator(range, { ...opts, limit: 1 }))
  }

  createRangeIterator (range, opts = {}) {
    // backwards compat range arg
    opts = opts ? { ...opts, ...range } : range

    const extension = (opts.extension === false && opts.limit !== 0) ? null : this.extension
    const keyEncoding = opts.keyEncoding ? codecs(opts.keyEncoding) : this.keyEncoding

    if (extension) {
      const { onseq, onwait } = opts
      let version = 0
      let next = 0

      opts = encRange(keyEncoding, {
        ...opts,
        sub: this._sub,
        onseq (seq) {
          if (!version) version = seq + 1
          if (next) next--
          if (onseq) onseq(seq)
        },
        onwait (seq) {
          if (!next) {
            next = Extension.BATCH_SIZE
            extension.iterator(ite.snapshot(version))
          }
          if (onwait) onwait(seq)
        }
      })
    } else {
      opts = encRange(keyEncoding, { ...opts, sub: this._sub })
    }

    const ite = new RangeIterator(new Batch(this, this._makeSnapshot(), null, false, opts), null, opts)
    return ite
  }

  createReadStream (range, opts) {
    const signal = (opts && opts.signal) || null
    return iteratorToStream(this.createRangeIterator(range, opts), signal)
  }

  createHistoryStream (opts) {
    const session = (opts && opts.live) ? this.core.session() : this._makeSnapshot()
    const signal = (opts && opts.signal) || null
    return iteratorToStream(new HistoryIterator(new Batch(this, session, null, false, opts), opts), signal)
  }

  createDiffStream (right, range, opts) {
    if (typeof right === 'number') right = this.checkout(Math.max(1, right), { reuseSession: true })

    // backwards compat range arg
    opts = opts ? { ...opts, ...range } : range

    const signal = (opts && opts.signal) || null

    const keyEncoding = opts && opts.keyEncoding ? codecs(opts.keyEncoding) : this.keyEncoding
    if (keyEncoding) opts = encRange(keyEncoding, { ...opts, sub: this._sub })

    let done
    let closing
    let ite

    const left = this

    const rs = new Readable({
      signal,
      eagerOpen: true,
      async open (cb) {
        try {
          if (right.opened === false) await right.ready()
          if (left.opened === false) await left.ready()
        } catch (err) {
          cb(err)
          return
        }

        if (closing) {
          cb(null)
          return
        }

        if (left.core.closing || right.core.closing) {
          cb(new Error('Bee closed'))
          return
        }

        const snapshot = right.version > left.version
          ? right._makeSnapshot()
          : left._makeSnapshot()

        done = cb
        ite = new DiffIterator(
          new Batch(left, snapshot, null, false, opts),
          new Batch(right, snapshot, null, false, opts),
          opts
        )
        ite.open().then(fin, fin)
      },
      read (cb) {
        done = cb
        ite.next().then(push, fin)
      },
      predestroy () {
        if (!ite) {
          closing = Promise.resolve()
        } else {
          closing = ite.close()
          closing.catch(noop)
        }
      },
      destroy (cb) {
        done = cb
        if (!closing) closing = ite.close()
        closing.then(fin, fin)
      }
    })

    return rs

    function fin (err) {
      done(err)
    }

    function push (val) {
      rs.push(val)
      done(null)
    }
  }

  get (key, opts) {
    const b = new Batch(this, this._makeSnapshot(), null, true, opts)
    return b.get(key)
  }

  getBySeq (seq, opts) {
    const b = new Batch(this, this._makeSnapshot(), null, true, opts)
    return b.getBySeq(seq)
  }

  put (key, value, opts) {
    const b = new Batch(this, this.core, null, true, opts)
    return b.put(key, value, opts)
  }

  batch (opts) {
    return new Batch(this, this.core, mutexify(), true, opts)
  }

  del (key, opts) {
    const b = new Batch(this, this.core, null, true, opts)
    return b.del(key, opts)
  }

  watch (range, opts) {
    if (!this._watchers) throw new Error('Can only watch the main bee instance')
    return new Watcher(this, range, opts)
  }

  async getAndWatch (key, opts) {
    if (!this._watchers) throw new Error('Can only watch the main bee instance')

    const watcher = new EntryWatcher(this, key, opts)
    await watcher._debouncedUpdate()

    if (this.closing) {
      await watcher.close()
      throw new Error('Bee closed')
    }

    return watcher
  }

  _onappend () {
    for (const watcher of this._watchers) {
      watcher._onappend()
    }

    for (const watcher of this._entryWatchers) {
      watcher._onappend()
    }
  }

  _ontruncate (length) {
    for (const watcher of this._watchers) {
      watcher._ontruncate()
    }

    for (const watcher of this._entryWatchers) {
      watcher._ontruncate()
    }

    this._nodeCache.gc(length)
    this._keyCache.gc(length)
  }

  _makeSnapshot () {
    if (this._sessions === false) return this.core
    // TODO: better if we could encapsulate this in hypercore in the future
    return (this._checkout <= this.core.length || this._checkout <= 1) ? this.core.snapshot() : this.core.session({ snapshot: false })
  }

  async clearUnlinked (options = {}) {
    await this.ready()

    const { gte = 0, lt = this.version - 1, batchSize = 4096, wait = true } = options
    const checkout = this.version

    let prev = this.batch({ wait: false, checkout: gte })
    let b = this.batch({ wait, checkout })

    const iteBatch = this.batch({ wait: false, checkout })
    const ite = new LocalBlockIterator(iteBatch, { gte, lt })
    await ite.open()

    let ticks = 0

    while (true) {
      const data = await ite.next()
      if (!data) break

      if (!(await isLinked(b, data))) {
        if (b.core.closing || this.core.closing || this.closing) break
        await this.core.clear(data.seq)
      }

      const prevNode = await prev.get(data.key, { finalize: false }).catch(toNull)

      if (prevNode && !(await isLinked(b, prevNode))) {
        if (b.core.closing || this.core.closing || this.closing) break
        await this.core.clear(prevNode.seq)
      }

      if (ticks++ >= batchSize) {
        ticks = 0
        await prev.close()
        await b.close()

        prev = this.batch({ wait: false, checkout: gte })
        b = this.batch({ wait, checkout })
      }
    }

    if (b.core.closing || this.core.closing || this.closing) throw new Error('Core is closed')

    await b.close()
    await prev.close()
    await ite.close()
    await iteBatch.close()

    return lt
  }

  checkout (version, opts = {}) {
    if (version === 0) version = 1

    // same as above, just checkout isn't set yet...
    const snap = (opts.reuseSession || this._sessions === false)
      ? this.core
      : (version <= this.core.length || version <= 1) ? this.core.snapshot() : this.core.session({ snapshot: false })

    return new Hyperbee(snap, {
      _view: true,
      _sub: false,
      prefix: this.prefix,
      sep: this.sep,
      lock: this.lock,
      checkout: version,
      keyEncoding: opts.keyEncoding || this.keyEncoding,
      valueEncoding: opts.valueEncoding || this.valueEncoding,
      extension: this.extension !== null ? this.extension : false
    })
  }

  snapshot (opts) {
    return this.checkout((this.core.opened === false || this._checkout <= 0) ? -1 : Math.max(1, this.version), opts)
  }

  sub (prefix, opts = {}) {
    let sep = opts.sep || this.sep
    if (!b4a.isBuffer(sep)) sep = b4a.from(sep)

    prefix = b4a.concat([this.prefix || EMPTY, b4a.from(prefix), sep])

    const valueEncoding = codecs(opts.valueEncoding || this.valueEncoding)
    const keyEncoding = codecs(opts.keyEncoding || this._unprefixedKeyEncoding)

    return new Hyperbee(this.core, {
      _view: true,
      _sub: true,
      prefix,
      sep: this.sep,
      lock: this.lock,
      checkout: this._checkout,
      valueEncoding,
      keyEncoding,
      extension: this.extension !== null ? this.extension : false,
      metadata: this.metadata
    })
  }

  async getHeader (opts) {
    const blk = await this.core.get(0, opts)
    try {
      return blk && Header.decode(blk)
    } catch {
      throw DECODING_ERROR()
    }
  }

  async _close () {
    if (!this._view) {
      if (this._keyCache) this._keyCache.clear()
      if (this._nodeCache) this._nodeCache.clear()
    }

    if (this._watchers) {
      this.core.off('append', this._onappendBound)
      this.core.off('truncate', this._ontruncateBound)

      while (this._watchers.length) {
        await this._watchers[this._watchers.length - 1].close()
      }
    }

    if (this._entryWatchers) {
      while (this._entryWatchers.length) {
        await this._entryWatchers[this._entryWatchers.length - 1].close()
      }
    }

    while (this._batches.length) {
      await this._batches[this._batches.length - 1].close()
    }

    return this.core.close()
  }

  static async isHyperbee (core, opts) {
    await core.ready()

    const blk0 = await core.get(0, opts)
    if (blk0 === null) throw BLOCK_NOT_AVAILABLE()

    try {
      return Header.decode(blk0).protocol === 'hyperbee'
    } catch (err) { // undecodable
      return false
    }
  }
}

class Batch {
  constructor (tree, core, batchLock, cache, options = {}) {
    this.tree = tree
    // this.feed is now deprecated, and will be this.core going forward
    this.feed = core
    this.core = core
    this.index = tree._batches.push(this) - 1
    this.blocks = cache ? new Map() : null
    this.autoFlush = !batchLock
    this.maxBlocksCached = options.maxBlocksCached || 128
    this.rootSeq = 0
    this.root = null
    this.length = 0
    this.checkout = options.checkout === undefined ? -1 : options.checkout
    this.options = options
    this.locked = null
    this.batchLock = batchLock
    this.onseq = this.options.onseq || noop
    this.appending = null
    this.isSnapshot = this.core !== this.tree.core
    this.shouldUpdate = this.options.update !== false
    this.updating = null
    this.encoding = {
      key: options.keyEncoding ? codecs(options.keyEncoding) : tree.keyEncoding,
      value: options.valueEncoding ? codecs(options.valueEncoding) : tree.valueEncoding
    }
  }

  async ready () {
    if (this.core.opened === false) await this.core.ready()
    if (this.tree.opened === false) await this.tree.ready()
  }

  async lock () {
    if (this.tree.readonly) throw new Error('Hyperbee is marked as read-only')
    if (this.locked === null) this.locked = await this.tree.lock()
  }

  get version () {
    if (this.checkout !== -1) return Math.max(1, this.checkout)
    return Math.max(1, this.tree._checkout || (this.core.length + this.length))
  }

  async getRoot (ensureHeader) {
    await this.ready()
    if (ensureHeader) {
      if (this.core.length === 0 && this.core.writable && !this.tree.readonly) {
        await this.core.append(Header.encode({
          protocol: 'hyperbee',
          metadata: this.tree.metadata
        }))
      }
    }
    if (this.tree._checkout === 0 && this.checkout === -1 && this.shouldUpdate) {
      if (this.updating === null) this.updating = this.core.update()
      await this.updating
    }
    if (this.version < 2) return null
    return (await this.getBlock(this.version - 1)).getTreeNode(0)
  }

  async getKey (seq) {
    await this.tree._cacheLock.enter(seq)

    try {
      const k = this.core.fork === this.tree.core.fork ? this.tree._keyCache.get(seq) : null
      if (k !== null) return k
      const key = (await this._getBlock(seq)).key
      if (this.core.fork === this.tree.core.fork) this.tree._keyCache.set(seq, key)
      return key
    } finally {
      this.tree._cacheLock.exit(seq)
    }
  }

  async _getNode (seq) {
    const cached = (this.tree._nodeCache !== null && this.core.fork === this.tree.core.fork) ? this.tree._nodeCache.get(seq) : null
    if (cached !== null) return cached
    const entry = await this.core.get(seq, { ...this.options, valueEncoding: Node })
    if (entry === null) throw BLOCK_NOT_AVAILABLE()
    const wrap = copyEntry(entry)
    if (this.core.fork === this.tree.core.fork && this.tree._nodeCache !== null) this.tree._nodeCache.set(seq, wrap)
    return wrap
  }

  async getBlock (seq) {
    if (this.rootSeq === 0) this.rootSeq = seq
    await this.tree._cacheLock.enter(seq)

    try {
      return await this._getBlock(seq)
    } finally {
      this.tree._cacheLock.exit(seq)
    }
  }

  async _getBlock (seq) {
    let b = this.blocks && this.blocks.get(seq)
    if (b) return b
    this.onseq(seq)
    const entry = await this._getNode(seq)
    b = this.blocks && this.blocks.get(seq)
    if (b) return b
    b = new BlockEntry(seq, this, entry)
    if (this.blocks && (this.blocks.size - this.length) < this.maxBlocksCached) this.blocks.set(seq, b)
    return b
  }

  _onwait (key) {
    this.options.onwait = null
    this.tree.extension.get(this.rootSeq + 1, key)
  }

  _getEncoding (opts) {
    if (!opts) return this.encoding
    return {
      key: opts.keyEncoding ? codecs(opts.keyEncoding) : this.encoding.key,
      value: opts.valueEncoding ? codecs(opts.valueEncoding) : this.encoding.value
    }
  }

  peek (range, opts) {
    return iteratorPeek(this.createRangeIterator(range, { ...opts, limit: 1 }))
  }

  createRangeIterator (range, opts = {}) {
    // backwards compat range arg
    opts = opts ? { ...opts, ...range } : range

    const encoding = this._getEncoding(opts)
    return new RangeIterator(this, encoding, encRange(encoding.key, { ...opts, sub: this.tree._sub }))
  }

  createReadStream (range, opts) {
    const signal = (opts && opts.signal) || null
    return iteratorToStream(this.createRangeIterator(range, opts), signal)
  }

  async getBySeq (seq, opts) {
    const encoding = this._getEncoding(opts)

    try {
      const block = (await this.getBlock(seq)).final(encoding)
      return { key: block.key, value: block.value }
    } finally {
      await this._closeSnapshot()
    }
  }

  async get (key, opts) {
    const encoding = this._getEncoding(opts)
    const finalize = opts ? opts.finalize !== false : true

    try {
      return await this._get(key, encoding, finalize)
    } finally {
      await this._closeSnapshot()
    }
  }

  async _get (key, encoding, finalize) {
    key = enc(encoding.key, key)

    if (this.tree.extension !== null && this.options.extension !== false) {
      this.options.onwait = this._onwait.bind(this, key)
    }

    let node = await this.getRoot(false)
    if (!node) return null

    while (true) {
      if (node.block.isTarget(key)) {
        return node.block.isDeletion() ? null : (finalize ? node.block.final(encoding) : node.block)
      }

      let s = 0
      let e = node.keys.length
      let c

      while (s < e) {
        const mid = (s + e) >> 1

        c = b4a.compare(key, await node.getKey(mid))

        if (c === 0) {
          const block = await this.getBlock(node.keys[mid].seq)
          return finalize ? block.final(encoding) : block
        }

        if (c < 0) e = mid
        else s = mid + 1
      }

      if (!node.children.length) return null

      const i = c < 0 ? e : s
      node = await node.getChildNode(i)
    }
  }

  async links (key, seq) {
    let node = await this.getRoot(false)
    if (!node) return false

    if (node.block.seq === seq) return true

    while (true) {
      if (node.block.isTarget(key)) return false

      let s = 0
      let e = node.keys.length
      let c

      while (s < e) {
        const mid = (s + e) >> 1

        if (node.keys[mid].seq === seq) return true
        c = b4a.compare(key, await node.getKey(mid))

        if (c === 0) return false

        if (c < 0) e = mid
        else s = mid + 1
      }

      if (!node.children.length) return false

      const i = c < 0 ? e : s
      node = await node.getChildNode(i)
      if (node.block.seq === seq) return true
    }
  }

  async put (key, value, opts) {
    const release = this.batchLock ? await this.batchLock() : null

    const cas = (opts && opts.cas) || null
    const encoding = this._getEncoding(opts)

    if (!this.locked) await this.lock()
    if (!release) return this._put(key, value, encoding, cas)

    try {
      return await this._put(key, value, encoding, cas)
    } finally {
      release()
    }
  }

  async _put (key, value, encoding, cas) {
    const newNode = {
      seq: 0,
      key,
      value
    }
    key = enc(encoding.key, key)
    value = enc(encoding.value, value)

    const stack = []

    let root
    let node = root = await this.getRoot(true)
    if (!node) node = root = TreeNode.create(null)

    const seq = newNode.seq = this.core.length + this.length
    const target = new Key(seq, key)

    while (node.children.length) {
      stack.push(node)
      node.changed = true // changed, but compressible

      let s = 0
      let e = node.keys.length
      let c

      while (s < e) {
        const mid = (s + e) >> 1
        c = b4a.compare(target.value, await node.getKey(mid))

        if (c === 0) {
          if (cas) {
            const prev = await node.getKeyNode(mid)
            if (!(await cas(prev.final(encoding), newNode))) return this._unlockMaybe()
          }
          if (!this.tree.alwaysDuplicate) {
            const prev = await node.getKeyNode(mid)
            if (sameValue(prev.value, value)) return this._unlockMaybe()
          }
          node.setKey(mid, target)
          return this._append(root, seq, key, value)
        }

        if (c < 0) e = mid
        else s = mid + 1
      }

      const i = c < 0 ? e : s
      node = await node.getChildNode(i)
    }

    let needsSplit = !(await node.insertKey(target, value, null, newNode, encoding, cas))
    if (!node.changed) return this._unlockMaybe()

    while (needsSplit) {
      const parent = stack.pop()
      const { median, right } = await node.split()

      if (parent) {
        needsSplit = !(await parent.insertKey(median, value, right, null, encoding, null))
        node = parent
      } else {
        root = TreeNode.create(node.block)
        root.changed = true
        root.keys.push(median)
        root.children.push(new Child(0, 0, node), new Child(0, 0, right))
        needsSplit = false
      }
    }

    return this._append(root, seq, key, value)
  }

  async del (key, opts) {
    const release = this.batchLock ? await this.batchLock() : null
    const cas = (opts && opts.cas) || null
    const encoding = this._getEncoding(opts)

    if (!this.locked) await this.lock()
    if (!release) return this._del(key, encoding, cas)

    try {
      return await this._del(key, encoding, cas)
    } finally {
      release()
    }
  }

  async _del (key, encoding, cas) {
    const delNode = {
      seq: 0,
      key,
      value: null
    }

    key = enc(encoding.key, key)

    const stack = []

    let node = await this.getRoot(true)
    if (!node) return this._unlockMaybe()

    const seq = delNode.seq = this.core.length + this.length

    while (true) {
      stack.push(node)

      let s = 0
      let e = node.keys.length
      let c

      while (s < e) {
        const mid = (s + e) >> 1
        c = b4a.compare(key, await node.getKey(mid))

        if (c === 0) {
          if (cas) {
            const prev = await node.getKeyNode(mid)
            if (!(await cas(prev.final(encoding), delNode))) return this._unlockMaybe()
          }
          if (node.children.length) await setKeyToNearestLeaf(node, mid, stack)
          else node.removeKey(mid)
          // we mark these as changed late, so we don't rewrite them if it is a 404
          for (const node of stack) node.changed = true
          return this._append(await rebalance(stack), seq, key, null)
        }

        if (c < 0) e = mid
        else s = mid + 1
      }

      if (!node.children.length) return this._unlockMaybe()

      const i = c < 0 ? e : s
      node = await node.getChildNode(i)
    }
  }

  async _closeSnapshot () {
    if (this.isSnapshot) {
      await this.core.close()
      this._finalize()
    }
  }

  async close () {
    if (this.isSnapshot) return this._closeSnapshot()

    this.root = null
    if (this.blocks) this.blocks.clear()
    this.length = 0
    this._unlock()
  }

  destroy () { // compat, remove later
    this.close().catch(noop)
  }

  toBlocks () {
    if (this.appending) return this.appending

    const batch = new Array(this.length)

    for (let i = 0; i < this.length; i++) {
      const seq = this.core.length + i
      const { pendingIndex, key, value } = this.blocks.get(seq)

      if (i < this.length - 1) {
        pendingIndex[0] = null
        let j = 0

        while (j < pendingIndex.length) {
          const idx = pendingIndex[j]
          if (idx !== null && idx.seq === seq) {
            idx.offset = j++
            continue
          }
          if (j === pendingIndex.length - 1) pendingIndex.pop()
          else pendingIndex[j] = pendingIndex.pop()
        }
      }

      batch[i] = Node.encode({
        key,
        value,
        index: deflate(pendingIndex)
      })
    }

    this.appending = batch
    return batch
  }

  flush () {
    if (!this.length) return this.close()

    const batch = this.toBlocks()

    this.root = null
    this.blocks.clear()
    this.length = 0

    return this._appendBatch(batch)
  }

  _unlockMaybe () {
    if (this.autoFlush) this._unlock()
  }

  _unlock () {
    const locked = this.locked
    this.locked = null
    if (locked !== null) locked()
    this._finalize()
  }

  _finalize () {
    // technically finalize can be called more than once, so here we just check if we already have been removed
    if (this.index >= this.tree._batches.length || this.tree._batches[this.index] !== this) return
    const top = this.tree._batches.pop()
    if (top === this) return
    top.index = this.index
    this.tree._batches[top.index] = top
  }

  _append (root, seq, key, value) {
    const index = []
    root.indexChanges(index, seq)
    index[0] = new Child(seq, 0, root)

    if (!this.autoFlush) {
      const block = new BatchEntry(seq, this, key, value, index)
      root.block = block
      this.root = root
      this.length++
      this.blocks.set(seq, block)

      root.updateChildren(seq, block)
      return
    }

    return this._appendBatch(Node.encode({
      key,
      value,
      index: deflate(index)
    }))
  }

  async _appendBatch (raw) {
    try {
      await this.core.append(raw)
    } finally {
      this._unlock()
    }
  }
}

class EntryWatcher extends ReadyResource {
  constructor (bee, key, opts = {}) {
    super()

    this.keyEncoding = opts.keyEncoding || bee.keyEncoding
    this.valueEncoding = opts.valueEncoding || bee.valueEncoding

    this.index = bee._entryWatchers.push(this) - 1
    this.bee = bee

    this.key = key
    this.node = null

    this._forceUpdate = false
    this._debouncedUpdate = debounce(this._processUpdate.bind(this))
  }

  _close () {
    const top = this.bee._entryWatchers.pop()
    if (top !== this) {
      top.index = this.index
      this.bee._entryWatchers[top.index] = top
    }
  }

  _onappend () {
    this._debouncedUpdate()
  }

  _ontruncate () {
    this._forceUpdate = true
    this._debouncedUpdate()
  }

  async _processUpdate () {
    const force = this._forceUpdate
    this._forceUpdate = false

    let newNode
    try {
      newNode = await this.bee.get(this.key, {
        keyEncoding: this.keyEncoding,
        valueEncoding: this.valueEncoding
      })
    } catch (e) {
      if (e.code === 'SNAPSHOT_NOT_AVAILABLE') {
        // There was a truncate event before the get resolved
        // So this handler will run again anyway
        return
      } else if (this.bee.closing) {
        this.close().catch(safetyCatch)
        return
      }
      this.emit('error', e)
      return
    }

    if (force || newNode?.seq !== this.node?.seq) {
      this.node = newNode
      this.emit('update')
    }
  }
}

class Watcher extends ReadyResource {
  constructor (bee, range, opts = {}) {
    super()

    this.keyEncoding = opts.keyEncoding || bee.keyEncoding
    this.valueEncoding = opts.valueEncoding || bee.valueEncoding
    this.index = bee._watchers.push(this) - 1
    this.bee = bee
    this.core = bee.core

    this.latestDiff = 0
    this.range = range
    this.map = opts.map || defaultWatchMap

    this.current = null
    this.previous = null
    this.currentMapped = null
    this.previousMapped = null
    this.stream = null

    this._lock = mutexify()
    this._flowing = false
    this._resolveOnChange = null
    this._differ = opts.differ || defaultDiffer
    this._eager = !!opts.eager
    this._onchange = opts.onchange || null

    this.on('newListener', autoFlowOnUpdate)

    this.ready().catch(safetyCatch)
  }

  async _consume () {
    if (this._flowing) return
    try {
      for await (const _ of this) {} // eslint-disable-line
    } catch {}
  }

  async _open () {
    await this.bee.ready()

    const opts = {
      keyEncoding: this.keyEncoding,
      valueEncoding: this.valueEncoding
    }

    // Point from which to start watching
    this.current = this._eager ? this.bee.checkout(1, opts) : this.bee.snapshot(opts)
    await this.current.ready()

    if (this._onchange) {
      if (this._eager) await this._onchange()
      this._consume()
    }
  }

  [Symbol.asyncIterator] () {
    this._flowing = true
    return this
  }

  _ontruncate () {
    this._onappend()
  }

  _onappend () {
    const resolve = this._resolveOnChange
    this._resolveOnChange = null
    if (resolve) resolve()
  }

  async _waitForChanges () {
    if (this.current.version < this.bee.version || this.closing) return

    await new Promise(resolve => {
      this._resolveOnChange = resolve
    })
  }

  async next () {
    try {
      return await this._next()
    } catch (err) {
      if (this.closing) return { value: undefined, done: true }
      await this.close()
      throw err
    }
  }

  async _next () {
    const release = await this._lock()

    try {
      if (this.closing) return { value: undefined, done: true }

      if (!this.opened) await this.ready()

      while (true) {
        await this._waitForChanges()

        if (this.closing) return { value: undefined, done: true }

        await this._closePrevious()
        this.previous = this.current.snapshot()

        await this._closeCurrent()
        this.current = this.bee.snapshot({
          keyEncoding: this.keyEncoding,
          valueEncoding: this.valueEncoding
        })

        await this.current.ready()
        await this.previous.ready()

        if (this.current.core.fork !== this.previous.core.fork) {
          return await this._yield()
        }

        this.stream = this._differ(this.current, this.previous, this.range)

        try {
          for await (const data of this.stream) { // eslint-disable-line
            return await this._yield()
          }
        } finally {
          this.stream = null
        }
      }
    } finally {
      release()
    }
  }

  async _yield () {
    this.currentMapped = this.map(this.current)
    this.previousMapped = this.map(this.previous)

    if (this._onchange) {
      try {
        await this._onchange()
      } catch (err) {
        safetyCatch(err)
      }
    }

    this.emit('update')
    return { done: false, value: [this.currentMapped, this.previousMapped] }
  }

  async return () {
    await this.close()
    return { done: true }
  }

  async _close () {
    const top = this.bee._watchers.pop()
    if (top !== this) {
      top.index = this.index
      this.bee._watchers[top.index] = top
    }

    if (this.stream && !this.stream.destroying) {
      this.stream.destroy()
    }

    this._onappend() // Continue execution being closed

    await this._closeCurrent().catch(safetyCatch)
    await this._closePrevious().catch(safetyCatch)

    const release = await this._lock()
    release()
  }

  destroy () {
    return this.close()
  }

  async _closeCurrent () {
    if (this.currentMapped) await this.currentMapped.close()
    if (this.current) await this.current.close()
    this.current = this.currentMapped = null
  }

  async _closePrevious () {
    if (this.previousMapped) await this.previousMapped.close()
    if (this.previous) await this.previous.close()
    this.previous = this.previousMapped = null
  }
}

function autoFlowOnUpdate (name) {
  if (name === 'update') this._consume()
}

function defaultWatchMap (snapshot) {
  return snapshot
}

async function leafSize (node, goLeft) {
  while (node.children.length) node = await node.getChildNode(goLeft ? 0 : node.children.length - 1)
  return node.keys.length
}

async function setKeyToNearestLeaf (node, index, stack) {
  let [left, right] = await Promise.all([node.getChildNode(index), node.getChildNode(index + 1)])
  const [ls, rs] = await Promise.all([leafSize(left, false), leafSize(right, true)])

  if (ls < rs) { // if fewer leaves on the left
    stack.push(right)
    while (right.children.length) stack.push(right = right.children[0].value)
    node.keys[index] = right.keys.shift()
  } else { // if fewer leaves on the right
    stack.push(left)
    while (left.children.length) stack.push(left = left.children[left.children.length - 1].value)
    node.keys[index] = left.keys.pop()
  }
}

async function rebalance (stack) {
  const root = stack[0]

  while (stack.length > 1) {
    const node = stack.pop()
    const parent = stack[stack.length - 1]

    if (node.keys.length >= MIN_KEYS) return root

    let { left, index, right } = await node.siblings(parent)

    // maybe borrow from left sibling?
    if (left && left.keys.length > MIN_KEYS) {
      left.changed = true
      node.keys.unshift(parent.keys[index - 1])
      if (left.children.length) node.children.unshift(left.children.pop())
      parent.keys[index - 1] = left.keys.pop()
      return root
    }

    // maybe borrow from right sibling?
    if (right && right.keys.length > MIN_KEYS) {
      right.changed = true
      node.keys.push(parent.keys[index])
      if (right.children.length) node.children.push(right.children.shift())
      parent.keys[index] = right.keys.shift()
      return root
    }

    // merge node with another sibling
    if (left) {
      index--
      right = node
    } else {
      left = node
    }

    left.merge(right, parent.keys[index])
    parent.removeKey(index)
  }

  // check if the tree shrunk
  if (!root.keys.length && root.children.length) return root.getChildNode(0)
  return root
}

function iteratorToStream (ite, signal) {
  let done
  let closing

  const rs = new Readable({
    signal,
    open (cb) {
      done = cb
      ite.open().then(fin, fin)
    },
    read (cb) {
      done = cb
      ite.next().then(push, fin)
    },
    predestroy () {
      closing = ite.close()
      closing.catch(noop)
    },
    destroy (cb) {
      done = cb
      if (!closing) closing = ite.close()
      closing.then(fin, fin)
    }
  })

  return rs

  function fin (err) {
    done(err)
  }

  function push (val) {
    rs.push(val)
    done(null)
  }
}

async function iteratorPeek (ite) {
  try {
    await ite.open()
    return await ite.next()
  } finally {
    await ite.close()
  }
}

function encRange (e, opts) {
  if (!e) return opts

  if (e.encodeRange) {
    const r = e.encodeRange({ gt: opts.gt, gte: opts.gte, lt: opts.lt, lte: opts.lte })
    opts.gt = r.gt
    opts.gte = r.gte
    opts.lt = r.lt
    opts.lte = r.lte
    return opts
  }

  if (opts.gt !== undefined) opts.gt = enc(e, opts.gt)
  if (opts.gte !== undefined) opts.gte = enc(e, opts.gte)
  if (opts.lt !== undefined) opts.lt = enc(e, opts.lt)
  if (opts.lte !== undefined) opts.lte = enc(e, opts.lte)
  if (opts.sub && !opts.gt && !opts.gte) opts.gt = enc(e, SEP)
  if (opts.sub && !opts.lt && !opts.lte) opts.lt = bump(enc(e, EMPTY))

  return opts
}

function bump (key) {
  // key should have been copied by enc above before hitting this
  key[key.length - 1]++
  return key
}

function enc (e, v) {
  if (v === undefined || v === null) return null
  if (e !== null) return e.encode(v)
  if (typeof v === 'string') return b4a.from(v)
  return v
}

function prefixEncoding (prefix, keyEncoding) {
  return {
    encode (key) {
      return b4a.concat([prefix, b4a.isBuffer(key) ? key : enc(keyEncoding, key)])
    },
    decode (key) {
      const sliced = key.slice(prefix.length, key.length)
      return keyEncoding ? keyEncoding.decode(sliced) : sliced
    }
  }
}

function copyEntry (entry) {
  let key = entry.key
  let value = entry.value
  let index = entry.index

  // key, value and index all refer to the same buffer (one hypercore block)
  // If together they are larger than half the buffer's byteLength,
  // this means that they got their own private slab (see Buffer.allocUnsafe docs)
  // so no need to unslab
  const size = key.byteLength + (value === null ? 0 : value.byteLength) + (index === null ? 0 : index.byteLength)
  if (2 * size < key.buffer.byteLength) {
    const [newKey, newValue, newIndex] = unslabAll([entry.key, entry.value, entry.index])
    key = newKey
    value = newValue
    index = newIndex
  }

  return {
    key,
    value,
    index,
    inflated: null
  }
}

function defaultDiffer (currentSnap, previousSnap, opts) {
  return currentSnap.createDiffStream(previousSnap, opts)
}

function toNull () {
  return null
}

function getBackingCore (core) {
  if (core.core) return core
  if (core.getBackingCore) return core.getBackingCore().session
  return null
}

function sameValue (a, b) {
  return a === b || (a !== null && b !== null && b4a.equals(a, b))
}

function noop () {}

function getExtension (db, opts) {
  if (opts.extension === false) return null
  if (opts.extension && opts.extension !== true) return opts.extension
  return Extension.register(db)
}

async function isLinked (batch, block) {
  const seq = block.seq
  block.inflate()

  const keys = [block.key]
  const wait = batch.options.wait

  for (const l of block.index.levels) {
    if (!l.keys.length) continue
    batch.options.wait = false
    try {
      keys.push(await batch.getKey(l.keys[0].seq))
    } catch {}
    batch.options.wait = wait
  }

  for (const k of keys) {
    if (await batch.links(k, seq)) return true
  }

  if (batch.core.closing) throw new Error('Core is closed')

  return false
}

module.exports = Hyperbee
const b4a = require('b4a')

class SubTree {
  constructor (node, parent) {
    this.node = node
    this.parent = parent

    this.isKey = node.children.length === 0
    this.i = this.isKey ? 1 : 0
    this.n = 0

    const child = this.isKey ? null : this.node.children[0]
    this.seq = child !== null ? child.seq : this.node.keys[0].seq
    this.offset = child !== null ? child.offset : 0
  }

  next () {
    this.i++
    this.isKey = (this.i & 1) === 1
    if (!this.isKey && !this.node.children.length) this.i++
    return this.update()
  }

  async bisect (key, incl) {
    let s = 0
    let e = this.node.keys.length
    let c

    while (s < e) {
      const mid = (s + e) >> 1
      c = cmp(key, await this.node.getKey(mid))

      if (c === 0) {
        if (incl) this.i = mid * 2 + 1
        else this.i = mid * 2 + (this.node.children.length ? 2 : 3)
        return true
      }

      if (c < 0) e = mid
      else s = mid + 1
    }

    const i = c < 0 ? e : s
    this.i = 2 * i + (this.node.children.length ? 0 : 1)
    return this.node.children.length === 0
  }

  update () {
    this.isKey = (this.i & 1) === 1
    this.n = this.i >> 1
    if (this.n >= (this.isKey ? this.node.keys.length : this.node.children.length)) return false
    const child = this.isKey ? null : this.node.children[this.n]
    this.seq = child !== null ? child.seq : this.node.keys[this.n].seq
    this.offset = child !== null ? child.offset : 0
    return true
  }

  async key () {
    return this.n < this.node.keys.length ? this.node.getKey(this.n) : (this.parent && this.parent.key())
  }

  async compare (tree) {
    const [a, b] = await Promise.all([this.key(), tree.key()])
    return cmp(a, b)
  }
}

class TreeIterator {
  constructor (batch, opts) {
    this.batch = batch
    this.stack = []
    this.lt = opts.lt || opts.lte || null
    this.lte = !!opts.lte
    this.gt = opts.gt || opts.gte || null
    this.gte = !!opts.gte
    this.seeking = !!this.gt
    this.encoding = opts.encoding || batch.encoding
  }

  async open () {
    const node = await this.batch.getRoot(false)
    if (!node || !node.keys.length) return
    const tree = new SubTree(node, null)
    if (this.seeking && !(await this._seek(tree))) return
    this.stack.push(tree)
  }

  async _seek (tree) {
    const done = await tree.bisect(this.gt, this.gte)
    const oob = !tree.update()
    if (done || oob) {
      this.seeking = false
      if (oob) return false
    }
    return true
  }

  peek () {
    if (!this.stack.length) return null
    return this.stack[this.stack.length - 1]
  }

  skip () {
    if (!this.stack.length) return
    if (!this.stack[this.stack.length - 1].next()) this.stack.pop()
  }

  async nextKey () {
    let n = null
    while (this.stack.length && n === null) n = await this.next()
    if (n === null) return null
    if (!this.lt) return n.final(this.encoding)

    const c = cmp(n.key, this.lt)
    if (this.lte ? c <= 0 : c < 0) return n.final(this.encoding)
    this.stack = []
    return null
  }

  async next () {
    if (!this.stack.length) return null

    const top = this.stack[this.stack.length - 1]
    const { isKey, n, seq } = top

    if (!top.next()) {
      this.stack.pop()
    }

    if (isKey) {
      this.seeking = false
      return this.batch.getBlock(seq)
    }

    const child = await top.node.getChildNode(n)
    top.node.children[n] = null // unlink to save memory
    const tree = new SubTree(child, top)
    if (this.seeking && !(await this._seek(tree))) return null
    this.stack.push(tree)

    return null
  }

  close () {
    return this.batch._closeSnapshot()
  }
}

module.exports = class DiffIterator {
  constructor (left, right, opts = {}) {
    this.left = new TreeIterator(left, opts)
    this.right = new TreeIterator(right, opts)
    this.limit = typeof opts.limit === 'number' ? opts.limit : -1
  }

  async open () {
    await Promise.all([this.left.open(), this.right.open()])
  }

  async next () {
    if (this.limit === 0) return null
    const res = await this._next()
    if (!res || (res.left === null && res.right === null)) return null
    this.limit--
    return res
  }

  async _next () {
    const a = this.left
    const b = this.right

    while (true) {
      const [l, r] = await Promise.all([a.peek(), b.peek()])

      if (!l && !r) return null
      if (!l) return { left: null, right: await b.nextKey() }
      if (!r) return { left: await a.nextKey(), right: null }

      if (l.seq === r.seq && l.isKey === r.isKey && l.offset === r.offset) {
        a.skip()
        b.skip()
        continue
      }

      const c = await l.compare(r)

      if (l.isKey && !r.isKey) {
        await b.next()
        continue
      }

      if (!l.isKey && r.isKey) {
        await a.next()
        continue
      }

      if (l.isKey && r.isKey) {
        if (c === 0) return { left: await a.nextKey(), right: await b.nextKey() }
        if (c < 0) return { left: await a.nextKey(), right: null }
        return { left: null, right: await b.nextKey() }
      }

      if (c === 0) await Promise.all([a.next(), b.next()])
      else if (c < 0) await b.next()
      else await a.next()
    }
  }

  async close () {
    await Promise.all([this.left.close(), this.right.close()])
  }
}

function cmp (a, b) {
  if (!a) return b ? 1 : 0
  if (!b) return a ? -1 : 0
  return b4a.compare(a, b)
}
module.exports = class HistoryIterator {
  constructor (batch, opts = {}) {
    this.batch = batch
    this.options = opts
    this.live = !!opts.live
    this.gte = 0
    this.lt = 0
    this.reverse = !!opts.reverse
    this.limit = typeof opts.limit === 'number' ? opts.limit : -1
    this.encoding = opts.encoding || batch.encoding
    if (this.live && this.reverse) {
      throw new Error('Cannot have both live and reverse enabled')
    }
  }

  async open () {
    await this.batch.getRoot(false) // does the update dance
    this.gte = gte(this.options, this.batch.version)
    this.lt = this.live ? Infinity : lt(this.options, this.batch.version)
  }

  async next () {
    if (this.limit === 0) return null
    if (this.limit > 0) this.limit--

    if (this.gte >= this.lt) return null

    if (this.reverse) {
      if (this.lt <= 1) return null
      return final(await this.batch.getBlock(--this.lt), this.encoding)
    }

    return final(await this.batch.getBlock(this.gte++), this.encoding)
  }

  close () {
    return this.batch._closeSnapshot()
  }
}

function final (node, encoding) {
  const type = node.isDeletion() ? 'del' : 'put'
  return { type, ...node.final(encoding) }
}

function gte (opts, version) {
  if (opts.gt) return (opts.gt < 0 ? (opts.gt + version) : opts.gt) + 1
  const gte = opts.gte || opts.since || 1
  return gte < 0 ? gte + version : gte
}

function lt (opts, version) {
  if (opts.lte === 0 || opts.lt === 0 || opts.end === 0) return 0
  if (opts.lte) return (opts.lte < 0 ? (opts.lte + version) : opts.lte) + 1
  const lt = opts.lt || opts.end || version
  return lt < 0 ? lt + version : lt
}
module.exports = class LocalBlocksIterator {
  constructor (batch, opts = {}) {
    this.batch = batch
    this.options = opts
    this.gte = 0
    this.lt = 0
    this.limit = typeof opts.limit === 'number' ? opts.limit : -1
  }

  async open () {
    await this.batch.getRoot(false) // does the update dance
    this.gte = gte(this.options, this.batch.version)
    this.lt = lt(this.options, this.batch.version)
  }

  async next () {
    if (this.limit === 0) return null
    if (this.limit > 0) this.limit--

    while (this.gte < this.lt) {
      try {
        return await this.batch.getBlock(this.gte++)
      } catch {
        continue
      }
    }

    return null
  }

  close () {
    return this.batch._closeSnapshot()
  }
}

function gte (opts, version) {
  if (opts.gt) return (opts.gt < 0 ? (opts.gt + version) : opts.gt) + 1
  const gte = opts.gte || opts.since || 1
  return gte < 0 ? gte + version : gte
}

function lt (opts, version) {
  if (opts.lte === 0 || opts.lt === 0 || opts.end === 0) return 0
  if (opts.lte) return (opts.lte < 0 ? (opts.lte + version) : opts.lte) + 1
  const lt = opts.lt || opts.end || version
  return lt < 0 ? lt + version : lt
}
const b4a = require('b4a')

module.exports = class RangeIterator {
  constructor (batch, encoding, opts = {}) {
    this.batch = batch
    this.stack = []
    this.opened = false
    this.encoding = encoding || batch.encoding

    this._limit = typeof opts.limit === 'number' ? opts.limit : -1
    this._gIncl = !opts.gt
    this._gKey = opts.gt || opts.gte || null
    this._lIncl = !opts.lt
    this._lKey = opts.lt || opts.lte || null
    this._reverse = !!opts.reverse
    this._version = 0
    this._checkpoint = (opts.checkpoint && opts.checkpoint.length) ? opts.checkpoint : null
    this._nexting = false
  }

  snapshot (version = this.batch.version) {
    const checkpoint = []
    for (const s of this.stack) {
      let { node, i } = s
      if (this._nexting && s === this.stack[this.stack.length - 1]) i = this._reverse ? i + 1 : i - 1
      if (!node.block) continue
      if (i < 0) continue
      checkpoint.push(node.block.seq, node.offset, i)
    }

    return {
      version,
      gte: this._gIncl ? this._gKey : null,
      gt: this._gIncl ? null : this._gKey,
      lte: this._lIncl ? this._lKey : null,
      lt: this._lIncl ? null : this._lKey,
      limit: this._limit,
      reverse: this._reverse,
      ended: this.opened && !checkpoint.length,
      checkpoint: this.opened ? checkpoint : []
    }
  }

  async open () {
    await this._open()
    this.opened = true
  }

  async _open () {
    if (this._checkpoint) {
      for (let j = 0; j < this._checkpoint.length; j += 3) {
        const seq = this._checkpoint[j]
        const offset = this._checkpoint[j + 1]
        const i = this._checkpoint[j + 2]
        this.stack.push({
          node: (await this.batch.getBlock(seq)).getTreeNode(offset),
          i
        })
      }
      return
    }

    this._nexting = true

    let node = await this.batch.getRoot(false)
    if (!node) {
      this._nexting = false
      return
    }

    const incl = this._reverse ? this._lIncl : this._gIncl
    const start = this._reverse ? this._lKey : this._gKey

    if (!start) {
      this.stack.push({ node, i: this._reverse ? node.keys.length << 1 : 0 })
      this._nexting = false
      return
    }

    while (true) {
      const entry = { node, i: this._reverse ? node.keys.length << 1 : 0 }

      let s = 0
      let e = node.keys.length
      let c

      while (s < e) {
        const mid = (s + e) >> 1
        c = b4a.compare(start, await node.getKey(mid))

        if (c === 0) {
          if (incl) entry.i = mid * 2 + 1
          else entry.i = mid * 2 + (this._reverse ? 0 : 2)
          this.stack.push(entry)
          this._nexting = false
          return
        }

        if (c < 0) e = mid
        else s = mid + 1
      }

      const i = c < 0 ? e : s
      entry.i = 2 * i + (this._reverse ? -1 : 1)

      if (entry.i >= 0 && entry.i <= (node.keys.length << 1)) this.stack.push(entry)
      if (!node.children.length) {
        this._nexting = false
        return
      }

      node = await node.getChildNode(i)
    }
  }

  async next () {
    // TODO: this nexting flag is only needed if someone asks for a snapshot during
    // a lookup (ie the extension, pretty important...).
    // A better solution would be to refactor this so top.i is incremented eagerly
    // to get the current block instead of the way it is done now (++i vs i++)
    this._nexting = true

    const end = this._reverse ? this._gKey : this._lKey
    const incl = this._reverse ? this._gIncl : this._lIncl

    while (this.stack.length && (this._limit === -1 || this._limit > 0)) {
      const top = this.stack[this.stack.length - 1]
      const isKey = (top.i & 1) === 1
      const n = this._reverse
        ? (top.i < 0 ? top.node.keys.length : top.i-- >> 1)
        : top.i++ >> 1

      if (!isKey) {
        if (!top.node.children.length) continue
        const node = await top.node.getChildNode(n)
        if (top.node.block.seq < this.batch.core.length) {
          top.node.children[n].value = null // unlink it to save memory
        }
        this.stack.push({ i: this._reverse ? node.keys.length << 1 : 0, node })
        continue
      }

      if (n >= top.node.keys.length) {
        this.stack.pop()
        continue
      }

      const key = top.node.keys[n]
      const block = await this.batch.getBlock(key.seq)
      if (end) {
        const c = b4a.compare(block.key, end)
        if (c === 0 ? !incl : (this._reverse ? c < 0 : c > 0)) {
          this._limit = 0
          break
        }
      }
      if (this._limit > 0) this._limit--
      this._nexting = false
      return block.final(this.encoding)
    }

    this._nexting = false
    return null
  }

  close () {
    return this.batch._closeSnapshot()
  }
}
const { Extension } = require('./messages')

// const MAX_ACTIVE = 32
const FLUSH_BATCH = 128
const MAX_PASSIVE_BATCH = 2048
const MAX_ACTIVE_BATCH = MAX_PASSIVE_BATCH + FLUSH_BATCH

class Batch {
  constructor (outgoing, from) {
    this.blocks = []
    this.start = 0
    this.end = 0
    this.outgoing = outgoing
    this.from = from
  }

  push (seq) {
    const len = this.blocks.push(seq)
    if (len === 1 || seq < this.start) this.start = seq
    if (len === 1 || seq >= this.end) this.end = seq + 1
    if (len >= FLUSH_BATCH) {
      this.send()
      this.clear()
    }
  }

  send () {
    if (!this.blocks.length) return
    this.outgoing.send(Extension.encode({ cache: { blocks: this.blocks, start: this.start, end: this.end } }), this.from)
  }

  clear () {
    this.start = this.end = 0
    this.blocks = []
  }
}

class HyperbeeExtension {
  constructor (db) {
    this.encoding = null
    this.outgoing = null
    this.db = db
    this.active = 0
  }

  get (version, key) {
    this.outgoing.broadcast(Extension.encode({ get: { version, key } }))
  }

  iterator (snapshot) {
    if (snapshot.ended) return
    if (snapshot.limit === 0) return
    if (snapshot.limit === -1) snapshot.limit = 0
    this.outgoing.broadcast(Extension.encode({ iterator: snapshot }))
  }

  onmessage (buf, from) {
    // TODO: handle max active extension messages
    // this.active++

    const message = decode(buf)
    if (!message) return

    if (message.cache) this.oncache(message.cache, from)
    if (message.get) this.onget(message.get, from)
    if (message.iterator) this.oniterator(message.iterator, from)
  }

  oncache (message, from) {
    if (!message.blocks.length) return
    this.db.core.download(message)
  }

  onget (message, from) {
    if (!message.version || message.version > this.db.version) return

    const b = new Batch(this.outgoing, from)
    const db = this.db.checkout(message.version)

    db.get(message.key, { extension: false, wait: false, update: false, onseq }).then(done, done)

    function done () {
      db.close().catch(noop)
      b.send()
    }

    function onseq (seq) {
      b.push(seq)
    }
  }

  async oniterator (message, from) {
    if (!message.version || message.version > this.db.version) return

    const b = new Batch(this.outgoing, from)
    const seqs = new Set()

    let skip = message.checkpoint.length
    let work = 0

    const db = this.db.checkout(message.version)
    const ite = db.createRangeIterator({
      ...message,
      wait: false,
      extension: false,
      update: false,
      limit: message.limit === 0 ? -1 : message.limit,
      onseq (seq) {
        if (skip && skip--) return
        if (seqs.has(seq)) return
        work++
        seqs.add(seq)
        b.push(seq)
      }
    })

    try {
      await ite.open()
      // eslint-disable-next-line no-unmodified-loop-condition
      while (work < MAX_ACTIVE_BATCH) {
        if (!(await ite.next())) break
      }
    } catch (_) {
      // do nothing
    } finally {
      ite.close().catch(noop)
      db.close().catch(noop)
      b.send()
    }
  }

  static register (db) {
    const e = new this(db)
    e.outgoing = db.core.registerExtension('hyperbee', e)
    return e
  }
}

HyperbeeExtension.BATCH_SIZE = MAX_PASSIVE_BATCH

module.exports = HyperbeeExtension

function decode (buf) {
  try {
    return Extension.decode(buf)
  } catch (err) {
    return null
  }
}

function noop () {}
// This file is auto generated by the protocol-buffers compiler

/* eslint-disable quotes */
/* eslint-disable indent */
/* eslint-disable no-redeclare */
/* eslint-disable camelcase */
/* eslint-disable no-var */

// Remember to `npm install --save protocol-buffers-encodings`
var encodings = require('protocol-buffers-encodings')
var b4a = require('b4a')
var varint = encodings.varint
var skip = encodings.skip

var YoloIndex = exports.YoloIndex = {
  buffer: true,
  encodingLength: null,
  encode: null,
  decode: null
}

var Header = exports.Header = {
  buffer: true,
  encodingLength: null,
  encode: null,
  decode: null
}

var Node = exports.Node = {
  buffer: true,
  encodingLength: null,
  encode: null,
  decode: null
}

var Extension = exports.Extension = {
  buffer: true,
  encodingLength: null,
  encode: null,
  decode: null
}

defineYoloIndex()
defineHeader()
defineNode()
defineExtension()

function defineYoloIndex () {
  var Level = YoloIndex.Level = {
    buffer: true,
    encodingLength: null,
    encode: null,
    decode: null
  }

  defineLevel()

  function defineLevel () {
    Level.encodingLength = encodingLength
    Level.encode = encode
    Level.decode = decode

    function encodingLength (obj) {
      var length = 0
      if (defined(obj.keys)) {
        var packedLen = 0
        for (var i = 0; i < obj.keys.length; i++) {
          if (!defined(obj.keys[i])) continue
          var len = encodings.varint.encodingLength(obj.keys[i])
          packedLen += len
        }
        if (packedLen) {
          length += 1 + packedLen + varint.encodingLength(packedLen)
        }
      }
      if (defined(obj.children)) {
        var packedLen = 0
        for (var i = 0; i < obj.children.length; i++) {
          if (!defined(obj.children[i])) continue
          var len = encodings.varint.encodingLength(obj.children[i])
          packedLen += len
        }
        if (packedLen) {
          length += 1 + packedLen + varint.encodingLength(packedLen)
        }
      }
      return length
    }

    function encode (obj, buf, offset) {
      if (!offset) offset = 0
      if (!buf) buf = b4a.allocUnsafe(encodingLength(obj))
      var oldOffset = offset
      if (defined(obj.keys)) {
        var packedLen = 0
        for (var i = 0; i < obj.keys.length; i++) {
          if (!defined(obj.keys[i])) continue
          packedLen += encodings.varint.encodingLength(obj.keys[i])
        }
        if (packedLen) {
          buf[offset++] = 10
          varint.encode(packedLen, buf, offset)
          offset += varint.encode.bytes
        }
        for (var i = 0; i < obj.keys.length; i++) {
          if (!defined(obj.keys[i])) continue
          encodings.varint.encode(obj.keys[i], buf, offset)
          offset += encodings.varint.encode.bytes
        }
      }
      if (defined(obj.children)) {
        var packedLen = 0
        for (var i = 0; i < obj.children.length; i++) {
          if (!defined(obj.children[i])) continue
          packedLen += encodings.varint.encodingLength(obj.children[i])
        }
        if (packedLen) {
          buf[offset++] = 18
          varint.encode(packedLen, buf, offset)
          offset += varint.encode.bytes
        }
        for (var i = 0; i < obj.children.length; i++) {
          if (!defined(obj.children[i])) continue
          encodings.varint.encode(obj.children[i], buf, offset)
          offset += encodings.varint.encode.bytes
        }
      }
      encode.bytes = offset - oldOffset
      return buf
    }

    function decode (buf, offset, end) {
      if (!offset) offset = 0
      if (!end) end = buf.length
      if (!(end <= buf.length && offset <= buf.length)) throw new Error("Decoded message is not valid")
      var oldOffset = offset
      var obj = {
        keys: [],
        children: []
      }
      while (true) {
        if (end <= offset) {
          decode.bytes = offset - oldOffset
          return obj
        }
        var prefix = varint.decode(buf, offset)
        offset += varint.decode.bytes
        var tag = prefix >> 3
        switch (tag) {
          case 1:
          var packedEnd = varint.decode(buf, offset)
          offset += varint.decode.bytes
          packedEnd += offset
          while (offset < packedEnd) {
            obj.keys.push(encodings.varint.decode(buf, offset))
            offset += encodings.varint.decode.bytes
          }
          break
          case 2:
          var packedEnd = varint.decode(buf, offset)
          offset += varint.decode.bytes
          packedEnd += offset
          while (offset < packedEnd) {
            obj.children.push(encodings.varint.decode(buf, offset))
            offset += encodings.varint.decode.bytes
          }
          break
          default:
          offset = skip(prefix & 7, buf, offset)
        }
      }
    }
  }

  YoloIndex.encodingLength = encodingLength
  YoloIndex.encode = encode
  YoloIndex.decode = decode

  function encodingLength (obj) {
    var length = 0
    if (defined(obj.levels)) {
      for (var i = 0; i < obj.levels.length; i++) {
        if (!defined(obj.levels[i])) continue
        var len = Level.encodingLength(obj.levels[i])
        length += varint.encodingLength(len)
        length += 1 + len
      }
    }
    return length
  }

  function encode (obj, buf, offset) {
    if (!offset) offset = 0
    if (!buf) buf = b4a.allocUnsafe(encodingLength(obj))
    var oldOffset = offset
    if (defined(obj.levels)) {
      for (var i = 0; i < obj.levels.length; i++) {
        if (!defined(obj.levels[i])) continue
        buf[offset++] = 10
        varint.encode(Level.encodingLength(obj.levels[i]), buf, offset)
        offset += varint.encode.bytes
        Level.encode(obj.levels[i], buf, offset)
        offset += Level.encode.bytes
      }
    }
    encode.bytes = offset - oldOffset
    return buf
  }

  function decode (buf, offset, end) {
    if (!offset) offset = 0
    if (!end) end = buf.length
    if (!(end <= buf.length && offset <= buf.length)) throw new Error("Decoded message is not valid")
    var oldOffset = offset
    var obj = {
      levels: []
    }
    while (true) {
      if (end <= offset) {
        decode.bytes = offset - oldOffset
        return obj
      }
      var prefix = varint.decode(buf, offset)
      offset += varint.decode.bytes
      var tag = prefix >> 3
      switch (tag) {
        case 1:
        var len = varint.decode(buf, offset)
        offset += varint.decode.bytes
        obj.levels.push(Level.decode(buf, offset, offset + len))
        offset += Level.decode.bytes
        break
        default:
        offset = skip(prefix & 7, buf, offset)
      }
    }
  }
}

function defineHeader () {
  var Metadata = Header.Metadata = {
    buffer: true,
    encodingLength: null,
    encode: null,
    decode: null
  }

  defineMetadata()

  function defineMetadata () {
    Metadata.encodingLength = encodingLength
    Metadata.encode = encode
    Metadata.decode = decode

    function encodingLength (obj) {
      var length = 0
      if (defined(obj.contentFeed)) {
        var len = encodings.bytes.encodingLength(obj.contentFeed)
        length += 1 + len
      }
      if (defined(obj.userData)) {
        var len = encodings.bytes.encodingLength(obj.userData)
        length += 1 + len
      }
      return length
    }

    function encode (obj, buf, offset) {
      if (!offset) offset = 0
      if (!buf) buf = b4a.allocUnsafe(encodingLength(obj))
      var oldOffset = offset
      if (defined(obj.contentFeed)) {
        buf[offset++] = 10
        encodings.bytes.encode(obj.contentFeed, buf, offset)
        offset += encodings.bytes.encode.bytes
      }
      if (defined(obj.userData)) {
        buf[offset++] = 18
        encodings.bytes.encode(obj.userData, buf, offset)
        offset += encodings.bytes.encode.bytes
      }
      encode.bytes = offset - oldOffset
      return buf
    }

    function decode (buf, offset, end) {
      if (!offset) offset = 0
      if (!end) end = buf.length
      if (!(end <= buf.length && offset <= buf.length)) throw new Error("Decoded message is not valid")
      var oldOffset = offset
      var obj = {
        contentFeed: null,
        userData: null
      }
      while (true) {
        if (end <= offset) {
          decode.bytes = offset - oldOffset
          return obj
        }
        var prefix = varint.decode(buf, offset)
        offset += varint.decode.bytes
        var tag = prefix >> 3
        switch (tag) {
          case 1:
          obj.contentFeed = encodings.bytes.decode(buf, offset)
          offset += encodings.bytes.decode.bytes
          break
          case 2:
          obj.userData = encodings.bytes.decode(buf, offset)
          offset += encodings.bytes.decode.bytes
          break
          default:
          offset = skip(prefix & 7, buf, offset)
        }
      }
    }
  }

  Header.encodingLength = encodingLength
  Header.encode = encode
  Header.decode = decode

  function encodingLength (obj) {
    var length = 0
    if (!defined(obj.protocol)) throw new Error("protocol is required")
    var len = encodings.string.encodingLength(obj.protocol)
    length += 1 + len
    if (defined(obj.metadata)) {
      var len = Metadata.encodingLength(obj.metadata)
      length += varint.encodingLength(len)
      length += 1 + len
    }
    return length
  }

  function encode (obj, buf, offset) {
    if (!offset) offset = 0
    if (!buf) buf = b4a.allocUnsafe(encodingLength(obj))
    var oldOffset = offset
    if (!defined(obj.protocol)) throw new Error("protocol is required")
    buf[offset++] = 10
    encodings.string.encode(obj.protocol, buf, offset)
    offset += encodings.string.encode.bytes
    if (defined(obj.metadata)) {
      buf[offset++] = 18
      varint.encode(Metadata.encodingLength(obj.metadata), buf, offset)
      offset += varint.encode.bytes
      Metadata.encode(obj.metadata, buf, offset)
      offset += Metadata.encode.bytes
    }
    encode.bytes = offset - oldOffset
    return buf
  }

  function decode (buf, offset, end) {
    if (!offset) offset = 0
    if (!end) end = buf.length
    if (!(end <= buf.length && offset <= buf.length)) throw new Error("Decoded message is not valid")
    var oldOffset = offset
    var obj = {
      protocol: "",
      metadata: null
    }
    var found0 = false
    while (true) {
      if (end <= offset) {
        if (!found0) throw new Error("Decoded message is not valid")
        decode.bytes = offset - oldOffset
        return obj
      }
      var prefix = varint.decode(buf, offset)
      offset += varint.decode.bytes
      var tag = prefix >> 3
      switch (tag) {
        case 1:
        obj.protocol = encodings.string.decode(buf, offset)
        offset += encodings.string.decode.bytes
        found0 = true
        break
        case 2:
        var len = varint.decode(buf, offset)
        offset += varint.decode.bytes
        obj.metadata = Metadata.decode(buf, offset, offset + len)
        offset += Metadata.decode.bytes
        break
        default:
        offset = skip(prefix & 7, buf, offset)
      }
    }
  }
}

function defineNode () {
  Node.encodingLength = encodingLength
  Node.encode = encode
  Node.decode = decode

  function encodingLength (obj) {
    var length = 0
    if (!defined(obj.index)) throw new Error("index is required")
    var len = encodings.bytes.encodingLength(obj.index)
    length += 1 + len
    if (!defined(obj.key)) throw new Error("key is required")
    var len = encodings.bytes.encodingLength(obj.key)
    length += 1 + len
    if (defined(obj.value)) {
      var len = encodings.bytes.encodingLength(obj.value)
      length += 1 + len
    }
    return length
  }

  function encode (obj, buf, offset) {
    if (!offset) offset = 0
    if (!buf) buf = b4a.allocUnsafe(encodingLength(obj))
    var oldOffset = offset
    if (!defined(obj.index)) throw new Error("index is required")
    buf[offset++] = 10
    encodings.bytes.encode(obj.index, buf, offset)
    offset += encodings.bytes.encode.bytes
    if (!defined(obj.key)) throw new Error("key is required")
    buf[offset++] = 18
    encodings.bytes.encode(obj.key, buf, offset)
    offset += encodings.bytes.encode.bytes
    if (defined(obj.value)) {
      buf[offset++] = 26
      encodings.bytes.encode(obj.value, buf, offset)
      offset += encodings.bytes.encode.bytes
    }
    encode.bytes = offset - oldOffset
    return buf
  }

  function decode (buf, offset, end) {
    if (!offset) offset = 0
    if (!end) end = buf.length
    if (!(end <= buf.length && offset <= buf.length)) throw new Error("Decoded message is not valid")
    var oldOffset = offset
    var obj = {
      index: null,
      key: null,
      value: null
    }
    var found0 = false
    var found1 = false
    while (true) {
      if (end <= offset) {
        if (!found0 || !found1) throw new Error("Decoded message is not valid")
        decode.bytes = offset - oldOffset
        return obj
      }
      var prefix = varint.decode(buf, offset)
      offset += varint.decode.bytes
      var tag = prefix >> 3
      switch (tag) {
        case 1:
        obj.index = encodings.bytes.decode(buf, offset)
        offset += encodings.bytes.decode.bytes
        found0 = true
        break
        case 2:
        obj.key = encodings.bytes.decode(buf, offset)
        offset += encodings.bytes.decode.bytes
        found1 = true
        break
        case 3:
        obj.value = encodings.bytes.decode(buf, offset)
        offset += encodings.bytes.decode.bytes
        break
        default:
        offset = skip(prefix & 7, buf, offset)
      }
    }
  }
}

function defineExtension () {
  var Get = Extension.Get = {
    buffer: true,
    encodingLength: null,
    encode: null,
    decode: null
  }

  var Iterator = Extension.Iterator = {
    buffer: true,
    encodingLength: null,
    encode: null,
    decode: null
  }

  var Cache = Extension.Cache = {
    buffer: true,
    encodingLength: null,
    encode: null,
    decode: null
  }

  defineGet()
  defineIterator()
  defineCache()

  function defineGet () {
    Get.encodingLength = encodingLength
    Get.encode = encode
    Get.decode = decode

    function encodingLength (obj) {
      var length = 0
      if (defined(obj.version)) {
        var len = encodings.varint.encodingLength(obj.version)
        length += 1 + len
      }
      if (defined(obj.key)) {
        var len = encodings.bytes.encodingLength(obj.key)
        length += 1 + len
      }
      return length
    }

    function encode (obj, buf, offset) {
      if (!offset) offset = 0
      if (!buf) buf = b4a.allocUnsafe(encodingLength(obj))
      var oldOffset = offset
      if (defined(obj.version)) {
        buf[offset++] = 8
        encodings.varint.encode(obj.version, buf, offset)
        offset += encodings.varint.encode.bytes
      }
      if (defined(obj.key)) {
        buf[offset++] = 18
        encodings.bytes.encode(obj.key, buf, offset)
        offset += encodings.bytes.encode.bytes
      }
      encode.bytes = offset - oldOffset
      return buf
    }

    function decode (buf, offset, end) {
      if (!offset) offset = 0
      if (!end) end = buf.length
      if (!(end <= buf.length && offset <= buf.length)) throw new Error("Decoded message is not valid")
      var oldOffset = offset
      var obj = {
        version: 0,
        key: null
      }
      while (true) {
        if (end <= offset) {
          decode.bytes = offset - oldOffset
          return obj
        }
        var prefix = varint.decode(buf, offset)
        offset += varint.decode.bytes
        var tag = prefix >> 3
        switch (tag) {
          case 1:
          obj.version = encodings.varint.decode(buf, offset)
          offset += encodings.varint.decode.bytes
          break
          case 2:
          obj.key = encodings.bytes.decode(buf, offset)
          offset += encodings.bytes.decode.bytes
          break
          default:
          offset = skip(prefix & 7, buf, offset)
        }
      }
    }
  }

  function defineIterator () {
    Iterator.encodingLength = encodingLength
    Iterator.encode = encode
    Iterator.decode = decode

    function encodingLength (obj) {
      var length = 0
      if (defined(obj.version)) {
        var len = encodings.varint.encodingLength(obj.version)
        length += 1 + len
      }
      if (defined(obj.gte)) {
        var len = encodings.bytes.encodingLength(obj.gte)
        length += 1 + len
      }
      if (defined(obj.gt)) {
        var len = encodings.bytes.encodingLength(obj.gt)
        length += 1 + len
      }
      if (defined(obj.lte)) {
        var len = encodings.bytes.encodingLength(obj.lte)
        length += 1 + len
      }
      if (defined(obj.lt)) {
        var len = encodings.bytes.encodingLength(obj.lt)
        length += 1 + len
      }
      if (defined(obj.limit)) {
        var len = encodings.varint.encodingLength(obj.limit)
        length += 1 + len
      }
      if (defined(obj.reverse)) {
        var len = encodings.bool.encodingLength(obj.reverse)
        length += 1 + len
      }
      if (defined(obj.checkpoint)) {
        var packedLen = 0
        for (var i = 0; i < obj.checkpoint.length; i++) {
          if (!defined(obj.checkpoint[i])) continue
          var len = encodings.varint.encodingLength(obj.checkpoint[i])
          packedLen += len
        }
        if (packedLen) {
          length += 1 + packedLen + varint.encodingLength(packedLen)
        }
      }
      return length
    }

    function encode (obj, buf, offset) {
      if (!offset) offset = 0
      if (!buf) buf = b4a.allocUnsafe(encodingLength(obj))
      var oldOffset = offset
      if (defined(obj.version)) {
        buf[offset++] = 8
        encodings.varint.encode(obj.version, buf, offset)
        offset += encodings.varint.encode.bytes
      }
      if (defined(obj.gte)) {
        buf[offset++] = 18
        encodings.bytes.encode(obj.gte, buf, offset)
        offset += encodings.bytes.encode.bytes
      }
      if (defined(obj.gt)) {
        buf[offset++] = 26
        encodings.bytes.encode(obj.gt, buf, offset)
        offset += encodings.bytes.encode.bytes
      }
      if (defined(obj.lte)) {
        buf[offset++] = 34
        encodings.bytes.encode(obj.lte, buf, offset)
        offset += encodings.bytes.encode.bytes
      }
      if (defined(obj.lt)) {
        buf[offset++] = 42
        encodings.bytes.encode(obj.lt, buf, offset)
        offset += encodings.bytes.encode.bytes
      }
      if (defined(obj.limit)) {
        buf[offset++] = 48
        encodings.varint.encode(obj.limit, buf, offset)
        offset += encodings.varint.encode.bytes
      }
      if (defined(obj.reverse)) {
        buf[offset++] = 56
        encodings.bool.encode(obj.reverse, buf, offset)
        offset += encodings.bool.encode.bytes
      }
      if (defined(obj.checkpoint)) {
        var packedLen = 0
        for (var i = 0; i < obj.checkpoint.length; i++) {
          if (!defined(obj.checkpoint[i])) continue
          packedLen += encodings.varint.encodingLength(obj.checkpoint[i])
        }
        if (packedLen) {
          buf[offset++] = 66
          varint.encode(packedLen, buf, offset)
          offset += varint.encode.bytes
        }
        for (var i = 0; i < obj.checkpoint.length; i++) {
          if (!defined(obj.checkpoint[i])) continue
          encodings.varint.encode(obj.checkpoint[i], buf, offset)
          offset += encodings.varint.encode.bytes
        }
      }
      encode.bytes = offset - oldOffset
      return buf
    }

    function decode (buf, offset, end) {
      if (!offset) offset = 0
      if (!end) end = buf.length
      if (!(end <= buf.length && offset <= buf.length)) throw new Error("Decoded message is not valid")
      var oldOffset = offset
      var obj = {
        version: 0,
        gte: null,
        gt: null,
        lte: null,
        lt: null,
        limit: 0,
        reverse: false,
        checkpoint: []
      }
      while (true) {
        if (end <= offset) {
          decode.bytes = offset - oldOffset
          return obj
        }
        var prefix = varint.decode(buf, offset)
        offset += varint.decode.bytes
        var tag = prefix >> 3
        switch (tag) {
          case 1:
          obj.version = encodings.varint.decode(buf, offset)
          offset += encodings.varint.decode.bytes
          break
          case 2:
          obj.gte = encodings.bytes.decode(buf, offset)
          offset += encodings.bytes.decode.bytes
          break
          case 3:
          obj.gt = encodings.bytes.decode(buf, offset)
          offset += encodings.bytes.decode.bytes
          break
          case 4:
          obj.lte = encodings.bytes.decode(buf, offset)
          offset += encodings.bytes.decode.bytes
          break
          case 5:
          obj.lt = encodings.bytes.decode(buf, offset)
          offset += encodings.bytes.decode.bytes
          break
          case 6:
          obj.limit = encodings.varint.decode(buf, offset)
          offset += encodings.varint.decode.bytes
          break
          case 7:
          obj.reverse = encodings.bool.decode(buf, offset)
          offset += encodings.bool.decode.bytes
          break
          case 8:
          var packedEnd = varint.decode(buf, offset)
          offset += varint.decode.bytes
          packedEnd += offset
          while (offset < packedEnd) {
            obj.checkpoint.push(encodings.varint.decode(buf, offset))
            offset += encodings.varint.decode.bytes
          }
          break
          default:
          offset = skip(prefix & 7, buf, offset)
        }
      }
    }
  }

  function defineCache () {
    Cache.encodingLength = encodingLength
    Cache.encode = encode
    Cache.decode = decode

    function encodingLength (obj) {
      var length = 0
      if (!defined(obj.start)) throw new Error("start is required")
      var len = encodings.varint.encodingLength(obj.start)
      length += 1 + len
      if (!defined(obj.end)) throw new Error("end is required")
      var len = encodings.varint.encodingLength(obj.end)
      length += 1 + len
      if (defined(obj.blocks)) {
        var packedLen = 0
        for (var i = 0; i < obj.blocks.length; i++) {
          if (!defined(obj.blocks[i])) continue
          var len = encodings.varint.encodingLength(obj.blocks[i])
          packedLen += len
        }
        if (packedLen) {
          length += 1 + packedLen + varint.encodingLength(packedLen)
        }
      }
      return length
    }

    function encode (obj, buf, offset) {
      if (!offset) offset = 0
      if (!buf) buf = b4a.allocUnsafe(encodingLength(obj))
      var oldOffset = offset
      if (!defined(obj.start)) throw new Error("start is required")
      buf[offset++] = 8
      encodings.varint.encode(obj.start, buf, offset)
      offset += encodings.varint.encode.bytes
      if (!defined(obj.end)) throw new Error("end is required")
      buf[offset++] = 16
      encodings.varint.encode(obj.end, buf, offset)
      offset += encodings.varint.encode.bytes
      if (defined(obj.blocks)) {
        var packedLen = 0
        for (var i = 0; i < obj.blocks.length; i++) {
          if (!defined(obj.blocks[i])) continue
          packedLen += encodings.varint.encodingLength(obj.blocks[i])
        }
        if (packedLen) {
          buf[offset++] = 26
          varint.encode(packedLen, buf, offset)
          offset += varint.encode.bytes
        }
        for (var i = 0; i < obj.blocks.length; i++) {
          if (!defined(obj.blocks[i])) continue
          encodings.varint.encode(obj.blocks[i], buf, offset)
          offset += encodings.varint.encode.bytes
        }
      }
      encode.bytes = offset - oldOffset
      return buf
    }

    function decode (buf, offset, end) {
      if (!offset) offset = 0
      if (!end) end = buf.length
      if (!(end <= buf.length && offset <= buf.length)) throw new Error("Decoded message is not valid")
      var oldOffset = offset
      var obj = {
        start: 0,
        end: 0,
        blocks: []
      }
      var found0 = false
      var found1 = false
      while (true) {
        if (end <= offset) {
          if (!found0 || !found1) throw new Error("Decoded message is not valid")
          decode.bytes = offset - oldOffset
          return obj
        }
        var prefix = varint.decode(buf, offset)
        offset += varint.decode.bytes
        var tag = prefix >> 3
        switch (tag) {
          case 1:
          obj.start = encodings.varint.decode(buf, offset)
          offset += encodings.varint.decode.bytes
          found0 = true
          break
          case 2:
          obj.end = encodings.varint.decode(buf, offset)
          offset += encodings.varint.decode.bytes
          found1 = true
          break
          case 3:
          var packedEnd = varint.decode(buf, offset)
          offset += varint.decode.bytes
          packedEnd += offset
          while (offset < packedEnd) {
            obj.blocks.push(encodings.varint.decode(buf, offset))
            offset += encodings.varint.decode.bytes
          }
          break
          default:
          offset = skip(prefix & 7, buf, offset)
        }
      }
    }
  }

  Extension.encodingLength = encodingLength
  Extension.encode = encode
  Extension.decode = decode

  function encodingLength (obj) {
    var length = 0
    if (defined(obj.cache)) {
      var len = Cache.encodingLength(obj.cache)
      length += varint.encodingLength(len)
      length += 1 + len
    }
    if (defined(obj.get)) {
      var len = Get.encodingLength(obj.get)
      length += varint.encodingLength(len)
      length += 1 + len
    }
    if (defined(obj.iterator)) {
      var len = Iterator.encodingLength(obj.iterator)
      length += varint.encodingLength(len)
      length += 1 + len
    }
    return length
  }

  function encode (obj, buf, offset) {
    if (!offset) offset = 0
    if (!buf) buf = b4a.allocUnsafe(encodingLength(obj))
    var oldOffset = offset
    if (defined(obj.cache)) {
      buf[offset++] = 10
      varint.encode(Cache.encodingLength(obj.cache), buf, offset)
      offset += varint.encode.bytes
      Cache.encode(obj.cache, buf, offset)
      offset += Cache.encode.bytes
    }
    if (defined(obj.get)) {
      buf[offset++] = 18
      varint.encode(Get.encodingLength(obj.get), buf, offset)
      offset += varint.encode.bytes
      Get.encode(obj.get, buf, offset)
      offset += Get.encode.bytes
    }
    if (defined(obj.iterator)) {
      buf[offset++] = 26
      varint.encode(Iterator.encodingLength(obj.iterator), buf, offset)
      offset += varint.encode.bytes
      Iterator.encode(obj.iterator, buf, offset)
      offset += Iterator.encode.bytes
    }
    encode.bytes = offset - oldOffset
    return buf
  }

  function decode (buf, offset, end) {
    if (!offset) offset = 0
    if (!end) end = buf.length
    if (!(end <= buf.length && offset <= buf.length)) throw new Error("Decoded message is not valid")
    var oldOffset = offset
    var obj = {
      cache: null,
      get: null,
      iterator: null
    }
    while (true) {
      if (end <= offset) {
        decode.bytes = offset - oldOffset
        return obj
      }
      var prefix = varint.decode(buf, offset)
      offset += varint.decode.bytes
      var tag = prefix >> 3
      switch (tag) {
        case 1:
        var len = varint.decode(buf, offset)
        offset += varint.decode.bytes
        obj.cache = Cache.decode(buf, offset, offset + len)
        offset += Cache.decode.bytes
        break
        case 2:
        var len = varint.decode(buf, offset)
        offset += varint.decode.bytes
        obj.get = Get.decode(buf, offset, offset + len)
        offset += Get.decode.bytes
        break
        case 3:
        var len = varint.decode(buf, offset)
        offset += varint.decode.bytes
        obj.iterator = Iterator.decode(buf, offset, offset + len)
        offset += Iterator.decode.bytes
        break
        default:
        offset = skip(prefix & 7, buf, offset)
      }
    }
  }
}

function defined (val) {
  return val !== null && val !== undefined && (typeof val !== 'number' || !isNaN(val))
}
{
  "name": "hyperbee",
  "version": "2.26.5",
  "description": "An append-only B-tree running on a Hypercore.",
  "main": "index.js",
  "files": [
    "index.js",
    "lib/**.js",
    "iterators/**.js"
  ],
  "dependencies": {
    "b4a": "^1.6.0",
    "codecs": "^3.0.0",
    "debounceify": "^1.0.0",
    "hypercore-errors": "^1.0.0",
    "mutexify": "^1.4.0",
    "protocol-buffers-encodings": "^1.2.0",
    "rache": "^1.0.0",
    "ready-resource": "^1.0.0",
    "resolve-reject-promise": "^1.1.0",
    "safety-catch": "^1.0.2",
    "streamx": "^2.12.4",
    "unslab": "^1.2.0"
  },
  "devDependencies": {
    "brittle": "^3.1.0",
    "hypercore": "^11.0.0",
    "protocol-buffers": "^4.2.0",
    "standard": "^17.0.0",
    "sub-encoder": "^1.0.6",
    "tree-to-string": "^1.1.1"
  },
  "scripts": {
    "test": "standard && brittle test/*.js",
    "protobuf": "protocol-buffers schema.proto -o ./lib/messages.js"
  },
  "repository": {
    "type": "git",
    "url": "https://github.com/holepunchto/hyperbee.git"
  },
  "author": "Mathias Buus (@mafintosh)",
  "license": "MIT",
  "bugs": {
    "url": "https://github.com/holepunchto/hyperbee/issues"
  },
  "homepage": "https://github.com/holepunchto/hyperbee"
}
const sodium = require('sodium-universal')
const c = require('compact-encoding')
const b4a = require('b4a')

// https://en.wikipedia.org/wiki/Merkle_tree#Second_preimage_attack
const LEAF_TYPE = b4a.from([0])
const PARENT_TYPE = b4a.from([1])
const ROOT_TYPE = b4a.from([2])

const HYPERCORE = b4a.from('hypercore')

exports.keyPair = function (seed) {
  // key pairs might stay around for a while, so better not to use a default slab to avoid retaining it completely
  const slab = b4a.allocUnsafeSlow(sodium.crypto_sign_PUBLICKEYBYTES + sodium.crypto_sign_SECRETKEYBYTES)
  const publicKey = slab.subarray(0, sodium.crypto_sign_PUBLICKEYBYTES)
  const secretKey = slab.subarray(sodium.crypto_sign_PUBLICKEYBYTES)

  if (seed) sodium.crypto_sign_seed_keypair(publicKey, secretKey, seed)
  else sodium.crypto_sign_keypair(publicKey, secretKey)

  return {
    publicKey,
    secretKey
  }
}

exports.validateKeyPair = function (keyPair) {
  const pk = b4a.allocUnsafe(sodium.crypto_sign_PUBLICKEYBYTES)
  sodium.crypto_sign_ed25519_sk_to_pk(pk, keyPair.secretKey)
  return b4a.equals(pk, keyPair.publicKey)
}

exports.sign = function (message, secretKey) {
  // Dedicated slab for the signature, to avoid retaining unneeded mem and for security
  const signature = b4a.allocUnsafeSlow(sodium.crypto_sign_BYTES)
  sodium.crypto_sign_detached(signature, message, secretKey)
  return signature
}

exports.verify = function (message, signature, publicKey) {
  if (signature.byteLength !== sodium.crypto_sign_BYTES) return false
  if (publicKey.byteLength !== sodium.crypto_sign_PUBLICKEYBYTES) return false
  return sodium.crypto_sign_verify_detached(signature, message, publicKey)
}

exports.encrypt = function (message, publicKey) {
  const ciphertext = b4a.alloc(message.byteLength + sodium.crypto_box_SEALBYTES)
  sodium.crypto_box_seal(ciphertext, message, publicKey)
  return ciphertext
}

exports.decrypt = function (ciphertext, keyPair) {
  if (ciphertext.byteLength < sodium.crypto_box_SEALBYTES) return null

  const plaintext = b4a.alloc(ciphertext.byteLength - sodium.crypto_box_SEALBYTES)

  if (!sodium.crypto_box_seal_open(plaintext, ciphertext, keyPair.publicKey, keyPair.secretKey)) {
    return null
  }

  return plaintext
}

exports.encryptionKeyPair = function (seed) {
  const publicKey = b4a.alloc(sodium.crypto_box_PUBLICKEYBYTES)
  const secretKey = b4a.alloc(sodium.crypto_box_SECRETKEYBYTES)

  if (seed) {
    sodium.crypto_box_seed_keypair(publicKey, secretKey, seed)
  } else {
    sodium.crypto_box_keypair(publicKey, secretKey)
  }

  return {
    publicKey,
    secretKey
  }
}

exports.data = function (data) {
  const out = b4a.allocUnsafe(32)

  sodium.crypto_generichash_batch(out, [
    LEAF_TYPE,
    c.encode(c.uint64, data.byteLength),
    data
  ])

  return out
}

exports.parent = function (a, b) {
  if (a.index > b.index) {
    const tmp = a
    a = b
    b = tmp
  }

  const out = b4a.allocUnsafe(32)

  sodium.crypto_generichash_batch(out, [
    PARENT_TYPE,
    c.encode(c.uint64, a.size + b.size),
    a.hash,
    b.hash
  ])

  return out
}

exports.tree = function (roots, out) {
  const buffers = new Array(3 * roots.length + 1)
  let j = 0

  buffers[j++] = ROOT_TYPE

  for (let i = 0; i < roots.length; i++) {
    const r = roots[i]
    buffers[j++] = r.hash
    buffers[j++] = c.encode(c.uint64, r.index)
    buffers[j++] = c.encode(c.uint64, r.size)
  }

  if (!out) out = b4a.allocUnsafe(32)
  sodium.crypto_generichash_batch(out, buffers)
  return out
}

exports.hash = function (data, out) {
  if (!out) out = b4a.allocUnsafe(32)
  if (!Array.isArray(data)) data = [data]

  sodium.crypto_generichash_batch(out, data)

  return out
}

exports.randomBytes = function (n) {
  const buf = b4a.allocUnsafe(n)
  sodium.randombytes_buf(buf)
  return buf
}

exports.discoveryKey = function (key) {
  if (!key || key.byteLength !== 32) throw new Error('Must pass a 32 byte buffer')
  // Discovery keys might stay around for a while, so better not to use slab memory (for better gc)
  const digest = b4a.allocUnsafeSlow(32)
  sodium.crypto_generichash(digest, HYPERCORE, key)
  return digest
}

if (sodium.sodium_free) {
  exports.free = function (secureBuf) {
    if (secureBuf.secure) sodium.sodium_free(secureBuf)
  }
} else {
  exports.free = function () {}
}

exports.namespace = function (name, count) {
  const ids = typeof count === 'number' ? range(count) : count

  // Namespaces are long-lived, so better to use a dedicated slab
  const buf = b4a.allocUnsafeSlow(32 * ids.length)

  const list = new Array(ids.length)

  // ns is emhemeral, so default slab
  const ns = b4a.allocUnsafe(33)
  sodium.crypto_generichash(ns.subarray(0, 32), typeof name === 'string' ? b4a.from(name) : name)

  for (let i = 0; i < list.length; i++) {
    list[i] = buf.subarray(32 * i, 32 * i + 32)
    ns[32] = ids[i]
    sodium.crypto_generichash(list[i], ns)
  }

  return list
}

function range (count) {
  const arr = new Array(count)
  for (let i = 0; i < count; i++) arr[i] = i
  return arr
}
{
  "name": "hypercore-crypto",
  "version": "3.6.1",
  "description": "The crypto primitives used in hypercore, extracted into a separate module",
  "main": "index.js",
  "files": [
    "index.js"
  ],
  "dependencies": {
    "b4a": "^1.6.6",
    "compact-encoding": "^2.15.0",
    "sodium-universal": "^5.0.0"
  },
  "devDependencies": {
    "brittle": "^3.5.0",
    "standard": "^17.1.0"
  },
  "scripts": {
    "test": "standard && brittle test.js"
  },
  "repository": {
    "type": "git",
    "url": "https://github.com/mafintosh/hypercore-crypto.git"
  },
  "author": "Mathias Buus (@mafintosh)",
  "license": "MIT",
  "bugs": {
    "url": "https://github.com/mafintosh/hypercore-crypto/issues"
  },
  "homepage": "https://github.com/mafintosh/hypercore-crypto"
}
const IdEnc = require('hypercore-id-encoding')

module.exports = class HypercoreError extends Error {
  constructor (msg, code, fn = HypercoreError, discoveryKey = null) {
    if (discoveryKey) msg = `${msg} (discovery key: ${IdEnc.normalize(discoveryKey)})`
    super(`${code}: ${msg}`)

    this.code = code
    this.discoveryKey = discoveryKey

    if (Error.captureStackTrace) {
      Error.captureStackTrace(this, fn)
    }
  }

  get name () {
    return 'HypercoreError'
  }

  static ASSERTION (msg, discoveryKey = null) { // ERR_ASSERTION is picked up by safety-catch also
    return new HypercoreError(msg, 'ERR_ASSERTION', HypercoreError.ASSERT, discoveryKey)
  }

  static BAD_ARGUMENT (msg, discoveryKey = null) {
    return new HypercoreError(msg, 'BAD_ARGUMENT', HypercoreError.BAD_ARGUMENT, discoveryKey)
  }

  static STORAGE_EMPTY (msg, discoveryKey = null) {
    return new HypercoreError(msg, 'STORAGE_EMPTY', HypercoreError.STORAGE_EMPTY, discoveryKey)
  }

  static STORAGE_CONFLICT (msg, discoveryKey = null) {
    return new HypercoreError(msg, 'STORAGE_CONFLICT', HypercoreError.STORAGE_CONFLICT, discoveryKey)
  }

  static INVALID_SIGNATURE (msg, discoveryKey = null) {
    return new HypercoreError(msg, 'INVALID_SIGNATURE', HypercoreError.INVALID_SIGNATURE, discoveryKey)
  }

  static INVALID_CAPABILITY (msg, discoveryKey = null) {
    return new HypercoreError(msg, 'INVALID_CAPABILITY', HypercoreError.INVALID_CAPABILITY, discoveryKey)
  }

  static INVALID_CHECKSUM (msg = 'Invalid checksum', discoveryKey = null) {
    return new HypercoreError(msg, 'INVALID_CHECKSUM', HypercoreError.INVALID_CHECKSUM, discoveryKey)
  }

  static INVALID_OPERATION (msg, discoveryKey = null) {
    return new HypercoreError(msg, 'INVALID_OPERATION', HypercoreError.INVALID_OPERATION, discoveryKey)
  }

  static INVALID_PROOF (msg = 'Proof not verifiable', discoveryKey = null) {
    return new HypercoreError(msg, 'INVALID_PROOF', HypercoreError.INVALID_PROOF, discoveryKey)
  }

  static BLOCK_NOT_AVAILABLE (msg = 'Block is not available', discoveryKey = null) {
    return new HypercoreError(msg, 'BLOCK_NOT_AVAILABLE', HypercoreError.BLOCK_NOT_AVAILABLE, discoveryKey)
  }

  static SNAPSHOT_NOT_AVAILABLE (msg = 'Snapshot is not available', discoveryKey = null) {
    return new HypercoreError(msg, 'SNAPSHOT_NOT_AVAILABLE', HypercoreError.SNAPSHOT_NOT_AVAILABLE, discoveryKey)
  }

  static REQUEST_CANCELLED (msg = 'Request was cancelled', discoveryKey = null) {
    return new HypercoreError(msg, 'REQUEST_CANCELLED', HypercoreError.REQUEST_CANCELLED, discoveryKey)
  }

  static REQUEST_TIMEOUT (msg = 'Request timed out', discoveryKey = null) {
    return new HypercoreError(msg, 'REQUEST_TIMEOUT', HypercoreError.REQUEST_TIMEOUT, discoveryKey)
  }

  static SESSION_NOT_WRITABLE (msg = 'Session is not writable', discoveryKey = null) {
    return new HypercoreError(msg, 'SESSION_NOT_WRITABLE', HypercoreError.SESSION_NOT_WRITABLE, discoveryKey)
  }

  static SESSION_CLOSED (msg = 'Session is closed', discoveryKey = null) {
    return new HypercoreError(msg, 'SESSION_CLOSED', HypercoreError.SESSION_CLOSED, discoveryKey)
  }

  static BATCH_UNFLUSHED (msg = 'Batch not yet flushed', discoveryKey = null) {
    return new HypercoreError(msg, 'BATCH_UNFLUSHED', HypercoreError.BATCH_UNFLUSHED, discoveryKey)
  }

  static BATCH_ALREADY_EXISTS (msg = 'Batch already exists', discoveryKey = null) {
    return new HypercoreError(msg, 'BATCH_ALREADY_EXISTS', HypercoreError.BATCH_ALREADY_EXISTS, discoveryKey)
  }

  static BATCH_ALREADY_FLUSHED (msg = 'Batch has already been flushed', discoveryKey = null) {
    return new HypercoreError(msg, 'BATCH_ALREADY_FLUSHED', HypercoreError.BATCH_ALREADY_FLUSHED, discoveryKey)
  }

  static OPLOG_CORRUPT (msg = 'Oplog file appears corrupt or out of date', discoveryKey = null) {
    return new HypercoreError(msg, 'OPLOG_CORRUPT', HypercoreError.OPLOG_CORRUPT, discoveryKey)
  }

  static OPLOG_HEADER_OVERFLOW (msg = 'Oplog header exceeds page size', discoveryKey = null) {
    return new HypercoreError(msg, 'OPLOG_HEADER_OVERFLOW', HypercoreError.OPLOG_HEADER_OVERFLOW, discoveryKey)
  }

  static INVALID_OPLOG_VERSION (msg = 'Invalid header version', discoveryKey = null) {
    return new HypercoreError(msg, 'INVALID_OPLOG_VERSION', HypercoreError.INVALID_OPLOG_VERSION, discoveryKey)
  }

  static WRITE_FAILED (msg = 'Write to storage failed', discoveryKey = null) {
    return new HypercoreError(msg, 'WRITE_FAILED', HypercoreError.WRITE_FAILED, discoveryKey)
  }

  static DECODING_ERROR (msg = 'Decoding error', discoveryKey = null) {
    return new HypercoreError(msg, 'DECODING_ERROR', HypercoreError.DECODING_ERROR, discoveryKey)
  }

  static SESSION_MOVED (msg = 'Session moved', discoveryKey = null) {
    return new HypercoreError(msg, 'SESSION_MOVED', HypercoreError.SESSION_MOVED, discoveryKey)
  }
}
{
  "name": "hypercore-errors",
  "version": "1.5.0",
  "description": "Hypercore errors",
  "main": "index.js",
  "files": [
    "index.js"
  ],
  "scripts": {
    "test": "standard && brittle test.js"
  },
  "repository": {
    "type": "git",
    "url": "https://github.com/holepunchto/hypercore-errors.git"
  },
  "author": "Holepunch",
  "license": "Apache-2.0",
  "bugs": {
    "url": "https://github.com/holepunchto/hypercore-errors/issues"
  },
  "homepage": "https://github.com/holepunchto/hypercore-errors#readme",
  "devDependencies": {
    "b4a": "^1.6.7",
    "brittle": "^3.1.3",
    "standard": "^17.0.0"
  },
  "dependencies": {
    "hypercore-id-encoding": "^1.3.0"
  }
}
const z32 = require('z32')
const b4a = require('b4a')

module.exports = {
  encode,
  decode,
  normalize,
  isValid
}

function encode (key) {
  if (!b4a.isBuffer(key)) throw new Error('Key must be a Buffer')
  if (key.byteLength !== 32) throw new Error('Key must be 32-bytes long')
  return z32.encode(key)
}

function decode (id) {
  if (b4a.isBuffer(id)) {
    if (id.byteLength !== 32) throw new Error('ID must be 32-bytes long')
    return id
  }
  if (typeof id === 'string') {
    if (id.startsWith('pear://')) id = id.slice(7).split('/')[0]
    if (id.length === 52) return z32.decode(id)
    if (id.length === 64) {
      const buf = b4a.from(id, 'hex')
      if (buf.byteLength === 32) return buf
    }
  }
  throw new Error('Invalid Hypercore key')
}

function normalize (any) {
  return encode(decode(any))
}

function isValid (any) {
  try {
    decode(any)
    return true
  } catch {
    return false
  }
}
{
  "name": "hypercore-id-encoding",
  "version": "1.3.0",
  "description": "Convert Hypercore keys to/from z-base32 or hex",
  "main": "index.js",
  "scripts": {
    "test": "standard && brittle test.js"
  },
  "files": [
    "index.js"
  ],
  "dependencies": {
    "b4a": "^1.5.3",
    "z32": "^1.0.0"
  },
  "devDependencies": {
    "brittle": "^3.1.1",
    "hypercore": "^10.0.0",
    "random-access-memory": "^6.0.0",
    "standard": "^17.1.0"
  },
  "license": "Apache-2.0",
  "repository": {
    "type": "git",
    "url": "git+https://github.com/holepunchto/hypercore-id-encoding.git"
  },
  "keywords": [],
  "author": "",
  "bugs": {
    "url": "https://github.com/holepunchto/hypercore-id-encoding/issues"
  },
  "homepage": "https://github.com/holepunchto/hypercore-id-encoding#readme"
}
const RocksDB = require('rocksdb-native')
const rrp = require('resolve-reject-promise')
const ScopeLock = require('scope-lock')
const DeviceFile = require('device-file')
const path = require('path')
const fs = require('fs')
const View = require('./lib/view.js')

const VERSION = 1
const COLUMN_FAMILY = 'corestore'

const { store, core } = require('./lib/keys.js')

const { CorestoreRX, CorestoreTX, CoreTX, CoreRX } = require('./lib/tx.js')

const {
  createCoreStream,
  createAliasStream,
  createDiscoveryKeyStream,
  createBlockStream,
  createBitfieldStream,
  createUserDataStream,
  createTreeNodeStream,
  createLocalStream
} = require('./lib/streams.js')

const EMPTY = new View()

class Atom {
  constructor(db) {
    this.db = db
    this.view = new View()
    this.flushedPromise = null
    this.flushing = false
    this.flushes = []
  }

  onflush(fn) {
    this.flushes.push(fn)
  }

  flushed() {
    if (!this.flushing) return Promise.resolve()
    if (this.flushedPromise !== null) return this.flushedPromise.promise
    this.flushedPromise = rrp()
    return this.flushedPromise.promise
  }

  _resolve() {
    const f = this.flushedPromise
    this.flushedPromise = null
    f.resolve()
  }

  async flush() {
    if (this.flushing) throw new Error('Atom already flushing')
    this.flushing = true

    try {
      await View.flush(this.view.changes, this.db)
      this.view.reset()

      const promises = []
      const len = this.flushes.length // in case of reentry
      for (let i = 0; i < len; i++) promises.push(this.flushes[i]())

      await Promise.all(promises)
    } finally {
      this.flushing = false
      if (this.flushedPromise !== null) this._resolve()
    }
  }
}

class HypercoreStorage {
  constructor(store, db, core, view, atom) {
    this.store = store
    this.db = db
    this.core = core
    this.view = view
    this.atom = atom

    this.view.readStart()
  }

  get readOnly() {
    return this.store.readOnly
  }

  get dependencies() {
    return this.core.dependencies
  }

  getDependencyLength() {
    return this.core.dependencies.length
      ? this.core.dependencies[this.core.dependencies.length - 1].length
      : -1
  }

  getDependency(length) {
    for (let i = this.core.dependencies.length - 1; i >= 0; i--) {
      const dep = this.core.dependencies[i]
      if (dep.length < length) return dep
    }

    return null
  }

  setDependencyHead(dep) {
    const deps = this.core.dependencies

    for (let i = deps.length - 1; i >= 0; i--) {
      const d = deps[i]

      if (d.dataPointer !== dep.dataPointer) continue

      // check if nothing changed
      if (d.length === dep.length && i === deps.length - 1) return

      this.core = {
        corePointer: this.core.corePointer,
        dataPointer: this.core.dataPointer,
        dependencies: deps.slice(0, i + 1)
      }

      this.core.dependencies[i] = {
        dataPointer: dep.dataPointer,
        length: dep.length
      }
    }

    this.core.dependencies = [
      {
        dataPointer: dep.dataPointer,
        length: dep.length
      }
    ]
  }

  // TODO: this might have to be async if the dependents have changed, but prop ok for now
  updateDependencyLength(length, truncated) {
    const deps = this.core.dependencies

    const i = this.findDependencyIndex(length, truncated)
    if (i === -1) throw new Error('Dependency not found')

    this.core = {
      corePointer: this.core.corePointer,
      dataPointer: this.core.dataPointer,
      dependencies: deps.slice(0, i + 1)
    }

    if (this.core.dependencies[i].length !== length) {
      this.core.dependencies[i] = {
        dataPointer: deps[i].dataPointer,
        length
      }
    }
  }

  findDependencyIndex(length, truncated) {
    const deps = this.core.dependencies

    if (truncated) {
      for (let i = 0; i < deps.length; i++) {
        if (deps[i].length >= length) return i
      }

      return -1
    }

    for (let i = deps.length - 1; i >= 0; i--) {
      if (deps[i].length <= length) return i
    }

    return -1
  }

  get snapshotted() {
    return this.db._snapshot !== null
  }

  snapshot() {
    return new HypercoreStorage(
      this.store,
      this.db.snapshot(),
      this.core,
      this.view.snapshot(),
      this.atom
    )
  }

  compact() {
    return Promise.all([
      this.db.compactRange(core.core(this.core.corePointer), core.core(this.core.corePointer)),
      this.db.compactRange(core.data(this.core.dataPointer), core.data(this.core.dataPointer))
    ])
  }

  atomize(atom) {
    if (this.atom && this.atom !== atom) {
      throw new Error('Cannot atomize and atomized session with a new atom')
    }
    return new HypercoreStorage(this.store, this.db.session(), this.core, atom.view, atom)
  }

  createAtom() {
    return this.store.createAtom()
  }

  createBlockStream(opts) {
    return createBlockStream(this.core, this.db, this.view, opts)
  }

  createTreeNodeStream(opts) {
    return createTreeNodeStream(this.core, this.db, this.view, opts)
  }

  createBitfieldStream(opts) {
    return createBitfieldStream(this.core, this.db, this.view, opts)
  }

  createUserDataStream(opts) {
    return createUserDataStream(this.core, this.db, this.view, opts)
  }

  createLocalStream(opts) {
    return createLocalStream(this.core, this.db, this.view, opts)
  }

  async resumeSession(name) {
    const rx = this.read()
    const existingSessionsPromise = rx.getSessions()

    rx.tryFlush()
    const existingSessions = await existingSessionsPromise

    const sessions = existingSessions || []
    const session = getBatch(sessions, name, false)

    if (session === null) return null

    const core = {
      corePointer: this.core.corePointer,
      dataPointer: session.dataPointer,
      dependencies: []
    }

    const coreRx = new CoreRX(core, this.db, this.view)

    const dependencyPromise = coreRx.getDependency()
    coreRx.tryFlush()

    const dependency = await dependencyPromise
    if (dependency) core.dependencies = this._addDependency(dependency)

    return new HypercoreStorage(
      this.store,
      this.db.session(),
      core,
      this.atom ? this.view : new View(),
      this.atom
    )
  }

  async createSession(name, head) {
    const rx = this.read()

    const existingSessionsPromise = rx.getSessions()
    const existingHeadPromise = rx.getHead()

    rx.tryFlush()

    const [existingSessions, existingHead] = await Promise.all([
      existingSessionsPromise,
      existingHeadPromise
    ])
    if (head === null) head = existingHead

    if (existingHead !== null && head.length > existingHead.length) {
      throw new Error('Invalid head passed, ahead of core')
    }

    const sessions = existingSessions || []
    const session = getBatch(sessions, name, true)
    const fresh = session.dataPointer === -1

    if (fresh) {
      session.dataPointer = await this.store._allocData()
    }

    const tx = this.write()

    tx.setSessions(sessions)

    const length = head === null ? 0 : head.length
    const core = {
      corePointer: this.core.corePointer,
      dataPointer: session.dataPointer,
      dependencies: this._addDependency({
        dataPointer: this.core.dataPointer,
        length
      })
    }

    const coreTx = new CoreTX(core, this.db, tx.view, tx.changes)

    if (length > 0) coreTx.setHead(head)
    coreTx.setDependency(core.dependencies[core.dependencies.length - 1])

    if (!fresh) {
      // nuke all existing state...
      coreTx.deleteBlockRange(0, -1)
      coreTx.deleteTreeNodeRange(0, -1)
      coreTx.deleteBitfieldPageRange(0, -1)
    }

    await tx.flush()

    return new HypercoreStorage(
      this.store,
      this.db.session(),
      core,
      this.atom ? this.view : new View(),
      this.atom
    )
  }

  async createAtomicSession(atom, head) {
    const length = head === null ? 0 : head.length
    const core = {
      corePointer: this.core.corePointer,
      dataPointer: this.core.dataPointer,
      dependencies: this._addDependency(null)
    }

    const coreTx = new CoreTX(core, this.db, atom.view, [])

    if (length > 0) coreTx.setHead(head)

    await coreTx.flush()

    return this.atomize(atom)
  }

  _addDependency(dep) {
    const deps = []

    for (let i = 0; i < this.core.dependencies.length; i++) {
      const d = this.core.dependencies[i]

      if (dep !== null && d.length > dep.length) {
        if (d.dataPointer !== dep.dataPointer) {
          deps.push({ dataPointer: d.dataPointer, length: dep.length })
        }
        return deps
      }

      deps.push(d)
    }

    if (
      dep !== null &&
      (deps.length === 0 || deps[deps.length - 1].dataPointer !== dep.dataPointer)
    ) {
      deps.push(dep)
    }
    return deps
  }

  read() {
    return new CoreRX(this.core, this.db, this.view)
  }

  write() {
    return new CoreTX(this.core, this.db, this.atom ? this.view : null, [])
  }

  close() {
    if (this.view !== null) {
      this.view.readStop()
      this.view = null
    }

    return this.db.close()
  }

  static async export(ptr, db, { batches = false } = {}) {
    const rx = new CoreRX(ptr, db, EMPTY)

    const core = {
      head: null,
      auth: null,
      sessions: [],
      data: null
    }

    const sessionsPromise = rx.getSessions()
    const headPromise = rx.getHead()
    const authPromise = rx.getAuth()

    rx.tryFlush()

    const [sessions, head, auth] = await Promise.all([sessionsPromise, headPromise, authPromise])

    core.head = head
    core.auth = { ...auth, keyPair: null }
    if (sessions) core.sessions = sessions.map((s) => s.name)

    const data = []

    data.push(exportData(ptr, db))

    if (batches) {
      for (const { dataPointer } of sessions) {
        data.push(exportData({ dataPointer, dependencies: [] }, db))
      }
    }

    core.data = await Promise.all(data)

    return core
  }
}

class CorestoreStorage {
  constructor(db, opts = {}) {
    const storage = typeof db === 'string' ? db : null

    this.bootstrap = storage !== null
    this.path = storage !== null ? storage : path.join(db.path, '..')
    this.readOnly = !!opts.readOnly
    this.allowBackup = !!opts.allowBackup
    this.deviceFile = null
    this.wait = !!opts.wait

    // tmp sync fix for simplicty since not super deployed yet
    if (this.bootstrap && !this.readOnly) tmpFixStorage(this.path)

    this.id = opts.id || null
    this.view = null
    this.enters = 0
    this.lock = new ScopeLock()
    this.flushing = null
    this.version = 0
    this.migrating = null

    if ((this.bootstrap && !this.readOnly && !this.allowBackup) || this.wait) {
      const corestoreFile = path.join(this.path, 'CORESTORE')

      this.deviceFile = new DeviceFile(corestoreFile, {
        lock: true,
        wait: this.wait,
        data: { id: this.id }
      })
    }

    const dbPath = path.join(this.path, 'db')

    this.rocks = storage === null ? db : new RocksDB(dbPath, { ...opts, lock: this.deviceFile })
    this.db = createColumnFamily(this.rocks, opts)
  }

  get opened() {
    return this.db.opened
  }

  get closed() {
    return this.db.closed
  }

  async ready() {
    if (this.version === 0) await this._migrateStore()
    return this.db.ready()
  }

  compact() {
    return this.db.compactRange()
  }

  async audit() {
    for await (const { core } of this.createCoreStream()) {
      const coreRx = new CoreRX(core, this.db, EMPTY)
      const authPromise = coreRx.getAuth()

      coreRx.tryFlush()

      const auth = await authPromise

      if (!auth.manifest || auth.manifest.version > 0) continue
      if (auth.manifest.linked === null) continue

      auth.manifest.linked = null
      const coreTx = new CoreTX(core, this.db, null, [])
      coreTx.setAuth(auth)
      await coreTx.flush()
    }
  }

  async deleteCore(ptr) {
    const rx = new CoreRX(ptr, this.db, EMPTY)

    const authPromise = rx.getAuth()
    const sessionsPromise = rx.getSessions()

    rx.tryFlush()

    const auth = await authPromise
    const sessions = await sessionsPromise

    // no core stored here
    if (!auth) return

    const tx = this.db.write({ autoDestroy: true })

    tx.tryDelete(store.core(auth.discoveryKey))

    // clear core
    const start = core.core(ptr.corePointer)
    const end = core.core(ptr.corePointer + 1)
    tx.tryDeleteRange(start, end)

    if (sessions) {
      for (const { dataPointer } of sessions) {
        const start = core.data(dataPointer)
        const end = core.data(dataPointer + 1)
        tx.tryDeleteRange(start, end)
      }
    }

    return tx.flush()
  }

  static isCoreStorage(db) {
    return isCorestoreStorage(db)
  }

  static from(db) {
    if (isCorestoreStorage(db)) return db
    return new this(db)
  }

  async _flush() {
    while (this.enters > 0) {
      await this.lock.lock()
      await this.lock.unlock()
    }
  }

  // runs pre any other mutation and read
  async _migrateStore() {
    const view = await this._enter()

    try {
      if (this.version === VERSION) return

      await this._preopen
      await this.db.ready()

      const rx = new CorestoreRX(this.db, view)
      const headPromise = rx.getHead()

      rx.tryFlush()
      const head = await headPromise

      const version = head === null ? 0 : head.version
      if (version === VERSION) {
        this.version = VERSION
        return
      }

      const target = { version: VERSION, dryRun: false }

      switch (version) {
        case 0: {
          await require('./migrations/0').store(this, target)
          break
        }
        default: {
          throw new Error(
            'Unsupported version: ' + version + ' - you should probably upgrade your dependencies'
          )
        }
      }

      this.version = VERSION
    } finally {
      await this._exit()
    }
  }

  // runs pre the core is returned to the user
  async _migrateCore(core, discoveryKey, version, locked) {
    const view = locked ? this.view : await this._enter()
    try {
      if (version === VERSION) return

      const target = { version: VERSION, dryRun: false }

      switch (version) {
        case 0: {
          await require('./migrations/0').core(core, target)
          break
        }
        default: {
          throw new Error(
            'Unsupported version: ' + version + ' - you should probably upgrade your dependencies'
          )
        }
      }

      if (locked === false) return

      // if its locked, then move the core state into the memview
      // in case the core is reopened from the memview, pre flush

      const rx = new CorestoreRX(this.db, EMPTY)
      const tx = new CorestoreTX(view)

      const corePromise = rx.getCore(discoveryKey)
      rx.tryFlush()

      tx.putCore(discoveryKey, await corePromise)
      tx.apply()
    } finally {
      if (!locked) await this._exit()
    }
  }

  async _enter() {
    this.enters++
    await this.lock.lock()
    if (this.view === null) this.view = new View()
    return this.view
  }

  async _exit() {
    this.enters--

    if (this.flushing === null) this.flushing = rrp()
    const flushed = this.flushing.promise

    if (this.enters === 0 || this.view.size() > 128) {
      try {
        await View.flush(this.view.changes, this.db)
        this.flushing.resolve()
      } catch (err) {
        this.flushing.reject(err)
      } finally {
        this.flushing = null
        this.view = null
      }
    }

    this.lock.unlock()
    return flushed
  }

  // when used with core catches this isnt transactional for simplicity, HOWEVER, its just a number
  // so worth the tradeoff
  async _allocData() {
    let dataPointer = 0

    const view = await this._enter()
    const tx = new CorestoreTX(view)

    try {
      const head = await this._getHead(view)

      dataPointer = head.allocated.datas++

      tx.setHead(head)
      tx.apply()
    } finally {
      await this._exit()
    }

    return dataPointer
  }

  // exposes here so migrations can easily access the head in an init state
  async _getHead(view) {
    const rx = new CorestoreRX(this.db, view)
    const headPromise = rx.getHead()
    rx.tryFlush()

    const head = await headPromise
    return head === null ? initStoreHead() : head
  }

  createAtom() {
    return new Atom(this.db)
  }

  async flush() {
    await this.rocks.flush()
  }

  async close() {
    if (this.db.closed) return
    await this._flush()
    await this.db.close()
    await this.rocks.close()
    if (this.deviceFile) await this.deviceFile.close()
  }

  async clear() {
    if (this.version === 0) await this._migrateStore()

    const view = await this._enter()
    const tx = new CorestoreTX(view)

    tx.clear()
    tx.apply()

    await this._exit()
  }

  createCoreStream() {
    // TODO: be nice to run the mgiration here also, but too much plumbing atm
    return createCoreStream(this.db, EMPTY)
  }

  createAliasStream(namespace) {
    // TODO: be nice to run the mgiration here also, but too much plumbing atm
    return createAliasStream(this.db, EMPTY, namespace)
  }

  createDiscoveryKeyStream(namespace) {
    return createDiscoveryKeyStream(this.db, EMPTY, namespace)
  }

  async getAlias(alias) {
    if (this.version === 0) await this._migrateStore()

    const rx = new CorestoreRX(this.db, EMPTY)
    const discoveryKeyPromise = rx.getCoreByAlias(alias)
    rx.tryFlush()
    return discoveryKeyPromise
  }

  async getSeed() {
    if (this.version === 0) await this._migrateStore()

    const rx = new CorestoreRX(this.db, EMPTY)
    const headPromise = rx.getHead()

    rx.tryFlush()

    const head = await headPromise
    return head === null ? null : head.seed
  }

  async setSeed(seed, { overwrite = true } = {}) {
    if (this.version === 0) await this._migrateStore()

    const view = await this._enter()
    const tx = new CorestoreTX(view)

    try {
      const rx = new CorestoreRX(this.db, view)
      const headPromise = rx.getHead()

      rx.tryFlush()

      const head = (await headPromise) || initStoreHead()

      if (head.seed === null || overwrite) head.seed = seed
      tx.setHead(head)
      tx.apply()

      return head.seed
    } finally {
      await this._exit()
    }
  }

  async getDefaultDiscoveryKey() {
    if (this.version === 0) await this._migrateStore()

    const rx = new CorestoreRX(this.db, EMPTY)
    const headPromise = rx.getHead()

    rx.tryFlush()

    const head = await headPromise
    return head === null ? null : head.defaultDiscoveryKey
  }

  async setDefaultDiscoveryKey(discoveryKey, { overwrite = true } = {}) {
    if (this.version === 0) await this._migrateStore()

    const view = await this._enter()
    const tx = new CorestoreTX(view)

    try {
      const rx = new CorestoreRX(this.db, view)
      const headPromise = rx.getHead()

      rx.tryFlush()

      const head = (await headPromise) || initStoreHead()

      if (head.defaultDiscoveryKey === null || overwrite) head.defaultDiscoveryKey = discoveryKey
      tx.setHead(head)
      tx.apply()

      return head.defaultDiscoveryKey
    } finally {
      await this._exit()
    }
  }

  async hasCore(discoveryKey, { ifMigrated = false } = {}) {
    if (this.version === 0) await this._migrateStore()

    const rx = new CorestoreRX(this.db, EMPTY)
    const promise = rx.getCore(discoveryKey)

    rx.tryFlush()

    const core = await promise

    if (core === null) return false
    if (core.version !== VERSION && ifMigrated) return false

    return true
  }

  async getAuth(discoveryKey) {
    if (this.version === 0) await this._migrateStore()

    const rx = new CorestoreRX(this.db, EMPTY)
    const corePromise = rx.getCore(discoveryKey)

    rx.tryFlush()

    const core = await corePromise
    if (core === null) return null

    const read = this.db.read({ autoDestroy: true })
    const authPromise = CoreRX.getAuth(read, core)

    read.tryFlush()

    return authPromise
  }

  async getInfo(discoveryKey, opts) {
    return (await this.getInfos([discoveryKey], opts))[0]
  }

  async getInfos(discoveryKeys, { auth = true, head = true, hints = true } = {}) {
    if (this.version === 0) await this._migrateStore()

    const rx = new CorestoreRX(this.db, EMPTY)
    const corePromises = new Array(discoveryKeys.length)

    for (let i = 0; i < discoveryKeys.length; i++) {
      corePromises[i] = rx.getCore(discoveryKeys[i])
    }

    rx.tryFlush()

    const cores = await Promise.all(corePromises)
    const read = this.db.read({ autoDestroy: true })

    const resultPromises = new Array(cores.length)

    for (let i = 0; i < cores.length; i++) {
      resultPromises[i] = cores[i]
        ? getInfoFromBatch(read, cores[i], discoveryKeys[i], auth, head, hints)
        : null
    }

    read.tryFlush()

    return Promise.all(resultPromises)
  }

  async suspend() {
    await this.db.suspend()
    if (this.deviceFile) await this.deviceFile.suspend()
  }

  async resume() {
    if (this.deviceFile) await this.deviceFile.resume()
    await this.db.resume()
  }

  async resumeCore(discoveryKey) {
    if (this.version === 0) await this._migrateStore()

    if (!discoveryKey) {
      discoveryKey = await this.getDefaultDiscoveryKey()
      if (!discoveryKey) return null
    }

    const rx = new CorestoreRX(this.db, EMPTY)
    const corePromise = rx.getCore(discoveryKey)

    rx.tryFlush()
    const core = await corePromise

    if (core === null) return null
    return this._resumeFromPointers(EMPTY, discoveryKey, false, core)
  }

  async export(discoveryKey, opts) {
    const rx = new CorestoreRX(this.db, EMPTY)
    const corePromise = rx.getCore(discoveryKey)

    rx.tryFlush()
    const core = await corePromise
    if (core === null) return null

    let { dataPointer, corePointer } = core

    const ptr = { corePointer, dataPointer, dependencies: [] }

    while (true) {
      const rx = new CoreRX({ dataPointer, corePointer: 0, dependencies: [] }, this.db, EMPTY)
      const dependencyPromise = rx.getDependency()
      rx.tryFlush()
      const dependency = await dependencyPromise
      if (!dependency) break
      ptr.dependencies.push(dependency)
      dataPointer = dependency.dataPointer
    }

    const session = this.db.session()
    try {
      return await HypercoreStorage.export(ptr, session, opts)
    } finally {
      await session.close()
    }
  }

  async _resumeFromPointers(view, discoveryKey, create, { version, corePointer, dataPointer }) {
    const core = { corePointer, dataPointer, dependencies: [] }

    while (true) {
      const rx = new CoreRX({ dataPointer, corePointer: 0, dependencies: [] }, this.db, view)
      const dependencyPromise = rx.getDependency()
      rx.tryFlush()
      const dependency = await dependencyPromise
      if (!dependency) break
      core.dependencies.push(dependency)
      dataPointer = dependency.dataPointer
    }

    const result = new HypercoreStorage(this, this.db.session(), core, EMPTY, null)

    if (version < VERSION) await this._migrateCore(result, discoveryKey, version, create)
    return result
  }

  // not allowed to throw validation errors as its a shared tx!
  async _create(
    view,
    { key, manifest, keyPair, encryptionKey, discoveryKey, alias, userData, core: ptrs }
  ) {
    const rx = new CorestoreRX(this.db, view)
    const tx = new CorestoreTX(view)

    const corePromise = rx.getCore(discoveryKey)
    const headPromise = rx.getHead()

    rx.tryFlush()

    let [core, head] = await Promise.all([corePromise, headPromise])
    if (core) return this._resumeFromPointers(view, discoveryKey, true, core)

    if (head === null) head = initStoreHead()
    if (head.defaultDiscoveryKey === null) head.defaultDiscoveryKey = discoveryKey

    const corePointer = ptrs ? ptrs.corePointer : head.allocated.cores++
    const dataPointer = ptrs ? ptrs.dataPointer : head.allocated.datas++

    core = { version: VERSION, corePointer, dataPointer, alias }

    tx.setHead(head)
    tx.putCore(discoveryKey, core)
    if (alias) tx.putCoreByAlias(alias, discoveryKey)

    const ptr = { corePointer, dataPointer, dependencies: [] }
    const ctx = new CoreTX(ptr, this.db, view, tx.changes)

    ctx.setAuth({
      key,
      discoveryKey,
      manifest,
      keyPair,
      encryptionKey
    })

    if (userData) {
      for (const { key, value } of userData) {
        ctx.putUserData(key, value)
      }
    }

    tx.apply()

    return new HypercoreStorage(this, this.db.session(), ptr, EMPTY, null)
  }

  async createCore(data) {
    if (this.version === 0) await this._migrateStore()

    const view = await this._enter()

    try {
      return await this._create(view, data)
    } finally {
      await this._exit()
    }
  }
}

module.exports = CorestoreStorage

function initStoreHead() {
  return {
    version: 0, // cause we wanna run the migration
    allocated: {
      datas: 0,
      cores: 0
    },
    seed: null,
    defaultDiscoveryKey: null
  }
}

function getBatch(sessions, name, alloc) {
  for (let i = 0; i < sessions.length; i++) {
    if (sessions[i].name === name) return sessions[i]
  }

  if (!alloc) return null

  const result = { name, dataPointer: -1 }
  sessions.push(result)
  return result
}

function isCorestoreStorage(s) {
  return typeof s === 'object' && !!s && typeof s.setDefaultDiscoveryKey === 'function'
}

function createColumnFamily(db, opts = {}) {
  const {
    tableCacheIndexAndFilterBlocks = true,
    blockCache = true,
    optimizeFiltersForMemory = false
  } = opts

  const col = new RocksDB.ColumnFamily(COLUMN_FAMILY, {
    enableBlobFiles: true,
    minBlobSize: 4096,
    blobFileSize: 256 * 1024 * 1024,
    enableBlobGarbageCollection: true,
    tableBlockSize: 8192,
    tableCacheIndexAndFilterBlocks,
    tableFormatVersion: 6,
    optimizeFiltersForMemory,
    blockCache
  })

  return db.columnFamily(col)
}

// TODO: remove in like 3-6 mo
function tmpFixStorage(p) {
  // if CORESTORE file is written, new format
  if (fs.existsSync(path.join(p, 'CORESTORE'))) return

  let files = []

  try {
    files = fs.readdirSync(p)
  } catch {}

  const notRocks = new Set([
    'CORESTORE',
    'primary-key',
    'cores',
    'app-preferences',
    'cache',
    'preferences.json',
    'db',
    'clone',
    'core',
    'notifications'
  ])

  for (const f of files) {
    if (notRocks.has(f)) continue

    try {
      fs.mkdirSync(path.join(p, 'db'))
    } catch {}

    fs.renameSync(path.join(p, f), path.join(p, 'db', f))
  }
}

async function exportData(ptr, db, opts) {
  // just need dataPointer
  const reads = [
    toArray(createBlockStream(ptr, db, EMPTY, opts)),
    toArray(createTreeNodeStream(ptr, db, EMPTY, opts)),
    toArray(createBitfieldStream(ptr, db, EMPTY, opts))
  ]

  const [blocks, tree, bitfield] = await Promise.all(reads)

  return {
    blocks,
    tree,
    bitfield
  }
}

async function toArray(stream) {
  const all = []
  for await (const e of stream) all.push(e)
  return all
}

function noop() {}

async function getInfoFromBatch(db, c, discoveryKey, getAuth, getHead, getHints) {
  const authPromise = getAuth ? CoreRX.getAuth(db, c) : null
  const headPromise = getHead ? CoreRX.getHead(db, c) : null
  const hintsPromise = getHints ? CoreRX.getHints(db, c) : null

  // ensure no uncaughts
  if (authPromise) authPromise.catch(noop)
  if (headPromise) headPromise.catch(noop)
  if (hintsPromise) hintsPromise.catch(noop)

  return {
    discoveryKey,
    core: c,
    auth: await authPromise,
    head: await headPromise,
    hints: await hintsPromise
  }
}
const { Readable, getStreamError } = require('streamx')
const { core } = require('./keys')

module.exports = class BlockStream extends Readable {
  constructor(core, db, updates, start, end, reverse) {
    super()

    this.core = core
    this.db = db
    this.updates = updates
    this.start = start
    this.end = end
    this.reverse = reverse === true

    this._drained = true
    this._consumed = 0
    this._stream = null
    this._oncloseBound = this._onclose.bind(this)
    this._maybeDrainBound = this._maybeDrain.bind(this)

    this._update()
  }

  _update() {
    if (this._consumed > this.core.dependencies.length) return

    const deps = this.core.dependencies
    const index = this._findDependencyIndex(deps)

    const curr = index < deps.length ? deps[index] : null
    const prev = index > 0 && index - 1 < deps.length ? deps[index - 1] : null

    const start = prev && prev.length > this.start ? prev.length : this.start
    const end = curr && (this.end === -1 || curr.length < this.end) ? curr.length : this.end

    const ptr = curr ? curr.dataPointer : this.core.dataPointer

    this._makeStream(core.block(ptr, start), core.block(ptr, end))
  }

  _findDependencyIndex(deps) {
    if (!this.reverse) return this._consumed++

    let i = deps.length - this._consumed++
    while (i > 0) {
      if (deps[i - 1].length <= this.end) return i
      i--
      this._consumed++
    }

    return 0
  }

  _predestroy() {
    if (this._stream !== null) this._stream.destroy()
  }

  _read(cb) {
    this._drained = this._onreadable()
    cb(null)
  }

  _maybeDrain() {
    if (this._drained === true) return
    this._drained = this._onreadable()
  }

  _onreadable() {
    if (this._stream === null) {
      this.push(null)
      return true
    }

    let data = this._stream.read()

    if (data === null) return false

    do {
      this.push(data)
      data = this._stream.read()
    } while (data !== null)

    return true
  }

  _onclose() {
    if (this.destroying) return

    const err = getStreamError(this._stream)

    if (err !== null) {
      this.destroy(err)
      return
    }

    // empty the current stream
    if (this._onreadable() === true) this._drained = true

    this._stream = null

    this._update()
    this._maybeDrain()
  }

  _makeStream(start, end) {
    this._stream = this.updates.iterator(this.db, start, end, this.reverse)
    this._stream.on('readable', this._maybeDrainBound)
    this._stream.on('error', noop)
    this._stream.on('close', this._oncloseBound)
  }
}

function noop() {}
const { Readable } = require('streamx')

// used for returned a stream that just errors (during read during teardown)

module.exports = class CloseErrorStream extends Readable {
  constructor(err) {
    super()
    this.error = err
  }

  _open(cb) {
    cb(this.error)
  }
}
const { UINT, STRING } = require('index-encoder')
const c = require('compact-encoding')
const b4a = require('b4a')

const TL_HEAD = 0
const TL_CORE_BY_DKEY = 1
const TL_CORE_BY_ALIAS = 2
const TL_CORE = 3
const TL_DATA = 4

const TL_END = TL_DATA + 1

const CORE_AUTH = 0
const CORE_SESSIONS = 1

const DATA_HEAD = 0
const DATA_DEPENDENCY = 1
const DATA_HINTS = 2
const DATA_BLOCK = 3
const DATA_TREE = 4
const DATA_BITFIELD = 5
const DATA_USER_DATA = 6
const DATA_LOCAL = 7

const slab = { buffer: b4a.allocUnsafe(65536), start: 0, end: 0 }

const store = {}
const core = {}

store.clear = function () {
  const state = alloc()
  let start = state.start
  UINT.encode(state, 0)
  const a = state.buffer.subarray(start, state.start)
  start = state.start
  UINT.encode(state, TL_END)
  const b = state.buffer.subarray(start, state.start)
  return [a, b]
}

store.head = function () {
  const state = alloc()
  const start = state.start
  UINT.encode(state, TL_HEAD)
  return state.buffer.subarray(start, state.start)
}

store.core = function (discoveryKey) {
  const state = alloc()
  const start = state.start
  UINT.encode(state, TL_CORE_BY_DKEY)
  c.fixed32.encode(state, discoveryKey)
  return state.buffer.subarray(start, state.start)
}

store.coreStart = function () {
  const state = alloc()
  const start = state.start
  UINT.encode(state, TL_CORE_BY_DKEY)
  return state.buffer.subarray(start, state.start)
}

store.coreEnd = function () {
  const state = alloc()
  const start = state.start
  UINT.encode(state, TL_CORE_BY_DKEY + 1)
  return state.buffer.subarray(start, state.start)
}

store.coreByAlias = function ({ namespace, name }) {
  const state = alloc()
  const start = state.start
  UINT.encode(state, TL_CORE_BY_ALIAS)
  c.fixed32.encode(state, namespace)
  STRING.encode(state, name)
  return state.buffer.subarray(start, state.start)
}

store.coreByAliasStart = function (namespace) {
  const state = alloc()
  const start = state.start
  UINT.encode(state, TL_CORE_BY_ALIAS)
  if (namespace) c.fixed32.encode(state, namespace)
  return state.buffer.subarray(start, state.start)
}

store.coreByAliasEnd = function (namespace) {
  const state = alloc()
  const start = state.start

  if (namespace) {
    UINT.encode(state, TL_CORE_BY_ALIAS)
    c.fixed32.encode(state, namespace)
    state.buffer[state.start++] = 0xff
  } else {
    UINT.encode(state, TL_CORE_BY_ALIAS + 1)
  }

  return state.buffer.subarray(start, state.start)
}

store.alias = function (buffer) {
  const state = { buffer, start: 0, end: buffer.byteLength }
  UINT.decode(state) // ns
  const namespace = c.fixed32.decode(state)
  const name = STRING.decode(state)
  return { namespace, name }
}

store.discoveryKey = function (buffer) {
  const state = { buffer, start: 0, end: buffer.byteLength }
  UINT.decode(state) // ns
  return c.fixed32.decode(state)
}

core.core = function (ptr) {
  const state = alloc()
  const start = state.start
  UINT.encode(state, TL_CORE)
  UINT.encode(state, ptr)
  return state.buffer.subarray(start, state.start)
}

core.data = function (ptr) {
  const state = alloc()
  const start = state.start
  UINT.encode(state, TL_DATA)
  UINT.encode(state, ptr)
  return state.buffer.subarray(start, state.start)
}

core.auth = function (ptr) {
  const state = alloc()
  const start = state.start
  UINT.encode(state, TL_CORE)
  UINT.encode(state, ptr)
  UINT.encode(state, CORE_AUTH)
  return state.buffer.subarray(start, state.start)
}

core.sessions = function (ptr) {
  const state = alloc()
  const start = state.start
  UINT.encode(state, TL_CORE)
  UINT.encode(state, ptr)
  UINT.encode(state, CORE_SESSIONS)
  return state.buffer.subarray(start, state.start)
}

core.head = function (ptr) {
  const state = alloc()
  const start = state.start
  UINT.encode(state, TL_DATA)
  UINT.encode(state, ptr)
  UINT.encode(state, DATA_HEAD)
  return state.buffer.subarray(start, state.start)
}

core.dependency = function (ptr) {
  const state = alloc()
  const start = state.start
  UINT.encode(state, TL_DATA)
  UINT.encode(state, ptr)
  UINT.encode(state, DATA_DEPENDENCY)
  return state.buffer.subarray(start, state.start)
}

core.hints = function (ptr) {
  const state = alloc()
  const start = state.start
  UINT.encode(state, TL_DATA)
  UINT.encode(state, ptr)
  UINT.encode(state, DATA_HINTS)
  return state.buffer.subarray(start, state.start)
}

core.block = function (ptr, index) {
  const state = alloc()
  const start = state.start
  UINT.encode(state, TL_DATA)
  UINT.encode(state, ptr)
  UINT.encode(state, DATA_BLOCK)
  UINT.encode(state, index)
  return state.buffer.subarray(start, state.start)
}

core.tree = function (ptr, index) {
  const state = alloc()
  const start = state.start
  UINT.encode(state, TL_DATA)
  UINT.encode(state, ptr)
  UINT.encode(state, DATA_TREE)
  UINT.encode(state, index)
  return state.buffer.subarray(start, state.start)
}

core.bitfield = function (ptr, index, type) {
  const state = alloc()
  const start = state.start
  UINT.encode(state, TL_DATA)
  UINT.encode(state, ptr)
  UINT.encode(state, DATA_BITFIELD)
  UINT.encode(state, index)
  UINT.encode(state, type)
  return state.buffer.subarray(start, state.start)
}

core.userData = function (ptr, key) {
  const state = alloc()
  const start = state.start
  UINT.encode(state, TL_DATA)
  UINT.encode(state, ptr)
  UINT.encode(state, DATA_USER_DATA)
  STRING.encode(state, key)
  return state.buffer.subarray(start, state.start)
}

core.userDataEnd = function (ptr) {
  const state = alloc()
  const start = state.start
  UINT.encode(state, TL_DATA)
  UINT.encode(state, ptr)
  UINT.encode(state, DATA_USER_DATA + 1)
  return state.buffer.subarray(start, state.start)
}

core.local = function (ptr, key) {
  if (key.byteLength > 2048) {
    throw new Error('local keys has an upper limit of 2048 bytes atm')
  }

  const state = alloc()
  const start = state.start
  UINT.encode(state, TL_DATA)
  UINT.encode(state, ptr)
  UINT.encode(state, DATA_LOCAL)

  state.buffer.set(key, state.start)
  state.start += key.byteLength
  return state.buffer.subarray(start, state.start)
}

core.localEnd = function (ptr) {
  const state = alloc()
  const start = state.start
  UINT.encode(state, TL_DATA)
  UINT.encode(state, ptr)
  UINT.encode(state, DATA_LOCAL + 1)
  return state.buffer.subarray(start, state.start)
}

core.blockIndex = function (buffer) {
  const state = { buffer, start: 0, end: buffer.byteLength }
  UINT.decode(state) // ns
  UINT.decode(state) // ptr
  UINT.decode(state) // type
  return UINT.decode(state)
}

core.bitfieldIndexAndType = function (buffer) {
  const state = { buffer, start: 0, end: buffer.byteLength }
  UINT.decode(state) // ns
  UINT.decode(state) // ptr
  UINT.decode(state) // type
  return [UINT.decode(state), UINT.decode(state)]
}

core.userDataKey = function (buffer) {
  const state = { buffer, start: 0, end: buffer.byteLength }
  UINT.decode(state) // ns
  UINT.decode(state) // ptr
  UINT.decode(state) // type
  return STRING.decode(state)
}

core.localKey = function (buffer) {
  const state = { buffer, start: 0, end: buffer.byteLength }
  UINT.decode(state) // ns
  UINT.decode(state) // ptr
  UINT.decode(state) // type
  return state.buffer.subarray(state.start, state.end)
}

module.exports = { store, core }

function alloc() {
  if (slab.buffer.byteLength - slab.start < 4096) {
    slab.buffer = b4a.allocUnsafe(slab.buffer.byteLength)
    slab.start = 0
  }
  return slab
}
const b4a = require('b4a')
const BlockDependencyStream = require('./block-dependency-stream.js')
const { core, store } = require('./keys.js')
const schema = require('../spec/hyperschema')

const CORESTORE_CORE = schema.getEncoding('@corestore/core')
const CORE_TREE_NODE = schema.getEncoding('@core/tree-node')
const EMPTY = b4a.alloc(0)

module.exports = {
  createBlockStream,
  createBitfieldStream,
  createUserDataStream,
  createCoreStream,
  createAliasStream,
  createDiscoveryKeyStream,
  createTreeNodeStream,
  createLocalStream
}

function createCoreStream(db, view) {
  const start = store.coreStart()
  const end = store.coreEnd()

  const ite = view.iterator(db, start, end, false)

  ite._readableState.map = mapCore
  return ite
}

function createDiscoveryKeyStream(db, view, namespace) {
  const start = namespace ? store.coreByAliasStart(namespace) : store.coreStart()
  const end = namespace ? store.coreByAliasEnd(namespace) : store.coreEnd()

  const ite = view.iterator(db, start, end, false)

  ite._readableState.map = namespace ? mapNamespaceDiscoveryKeys : mapAllDiscoveryKeys
  return ite
}

function createAliasStream(db, view, namespace) {
  const start = store.coreByAliasStart(namespace)
  const end = store.coreByAliasEnd(namespace)

  const ite = view.iterator(db, start, end, false)

  ite._readableState.map = mapAlias
  return ite
}

function createBlockIterator(ptr, db, view, start, end, reverse) {
  if (ptr.dependencies.length > 0) {
    return new BlockDependencyStream(ptr, db, view, start, end, reverse)
  }

  const s = core.block(ptr.dataPointer, start)
  const e = core.block(ptr.dataPointer, end === -1 ? Infinity : end)
  return view.iterator(db, s, e, reverse)
}

function createBlockStream(
  ptr,
  db,
  view,
  { gt = -1, gte = gt + 1, lte = -1, lt = lte === -1 ? -1 : lte + 1, reverse = false } = {}
) {
  const ite = createBlockIterator(ptr, db, view, gte, lt, reverse)

  ite._readableState.map = mapBlock
  return ite
}

function createBitfieldStream(
  ptr,
  db,
  view,
  { gt = -1, gte = gt + 1, lte = -1, lt = lte === -1 ? -1 : lte + 1, reverse = false } = {}
) {
  const s = core.bitfield(ptr.dataPointer, gte, 0)
  const e = core.bitfield(ptr.dataPointer, lt === -1 ? Infinity : lt, 0)
  const ite = view.iterator(db, s, e, false)

  ite._readableState.map = mapBitfield
  return ite
}

// NOTE: this does not do dependency lookups atm
function createTreeNodeStream(
  ptr,
  db,
  view,
  { gt = -1, gte = gt + 1, lte = -1, lt = lte === -1 ? -1 : lte + 1, reverse = false } = {}
) {
  const s = core.tree(ptr.dataPointer, gte, 0)
  const e = core.tree(ptr.dataPointer, lt === -1 ? Infinity : lt, 0)
  const ite = view.iterator(db, s, e, false)

  ite._readableState.map = mapTreeNode
  return ite
}

function createUserDataStream(
  ptr,
  db,
  view,
  { gt = null, gte = '', lte = null, lt = null, reverse = false } = {}
) {
  if (gt !== null || lte !== null) {
    throw new Error('gt and lte not yet supported for user data streams')
  }

  const s = core.userData(ptr.dataPointer, gte)
  const e = lt === null ? core.userDataEnd(ptr.dataPointer) : core.userData(ptr.dataPointer, lt)
  const ite = view.iterator(db, s, e, false)

  ite._readableState.map = mapUserData
  return ite
}

function createLocalStream(
  ptr,
  db,
  view,
  { gt = null, gte = EMPTY, lte = null, lt = null, reverse = false } = {}
) {
  if (gt !== null || lte !== null) throw new Error('gt and lte not yet supported for local streams')

  const s = core.local(ptr.dataPointer, gte)
  const e = lt === null ? core.localEnd(ptr.dataPointer) : core.local(ptr.dataPointer, lt)
  const ite = view.iterator(db, s, e, false)

  ite._readableState.map = mapLocal
  return ite
}

function mapBitfield(data) {
  const [index, type] = core.bitfieldIndexAndType(data.key)
  if (type !== 0) return null // ignore for now
  return { index, page: data.value }
}

function mapLocal(data) {
  const key = core.localKey(data.key)
  return { key, value: data.value }
}

function mapUserData(data) {
  const key = core.userDataKey(data.key)
  return { key, value: data.value }
}

function mapCore(data) {
  const discoveryKey = store.discoveryKey(data.key)
  const core = CORESTORE_CORE.decode({
    start: 0,
    end: data.value.byteLength,
    buffer: data.value
  })
  return { discoveryKey, core }
}

function mapAllDiscoveryKeys(data) {
  return store.discoveryKey(data.key)
}

function mapNamespaceDiscoveryKeys(data) {
  return data.value
}

function mapAlias(data) {
  const alias = store.alias(data.key)
  return { alias, discoveryKey: data.value }
}

function mapBlock(data) {
  return { index: core.blockIndex(data.key), value: data.value }
}

function mapTreeNode(data) {
  return CORE_TREE_NODE.decode({
    start: 0,
    end: data.value.byteLength,
    buffer: data.value
  })
}
const schema = require('../spec/hyperschema')
const { store, core } = require('./keys.js')
const View = require('./view.js')
const b4a = require('b4a')
const flat = require('flat-tree')

const CORESTORE_HEAD = schema.getEncoding('@corestore/head')
const CORESTORE_CORE = schema.getEncoding('@corestore/core')

const CORE_AUTH = schema.getEncoding('@core/auth')
const CORE_SESSIONS = schema.getEncoding('@core/sessions')
const CORE_HEAD = schema.getEncoding('@core/head')
const CORE_TREE_NODE = schema.getEncoding('@core/tree-node')
const CORE_DEPENDENCY = schema.getEncoding('@core/dependency')
const CORE_HINTS = schema.getEncoding('@core/hints')

class CoreTX {
  constructor(core, db, view, changes) {
    if (db.snapshotted) throw new Error('Cannot open core tx on snapshot')
    this.core = core
    this.db = db
    this.view = view
    this.changes = changes
  }

  setAuth(auth) {
    this.changes.push([core.auth(this.core.corePointer), encode(CORE_AUTH, auth), null])
  }

  setSessions(sessions) {
    this.changes.push([core.sessions(this.core.corePointer), encode(CORE_SESSIONS, sessions), null])
  }

  setHead(head) {
    this.changes.push([core.head(this.core.dataPointer), encode(CORE_HEAD, head), null])
  }

  deleteHead() {
    this.changes.push([core.head(this.core.dataPointer), null, null])
  }

  setDependency(dep) {
    this.changes.push([core.dependency(this.core.dataPointer), encode(CORE_DEPENDENCY, dep), null])
  }

  setHints(hints) {
    this.changes.push([core.hints(this.core.dataPointer), encode(CORE_HINTS, hints), null])
  }

  putBlock(index, data) {
    this.changes.push([core.block(this.core.dataPointer, index), data, null])
  }

  deleteBlock(index) {
    this.changes.push([core.block(this.core.dataPointer, index), null, null])
  }

  deleteBlockRange(start, end) {
    this.changes.push([
      core.block(this.core.dataPointer, start),
      null,
      core.block(this.core.dataPointer, end === -1 ? Infinity : end)
    ])
  }

  putBitfieldPage(index, data) {
    this.changes.push([core.bitfield(this.core.dataPointer, index, 0), data, null])
  }

  deleteBitfieldPage(index) {
    this.changes.push([core.bitfield(this.core.dataPointer, index, 0), null, null])
  }

  deleteBitfieldPageRange(start, end) {
    this.changes.push([
      core.bitfield(this.core.dataPointer, start, 0),
      null,
      core.bitfield(this.core.dataPointer, end === -1 ? Infinity : end, 0)
    ])
  }

  putTreeNode(node) {
    this.changes.push([
      core.tree(this.core.dataPointer, node.index),
      encode(CORE_TREE_NODE, node),
      null
    ])
  }

  deleteTreeNode(index) {
    this.changes.push([core.tree(this.core.dataPointer, index), null, null])
  }

  deleteTreeNodeRange(start, end) {
    this.changes.push([
      core.tree(this.core.dataPointer, start),
      null,
      core.tree(this.core.dataPointer, end === -1 ? Infinity : end)
    ])
  }

  putUserData(key, value) {
    const buffer = typeof value === 'string' ? b4a.from(value) : value
    this.changes.push([core.userData(this.core.dataPointer, key), buffer, null])
  }

  deleteUserData(key) {
    this.changes.push([core.userData(this.core.dataPointer, key), null, null])
  }

  putLocal(key, value) {
    this.changes.push([core.local(this.core.dataPointer, key), value, null])
  }

  deleteLocal(key) {
    this.changes.push([core.local(this.core.dataPointer, key), null, null])
  }

  deleteLocalRange(start, end) {
    this.changes.push([
      core.local(this.core.dataPointer, start),
      null,
      end === null ? core.localEnd(this.core.dataPointer) : core.local(this.core.dataPointer, end)
    ])
  }

  flush() {
    const changes = this.changes
    if (changes === null) return Promise.resolve(!this.view)

    this.changes = null

    if (this.view) {
      this.view.apply(changes)
      return Promise.resolve(false)
    }

    return View.flush(changes, this.db)
  }
}

class CoreRX {
  constructor(core, db, view) {
    this.core = core
    this.read = db.read({ autoDestroy: true })
    this.view = view

    view.readStart()
  }

  static async getAuth(db, c) {
    return await decode(CORE_AUTH, await db.get(core.auth(c.corePointer)))
  }

  async getAuth() {
    return await decode(CORE_AUTH, await this.view.get(this.read, core.auth(this.core.corePointer)))
  }

  async getSessions() {
    return await decode(
      CORE_SESSIONS,
      await this.view.get(this.read, core.sessions(this.core.corePointer))
    )
  }

  static async getHead(db, c) {
    return await decode(CORE_HEAD, await db.get(core.head(c.dataPointer)))
  }

  async getHead() {
    return await decode(CORE_HEAD, await this.view.get(this.read, core.head(this.core.dataPointer)))
  }

  async getDependency() {
    return await decode(
      CORE_DEPENDENCY,
      await this.view.get(this.read, core.dependency(this.core.dataPointer))
    )
  }

  static async getHints(db, c) {
    return await decode(CORE_HINTS, await db.get(core.hints(c.dataPointer)))
  }

  async getHints() {
    return await decode(
      CORE_HINTS,
      await this.view.get(this.read, core.hints(this.core.dataPointer))
    )
  }

  getBlock(index) {
    const dep = findBlockDependency(this.core.dependencies, index)
    const data = dep === null ? this.core.dataPointer : dep.dataPointer
    return this.view.get(this.read, core.block(data, index))
  }

  getBitfieldPage(index) {
    return this.view.get(this.read, core.bitfield(this.core.dataPointer, index, 0))
  }

  async getTreeNode(index) {
    const dep = findTreeDependency(this.core.dependencies, index)
    const data = dep === null ? this.core.dataPointer : dep.dataPointer
    return decode(CORE_TREE_NODE, await this.view.get(this.read, core.tree(data, index)))
  }

  async hasTreeNode(index) {
    return (await this.getTreeNode(index)) !== null
  }

  getUserData(key) {
    return this.view.get(this.read, core.userData(this.core.dataPointer, key))
  }

  getLocal(key) {
    return this.view.get(this.read, core.local(this.core.dataPointer, key))
  }

  tryFlush() {
    this.read.tryFlush()
    this._free()
  }

  destroy() {
    this.read.destroy()
    this._free()
  }

  _free() {
    if (this.view === null) return
    this.view.readStop()
    this.view = null
  }
}

class CorestoreTX {
  constructor(view) {
    this.view = view
    this.changes = []
  }

  setHead(head) {
    this.changes.push([store.head(), encode(CORESTORE_HEAD, head), null])
  }

  putCore(discoveryKey, ptr) {
    this.changes.push([store.core(discoveryKey), encode(CORESTORE_CORE, ptr), null])
  }

  putCoreByAlias(alias, discoveryKey) {
    this.changes.push([store.coreByAlias(alias), discoveryKey, null])
  }

  clear() {
    const [start, end] = store.clear()
    this.changes.push([start, null, end])
  }

  apply() {
    if (this.changes === null) return
    this.view.apply(this.changes)
    this.changes = null
  }
}

class CorestoreRX {
  constructor(db, view) {
    this.read = db.read({ autoDestroy: true })
    this.view = view

    view.readStart()
  }

  async getHead() {
    return decode(CORESTORE_HEAD, await this.view.get(this.read, store.head()))
  }

  async getCore(discoveryKey) {
    return decode(CORESTORE_CORE, await this.view.get(this.read, store.core(discoveryKey)))
  }

  getCoreByAlias(alias) {
    return this.view.get(this.read, store.coreByAlias(alias))
  }

  tryFlush() {
    this.read.tryFlush()
    this._free()
  }

  destroy() {
    this.read.destroy()
    this._free()
  }

  _free() {
    if (this.view === null) return
    this.view.readStop()
    this.view = null
  }
}

module.exports = { CorestoreTX, CorestoreRX, CoreTX, CoreRX }

function findBlockDependency(dependencies, index) {
  for (let i = 0; i < dependencies.length; i++) {
    const dep = dependencies[i]
    if (index < dep.length) return dep
  }

  return null
}

function findTreeDependency(dependencies, index) {
  for (let i = 0; i < dependencies.length; i++) {
    const dep = dependencies[i]
    if (flat.rightSpan(index) <= (dep.length - 1) * 2) return dep
  }

  return null
}

function decode(enc, buffer) {
  if (buffer === null) return null
  return enc.decode({ start: 0, end: buffer.byteLength, buffer })
}

function encode(enc, m) {
  // TODO: use fancy slab for small messages
  const state = { start: 0, end: 0, buffer: null }
  enc.preencode(state, m)
  state.buffer = b4a.allocUnsafe(state.end)
  enc.encode(state, m)
  return state.buffer
}
const { Readable, getStreamError } = require('streamx')
const CloseErrorStream = require('./close-error-stream.js')
const b4a = require('b4a')

class OverlayStream extends Readable {
  constructor(stream, start, end, reverse, changes, cleared) {
    super()

    this.start = start
    this.end = end
    this.reverse = reverse
    this.changes = changes
    this.cleared = cleared
    this.change = 0
    this.range = 0

    this._stream = stream
    this._drained = false

    this._stream.on('readable', this._drainMaybe.bind(this))
    this._stream.on('error', noop)
    this._stream.on('close', this._onclose.bind(this))
  }

  _drainMaybe() {
    if (this._drained === true) return
    this._drained = this._onreadable()
  }

  _onclose() {
    if (this.destroying) return

    const err = getStreamError(this._stream)

    if (err !== null) {
      this.destroy(err)
      return
    }

    while (this.change < this.changes.length) {
      const c = this.changes[this.change++]
      const key = c[0]
      const value = c[1]

      if (value !== null && this._inRange(key)) this.push({ key, value })
    }

    this.push(null)
    this._stream = null
  }

  _onreadable() {
    let data = this._stream.read()
    if (data === null) return false

    let drained = false

    do {
      if (this._push(data) === true) drained = true
      data = this._stream.read()
    } while (data !== null)

    return drained
  }

  _read(cb) {
    this._drained = this._onreadable()
    cb(null)
  }

  _predestroy() {
    this.stream.destroy()
  }

  _push(entry) {
    const key = entry.key

    while (this.range < this.cleared.length) {
      const c = this.cleared[this.range]

      // we moved past the range
      if (this.reverse ? b4a.compare(key, c[0]) < 0 : b4a.compare(c[2], key) <= 0) {
        this.range++
        continue
      }

      // we didnt move past and are in, drop
      if (b4a.compare(c[0], key) <= 0 && b4a.compare(key, c[2]) < 0) {
        return false
      }

      break
    }

    let updated = false

    while (this.change < this.changes.length) {
      const c = this.changes[this.change]
      const key = c[0]
      const value = typeof c[1] === 'string' ? b4a.from(c[1]) : c[1]
      const cmp = b4a.compare(key, entry.key)

      // same value, if not deleted, return new one
      if (cmp === 0) {
        this.change++
        if (value === null || this._inRange(key) === false) return updated
        this.push({ key, value })
        return true
      }

      // we moved past the change, push it
      if (this.reverse ? cmp > 0 : cmp < 0) {
        this.change++
        if (value === null || this._inRange(key) === false) continue
        this.push({ key, value })
        updated = true
        continue
      }

      this.push(entry)
      return true
    }

    this.push(entry)
    return true
  }

  _inRange(key) {
    return b4a.compare(this.start, key) <= 0 && b4a.compare(key, this.end) < 0
  }
}

class Overlay {
  constructor() {
    this.indexed = 0
    this.changes = null
    this.cleared = null
    this.reverse = false
  }

  update(view, reverse) {
    if (view.indexed === this.indexed) return

    const changes = view.map === null ? [] : [...view.map.values()]
    const cleared = view.cleared === null ? [] : view.cleared.slice(0)

    const cmp = reverse ? cmpChangeReverse : cmpChange

    changes.sort(cmp)
    cleared.sort(cmp)

    this.indexed = view.indexed
    this.changes = changes
    this.cleared = cleared
    this.reverse = reverse
  }

  createStream(stream, start, end, reverse) {
    return new OverlayStream(
      stream,
      start,
      end,
      reverse,
      this.reverse === reverse ? this.changes : reverseArray(this.changes),
      this.reverse === reverse ? this.cleared : reverseArray(this.cleared)
    )
  }
}

class View {
  constructor() {
    this.map = null
    this.indexed = 0
    this.changes = null
    this.cleared = null
    this.overlay = null
    this.snap = null
    this.readers = 0
  }

  snapshot() {
    if (this._attached()) return this.snap.snapshot()

    const snap = new View()

    snap.map = this.map
    snap.indexed = this.indexed
    snap.changes = this.changes
    snap.cleared = this.cleared

    if (this._frozen()) return snap

    this.readers++
    snap.snap = this

    return snap
  }

  readStart() {
    if (this.snap !== null) this.readers++
  }

  readStop() {
    if (this.snap !== null && --this.readers === 0) this.snap.readers--
  }

  size() {
    return this.changes === null ? 0 : this.changes.length
  }

  updated() {
    return this.changes === null
  }

  get(read, key) {
    return this.changes === null ? read.get(key) : this._indexAndGet(read, key)
  }

  reset() {
    this.indexed = 0
    this.snap = this.map = this.changes = this.cleared = this.overlay = null
  }

  iterator(db, start, end, reverse) {
    if (dbClosing(db)) return new CloseErrorStream(new Error('RocksDB session is closed'))

    const stream = db.iterator({ gte: start, lt: end, reverse })
    if (this.changes === null) return stream

    this._index()

    if (this.overlay === null) this.overlay = new Overlay()
    this.overlay.update(this, reverse)
    return this.overlay.createStream(stream, start, end, reverse)
  }

  _indexAndGet(read, key) {
    this._index()
    const change = this.map.get(b4a.toString(key, 'hex'))

    if (change === undefined) {
      return this.cleared === null ? read.get(key) : this._readAndMaybeDrop(read, key)
    }

    return Promise.resolve(change[1])
  }

  async _readAndMaybeDrop(read, key) {
    const cleared = this.cleared // in case its cleared
    const value = await read.get(key)
    if (value === null) return null

    for (let i = 0; i < cleared.length; i++) {
      const c = cleared[i]
      // check if in range
      if (b4a.compare(c[0], key) <= 0 && b4a.compare(key, c[2]) < 0) return null
    }

    return value
  }

  _attached() {
    return this.snap !== null && this.changes === this.snap.changes
  }

  _frozen() {
    return this.changes === null || (this.snap !== null && this.changes !== this.snap.changes)
  }

  _index() {
    // if we are a snap and we are still attached (ie no mutations), simply copy the refs
    if (this._attached()) {
      this.snap._index()
      this.map = this.snap.map
      this.cleared = this.snap.cleared
      this.indexed = this.snap.indexed
      return
    }

    if (this.map === null) this.map = new Map()
    if (this.changes.length === this.indexed) return

    while (this.indexed < this.changes.length) {
      const c = this.changes[this.indexed++]

      if (c[2] === null) this.map.set(b4a.toString(c[0], 'hex'), c)
      else this._indexRange(c)
    }
  }

  _indexRange(range) {
    const s = b4a.toString(range[0], 'hex')
    const e = b4a.toString(range[2], 'hex')

    for (const [key, c] of this.map) {
      if (s <= key && key < e) this.map.set(key, [c[0], null, null])
    }

    if (this.cleared === null) this.cleared = []
    this.cleared.push(range)
  }

  apply(changes) {
    if (this.snap !== null) throw new Error('Illegal to push changes to a snapshot')

    if (this.readers !== 0 && this.changes !== null) {
      this.changes = this.changes.slice(0)
      this.cleared = this.cleared === null ? null : this.cleared.slice(0)
      this.map = this.map === null ? null : new Map([...this.map])
    }

    if (this.changes === null) {
      this.changes = changes
      return
    }

    for (let i = 0; i < changes.length; i++) {
      this.changes.push(changes[i])
    }
  }

  static async flush(changes, db) {
    if (changes === null) return true

    const w = db.write({ autoDestroy: true })

    for (const [start, value, end] of changes) {
      if (end !== null) w.tryDeleteRange(start, end)
      else if (value !== null) w.tryPut(start, value)
      else w.tryDelete(start)
    }

    await w.flush()

    return true
  }
}

module.exports = View

function cmpChange(a, b) {
  const c = b4a.compare(a[0], b[0])
  return c === 0 ? b4a.compare(a[2], b[2]) : c
}

function cmpChangeReverse(a, b) {
  return cmpChange(b, a)
}

function noop() {}

function reverseArray(list) {
  const r = new Array(list.length)
  for (let i = 0; i < list.length; i++) r[r.length - 1 - i] = list[i]
  return r
}

// TODO: expose from rocks instead
function dbClosing(db) {
  return db._state.closing || db._index === -1
}
const fs = require('fs')
const path = require('path')
const { Readable } = require('streamx')
const b4a = require('b4a')
const flat = require('flat-tree')
const crypto = require('hypercore-crypto')
const c = require('compact-encoding')
const m = require('./messages.js')
const View = require('../../lib/view.js')
const { CorestoreTX, CoreTX, CorestoreRX } = require('../../lib/tx.js')

const EMPTY_NODE = b4a.alloc(40)
const EMPTY_PAGE = b4a.alloc(4096)

let TREE_01_SKIP = null
let TREE_04_SKIP = null
let TREE_16_SKIP = null

class CoreListStream extends Readable {
  constructor (storage) {
    super()

    this.storage = storage
    this.stack = []
  }

  async _open (cb) {
    for (const a of await readdir(path.join(this.storage, 'cores'))) {
      for (const b of await readdir(path.join(this.storage, 'cores', a))) {
        for (const dkey of await readdir(path.join(this.storage, 'cores', a, b))) {
          this.stack.push(path.join(this.storage, 'cores', a, b, dkey))
        }
      }
    }

    cb(null)
  }

  async _read (cb) {
    while (true) {
      const next = this.stack.pop()
      if (!next) {
        this.push(null)
        break
      }

      const oplog = path.join(next, 'oplog')
      const result = await readOplog(oplog)
      if (!result) continue

      this.push(result)
      break
    }

    cb(null)
  }
}

function decodeOplogHeader (state) {
  c.uint32.decode(state) // cksum, ignore for now

  const l = c.uint32.decode(state)
  const length = l >> 2
  const headerBit = l & 1
  const partialBit = l & 2

  if (state.end - state.start < length) return null

  const end = state.start + length
  const result = { header: headerBit, partial: partialBit !== 0, byteLength: length + 8, message: null }

  try {
    result.message = m.oplog.header.decode({ start: state.start, end, buffer: state.buffer })
  } catch {
    return null
  }

  state.start = end
  return result
}

function decodeOplogEntry (state) {
  if (state.end - state.start < 8) return null

  c.uint32.decode(state) // cksum, ignore for now

  const l = c.uint32.decode(state)
  const length = l >>> 2
  const headerBit = l & 1
  const partialBit = l & 2

  if (state.end - state.start < length) return null

  const end = state.start + length

  const result = { header: headerBit, partial: partialBit !== 0, byteLength: length + 8, message: null }

  try {
    result.message = m.oplog.entry.decode({ start: state.start, end, buffer: state.buffer })
  } catch {
    return null
  }

  state.start = end

  return result
}

module.exports = { store, core }

async function store (storage, { version, dryRun = true, gc = true }) {
  const stream = new CoreListStream(storage.path)
  const view = new View()

  const tx = new CorestoreTX(view)
  const head = await storage._getHead(view)
  const primaryKeyFile = path.join(storage.path, 'primary-key')

  const primaryKey = await readFile(primaryKeyFile)

  if (!head.seed) head.seed = primaryKey

  for await (const data of stream) {
    const key = data.header.key
    const discoveryKey = crypto.discoveryKey(data.header.key)
    const files = getFiles(data.path)

    if (head.defaultDiscoveryKey === null) head.defaultDiscoveryKey = discoveryKey

    const core = {
      version: 0, // need later migration
      corePointer: head.allocated.cores++,
      dataPointer: head.allocated.datas++,
      alias: null
    }

    const ptr = { version: 0, corePointer: core.corePointer, dataPointer: core.dataPointer, dependencies: [] }
    const ctx = new CoreTX(ptr, storage.db, view, [])
    const userData = new Map()
    const treeNodes = new Map()

    const auth = {
      key,
      discoveryKey,
      manifest: data.header.manifest,
      keyPair: data.header.keyPair,
      encryptionKey: null
    }

    const tree = {
      length: 0,
      fork: 0,
      rootHash: null,
      signature: null
    }

    if (data.header.tree && data.header.tree.length) {
      tree.length = data.header.tree.length
      tree.fork = data.header.tree.fork
      tree.rootHash = data.header.tree.rootHash
      tree.signature = data.header.tree.signature
    }

    for (const { key, value } of data.header.userData) {
      userData.set(key, value)
    }

    for (const e of data.entries) {
      if (e.userData) userData.set(e.userData.key, e.userData.value)

      if (e.treeNodes) {
        for (const node of e.treeNodes) {
          treeNodes.set(node.index, node)
          ctx.putTreeNode(node)
        }
      }

      if (e.treeUpgrade) {
        if (e.treeUpgrade.ancestors !== tree.length) {
          throw new Error('Unflushed truncations not migrate-able atm')
        }

        tree.length = e.treeUpgrade.length
        tree.fork = e.treeUpgrade.fork
        tree.rootHash = null
        tree.signature = e.treeUpgrade.signature
      }
    }

    if (userData.has('corestore/name') && userData.has('corestore/namespace')) {
      core.alias = {
        name: b4a.toString(userData.get('corestore/name')),
        namespace: userData.get('corestore/namespace')
      }
      userData.delete('corestore/name')
      userData.delete('corestore/namespace')
    }

    for (const [key, value] of userData) {
      ctx.putUserData(key, value)
    }

    ctx.setAuth(auth)

    const getTreeNode = (index) => (treeNodes.get(index) || getTreeNodeFromFile(files.tree, index))

    if (tree.length) {
      if (tree.rootHash === null) tree.rootHash = crypto.tree(await getRoots(tree.length, getTreeNode))
      ctx.setHead(tree)
    }

    tx.putCore(discoveryKey, core)
    if (core.alias) tx.putCoreByAlias(core.alias, discoveryKey)

    await ctx.flush()
  }

  head.version = version
  tx.setHead(head)
  tx.apply()

  if (dryRun) return

  await View.flush(view.changes, storage.db)

  if (gc) await rm(primaryKeyFile)
}

class BlockSlicer {
  constructor (filename) {
    this.stream = fs.createReadStream(filename)
    this.closed = new Promise(resolve => this.stream.once('close', resolve))
    this.offset = 0
    this.overflow = null
  }

  async take (offset, size) {
    let buffer = null
    if (offset < this.offset) throw new Error('overread')

    while (true) {
      let data = null

      if (this.overflow) {
        data = this.overflow
        this.overflow = null
      } else {
        data = this.stream.read()

        if (!data) {
          await new Promise(resolve => this.stream.once('readable', resolve))
          continue
        }
      }

      let chunk = null

      if (this.offset === offset || buffer) {
        chunk = data
      } else if (this.offset + data.byteLength > offset) {
        chunk = data.subarray(offset - this.offset)
      }

      this.offset += data.byteLength
      if (!chunk) continue

      if (buffer) buffer = b4a.concat([buffer, chunk])
      else buffer = chunk

      if (buffer.byteLength < size) continue

      const result = buffer.subarray(0, size)
      this.overflow = size === buffer.byteLength ? null : buffer.subarray(result.byteLength)
      this.offset -= (this.overflow ? this.overflow.byteLength : 0)
      return result
    }
  }

  close () {
    this.stream.on('error', noop)
    this.stream.destroy()
    return this.closed
  }
}

class TreeSlicer {
  constructor () {
    this.buffer = null
    this.offset = 0
  }

  get size () {
    return this.buffer === null ? 0 : this.buffer.byteLength
  }

  push (data) {
    if (this.buffer === null) this.buffer = data
    else this.buffer = b4a.concat([this.buffer, data])
    this.offset += data.byteLength
  }

  skip () {
    let skipped = 0

    if (TREE_01_SKIP === null) {
      TREE_16_SKIP = b4a.alloc(16 * 40 * 100)
      TREE_04_SKIP = TREE_16_SKIP.subarray(0, 4 * 40 * 100)
      TREE_01_SKIP = TREE_16_SKIP.subarray(0, 1 * 40 * 100)
    }

    while (true) {
      if (this.buffer.byteLength >= TREE_16_SKIP.byteLength) {
        if (b4a.equals(this.buffer.subarray(0, TREE_16_SKIP.byteLength), TREE_16_SKIP)) {
          this.buffer = this.buffer.subarray(TREE_16_SKIP.byteLength)
          skipped += 1600
          continue
        }
      }

      if (this.buffer.byteLength >= TREE_04_SKIP.byteLength) {
        if (b4a.equals(this.buffer.subarray(0, TREE_04_SKIP.byteLength), TREE_04_SKIP)) {
          this.buffer = this.buffer.subarray(TREE_04_SKIP.byteLength)
          skipped += 400
          continue
        }
      }

      if (this.buffer.byteLength >= TREE_01_SKIP.byteLength) {
        if (b4a.equals(this.buffer.subarray(0, TREE_01_SKIP.byteLength), TREE_01_SKIP)) {
          this.buffer = this.buffer.subarray(TREE_01_SKIP.byteLength)
          skipped += 100
          continue
        }
      }
      break
    }

    return skipped
  }

  take () {
    const len = 40

    if (len <= this.size) {
      const chunk = this.buffer.subarray(0, len)
      this.buffer = this.buffer.subarray(len)
      return chunk
    }

    return null
  }
}

async function core (core, { version, dryRun = true, gc = true }) {
  if (dryRun) return // dryRun mode not supported atm

  const rx = core.read()

  const promises = [rx.getAuth(), rx.getHead()]
  rx.tryFlush()

  const [auth, head] = await Promise.all(promises)

  if (!auth) return

  const dk = b4a.toString(auth.discoveryKey, 'hex')
  const files = getFiles(path.join(core.store.path, 'cores', dk.slice(0, 2), dk.slice(2, 4), dk))

  if (head === null || head.length === 0) {
    await commitCoreMigration(auth, core, version)
    if (gc) await runGC()
    return // no data
  }

  const oplog = await readOplog(files.oplog)
  if (!oplog) {
    const writable = !!auth.keyPair

    if (writable) {
      throw new Error('No oplog available writable core for ' + files.oplog + ', length = ' + (head ? head.length : 0))
    }

    // if not writable, just nuke it to recover, some bad state happened here, prop corruption from earlier versions
    const w = core.write()

    w.deleteBlockRange(0, -1)
    w.deleteTreeNodeRange(0, -1)
    w.deleteBitfieldPageRange(0, -1)
    w.deleteHead()

    await w.flush()

    await commitCoreMigration(auth, core, version)
    if (gc) await runGC()
    return // no data
  }

  const treeData = new TreeSlicer()

  let treeIndex = 0

  if (await exists(files.tree)) {
    for await (const data of fs.createReadStream(files.tree)) {
      treeData.push(data)

      let write = null

      while (true) {
        const skip = treeData.skip()
        treeIndex += skip

        const buf = treeData.take()
        if (buf === null) break

        const index = treeIndex++
        if (b4a.equals(buf, EMPTY_NODE)) continue

        if (write === null) write = core.write()
        write.putTreeNode(decodeTreeNode(index, buf))
      }

      if (write !== null) await write.flush()
    }
  }

  const buf = []
  if (await exists(files.bitfield)) {
    for await (const data of fs.createReadStream(files.bitfield)) {
      buf.push(data)
    }
  }

  let bitfield = b4a.concat(buf)
  if (bitfield.byteLength & 4095) bitfield = b4a.concat([bitfield, b4a.alloc(4096 - (bitfield.byteLength & 4095))])

  const pages = new Map()
  const headerBits = new Map()

  const roots = await getRootsFromStorage(core, head.length)

  for (const e of oplog.entries) {
    if (!e.bitfield) continue

    for (let i = 0; i < e.bitfield.length; i++) {
      headerBits.set(i + e.bitfield.start, !e.bitfield.drop)
    }
  }

  let batch = []

  const cache = new Map()
  const blocks = new BlockSlicer(files.data)

  for (const index of allBits(bitfield)) {
    if (headerBits.get(index) === false) continue
    if (index >= head.length) continue

    setBitInPage(index)

    batch.push(index)
    if (batch.length < 1024) continue

    await writeBlocksBatch()
    continue
  }

  if (batch.length) await writeBlocksBatch()

  await blocks.close()

  const w = core.write()

  for (const [index, bit] of headerBits) {
    if (!bit) continue
    if (index >= head.length) continue

    setBitInPage(index)

    const blk = await getBlockFromFile(files.data, core, index, roots, cache)
    w.putBlock(index, blk)
  }

  for (const [index, page] of pages) {
    w.putBitfieldPage(index, b4a.from(page.buffer, page.byteOffset, page.byteLength))
  }

  await w.flush()

  let contiguousLength = 0
  for await (const data of core.createBlockStream()) {
    if (data.index === contiguousLength) contiguousLength++
    else break
  }

  if (contiguousLength) {
    const w = core.write()
    w.setHints({ contiguousLength })
    await w.flush()
  }

  await commitCoreMigration(auth, core, version)

  if (gc) await runGC()

  async function runGC () {
    await rm(files.path)
    await rmdir(path.join(files.path, '..'))
    await rmdir(path.join(files.path, '../..'))
    await rmdir(path.join(core.store.path, 'cores'))
  }

  function setBitInPage (index) {
    const n = index & 32767
    const p = (index - n) / 32768

    let page = pages.get(p)

    if (!page) {
      page = new Uint32Array(1024)
      pages.set(p, page)
    }

    const o = n & 31
    const b = (n - o) / 32
    const v = 1 << o

    page[b] |= v
  }

  async function writeBlocksBatch () {
    const read = core.read()
    const promises = []
    for (const index of batch) promises.push(getByteRangeFromStorage(read, 2 * index, roots, cache))
    read.tryFlush()

    const r = await Promise.all(promises)
    const tx = core.write()

    for (let i = 0; i < r.length; i++) {
      const index = batch[i]
      const [offset, size] = r[i]

      const blk = await blocks.take(offset, size)
      tx.putBlock(index, blk)
    }

    batch = []
    if (cache.size > 16384) cache.clear()

    await tx.flush()
  }
}

async function commitCoreMigration (auth, core, version) {
  const view = new View()
  const rx = new CorestoreRX(core.db, view)

  const storeCorePromise = rx.getCore(auth.discoveryKey)
  rx.tryFlush()

  const storeCore = await storeCorePromise

  storeCore.version = version

  const tx = new CorestoreTX(view)

  tx.putCore(auth.discoveryKey, storeCore)
  tx.apply()

  await View.flush(view.changes, core.db)
}

async function getBlockFromFile (file, core, index, roots, cache) {
  const rx = core.read()
  const promise = getByteRangeFromStorage(rx, 2 * index, roots, cache)
  rx.tryFlush()
  const [offset, size] = await promise

  return new Promise(function (resolve) {
    readAll(file, size, offset, function (err, buf) {
      if (err) return resolve(null)
      resolve(buf)
    })
  })
}

function getFiles (dir) {
  return {
    path: dir,
    oplog: path.join(dir, 'oplog'),
    data: path.join(dir, 'data'),
    tree: path.join(dir, 'tree'),
    bitfield: path.join(dir, 'bitfield')
  }
}

async function getRootsFromStorage (core, length) {
  const all = []
  const rx = core.read()
  for (const index of flat.fullRoots(2 * length)) {
    all.push(rx.getTreeNode(index))
  }
  rx.tryFlush()
  return Promise.all(all)
}

async function getRoots (length, getTreeNode) {
  const all = []
  for (const index of flat.fullRoots(2 * length)) {
    all.push(await getTreeNode(index))
  }
  return all
}

function getCached (read, cache, index) {
  if (cache.has(index)) return cache.get(index)
  const p = read.getTreeNode(index)
  cache.set(index, p)
  return p
}

async function getByteRangeFromStorage (read, index, roots, cache) {
  const promises = [getCached(read, cache, index), getByteOffsetFromStorage(read, index, roots, cache)]
  const [node, offset] = await Promise.all(promises)
  if (!node) throw new Error('Node not found during migration: ' + index)
  return [offset, node.size]
}

async function getByteOffsetFromStorage (rx, index, roots, cache) {
  if (index === 0) return 0
  if ((index & 1) === 1) index = flat.leftSpan(index)

  let head = 0
  let offset = 0

  for (const node of roots) { // all async ticks happen once we find the root so safe
    head += 2 * ((node.index - head) + 1)

    if (index >= head) {
      offset += node.size
      continue
    }

    const ite = flat.iterator(node.index)
    const promises = []

    while (ite.index !== index) {
      if (index < ite.index) {
        ite.leftChild()
      } else {
        promises.push(getCached(rx, cache, ite.leftChild()))
        ite.sibling()
      }
    }

    const nodes = await Promise.all(promises)
    for (const node of nodes) offset += node.size

    return offset
  }

  throw new Error('Failed to find offset')
}

function decodeTreeNode (index, buf) {
  return { index, size: c.decode(c.uint64, buf), hash: buf.subarray(8) }
}

async function getTreeNodeFromFile (file, index) {
  return new Promise(function (resolve) {
    readAll(file, 40, index * 40, function (err, buf) {
      if (err) return resolve(null)
      resolve(decodeTreeNode(index, buf))
    })
  })
}

function readAll (filename, length, pos, cb) {
  const buf = b4a.alloc(length)

  fs.open(filename, 'r', function (err, fd) {
    if (err) return cb(err)

    let offset = 0

    fs.read(fd, buf, offset, buf.byteLength, pos, function loop (err, read) {
      if (err) return done(err)
      if (read === 0) return done(new Error('Partial read'))
      offset += read
      if (offset === buf.byteLength) return done(null, buf)
      fs.read(fd, offset, buf.byteLength - offset, buf, pos + offset, loop)
    })

    function done (err, value) {
      fs.close(fd, () => cb(err, value))
    }
  })
}

async function readdir (dir) {
  try {
    return await fs.promises.readdir(dir)
  } catch {
    return []
  }
}

async function exists (file) {
  try {
    await fs.promises.stat(file)
    return true
  } catch {
    return false
  }
}

async function readFile (file) {
  try {
    return await fs.promises.readFile(file)
  } catch {
    return null
  }
}

async function rm (dir) {
  try {
    await fs.promises.rm(dir, { recursive: true })
  } catch {}
}

async function rmdir (dir) {
  try {
    await fs.promises.rmdir(dir)
  } catch {}
}

function * allBits (buffer) {
  for (let i = 0; i < buffer.byteLength; i += EMPTY_PAGE.byteLength) {
    const page = buffer.subarray(i, i + EMPTY_NODE.byteLength)
    if (b4a.equals(page, EMPTY_PAGE)) continue

    const view = new Uint32Array(page.buffer, page.byteOffset, EMPTY_PAGE.byteLength / 4)

    for (let j = 0; j < view.length; j++) {
      const n = view[j]
      if (n === 0) continue

      for (let k = 0; k < 32; k++) {
        const m = 1 << k
        if (n & m) yield i * 8 + j * 32 + k
      }
    }
  }
}

function readOplog (oplog) {
  return new Promise(function (resolve) {
    fs.readFile(oplog, function (err, buffer) {
      if (err) return resolve(null)

      const state = { start: 0, end: buffer.byteLength, buffer }
      const headers = [1, 0]

      const h1 = decodeOplogHeader(state)
      state.start = 4096

      const h2 = decodeOplogHeader(state)
      state.start = 4096 * 2

      if (!h1 && !h2) return resolve(null)

      if (h1 && !h2) {
        headers[0] = h1.header
        headers[1] = h1.header
      } else if (!h1 && h2) {
        headers[0] = (h2.header + 1) & 1
        headers[1] = h2.header
      } else {
        headers[0] = h1.header
        headers[1] = h2.header
      }

      const header = (headers[0] + headers[1]) & 1
      const result = { path: path.dirname(oplog), header: null, entries: [] }
      const decoded = []

      result.header = header ? h2.message : h1.message

      if (result.header.external) {
        fs.readFile(path.join(oplog, '../header'), function (err, buffer) {
          if (err) return resolve(null)
          const start = result.header.external.start
          const end = start + result.header.external.length
          result.header = m.oplog.header.decode({ buffer, start, end })
          finish()
        })
        return
      }

      finish()

      function finish () {
        while (true) {
          const entry = decodeOplogEntry(state)
          if (!entry) break
          if (entry.header !== header) break

          decoded.push(entry)
        }

        while (decoded.length > 0 && decoded[decoded.length - 1].partial) decoded.pop()

        for (const e of decoded) {
          result.entries.push(e.message)
        }

        resolve(result)
      }
    })
  })
}

function noop () {}
// needed here for compat, copied from old hypercore, do not change this

const c = require('compact-encoding')
const b4a = require('b4a')

const EMPTY = b4a.alloc(0)
const DEFAULT_NAMESPACE = b4a.from('4144eea531e483d54e0c14f4ca68e0644f355343ff6fcb0f005200e12cd747cb', 'hex')

const hashes = {
  preencode (state, m) {
    state.end++ // small uint
  },
  encode (state, m) {
    if (m === 'blake2b') {
      c.uint.encode(state, 0)
      return
    }

    throw new Error('Unknown hash: ' + m)
  },
  decode (state) {
    const n = c.uint.decode(state)
    if (n === 0) return 'blake2b'
    throw new Error('Unknown hash id: ' + n)
  }
}

const signatures = {
  preencode (state, m) {
    state.end++ // small uint
  },
  encode (state, m) {
    if (m === 'ed25519') {
      c.uint.encode(state, 0)
      return
    }

    throw new Error('Unknown signature: ' + m)
  },
  decode (state) {
    const n = c.uint.decode(state)
    if (n === 0) return 'ed25519'
    throw new Error('Unknown signature id: ' + n)
  }
}

const signer = {
  preencode (state, m) {
    signatures.preencode(state, m.signature)
    c.fixed32.preencode(state, m.namespace)
    c.fixed32.preencode(state, m.publicKey)
  },
  encode (state, m) {
    signatures.encode(state, m.signature)
    c.fixed32.encode(state, m.namespace)
    c.fixed32.encode(state, m.publicKey)
  },
  decode (state) {
    return {
      signature: signatures.decode(state),
      namespace: c.fixed32.decode(state),
      publicKey: c.fixed32.decode(state)
    }
  }
}

const signerArray = c.array(signer)

const prologue = {
  preencode (state, p) {
    c.fixed32.preencode(state, p.hash)
    c.uint.preencode(state, p.length)
  },
  encode (state, p) {
    c.fixed32.encode(state, p.hash)
    c.uint.encode(state, p.length)
  },
  decode (state) {
    return {
      hash: c.fixed32.decode(state),
      length: c.uint.decode(state)
    }
  }
}

const manifestv0 = {
  preencode (state, m) {
    hashes.preencode(state, m.hash)
    state.end++ // type

    if (m.prologue && m.signers.length === 0) {
      c.fixed32.preencode(state, m.prologue.hash)
      return
    }

    if (m.quorum === 1 && m.signers.length === 1 && !m.allowPatch) {
      signer.preencode(state, m.signers[0])
    } else {
      state.end++ // flags
      c.uint.preencode(state, m.quorum)
      signerArray.preencode(state, m.signers)
    }
  },
  encode (state, m) {
    hashes.encode(state, m.hash)

    if (m.prologue && m.signers.length === 0) {
      c.uint.encode(state, 0)
      c.fixed32.encode(state, m.prologue.hash)
      return
    }

    if (m.quorum === 1 && m.signers.length === 1 && !m.allowPatch) {
      c.uint.encode(state, 1)
      signer.encode(state, m.signers[0])
    } else {
      c.uint.encode(state, 2)
      c.uint.encode(state, m.allowPatch ? 1 : 0)
      c.uint.encode(state, m.quorum)
      signerArray.encode(state, m.signers)
    }
  },
  decode (state) {
    const hash = hashes.decode(state)
    const type = c.uint.decode(state)

    if (type > 2) throw new Error('Unknown type: ' + type)

    if (type === 0) {
      return {
        version: 0,
        hash,
        allowPatch: false,
        quorum: 0,
        signers: [],
        prologue: {
          hash: c.fixed32.decode(state),
          length: 0
        }
      }
    }

    if (type === 1) {
      return {
        version: 0,
        hash,
        allowPatch: false,
        quorum: 1,
        signers: [signer.decode(state)],
        prologue: null
      }
    }

    const flags = c.uint.decode(state)

    return {
      version: 0,
      hash,
      allowPatch: (flags & 1) !== 0,
      quorum: c.uint.decode(state),
      signers: signerArray.decode(state),
      prologue: null
    }
  }
}

const manifest = exports.manifest = {
  preencode (state, m) {
    state.end++ // version
    if (m.version === 0) return manifestv0.preencode(state, m)

    state.end++ // flags
    hashes.preencode(state, m.hash)

    c.uint.preencode(state, m.quorum)
    signerArray.preencode(state, m.signers)
    if (m.prologue) prologue.preencode(state, m.prologue)
  },
  encode (state, m) {
    c.uint.encode(state, m.version)
    if (m.version === 0) return manifestv0.encode(state, m)

    c.uint.encode(state, (m.allowPatch ? 1 : 0) | (m.prologue ? 2 : 0) | (m.unencrypted ? 4 : 0))
    hashes.encode(state, m.hash)

    c.uint.encode(state, m.quorum)
    signerArray.encode(state, m.signers)
    if (m.prologue) prologue.encode(state, m.prologue)
  },
  decode (state) {
    const v = c.uint.decode(state)
    if (v === 0) return manifestv0.decode(state)
    if (v !== 1) throw new Error('Unknown version: ' + v)

    const flags = c.uint.decode(state)
    const hash = hashes.decode(state)
    const quorum = c.uint.decode(state)
    const signers = signerArray.decode(state)
    const unencrypted = (flags & 4) !== 0

    return {
      version: 1,
      hash,
      allowPatch: (flags & 1) !== 0,
      quorum,
      signers,
      prologue: (flags & 2) === 0 ? null : prologue.decode(state),
      unencrypted
    }
  }
}

const node = {
  preencode (state, n) {
    c.uint.preencode(state, n.index)
    c.uint.preencode(state, n.size)
    c.fixed32.preencode(state, n.hash)
  },
  encode (state, n) {
    c.uint.encode(state, n.index)
    c.uint.encode(state, n.size)
    c.fixed32.encode(state, n.hash)
  },
  decode (state) {
    return {
      index: c.uint.decode(state),
      size: c.uint.decode(state),
      hash: c.fixed32.decode(state)
    }
  }
}

const nodeArray = c.array(node)

const wire = exports.wire = {}

wire.handshake = {
  preencode (state, m) {
    c.uint.preencode(state, 1)
    c.fixed32.preencode(state, m.capability)
  },
  encode (state, m) {
    c.uint.encode(state, m.seeks ? 1 : 0)
    c.fixed32.encode(state, m.capability)
  },
  decode (state) {
    const flags = c.uint.decode(state)
    return {
      seeks: (flags & 1) !== 0,
      capability: c.fixed32.decode(state)
    }
  }
}

const requestBlock = {
  preencode (state, b) {
    c.uint.preencode(state, b.index)
    c.uint.preencode(state, b.nodes)
  },
  encode (state, b) {
    c.uint.encode(state, b.index)
    c.uint.encode(state, b.nodes)
  },
  decode (state) {
    return {
      index: c.uint.decode(state),
      nodes: c.uint.decode(state)
    }
  }
}

const requestSeek = {
  preencode (state, s) {
    c.uint.preencode(state, s.bytes)
    c.uint.preencode(state, s.padding)
  },
  encode (state, s) {
    c.uint.encode(state, s.bytes)
    c.uint.encode(state, s.padding)
  },
  decode (state) {
    return {
      bytes: c.uint.decode(state),
      padding: c.uint.decode(state)
    }
  }
}

const requestUpgrade = {
  preencode (state, u) {
    c.uint.preencode(state, u.start)
    c.uint.preencode(state, u.length)
  },
  encode (state, u) {
    c.uint.encode(state, u.start)
    c.uint.encode(state, u.length)
  },
  decode (state) {
    return {
      start: c.uint.decode(state),
      length: c.uint.decode(state)
    }
  }
}

wire.request = {
  preencode (state, m) {
    state.end++ // flags
    c.uint.preencode(state, m.id)
    c.uint.preencode(state, m.fork)

    if (m.block) requestBlock.preencode(state, m.block)
    if (m.hash) requestBlock.preencode(state, m.hash)
    if (m.seek) requestSeek.preencode(state, m.seek)
    if (m.upgrade) requestUpgrade.preencode(state, m.upgrade)
    if (m.priority) c.uint.preencode(state, m.priority)
  },
  encode (state, m) {
    const flags = (m.block ? 1 : 0) | (m.hash ? 2 : 0) | (m.seek ? 4 : 0) | (m.upgrade ? 8 : 0) | (m.manifest ? 16 : 0) | (m.priority ? 32 : 0)

    c.uint.encode(state, flags)
    c.uint.encode(state, m.id)
    c.uint.encode(state, m.fork)

    if (m.block) requestBlock.encode(state, m.block)
    if (m.hash) requestBlock.encode(state, m.hash)
    if (m.seek) requestSeek.encode(state, m.seek)
    if (m.upgrade) requestUpgrade.encode(state, m.upgrade)
    if (m.priority) c.uint.encode(state, m.priority)
  },
  decode (state) {
    const flags = c.uint.decode(state)

    return {
      id: c.uint.decode(state),
      fork: c.uint.decode(state),
      block: flags & 1 ? requestBlock.decode(state) : null,
      hash: flags & 2 ? requestBlock.decode(state) : null,
      seek: flags & 4 ? requestSeek.decode(state) : null,
      upgrade: flags & 8 ? requestUpgrade.decode(state) : null,
      manifest: (flags & 16) !== 0,
      priority: flags & 32 ? c.uint.decode(state) : 0
    }
  }
}

wire.cancel = {
  preencode (state, m) {
    c.uint.preencode(state, m.request)
  },
  encode (state, m) {
    c.uint.encode(state, m.request)
  },
  decode (state, m) {
    return {
      request: c.uint.decode(state)
    }
  }
}

const dataUpgrade = {
  preencode (state, u) {
    c.uint.preencode(state, u.start)
    c.uint.preencode(state, u.length)
    nodeArray.preencode(state, u.nodes)
    nodeArray.preencode(state, u.additionalNodes)
    c.buffer.preencode(state, u.signature)
  },
  encode (state, u) {
    c.uint.encode(state, u.start)
    c.uint.encode(state, u.length)
    nodeArray.encode(state, u.nodes)
    nodeArray.encode(state, u.additionalNodes)
    c.buffer.encode(state, u.signature)
  },
  decode (state) {
    return {
      start: c.uint.decode(state),
      length: c.uint.decode(state),
      nodes: nodeArray.decode(state),
      additionalNodes: nodeArray.decode(state),
      signature: c.buffer.decode(state)
    }
  }
}

const dataSeek = {
  preencode (state, s) {
    c.uint.preencode(state, s.bytes)
    nodeArray.preencode(state, s.nodes)
  },
  encode (state, s) {
    c.uint.encode(state, s.bytes)
    nodeArray.encode(state, s.nodes)
  },
  decode (state) {
    return {
      bytes: c.uint.decode(state),
      nodes: nodeArray.decode(state)
    }
  }
}

const dataBlock = {
  preencode (state, b) {
    c.uint.preencode(state, b.index)
    c.buffer.preencode(state, b.value)
    nodeArray.preencode(state, b.nodes)
  },
  encode (state, b) {
    c.uint.encode(state, b.index)
    c.buffer.encode(state, b.value)
    nodeArray.encode(state, b.nodes)
  },
  decode (state) {
    return {
      index: c.uint.decode(state),
      value: c.buffer.decode(state) || EMPTY,
      nodes: nodeArray.decode(state)
    }
  }
}

const dataHash = {
  preencode (state, b) {
    c.uint.preencode(state, b.index)
    nodeArray.preencode(state, b.nodes)
  },
  encode (state, b) {
    c.uint.encode(state, b.index)
    nodeArray.encode(state, b.nodes)
  },
  decode (state) {
    return {
      index: c.uint.decode(state),
      nodes: nodeArray.decode(state)
    }
  }
}

wire.data = {
  preencode (state, m) {
    state.end++ // flags
    c.uint.preencode(state, m.request)
    c.uint.preencode(state, m.fork)

    if (m.block) dataBlock.preencode(state, m.block)
    if (m.hash) dataHash.preencode(state, m.hash)
    if (m.seek) dataSeek.preencode(state, m.seek)
    if (m.upgrade) dataUpgrade.preencode(state, m.upgrade)
    if (m.manifest) manifest.preencode(state, m.manifest)
  },
  encode (state, m) {
    const flags = (m.block ? 1 : 0) | (m.hash ? 2 : 0) | (m.seek ? 4 : 0) | (m.upgrade ? 8 : 0) | (m.manifest ? 16 : 0)

    c.uint.encode(state, flags)
    c.uint.encode(state, m.request)
    c.uint.encode(state, m.fork)

    if (m.block) dataBlock.encode(state, m.block)
    if (m.hash) dataHash.encode(state, m.hash)
    if (m.seek) dataSeek.encode(state, m.seek)
    if (m.upgrade) dataUpgrade.encode(state, m.upgrade)
    if (m.manifest) manifest.encode(state, m.manifest)
  },
  decode (state) {
    const flags = c.uint.decode(state)

    return {
      request: c.uint.decode(state),
      fork: c.uint.decode(state),
      block: flags & 1 ? dataBlock.decode(state) : null,
      hash: flags & 2 ? dataHash.decode(state) : null,
      seek: flags & 4 ? dataSeek.decode(state) : null,
      upgrade: flags & 8 ? dataUpgrade.decode(state) : null,
      manifest: flags & 16 ? manifest.decode(state) : null
    }
  }
}

wire.noData = {
  preencode (state, m) {
    c.uint.preencode(state, m.request)
  },
  encode (state, m) {
    c.uint.encode(state, m.request)
  },
  decode (state, m) {
    return {
      request: c.uint.decode(state)
    }
  }
}

wire.want = {
  preencode (state, m) {
    c.uint.preencode(state, m.start)
    c.uint.preencode(state, m.length)
  },
  encode (state, m) {
    c.uint.encode(state, m.start)
    c.uint.encode(state, m.length)
  },
  decode (state) {
    return {
      start: c.uint.decode(state),
      length: c.uint.decode(state)
    }
  }
}

wire.unwant = {
  preencode (state, m) {
    c.uint.preencode(state, m.start)
    c.uint.preencode(state, m.length)
  },
  encode (state, m) {
    c.uint.encode(state, m.start)
    c.uint.encode(state, m.length)
  },
  decode (state, m) {
    return {
      start: c.uint.decode(state),
      length: c.uint.decode(state)
    }
  }
}

wire.range = {
  preencode (state, m) {
    state.end++ // flags
    c.uint.preencode(state, m.start)
    if (m.length !== 1) c.uint.preencode(state, m.length)
  },
  encode (state, m) {
    c.uint.encode(state, (m.drop ? 1 : 0) | (m.length === 1 ? 2 : 0))
    c.uint.encode(state, m.start)
    if (m.length !== 1) c.uint.encode(state, m.length)
  },
  decode (state) {
    const flags = c.uint.decode(state)

    return {
      drop: (flags & 1) !== 0,
      start: c.uint.decode(state),
      length: (flags & 2) !== 0 ? 1 : c.uint.decode(state)
    }
  }
}

wire.bitfield = {
  preencode (state, m) {
    c.uint.preencode(state, m.start)
    c.uint32array.preencode(state, m.bitfield)
  },
  encode (state, m) {
    c.uint.encode(state, m.start)
    c.uint32array.encode(state, m.bitfield)
  },
  decode (state, m) {
    return {
      start: c.uint.decode(state),
      bitfield: c.uint32array.decode(state)
    }
  }
}

wire.sync = {
  preencode (state, m) {
    state.end++ // flags
    c.uint.preencode(state, m.fork)
    c.uint.preencode(state, m.length)
    c.uint.preencode(state, m.remoteLength)
  },
  encode (state, m) {
    c.uint.encode(state, (m.canUpgrade ? 1 : 0) | (m.uploading ? 2 : 0) | (m.downloading ? 4 : 0) | (m.hasManifest ? 8 : 0))
    c.uint.encode(state, m.fork)
    c.uint.encode(state, m.length)
    c.uint.encode(state, m.remoteLength)
  },
  decode (state) {
    const flags = c.uint.decode(state)

    return {
      fork: c.uint.decode(state),
      length: c.uint.decode(state),
      remoteLength: c.uint.decode(state),
      canUpgrade: (flags & 1) !== 0,
      uploading: (flags & 2) !== 0,
      downloading: (flags & 4) !== 0,
      hasManifest: (flags & 8) !== 0
    }
  }
}

wire.reorgHint = {
  preencode (state, m) {
    c.uint.preencode(state, m.from)
    c.uint.preencode(state, m.to)
    c.uint.preencode(state, m.ancestors)
  },
  encode (state, m) {
    c.uint.encode(state, m.from)
    c.uint.encode(state, m.to)
    c.uint.encode(state, m.ancestors)
  },
  decode (state) {
    return {
      from: c.uint.encode(state),
      to: c.uint.encode(state),
      ancestors: c.uint.encode(state)
    }
  }
}

wire.extension = {
  preencode (state, m) {
    c.string.preencode(state, m.name)
    c.raw.preencode(state, m.message)
  },
  encode (state, m) {
    c.string.encode(state, m.name)
    c.raw.encode(state, m.message)
  },
  decode (state) {
    return {
      name: c.string.decode(state),
      message: c.raw.decode(state)
    }
  }
}

const keyValue = {
  preencode (state, p) {
    c.string.preencode(state, p.key)
    c.buffer.preencode(state, p.value)
  },
  encode (state, p) {
    c.string.encode(state, p.key)
    c.buffer.encode(state, p.value)
  },
  decode (state) {
    return {
      key: c.string.decode(state),
      value: c.buffer.decode(state)
    }
  }
}

const treeUpgrade = {
  preencode (state, u) {
    c.uint.preencode(state, u.fork)
    c.uint.preencode(state, u.ancestors)
    c.uint.preencode(state, u.length)
    c.buffer.preencode(state, u.signature)
  },
  encode (state, u) {
    c.uint.encode(state, u.fork)
    c.uint.encode(state, u.ancestors)
    c.uint.encode(state, u.length)
    c.buffer.encode(state, u.signature)
  },
  decode (state) {
    return {
      fork: c.uint.decode(state),
      ancestors: c.uint.decode(state),
      length: c.uint.decode(state),
      signature: c.buffer.decode(state)
    }
  }
}

const bitfieldUpdate = { // TODO: can maybe be folded into a HAVE later on with the most recent spec
  preencode (state, b) {
    state.end++ // flags
    c.uint.preencode(state, b.start)
    c.uint.preencode(state, b.length)
  },
  encode (state, b) {
    state.buffer[state.start++] = b.drop ? 1 : 0
    c.uint.encode(state, b.start)
    c.uint.encode(state, b.length)
  },
  decode (state) {
    const flags = c.uint.decode(state)
    return {
      drop: (flags & 1) !== 0,
      start: c.uint.decode(state),
      length: c.uint.decode(state)
    }
  }
}

const oplog = exports.oplog = {}

oplog.entry = {
  preencode (state, m) {
    state.end++ // flags
    if (m.userData) keyValue.preencode(state, m.userData)
    if (m.treeNodes) nodeArray.preencode(state, m.treeNodes)
    if (m.treeUpgrade) treeUpgrade.preencode(state, m.treeUpgrade)
    if (m.bitfield) bitfieldUpdate.preencode(state, m.bitfield)
  },
  encode (state, m) {
    const s = state.start++
    let flags = 0

    if (m.userData) {
      flags |= 1
      keyValue.encode(state, m.userData)
    }
    if (m.treeNodes) {
      flags |= 2
      nodeArray.encode(state, m.treeNodes)
    }
    if (m.treeUpgrade) {
      flags |= 4
      treeUpgrade.encode(state, m.treeUpgrade)
    }
    if (m.bitfield) {
      flags |= 8
      bitfieldUpdate.encode(state, m.bitfield)
    }

    state.buffer[s] = flags
  },
  decode (state) {
    const flags = c.uint.decode(state)
    return {
      userData: (flags & 1) !== 0 ? keyValue.decode(state) : null,
      treeNodes: (flags & 2) !== 0 ? nodeArray.decode(state) : null,
      treeUpgrade: (flags & 4) !== 0 ? treeUpgrade.decode(state) : null,
      bitfield: (flags & 8) !== 0 ? bitfieldUpdate.decode(state) : null
    }
  }
}

const keyPair = {
  preencode (state, kp) {
    c.buffer.preencode(state, kp.publicKey)
    c.buffer.preencode(state, kp.secretKey)
  },
  encode (state, kp) {
    c.buffer.encode(state, kp.publicKey)
    c.buffer.encode(state, kp.secretKey)
  },
  decode (state) {
    return {
      publicKey: c.buffer.decode(state),
      secretKey: c.buffer.decode(state)
    }
  }
}

const reorgHint = {
  preencode (state, r) {
    c.uint.preencode(state, r.from)
    c.uint.preencode(state, r.to)
    c.uint.preencode(state, r.ancestors)
  },
  encode (state, r) {
    c.uint.encode(state, r.from)
    c.uint.encode(state, r.to)
    c.uint.encode(state, r.ancestors)
  },
  decode (state) {
    return {
      from: c.uint.decode(state),
      to: c.uint.decode(state),
      ancestors: c.uint.decode(state)
    }
  }
}

const reorgHintArray = c.array(reorgHint)

const hints = {
  preencode (state, h) {
    reorgHintArray.preencode(state, h.reorgs)
    c.uint.preencode(state, h.contiguousLength)
  },
  encode (state, h) {
    reorgHintArray.encode(state, h.reorgs)
    c.uint.encode(state, h.contiguousLength)
  },
  decode (state) {
    return {
      reorgs: reorgHintArray.decode(state),
      contiguousLength: state.start < state.end ? c.uint.decode(state) : 0
    }
  }
}

const treeHeader = {
  preencode (state, t) {
    c.uint.preencode(state, t.fork)
    c.uint.preencode(state, t.length)
    c.buffer.preencode(state, t.rootHash)
    c.buffer.preencode(state, t.signature)
  },
  encode (state, t) {
    c.uint.encode(state, t.fork)
    c.uint.encode(state, t.length)
    c.buffer.encode(state, t.rootHash)
    c.buffer.encode(state, t.signature)
  },
  decode (state) {
    return {
      fork: c.uint.decode(state),
      length: c.uint.decode(state),
      rootHash: c.buffer.decode(state),
      signature: c.buffer.decode(state)
    }
  }
}

const types = {
  preencode (state, t) {
    c.string.preencode(state, t.tree)
    c.string.preencode(state, t.bitfield)
    c.string.preencode(state, t.signer)
  },
  encode (state, t) {
    c.string.encode(state, t.tree)
    c.string.encode(state, t.bitfield)
    c.string.encode(state, t.signer)
  },
  decode (state) {
    return {
      tree: c.string.decode(state),
      bitfield: c.string.decode(state),
      signer: c.string.decode(state)
    }
  }
}

const externalHeader = {
  preencode (state, m) {
    c.uint.preencode(state, m.start)
    c.uint.preencode(state, m.length)
  },
  encode (state, m) {
    c.uint.encode(state, m.start)
    c.uint.encode(state, m.length)
  },
  decode (state) {
    return {
      start: c.uint.decode(state),
      length: c.uint.decode(state)
    }
  }
}

const keyValueArray = c.array(keyValue)

oplog.header = {
  preencode (state, h) {
    state.end += 2 // version + flags
    if (h.external) {
      externalHeader.preencode(state, h.external)
      return
    }
    c.fixed32.preencode(state, h.key)
    if (h.manifest) manifest.preencode(state, h.manifest)
    if (h.keyPair) keyPair.preencode(state, h.keyPair)
    keyValueArray.preencode(state, h.userData)
    treeHeader.preencode(state, h.tree)
    hints.preencode(state, h.hints)
  },
  encode (state, h) {
    c.uint.encode(state, 1)
    if (h.external) {
      c.uint.encode(state, 1) // ONLY set the first big for clarity
      externalHeader.encode(state, h.external)
      return
    }
    c.uint.encode(state, (h.manifest ? 2 : 0) | (h.keyPair ? 4 : 0))
    c.fixed32.encode(state, h.key)
    if (h.manifest) manifest.encode(state, h.manifest)
    if (h.keyPair) keyPair.encode(state, h.keyPair)
    keyValueArray.encode(state, h.userData)
    treeHeader.encode(state, h.tree)
    hints.encode(state, h.hints)
  },
  decode (state) {
    const version = c.uint.decode(state)

    if (version > 1) {
      throw new Error('Invalid header version. Expected <= 1, got ' + version)
    }

    if (version === 0) {
      const old = {
        types: types.decode(state),
        userData: keyValueArray.decode(state),
        tree: treeHeader.decode(state),
        signer: keyPair.decode(state),
        hints: hints.decode(state)
      }

      return {
        external: null,
        key: old.signer.publicKey,
        manifest: {
          version: 0,
          hash: old.types.tree,
          allowPatch: false,
          quorum: 1,
          signers: [{
            signature: old.types.signer,
            namespace: DEFAULT_NAMESPACE,
            publicKey: old.signer.publicKey
          }],
          prologue: null
        },
        keyPair: old.signer.secretKey ? old.signer : null,
        userData: old.userData,
        tree: old.tree,
        hints: old.hints
      }
    }

    const flags = c.uint.decode(state)

    if (flags & 1) {
      return {
        external: externalHeader.decode(state),
        key: null,
        manifest: null,
        keyPair: null,
        userData: null,
        tree: null,
        hints: null
      }
    }

    return {
      external: null,
      key: c.fixed32.decode(state),
      manifest: (flags & 2) !== 0 ? manifest.decode(state) : null,
      keyPair: (flags & 4) !== 0 ? keyPair.decode(state) : null,
      userData: keyValueArray.decode(state),
      tree: treeHeader.decode(state),
      hints: hints.decode(state)
    }
  }
}

const uintArray = c.array(c.uint)

const multisigInput = {
  preencode (state, inp) {
    c.uint.preencode(state, inp.signer)
    c.fixed64.preencode(state, inp.signature)
    c.uint.preencode(state, inp.patch)
  },
  encode (state, inp) {
    c.uint.encode(state, inp.signer)
    c.fixed64.encode(state, inp.signature)
    c.uint.encode(state, inp.patch)
  },
  decode (state) {
    return {
      signer: c.uint.decode(state),
      signature: c.fixed64.decode(state),
      patch: c.uint.decode(state)
    }
  }
}

const patchEncodingv0 = {
  preencode (state, n) {
    c.uint.preencode(state, n.start)
    c.uint.preencode(state, n.length)
    uintArray.preencode(state, n.nodes)
  },
  encode (state, n) {
    c.uint.encode(state, n.start)
    c.uint.encode(state, n.length)
    uintArray.encode(state, n.nodes)
  },
  decode (state) {
    return {
      start: c.uint.decode(state),
      length: c.uint.decode(state),
      nodes: uintArray.decode(state)
    }
  }
}

const multisigInputv0 = {
  preencode (state, n) {
    state.end++
    c.uint.preencode(state, n.signer)
    c.fixed64.preencode(state, n.signature)
    if (n.patch) patchEncodingv0.preencode(state, n.patch)
  },
  encode (state, n) {
    c.uint.encode(state, n.patch ? 1 : 0)
    c.uint.encode(state, n.signer)
    c.fixed64.encode(state, n.signature)
    if (n.patch) patchEncodingv0.encode(state, n.patch)
  },
  decode (state) {
    const flags = c.uint.decode(state)
    return {
      signer: c.uint.decode(state),
      signature: c.fixed64.decode(state),
      patch: (flags & 1) ? patchEncodingv0.decode(state) : null
    }
  }
}

const multisigInputArrayv0 = c.array(multisigInputv0)
const multisigInputArray = c.array(multisigInput)

const compactNode = {
  preencode (state, n) {
    c.uint.preencode(state, n.index)
    c.uint.preencode(state, n.size)
    c.fixed32.preencode(state, n.hash)
  },
  encode (state, n) {
    c.uint.encode(state, n.index)
    c.uint.encode(state, n.size)
    c.fixed32.encode(state, n.hash)
  },
  decode (state) {
    return {
      index: c.uint.decode(state),
      size: c.uint.decode(state),
      hash: c.fixed32.decode(state)
    }
  }
}

const compactNodeArray = c.array(compactNode)

exports.multiSignaturev0 = {
  preencode (state, s) {
    multisigInputArrayv0.preencode(state, s.proofs)
    compactNodeArray.preencode(state, s.patch)
  },
  encode (state, s) {
    multisigInputArrayv0.encode(state, s.proofs)
    compactNodeArray.encode(state, s.patch)
  },
  decode (state) {
    return {
      proofs: multisigInputArrayv0.decode(state),
      patch: compactNodeArray.decode(state)
    }
  }
}

exports.multiSignature = {
  preencode (state, s) {
    multisigInputArray.preencode(state, s.proofs)
    compactNodeArray.preencode(state, s.patch)
  },
  encode (state, s) {
    multisigInputArray.encode(state, s.proofs)
    compactNodeArray.encode(state, s.patch)
  },
  decode (state) {
    return {
      proofs: multisigInputArray.decode(state),
      patch: compactNodeArray.decode(state)
    }
  }
}
{
  "name": "hypercore-storage",
  "version": "2.4.1",
  "main": "index.js",
  "files": [
    "index.js",
    "lib/*.js",
    "spec/hyperschema/*.js",
    "migrations/0/*.js"
  ],
  "scripts": {
    "format": "prettier --write .",
    "lint": "prettier --check . && lunte",
    "test": "node test/all.js",
    "test:bare": "bare test/all.js",
    "test:generate": "brittle -r test/all.js test/*.js"
  },
  "author": "Holepunch Inc.",
  "license": "Apache-2.0",
  "description": "Storage engine for Hypercore",
  "imports": {
    "fs": {
      "bare": "bare-fs",
      "default": "fs"
    },
    "path": {
      "bare": "bare-path",
      "default": "path"
    }
  },
  "dependencies": {
    "b4a": "^1.6.7",
    "bare-fs": "^4.0.1",
    "bare-path": "^3.0.0",
    "compact-encoding": "^2.16.0",
    "device-file": "^2.1.2",
    "flat-tree": "^1.12.1",
    "hypercore-crypto": "^3.4.2",
    "hyperschema": "^1.7.0",
    "index-encoder": "^3.3.2",
    "resolve-reject-promise": "^1.0.0",
    "rocksdb-native": "^3.11.0",
    "scope-lock": "^1.2.4",
    "streamx": "^2.21.1"
  },
  "devDependencies": {
    "brittle": "^3.7.0",
    "lunte": "^1.2.0",
    "prettier": "^3.6.2",
    "prettier-config-holepunch": "^2.0.0",
    "test-tmp": "^1.3.1"
  }
}
// This file is autogenerated by the hyperschema compiler
// Schema Version: 1
/* eslint-disable camelcase */
/* eslint-disable quotes */
/* eslint-disable space-before-function-paren */

const { c } = require('hyperschema/runtime')

const VERSION = 1

// eslint-disable-next-line no-unused-vars
let version = VERSION

// @corestore/allocated
const encoding0 = {
  preencode(state, m) {
    c.uint.preencode(state, m.cores)
    c.uint.preencode(state, m.datas)
  },
  encode(state, m) {
    c.uint.encode(state, m.cores)
    c.uint.encode(state, m.datas)
  },
  decode(state) {
    const r0 = c.uint.decode(state)
    const r1 = c.uint.decode(state)

    return {
      cores: r0,
      datas: r1
    }
  }
}

// @corestore/head
const encoding1 = {
  preencode(state, m) {
    c.uint.preencode(state, m.version)
    state.end++ // max flag is 4 so always one byte

    if (m.allocated) encoding0.preencode(state, m.allocated)
    if (m.seed) c.fixed32.preencode(state, m.seed)
    if (m.defaultDiscoveryKey) c.fixed32.preencode(state, m.defaultDiscoveryKey)
  },
  encode(state, m) {
    const flags =
      (m.allocated ? 1 : 0) |
      (m.seed ? 2 : 0) |
      (m.defaultDiscoveryKey ? 4 : 0)

    c.uint.encode(state, m.version)
    c.uint.encode(state, flags)

    if (m.allocated) encoding0.encode(state, m.allocated)
    if (m.seed) c.fixed32.encode(state, m.seed)
    if (m.defaultDiscoveryKey) c.fixed32.encode(state, m.defaultDiscoveryKey)
  },
  decode(state) {
    const r0 = c.uint.decode(state)
    const flags = c.uint.decode(state)

    return {
      version: r0,
      allocated: (flags & 1) !== 0 ? encoding0.decode(state) : null,
      seed: (flags & 2) !== 0 ? c.fixed32.decode(state) : null,
      defaultDiscoveryKey: (flags & 4) !== 0 ? c.fixed32.decode(state) : null
    }
  }
}

// @corestore/alias
const encoding2 = {
  preencode(state, m) {
    c.string.preencode(state, m.name)
    c.fixed32.preencode(state, m.namespace)
  },
  encode(state, m) {
    c.string.encode(state, m.name)
    c.fixed32.encode(state, m.namespace)
  },
  decode(state) {
    const r0 = c.string.decode(state)
    const r1 = c.fixed32.decode(state)

    return {
      name: r0,
      namespace: r1
    }
  }
}

// @corestore/core
const encoding3 = {
  preencode(state, m) {
    c.uint.preencode(state, m.version)
    c.uint.preencode(state, m.corePointer)
    c.uint.preencode(state, m.dataPointer)
    state.end++ // max flag is 1 so always one byte

    if (m.alias) encoding2.preencode(state, m.alias)
  },
  encode(state, m) {
    const flags = m.alias ? 1 : 0

    c.uint.encode(state, m.version)
    c.uint.encode(state, m.corePointer)
    c.uint.encode(state, m.dataPointer)
    c.uint.encode(state, flags)

    if (m.alias) encoding2.encode(state, m.alias)
  },
  decode(state) {
    const r0 = c.uint.decode(state)
    const r1 = c.uint.decode(state)
    const r2 = c.uint.decode(state)
    const flags = c.uint.decode(state)

    return {
      version: r0,
      corePointer: r1,
      dataPointer: r2,
      alias: (flags & 1) !== 0 ? encoding2.decode(state) : null
    }
  }
}

const encoding4_enum = {
  blake2b: 'blake2b'
}

// @core/hashes enum
const encoding4 = {
  preencode (state, m) {
    state.end++ // max enum is 0 so always one byte
  },
  encode (state, m) {
    switch (m) {
      case 'blake2b':
        c.uint.encode(state, 0)
        break
      default:
        throw new Error('Unknown enum')
    }
  },
  decode (state) {
    switch (c.uint.decode(state)) {
      case 0:
        return 'blake2b'
      default: return null
    }
  }
}

const encoding5_enum = {
  ed25519: 'ed25519'
}

// @core/signatures enum
const encoding5 = {
  preencode (state, m) {
    state.end++ // max enum is 0 so always one byte
  },
  encode (state, m) {
    switch (m) {
      case 'ed25519':
        c.uint.encode(state, 0)
        break
      default:
        throw new Error('Unknown enum')
    }
  },
  decode (state) {
    switch (c.uint.decode(state)) {
      case 0:
        return 'ed25519'
      default: return null
    }
  }
}

// @core/tree-node
const encoding6 = {
  preencode(state, m) {
    c.uint.preencode(state, m.index)
    c.uint.preencode(state, m.size)
    c.fixed32.preencode(state, m.hash)
  },
  encode(state, m) {
    c.uint.encode(state, m.index)
    c.uint.encode(state, m.size)
    c.fixed32.encode(state, m.hash)
  },
  decode(state) {
    const r0 = c.uint.decode(state)
    const r1 = c.uint.decode(state)
    const r2 = c.fixed32.decode(state)

    return {
      index: r0,
      size: r1,
      hash: r2
    }
  }
}

// @core/signer
const encoding7 = {
  preencode(state, m) {
    encoding5.preencode(state, m.signature)
    c.fixed32.preencode(state, m.namespace)
    c.fixed32.preencode(state, m.publicKey)
  },
  encode(state, m) {
    encoding5.encode(state, m.signature)
    c.fixed32.encode(state, m.namespace)
    c.fixed32.encode(state, m.publicKey)
  },
  decode(state) {
    const r0 = encoding5.decode(state)
    const r1 = c.fixed32.decode(state)
    const r2 = c.fixed32.decode(state)

    return {
      signature: r0,
      namespace: r1,
      publicKey: r2
    }
  }
}

// @core/prologue
const encoding8 = {
  preencode(state, m) {
    c.fixed32.preencode(state, m.hash)
    c.uint.preencode(state, m.length)
  },
  encode(state, m) {
    c.fixed32.encode(state, m.hash)
    c.uint.encode(state, m.length)
  },
  decode(state) {
    const r0 = c.fixed32.decode(state)
    const r1 = c.uint.decode(state)

    return {
      hash: r0,
      length: r1
    }
  }
}

// @core/manifest.signers
const encoding9_4 = c.array(encoding7)
// @core/manifest.linked
const encoding9_6 = c.array(c.fixed32)

// @core/manifest
const encoding9 = {
  preencode(state, m) {
    c.uint.preencode(state, m.version)
    state.end++ // max flag is 8 so always one byte
    encoding4.preencode(state, m.hash)
    c.uint.preencode(state, m.quorum)
    encoding9_4.preencode(state, m.signers)

    if (m.prologue) encoding8.preencode(state, m.prologue)
    if (m.linked) encoding9_6.preencode(state, m.linked)
    if (m.userData) c.buffer.preencode(state, m.userData)
  },
  encode(state, m) {
    const flags =
      (m.allowPatch ? 1 : 0) |
      (m.prologue ? 2 : 0) |
      (m.linked ? 4 : 0) |
      (m.userData ? 8 : 0)

    c.uint.encode(state, m.version)
    c.uint.encode(state, flags)
    encoding4.encode(state, m.hash)
    c.uint.encode(state, m.quorum)
    encoding9_4.encode(state, m.signers)

    if (m.prologue) encoding8.encode(state, m.prologue)
    if (m.linked) encoding9_6.encode(state, m.linked)
    if (m.userData) c.buffer.encode(state, m.userData)
  },
  decode(state) {
    const r0 = c.uint.decode(state)
    const flags = c.uint.decode(state)

    return {
      version: r0,
      hash: encoding4.decode(state),
      quorum: c.uint.decode(state),
      allowPatch: (flags & 1) !== 0,
      signers: encoding9_4.decode(state),
      prologue: (flags & 2) !== 0 ? encoding8.decode(state) : null,
      linked: (flags & 4) !== 0 ? encoding9_6.decode(state) : null,
      userData: (flags & 8) !== 0 ? c.buffer.decode(state) : null
    }
  }
}

// @core/keyPair
const encoding10 = {
  preencode(state, m) {
    c.buffer.preencode(state, m.publicKey)
    c.buffer.preencode(state, m.secretKey)
  },
  encode(state, m) {
    c.buffer.encode(state, m.publicKey)
    c.buffer.encode(state, m.secretKey)
  },
  decode(state) {
    const r0 = c.buffer.decode(state)
    const r1 = c.buffer.decode(state)

    return {
      publicKey: r0,
      secretKey: r1
    }
  }
}

// @core/auth.manifest
const encoding11_2 = c.frame(encoding9)

// @core/auth
const encoding11 = {
  preencode(state, m) {
    c.fixed32.preencode(state, m.key)
    c.fixed32.preencode(state, m.discoveryKey)
    state.end++ // max flag is 4 so always one byte

    if (m.manifest) encoding11_2.preencode(state, m.manifest)
    if (m.keyPair) encoding10.preencode(state, m.keyPair)
    if (m.encryptionKey) c.buffer.preencode(state, m.encryptionKey)
  },
  encode(state, m) {
    const flags =
      (m.manifest ? 1 : 0) |
      (m.keyPair ? 2 : 0) |
      (m.encryptionKey ? 4 : 0)

    c.fixed32.encode(state, m.key)
    c.fixed32.encode(state, m.discoveryKey)
    c.uint.encode(state, flags)

    if (m.manifest) encoding11_2.encode(state, m.manifest)
    if (m.keyPair) encoding10.encode(state, m.keyPair)
    if (m.encryptionKey) c.buffer.encode(state, m.encryptionKey)
  },
  decode(state) {
    const r0 = c.fixed32.decode(state)
    const r1 = c.fixed32.decode(state)
    const flags = c.uint.decode(state)

    return {
      key: r0,
      discoveryKey: r1,
      manifest: (flags & 1) !== 0 ? encoding11_2.decode(state) : null,
      keyPair: (flags & 2) !== 0 ? encoding10.decode(state) : null,
      encryptionKey: (flags & 4) !== 0 ? c.buffer.decode(state) : null
    }
  }
}

// @core/head
const encoding12 = {
  preencode(state, m) {
    c.uint.preencode(state, m.fork)
    c.uint.preencode(state, m.length)
    c.fixed32.preencode(state, m.rootHash)
    c.buffer.preencode(state, m.signature)
  },
  encode(state, m) {
    c.uint.encode(state, m.fork)
    c.uint.encode(state, m.length)
    c.fixed32.encode(state, m.rootHash)
    c.buffer.encode(state, m.signature)
  },
  decode(state) {
    const r0 = c.uint.decode(state)
    const r1 = c.uint.decode(state)
    const r2 = c.fixed32.decode(state)
    const r3 = c.buffer.decode(state)

    return {
      fork: r0,
      length: r1,
      rootHash: r2,
      signature: r3
    }
  }
}

// @core/hints
const encoding13 = {
  preencode(state, m) {
    state.end++ // max flag is 2 so always one byte

    if (m.contiguousLength) c.uint.preencode(state, m.contiguousLength)
    if (m.remoteContiguousLength) c.uint.preencode(state, m.remoteContiguousLength)
  },
  encode(state, m) {
    const flags =
      (m.contiguousLength ? 1 : 0) |
      (m.remoteContiguousLength ? 2 : 0)

    c.uint.encode(state, flags)

    if (m.contiguousLength) c.uint.encode(state, m.contiguousLength)
    if (m.remoteContiguousLength) c.uint.encode(state, m.remoteContiguousLength)
  },
  decode(state) {
    const flags = c.uint.decode(state)

    return {
      contiguousLength: (flags & 1) !== 0 ? c.uint.decode(state) : 0,
      remoteContiguousLength: (flags & 2) !== 0 ? c.uint.decode(state) : 0
    }
  }
}

// @core/session
const encoding14 = {
  preencode(state, m) {
    c.string.preencode(state, m.name)
    c.uint.preencode(state, m.dataPointer)
  },
  encode(state, m) {
    c.string.encode(state, m.name)
    c.uint.encode(state, m.dataPointer)
  },
  decode(state) {
    const r0 = c.string.decode(state)
    const r1 = c.uint.decode(state)

    return {
      name: r0,
      dataPointer: r1
    }
  }
}

// @core/sessions
const encoding15 = c.array(encoding14)

// @core/dependency
const encoding16 = {
  preencode(state, m) {
    c.uint.preencode(state, m.dataPointer)
    c.uint.preencode(state, m.length)
  },
  encode(state, m) {
    c.uint.encode(state, m.dataPointer)
    c.uint.encode(state, m.length)
  },
  decode(state) {
    const r0 = c.uint.decode(state)
    const r1 = c.uint.decode(state)

    return {
      dataPointer: r0,
      length: r1
    }
  }
}

function setVersion(v) {
  version = v
}

function encode(name, value, v = VERSION) {
  version = v
  return c.encode(getEncoding(name), value)
}

function decode(name, buffer, v = VERSION) {
  version = v
  return c.decode(getEncoding(name), buffer)
}

function getEnum(name) {
  switch (name) {
    case '@core/hashes':
      return encoding4_enum
    case '@core/signatures':
      return encoding5_enum
    default:
      throw new Error('Enum not found ' + name)
  }
}

function getEncoding(name) {
  switch (name) {
    case '@corestore/allocated':
      return encoding0
    case '@corestore/head':
      return encoding1
    case '@corestore/alias':
      return encoding2
    case '@corestore/core':
      return encoding3
    case '@core/hashes':
      return encoding4
    case '@core/signatures':
      return encoding5
    case '@core/tree-node':
      return encoding6
    case '@core/signer':
      return encoding7
    case '@core/prologue':
      return encoding8
    case '@core/manifest':
      return encoding9
    case '@core/keyPair':
      return encoding10
    case '@core/auth':
      return encoding11
    case '@core/head':
      return encoding12
    case '@core/hints':
      return encoding13
    case '@core/session':
      return encoding14
    case '@core/sessions':
      return encoding15
    case '@core/dependency':
      return encoding16
    default:
      throw new Error('Encoder not found ' + name)
  }
}

function getStruct(name, v = VERSION) {
  const enc = getEncoding(name)
  return {
    preencode(state, m) {
      version = v
      enc.preencode(state, m)
    },
    encode(state, m) {
      version = v
      enc.encode(state, m)
    },
    decode(state) {
      version = v
      return enc.decode(state)
    }
  }
}

const resolveStruct = getStruct // compat

module.exports = {
  resolveStruct,
  getStruct,
  getEnum,
  getEncoding,
  encode,
  decode,
  setVersion,
  version
}
const { EventEmitter } = require('events')
const isOptions = require('is-options')
const crypto = require('hypercore-crypto')
const CoreStorage = require('hypercore-storage')
const c = require('compact-encoding')
const b4a = require('b4a')
const NoiseSecretStream = require('@hyperswarm/secret-stream')
const Protomux = require('protomux')
const id = require('hypercore-id-encoding')
const safetyCatch = require('safety-catch')
const unslab = require('unslab')

const inspect = require('./lib/inspect')
const Core = require('./lib/core')
const Info = require('./lib/info')
const Download = require('./lib/download')
const DefaultEncryption = require('./lib/default-encryption')
const caps = require('./lib/caps')
const Replicator = require('./lib/replicator')
const { manifestHash, createManifest } = require('./lib/verifier')
const { ReadStream, WriteStream, ByteStream } = require('./lib/streams')
const { MerkleTree } = require('./lib/merkle-tree')
const {
  ASSERTION,
  BAD_ARGUMENT,
  SESSION_CLOSED,
  SESSION_MOVED,
  SESSION_NOT_WRITABLE,
  SNAPSHOT_NOT_AVAILABLE,
  DECODING_ERROR
} = require('hypercore-errors')

// Hypercore actually does not have any notion of max/min block sizes
// but we enforce 15mb to ensure smooth replication (each block is transmitted atomically)
const MAX_SUGGESTED_BLOCK_SIZE = 15 * 1024 * 1024

class Hypercore extends EventEmitter {
  constructor(storage, key, opts) {
    super()

    if (isOptions(storage) && !storage.db) {
      opts = storage
      storage = null
      key = opts.key || null
    } else if (isOptions(key)) {
      opts = key
      key = opts.key || null
    }

    if (key && typeof key === 'string') key = id.decode(key)
    if (!opts) opts = {}

    if (!storage) storage = opts.storage

    this.core = null
    this.state = null
    this.encryption = null
    this.extensions = new Map()

    this.valueEncoding = null
    this.encodeBatch = null
    this.activeRequests = []
    this.sessions = null
    this.ongc = null

    this.keyPair = opts.keyPair || null
    this.readable = true
    this.writable = false
    this.exclusive = false
    this.opened = false
    this.closed = false
    this.weak = !!opts.weak
    this.snapshotted = !!opts.snapshot
    this.onseq = opts.onseq || null
    this.onwait = opts.onwait || null
    this.wait = opts.wait !== false
    this.timeout = opts.timeout || 0
    this.preload = null
    this.closing = null
    this.opening = null

    this._readonly = opts.writable === false
    this._preappend = preappend.bind(this)
    this._snapshot = null
    this._findingPeers = 0
    this._active = opts.weak ? !!opts.active : opts.active !== false

    this._sessionIndex = -1
    this._stateIndex = -1 // maintained by session state
    this._monitorIndex = -1 // maintained by replication state

    this.opening = this._open(storage, key, opts)
    this.opening.catch(safetyCatch)

    this.on('newListener', maybeAddMonitor)
  }

  [Symbol.for('nodejs.util.inspect.custom')](depth, opts) {
    return inspect(this, depth, opts)
  }

  static MAX_SUGGESTED_BLOCK_SIZE = MAX_SUGGESTED_BLOCK_SIZE

  static DefaultEncryption = DefaultEncryption

  static key(manifest, { compat, version, namespace } = {}) {
    if (b4a.isBuffer(manifest)) {
      manifest = { version, signers: [{ publicKey: manifest, namespace }] }
    }
    return compat ? manifest.signers[0].publicKey : manifestHash(createManifest(manifest))
  }

  static discoveryKey(key) {
    return crypto.discoveryKey(key)
  }

  static blockEncryptionKey(key, encryptionKey) {
    return DefaultEncryption.blockEncryptionKey(key, encryptionKey)
  }

  static getProtocolMuxer(stream) {
    return stream.noiseStream.userData
  }

  static createCore(storage, opts) {
    return new Core(Hypercore.defaultStorage(storage), { autoClose: false, ...opts })
  }

  static createProtocolStream(isInitiator, opts = {}) {
    let outerStream = Protomux.isProtomux(isInitiator)
      ? isInitiator.stream
      : isStream(isInitiator)
        ? isInitiator
        : opts.stream

    let noiseStream = null

    if (outerStream) {
      noiseStream = outerStream.noiseStream
    } else {
      noiseStream = new NoiseSecretStream(isInitiator, null, opts)
      outerStream = noiseStream.rawStream
    }
    if (!noiseStream) throw BAD_ARGUMENT('Invalid stream', this.discoveryKey)

    if (!noiseStream.userData) {
      const protocol = Protomux.from(noiseStream)

      if (opts.keepAlive !== false && noiseStream.keepAlive === 0) {
        noiseStream.setKeepAlive(5000)
      }
      noiseStream.userData = protocol
    }

    if (opts.ondiscoverykey) {
      noiseStream.userData.pair({ protocol: 'hypercore/alpha' }, opts.ondiscoverykey)
    }

    return outerStream
  }

  static defaultStorage(storage, opts = {}) {
    if (CoreStorage.isCoreStorage(storage)) return storage

    const directory = storage
    return new CoreStorage(directory, opts)
  }

  static clearRequests(session, err) {
    return Replicator.clearRequests(session, err)
  }

  snapshot(opts) {
    return this.session({ ...opts, snapshot: true })
  }

  compact() {
    return this.core.compact()
  }

  session(opts = {}) {
    if (this.closing) {
      // This makes the closing logic a lot easier. If this turns out to be a problem
      // in practice, open an issue and we'll try to make a solution for it.
      throw SESSION_CLOSED('Cannot make sessions on a closing core', this.discoveryKey)
    }
    if (opts.checkout !== undefined && !opts.name && !opts.atom) {
      throw ASSERTION('Checkouts are only supported on atoms or named sessions', this.discoveryKey)
    }

    const wait = opts.wait === false ? false : this.wait
    const writable = opts.writable === undefined ? !this._readonly : opts.writable === true
    const onwait = opts.onwait === undefined ? this.onwait : opts.onwait
    const onseq = opts.onseq === undefined ? this.onseq : opts.onseq
    const timeout = opts.timeout === undefined ? this.timeout : opts.timeout
    const weak = opts.weak === undefined ? this.weak : opts.weak
    const Clz = opts.class || Hypercore
    const s = new Clz(null, this.key, {
      ...opts,
      wait,
      onwait,
      onseq,
      timeout,
      writable,
      weak,
      parent: this
    })

    return s
  }

  async setEncryptionKey(key, opts) {
    if (!this.opened) await this.opening
    const encryption = this._getEncryptionProvider({ key, block: !!(opts && opts.block) })
    return this.setEncryption(encryption)
  }

  async setEncryption(encryption) {
    if (!this.opened) await this.opening

    if (encryption === null) {
      this.encryption = encryption
      return
    }

    if (!isEncryptionProvider(encryption)) {
      throw ASSERTION('Provider does not satisfy HypercoreEncryption interface', this.discoveryKey)
    }

    this.encryption = encryption
  }

  setKeyPair(keyPair) {
    this.keyPair = keyPair
  }

  setActive(bool) {
    const active = !!bool
    if (active === this._active || this.closing) return
    this._active = active
    if (!this.opened) return
    this.core.replicator.updateActivity(this._active ? 1 : -1)
  }

  async _open(storage, key, opts) {
    const preload = opts.preload || (opts.parent && opts.parent.preload)

    if (preload) {
      this.sessions = [] // in case someone looks at it like with peers
      this.preload = preload
      opts = { ...opts, ...(await this.preload) }
      this.preload = null
    }

    const parent = opts.parent || null
    const core = opts.core || (parent && parent.core)
    const sessions = opts.sessions || (parent && parent.sessions)
    const ongc = opts.ongc || (parent && parent.ongc)

    if (core) this.core = core
    if (ongc) this.ongc = ongc
    if (sessions) this.sessions = sessions

    if (this.sessions === null) this.sessions = []
    this._sessionIndex = this.sessions.push(this) - 1

    if (this.core === null) initOnce(this, storage, key, opts)
    if (this._monitorIndex === -2) this.core.addMonitor(this)

    try {
      await this._openSession(opts)
    } catch (err) {
      if (this.core.autoClose && this.core.hasSession() === false) await this.core.close()

      if (this.exclusive) this.core.unlockExclusive()

      this.core.removeMonitor(this)
      this._removeSession()

      if (this.state !== null) this.state.removeSession(this)

      this.closed = true
      this.emit('close')
      throw err
    }

    this.emit('ready')

    // if we are a weak session the core might have closed...
    if (this.core.closing) this.close().catch(safetyCatch)
  }

  _removeSession() {
    if (this._sessionIndex === -1) return
    const head = this.sessions.pop()
    if (head !== this) this.sessions[(head._sessionIndex = this._sessionIndex)] = head
    this._sessionIndex = -1
    if (this.ongc !== null) this.ongc(this)
  }

  async _openSession(opts) {
    if (this.core.opened === false) await this.core.ready()

    if (this.keyPair === null) this.keyPair = opts.keyPair || this.core.header.keyPair

    const parent = opts.parent || null
    if (parent && parent.encryption) this.encryption = parent.encryption

    const e = getEncryptionOption(opts)
    if (!this.encryption) this.encryption = this._getEncryptionProvider(e)

    this.writable = this._isWritable()

    if (opts.valueEncoding) {
      this.valueEncoding = c.from(opts.valueEncoding)
    }
    if (opts.encodeBatch) {
      this.encodeBatch = opts.encodeBatch
    }

    if (parent) {
      if (parent._stateIndex === -1) await parent.ready()
      if (!this.keyPair) this.keyPair = parent.keyPair

      const ps = parent.state

      if (ps) {
        const shouldSnapshot = this.snapshotted && !ps.isSnapshot()
        this.state = shouldSnapshot ? await ps.snapshot() : ps.ref()
      }

      if (this.snapshotted && this.core && !this._snapshot) {
        this._updateSnapshot()
      }
    }

    if (opts.exclusive && opts.writable !== false) {
      this.exclusive = true
      await this.core.lockExclusive()
    }

    const parentState = parent ? parent.state : this.core.state
    const checkout = opts.checkout === undefined ? -1 : opts.checkout
    const state = this.state

    if (opts.atom) {
      this.state = await parentState.createSession(null, false, opts.atom)
      if (state) state.unref()
    } else if (opts.name) {
      // todo: need to make named sessions safe before ready
      // atm we always copy the state in passCapabilities
      this.state = await parentState.createSession(opts.name, !!opts.overwrite, null)
      if (state) state.unref() // ref'ed above in setup session
    }

    if (this.state && checkout !== -1) {
      if (!opts.name && !opts.atom) {
        throw ASSERTION('Checkouts must be named or atomized', this.discoveryKey)
      }
      if (checkout > this.state.length) {
        throw ASSERTION(
          `Invalid checkout ${checkout} for ${opts.name}, length is ${this.state.length}`,
          this.discoveryKey
        )
      }
      if (this.state.prologue && checkout < this.state.prologue.length) {
        throw ASSERTION(
          `Invalid checkout ${checkout} for ${opts.name}, prologue length is ${this.state.prologue.length}`,
          this.discoveryKey
        )
      }
      if (checkout < this.state.length) await this.state.truncate(checkout, this.fork)
    }

    if (this.state === null) {
      this.state = this.core.state.ref()
    }

    this.writable = this._isWritable()

    if (this.snapshotted && this.core) this._updateSnapshot()

    this.state.addSession(this)
    // TODO: we need to rework the core reference flow, as the state and session do not always agree now due to moveTo
    this.core = this.state.core // in case it was wrong...

    if (opts.userData) {
      const tx = this.state.storage.write()
      for (const [key, value] of Object.entries(opts.userData)) {
        tx.putUserData(key, value)
      }
      await tx.flush()
    }

    if (opts.manifest && !this.core.header.manifest) {
      await this.core.setManifest(createManifest(opts.manifest))
    }

    this.core.replicator.updateActivity(this._active ? 1 : 0)

    this.opened = true
  }

  get replicator() {
    return this.core === null ? null : this.core.replicator
  }

  _getSnapshot() {
    return {
      length: this.state.length,
      byteLength: this.state.byteLength,
      fork: this.state.fork
    }
  }

  _updateSnapshot() {
    const prev = this._snapshot
    const next = (this._snapshot = this._getSnapshot())

    if (!prev) return true
    return prev.length !== next.length || prev.fork !== next.fork
  }

  _isWritable() {
    if (this._readonly) return false
    if (this.state && !this.state.isDefault()) return true
    return !!(this.keyPair && this.keyPair.secretKey)
  }

  close({ error } = {}) {
    if (this.closing) return this.closing

    this.closing = this._close(error || null)
    return this.closing
  }

  clearRequests(activeRequests, error) {
    if (!activeRequests.length) return
    if (this.core) this.core.replicator.clearRequests(activeRequests, error)
  }

  async _close(error) {
    if (this.opened === false) {
      try {
        await this.opening
      } catch (err) {
        if (!this.closed) throw err
      }
    }

    if (this.closed === true) return

    this.core.removeMonitor(this)
    this.state.removeSession(this)
    this._removeSession()

    this.readable = false
    this.writable = false
    this.opened = false

    const gc = []
    for (const ext of this.extensions.values()) {
      if (ext.session === this) gc.push(ext)
    }
    for (const ext of gc) ext.destroy()

    this.core.replicator.findingPeers -= this._findingPeers
    this.core.replicator.clearRequests(this.activeRequests, error)
    this.core.replicator.updateActivity(this._active ? -1 : 0)

    this._findingPeers = 0

    this.state.unref()

    if (this.exclusive) this.core.unlockExclusive()

    if (this.core.hasSession()) {
      // emit "fake" close as this is a session
      this.closed = true
      this.emit('close')
      return
    }

    if (this.core.autoClose) await this.core.close()

    this.closed = true
    this.emit('close')
  }

  async commit(session, opts) {
    await this.ready()
    await session.ready()

    return this.state.commit(session.state, { keyPair: this.keyPair, ...opts })
  }

  replicate(isInitiator, opts = {}) {
    // Only limitation here is that ondiscoverykey doesn't work atm when passing a muxer directly,
    // because it doesn't really make a lot of sense.
    if (Protomux.isProtomux(isInitiator)) return this._attachToMuxer(isInitiator)

    // if same stream is passed twice, ignore the 2nd one before we make sessions etc
    if (isStream(isInitiator) && this._isAttached(isInitiator)) return isInitiator

    const protocolStream = Hypercore.createProtocolStream(isInitiator, opts)
    const noiseStream = protocolStream.noiseStream
    const protocol = noiseStream.userData

    this._attachToMuxer(protocol)

    return protocolStream
  }

  _isAttached(stream) {
    return (
      stream.userData &&
      this.core &&
      this.core.replicator &&
      this.core.replicator.attached(stream.userData)
    )
  }

  _attachToMuxer(mux) {
    if (this.opened) {
      this.core.replicator.attachTo(mux)
    } else {
      this.opening.then(() => this.core.replicator.attachTo(mux), mux.destroy.bind(mux))
    }

    return mux
  }

  get id() {
    return this.core === null ? null : this.core.id
  }

  get key() {
    return this.core === null ? null : this.core.key
  }

  get discoveryKey() {
    return this.core === null ? null : this.core.discoveryKey
  }

  get manifest() {
    return this.core === null ? null : this.core.manifest
  }

  get length() {
    if (this._snapshot) return this._snapshot.length
    return this.opened === false ? 0 : this.state.length
  }

  get signedLength() {
    return this.opened === false ? 0 : this.state.signedLength()
  }

  /**
   * Deprecated. Use `const { byteLength } = await core.info()`.
   */
  get byteLength() {
    if (this.opened === false) return 0
    if (this._snapshot) return this._snapshot.byteLength
    return this.state.byteLength - this.state.length * this.padding
  }

  get remoteContiguousLength() {
    if (this.opened === false) return 0
    return Math.min(this.core.state.length, this.core.header.hints.remoteContiguousLength)
  }

  get contiguousLength() {
    if (this.opened === false) return 0
    return Math.min(this.core.state.length, this.core.header.hints.contiguousLength)
  }

  get contiguousByteLength() {
    return 0
  }

  get fork() {
    if (this.opened === false) return 0
    return this.state.fork
  }

  get padding() {
    if (this.encryption && this.key && this.manifest) {
      return this.encryption.padding(this.core, this.length)
    }

    return 0
  }

  get peers() {
    return this.opened === false ? [] : this.core.replicator.peers
  }

  get globalCache() {
    return this.opened === false ? null : this.core.globalCache
  }

  ready() {
    return this.opening
  }

  async setUserData(key, value) {
    if (this.opened === false) await this.opening
    const existing = await this.getUserData(key)
    if (existing && b4a.isBuffer(value) && b4a.equals(existing, value)) return
    await this.state.setUserData(key, value)
  }

  async getUserData(key) {
    if (this.opened === false) await this.opening
    const batch = this.state.storage.read()
    const p = batch.getUserData(key)
    batch.tryFlush()
    return p
  }

  transferSession(core) {
    // todo: validate we can move

    if (this.weak === false) {
      this.core.activeSessions--
      core.activeSessions++
    }

    if (this._monitorIndex >= 0) {
      this.core.removeMonitor(this)
      core.addMonitor(this)
    }

    const old = this.core

    this.core = core

    old.replicator.clearRequests(this.activeRequests, SESSION_MOVED())

    this.emit('migrate', this.key)
  }

  findingPeers() {
    this._findingPeers++
    if (this.core !== null && !this.closing) this.core.replicator.findingPeers++

    let once = true

    return () => {
      if (this.closing || !once) return
      once = false
      this._findingPeers--
      if (this.core !== null && --this.core.replicator.findingPeers === 0) {
        this.core.replicator.updateAll()
      }
    }
  }

  async info(opts) {
    if (this.opened === false) await this.opening

    return Info.from(this, opts)
  }

  async update(opts) {
    if (this.opened === false) await this.opening
    if (this.closing !== null) return false
    if (this.snapshotted) return false

    if (this.writable && (!opts || opts.force !== true)) return false

    const remoteWait = this._shouldWait(opts, this.core.replicator.findingPeers > 0)

    let upgraded = false

    if (await this.core.replicator.applyPendingReorg()) {
      upgraded = true
    }

    if (!upgraded && remoteWait) {
      const activeRequests = (opts && opts.activeRequests) || this.activeRequests
      const req = this.core.replicator.addUpgrade(activeRequests)

      try {
        upgraded = await req.promise
      } catch (err) {
        if (isSessionMoved(err)) return this.update(opts)
        throw err
      }
    }

    if (!upgraded) return false
    return true
  }

  async seek(bytes, opts) {
    if (this.opened === false) await this.opening
    if (!isValidIndex(bytes)) throw ASSERTION('seek is invalid', this.discoveryKey)

    const activeRequests = (opts && opts.activeRequests) || this.activeRequests

    if (this.encryption && !this.core.manifest) {
      const req = this.replicator.addUpgrade(activeRequests)
      try {
        await req.promise
      } catch (err) {
        if (isSessionMoved(err)) return this.seek(bytes, opts)
        throw err
      }
    }

    const s = MerkleTree.seek(this.state, bytes, this.padding)

    const offset = await s.update()
    if (offset) return offset

    if (this.closing !== null) {
      throw SESSION_CLOSED('cannot seek on a closed session', this.discoveryKey)
    }

    if (!this._shouldWait(opts, this.wait)) return null

    const req = this.core.replicator.addSeek(activeRequests, s)

    const timeout = opts && opts.timeout !== undefined ? opts.timeout : this.timeout
    if (timeout) req.context.setTimeout(req, timeout)

    try {
      return await req.promise
    } catch (err) {
      if (isSessionMoved(err)) return this.seek(bytes, opts)
      throw err
    }
  }

  async has(start, end = start + 1) {
    if (this.opened === false) await this.opening
    if (!isValidIndex(start) || !isValidIndex(end)) {
      throw ASSERTION('has range is invalid', this.discoveryKey)
    }

    if (this.state.isDefault()) {
      if (end === start + 1) return this.core.bitfield.get(start)

      const i = this.core.bitfield.firstUnset(start)
      return i === -1 || i >= end
    }

    if (end === start + 1) {
      const rx = this.state.storage.read()
      const block = rx.getBlock(start)
      rx.tryFlush()

      return (await block) !== null
    }

    let count = 0

    const stream = this.state.storage.createBlockStream({ gte: start, lt: end })
    for await (const block of stream) {
      if (block === null) return false
      count++
    }

    return count === end - start
  }

  async get(index, opts) {
    if (this.opened === false) await this.opening
    if (!isValidIndex(index)) throw ASSERTION('block index is invalid', this.discoveryKey)

    if (this.closing !== null) {
      throw SESSION_CLOSED('cannot get on a closed session', this.discoveryKey)
    }

    const encoding =
      (opts && opts.valueEncoding && c.from(opts.valueEncoding)) || this.valueEncoding

    if (this.onseq !== null) this.onseq(index, this)

    const req = this._get(index, opts)

    let block = await req
    if (!block) return null

    if (opts && opts.raw) return block

    if (this.encryption && (!opts || opts.decrypt !== false)) {
      // Copy the block as it might be shared with other sessions.
      block = b4a.from(block)

      await this.encryption.decrypt(index, block, this.core)
    }

    return this._decode(encoding, block, index)
  }

  async clear(start, end = start + 1, opts) {
    if (this.opened === false) await this.opening
    if (this.closing !== null) {
      throw SESSION_CLOSED('cannot clear on a closed session', this.discoveryKey)
    }

    if (typeof end === 'object') {
      opts = end
      end = start + 1
    }

    if (!isValidIndex(start) || !isValidIndex(end)) {
      throw ASSERTION('clear range is invalid', this.discoveryKey)
    }

    const cleared = opts && opts.diff ? { blocks: 0 } : null

    if (start >= end) return cleared
    if (start >= this.length) return cleared

    await this.state.clear(start, end, cleared)

    return cleared
  }

  async purge() {
    await this._closeAllSessions(null)
    await this.core.purge()
  }

  async _get(index, opts) {
    const block = await readBlock(this.state.storage.read(), index)

    if (block !== null) return block

    if (this.closing !== null) {
      throw SESSION_CLOSED('cannot get on a closed session', this.discoveryKey)
    }

    // snapshot should check if core has block
    if (this._snapshot !== null) {
      checkSnapshot(this, index)
      const coreBlock = await readBlock(this.core.state.storage.read(), index)

      checkSnapshot(this, index)
      if (coreBlock !== null) return coreBlock
    }

    // lets check the bitfield to see if we got it during the above async calls
    // this is the last resort before replication, so always safe.
    if (this.core.bitfield.get(index)) {
      const coreBlock = await readBlock(this.state.storage.read(), index)
      // TODO: this should not be needed, only needed atm in case we are doing a moveTo during this (we should fix)
      if (coreBlock !== null) return coreBlock
    }

    if (!this._shouldWait(opts, this.wait)) return null

    if (opts && opts.onwait) opts.onwait(index, this)
    if (this.onwait) this.onwait(index, this)

    const activeRequests = (opts && opts.activeRequests) || this.activeRequests

    const req = this.core.replicator.addBlock(activeRequests, index)
    req.snapshot = index < this.length

    const timeout = opts && opts.timeout !== undefined ? opts.timeout : this.timeout
    if (timeout) req.context.setTimeout(req, timeout)

    let replicatedBlock = null

    try {
      replicatedBlock = await req.promise
    } catch (err) {
      if (isSessionMoved(err)) return this._get(index, opts)
      throw err
    }

    if (this._snapshot !== null) checkSnapshot(this, index)
    return maybeUnslab(replicatedBlock)
  }

  _shouldWait(opts, defaultValue) {
    if (opts) {
      if (opts.wait === false) return false
      if (opts.wait === true) return true
    }
    return defaultValue
  }

  createReadStream(opts) {
    return new ReadStream(this, opts)
  }

  createWriteStream() {
    return new WriteStream(this)
  }

  createByteStream(opts) {
    return new ByteStream(this, opts)
  }

  download(range) {
    return new Download(this, range)
  }

  // TODO: get rid of this / deprecate it?
  undownload(range) {
    range.destroy(null)
  }

  // TODO: get rid of this / deprecate it?
  cancel(request) {
    // Do nothing for now
  }

  async truncate(newLength = 0, opts = {}) {
    if (this.opened === false) await this.opening

    const {
      fork = this.state.fork + 1,
      keyPair = this.keyPair,
      signature = null
    } = typeof opts === 'number' ? { fork: opts } : opts

    const isDefault = this.state === this.core.state
    const writable = !this._readonly && !!(signature || (keyPair && keyPair.secretKey))
    if (isDefault && writable === false && (newLength > 0 || fork !== this.state.fork)) {
      throw SESSION_NOT_WRITABLE('cannot append to a non-writable core', this.discoveryKey)
    }

    await this.state.truncate(newLength, fork, { keyPair, signature })

    // TODO: Should propagate from an event triggered by the oplog
    if (this.state === this.core.state) this.core.replicator.updateAll()
  }

  async append(blocks, opts = {}) {
    if (this.opened === false) await this.opening

    const isDefault = this.state === this.core.state
    const defaultKeyPair = this.state.name === null ? this.keyPair : null

    const { keyPair = defaultKeyPair, signature = null, maxLength, postappend = null } = opts
    const writable =
      !isDefault || !!signature || !!(keyPair && keyPair.secretKey) || opts.writable === true

    if (this._readonly || writable === false) {
      throw SESSION_NOT_WRITABLE('cannot append to a readonly core', this.discoveryKey)
    }

    blocks = Array.isArray(blocks) ? blocks : [blocks]

    const preappend = this.encryption && this._preappend

    const buffers = this.encodeBatch !== null ? this.encodeBatch(blocks) : new Array(blocks.length)

    if (this.encodeBatch === null) {
      for (let i = 0; i < blocks.length; i++) {
        buffers[i] = this._encode(this.valueEncoding, blocks[i])
      }
    }
    for (const b of buffers) {
      if (b.byteLength > MAX_SUGGESTED_BLOCK_SIZE) {
        throw BAD_ARGUMENT(
          'Appended block exceeds the maximum suggested block size',
          this.discoveryKey
        )
      }
    }

    return this.state.append(buffers, { keyPair, signature, preappend, postappend, maxLength })
  }

  async signable(length = -1, fork = -1) {
    if (this.opened === false) await this.opening
    if (length === -1) length = this.length
    if (fork === -1) fork = this.fork

    return caps.treeSignable(this.key, await this.treeHash(length), length, fork)
  }

  async treeHash(length = -1) {
    if (this.opened === false) await this.opening
    if (length === -1) length = this.length

    const roots = await MerkleTree.getRoots(this.state, length)
    return crypto.tree(roots)
  }

  async missingNodes(index) {
    if (this.opened === false) await this.opening
    return await MerkleTree.missingNodes(this.core.state, 2 * index, this.core.state.length)
  }

  async proof(opts) {
    if (this.opened === false) await this.opening
    const rx = this.state.storage.read()
    const proofPromise = MerkleTree.proof(this.state, rx, opts)
    const blockPromise = opts && opts.block ? rx.getBlock(opts.block.index) : null
    rx.tryFlush()
    const [proof, block] = await Promise.all([proofPromise, blockPromise])
    const settled = await proof.settle()
    if (block) settled.block.value = block
    return settled
  }

  async applyProof(proof, from) {
    if (this.opened === false) await this.opening
    return this.core.verify(proof, from)
  }

  async verifyFullyRemote(proof) {
    if (this.opened === false) await this.opening
    const batch = await MerkleTree.verifyFullyRemote(this.state, proof)
    await this.core._verifyBatchUpgrade(batch, proof.manifest)
    return batch
  }

  registerExtension(name, handlers = {}) {
    if (this.extensions.has(name)) {
      const ext = this.extensions.get(name)
      ext.handlers = handlers
      ext.encoding = c.from(handlers.encoding || c.buffer)
      ext.session = this
      return ext
    }

    const ext = {
      name,
      handlers,
      encoding: c.from(handlers.encoding || c.buffer),
      session: this,
      send(message, peer) {
        const buffer = c.encode(this.encoding, message)
        peer.extension(name, buffer)
      },
      broadcast(message) {
        const buffer = c.encode(this.encoding, message)
        for (const peer of this.session.peers) {
          peer.extension(name, buffer)
        }
      },
      destroy() {
        for (const peer of this.session.peers) {
          if (peer.extensions.get(name) === ext) peer.extensions.delete(name)
        }
        this.session.extensions.delete(name)
      },
      _onmessage(state, peer) {
        const m = this.encoding.decode(state)
        if (this.handlers.onmessage) this.handlers.onmessage(m, peer)
      }
    }

    this.extensions.set(name, ext)

    if (this.core === null) this._monitorIndex = -2
    else this.core.addMonitor(this)

    for (const peer of this.peers) {
      peer.extensions.set(name, ext)
    }

    return ext
  }

  _encode(enc, val) {
    const state = { start: this.padding, end: this.padding, buffer: null }

    if (b4a.isBuffer(val)) {
      if (state.start === 0) return val
      state.end += val.byteLength
    } else if (enc) {
      enc.preencode(state, val)
    } else {
      val = b4a.from(val)
      if (state.start === 0) return val
      state.end += val.byteLength
    }

    state.buffer = b4a.allocUnsafe(state.end)

    if (enc) enc.encode(state, val)
    else state.buffer.set(val, state.start)

    return state.buffer
  }

  _decode(enc, block, index) {
    if (this.encryption) block = block.subarray(this.encryption.padding(this.core, index))
    try {
      if (enc) return c.decode(enc, block)
    } catch (err) {
      throw DECODING_ERROR(err.message, this.discoveryKey)
    }
    return block
  }

  _getEncryptionProvider(e) {
    if (isEncryptionProvider(e)) return e
    if (!e || !e.key) return null
    return new DefaultEncryption(e.key, this.key, { block: e.block, compat: this.core.compat })
  }
}

module.exports = Hypercore

function isStream(s) {
  return typeof s === 'object' && s && typeof s.pipe === 'function'
}

async function preappend(blocks) {
  const offset = this.state.length
  const fork = this.state.encryptionFork

  for (let i = 0; i < blocks.length; i++) {
    await this.encryption.encrypt(offset + i, blocks[i], fork, this.core)
  }
}

function isValidIndex(index) {
  return index === 0 || index > 0
}

function maybeUnslab(block) {
  // Unslab only when it takes up less then half the slab
  return block !== null && 2 * block.byteLength < block.buffer.byteLength ? unslab(block) : block
}

function checkSnapshot(snapshot, index) {
  if (index >= snapshot.state.snapshotCompatLength) {
    throw SNAPSHOT_NOT_AVAILABLE(
      `snapshot at index ${index} not available (max compat length ${snapshot.state.snapshotCompatLength})`,
      snapshot.discoveryKey
    )
  }
}

function readBlock(rx, index) {
  const promise = rx.getBlock(index)
  rx.tryFlush()
  return promise
}

function initOnce(session, storage, key, opts) {
  if (storage === null) storage = opts.storage || null
  if (key === null) key = opts.key || null

  session.core = new Core(Hypercore.defaultStorage(storage), {
    preopen: opts.preopen,
    eagerUpgrade: opts.eagerUpgrade !== false,
    notDownloadingLinger: opts.notDownloadingLinger,
    allowFork: opts.allowFork !== false,
    allowPush: !!opts.allowPush,
    alwaysLatestBlock: !!opts.allowLatestBlock,
    inflightRange: opts.inflightRange,
    compat: opts.compat === true,
    force: opts.force,
    createIfMissing: opts.createIfMissing,
    discoveryKey: opts.discoveryKey,
    overwrite: opts.overwrite,
    key,
    keyPair: opts.keyPair,
    legacy: opts.legacy,
    manifest: opts.manifest,
    globalCache: opts.globalCache || null // session is a temp option, not to be relied on unless you know what you are doing (no semver guarantees)
  })
}

function maybeAddMonitor(name) {
  if (name === 'append' || name === 'truncate') return
  if (this._monitorIndex >= 0 || this.closing) return

  if (this.core === null) {
    this._monitorIndex = -2
  } else {
    this.core.addMonitor(this)
  }
}

function isSessionMoved(err) {
  return err.code === 'SESSION_MOVED'
}

function getEncryptionOption(opts) {
  // old style, supported for now but will go away
  if (opts.encryptionKey) return { key: opts.encryptionKey, block: !!opts.isBlockKey }
  if (!opts.encryption) return null
  return b4a.isBuffer(opts.encryption) ? { key: opts.encryption } : opts.encryption
}

function isEncryptionProvider(e) {
  return e && isFunction(e.padding) && isFunction(e.encrypt) && isFunction(e.decrypt)
}

function isFunction(fn) {
  return !!fn && typeof fn === 'function'
}
const crypto = require('hypercore-crypto')
const flat = require('flat-tree')
const b4a = require('b4a')
const { MerkleTree } = require('./merkle-tree')

module.exports = async function auditCore(
  core,
  { tree = true, blocks = true, bitfield = true, dryRun = false } = {}
) {
  const length = core.state.length
  const stats = {
    treeNodes: 0,
    blocks: 0,
    bits: 0,
    droppedTreeNodes: 0,
    droppedBlocks: 0,
    droppedBits: 0,
    corrupt: false
  }

  // audit the tree
  if (tree) {
    let tx = null

    const roots = await MerkleTree.getRootsFromStorage(core.state.storage, length)
    const stack = []

    for (const r of roots) {
      if (r === null) {
        if (!dryRun) {
          const storage = core.state.storage
          await storage.store.deleteCore(storage.core)
          return null
        }

        stats.corrupt = true
      }

      stack.push(r)
    }

    stats.treeNodes += roots.length

    while (stack.length > 0) {
      const node = stack.pop()

      if ((node.index & 1) === 0) continue

      const [left, right] = flat.children(node.index)

      const rx = core.state.storage.read()
      const leftNodePromise = rx.getTreeNode(left)
      const rightNodePromise = rx.getTreeNode(right)

      rx.tryFlush()

      const [leftNode, rightNode] = await Promise.all([leftNodePromise, rightNodePromise])

      if (isBadTree(node, leftNode, rightNode)) {
        if (!tx && !stats.corrupt) tx = core.state.storage.write()
        const [l, r] = flat.spans(node.index)
        tx.deleteTreeNodeRange(l, r + 1)
        stats.droppedTreeNodes++
        continue
      }

      if (!leftNode) continue

      stats.treeNodes += 2
      stack.push(leftNode, rightNode)
    }

    if (tx && !dryRun) await tx.flush()
  }

  // audit the blocks
  if (blocks) {
    let tx = null

    for await (const block of core.state.storage.createBlockStream()) {
      if (!core.bitfield.get(block.index)) {
        if (!tx && !stats.corrupt) tx = core.state.storage.write()
        tx.deleteBlock(block.index)
        stats.droppedBlocks++
      }

      const rx = core.state.storage.read()
      const treeNodePromise = rx.getTreeNode(2 * block.index)

      rx.tryFlush()

      const treeNode = await treeNodePromise

      if (isBadBlock(treeNode, block.value)) {
        if (!tx && !stats.corrupt) tx = core.state.storage.write()
        tx.deleteBlock(block.index)
        stats.droppedBlocks++
        continue
      }

      stats.blocks++
    }

    if (tx && !dryRun) await tx.flush()
  }

  if (bitfield) {
    let tx = null

    for (const index of allBits(core.bitfield)) {
      const rx = core.state.storage.read()
      const blockPromise = rx.getBlock(index)

      rx.tryFlush()

      const block = await blockPromise
      if (!block) {
        stats.droppedBits++
        if (dryRun) continue

        if (!tx && !stats.corrupt) tx = core.state.storage.write()

        core.bitfield.set(index, false)

        const page = core.bitfield.getBitfield(index)
        if (page.bitfield) tx.putBitfieldPage(page.index, page.bitfield)
        else tx.deleteBitfieldPage(page.index)
        continue
      }

      stats.bits++
    }

    if (tx && !dryRun) await tx.flush()
  }

  return stats
}

function isBadBlock(node, block) {
  if (!node) return true
  const hash = crypto.data(block)
  return !b4a.equals(hash, node.hash) || node.size !== block.byteLength
}

function isBadTree(parent, left, right) {
  if (!left && !right) return false
  if (!left || !right) return true
  const hash = crypto.parent(left, right)
  return !b4a.equals(hash, parent.hash) || parent.size !== left.size + right.size
}

function* allBits(bitfield) {
  let i = 0
  if (bitfield.get(0)) yield 0
  while (true) {
    i = bitfield.findFirst(true, i + 1)
    if (i === -1) break
    yield i
  }
}
const b4a = require('b4a')
const quickbit = require('./compat').quickbit

module.exports = class BitInterlude {
  constructor() {
    this.ranges = []
  }

  contiguousLength(from) {
    for (const r of this.ranges) {
      if (r.start > from) break
      if (!r.value && r.start <= from) return r.start
    }

    // TODO: be smarter
    while (this.get(from) === true) from++
    return from
  }

  get(index) {
    let start = 0
    let end = this.ranges.length

    while (start < end) {
      const mid = (start + end) >> 1
      const r = this.ranges[mid]

      if (index < r.start) {
        end = mid
        continue
      }

      if (index >= r.end) {
        if (mid === start) break
        start = mid
        continue
      }

      return r.value
    }

    return false
  }

  setRange(start, end, value) {
    if (start === end) return

    let r = null

    for (let i = 0; i < this.ranges.length; i++) {
      r = this.ranges[i]

      // if already inside, stop
      if (r.start <= start && end <= r.end) {
        if (value === r.value) return

        const ranges = mergeRanges(r, { start, end, value })
        this.ranges.splice(i, 1, ...ranges)

        return
      }

      // we wanna overun the interval
      if (start > r.end) {
        continue
      }

      // we overran but this interval is ending after us, move it back
      if (end >= r.start && end <= r.end) {
        r.start = r.value === value ? start : end
        if (r.value !== value) this.ranges.splice(i, 0, { start, end, value })
        return
      }

      // we overran but our start is contained in this interval, move start back
      if (start >= r.start && start <= r.end) {
        if (r.value !== value) {
          r.end = start // Crop current comparison

          // If there's a next range
          if (this.ranges.length - 1 > i) {
            const next = this.ranges[i + 1]
            // If the same, go next
            if (next.value === value) continue
            // Else we overlap so update next range
            if (next.start < end) next.start = end
          }

          this.ranges.splice(++i, 0, { start, end, value })
          return
        }

        start = r.start
      }

      let remove = 0

      for (let j = i; j < this.ranges.length; j++) {
        const n = this.ranges[j]
        if (n.start > end || n.value !== value) break
        if (n.start <= end && n.end > end) end = n.end
        remove++
      }

      this.ranges.splice(i, remove, { start, end, value })
      return
    }

    if (r !== null) {
      if (start <= r.end && end > r.end) {
        r.end = end
        return
      }

      // we never
      if (r.end > start) return
    }

    this.ranges.push({ start, end, value })
  }

  flush(tx, bitfield) {
    if (!this.ranges.length) return []

    let index = this.ranges[0].start
    const final = this.ranges[this.ranges.length - 1].end

    let i = 0

    while (index < final) {
      const page = bitfield.getBitfield(index) // read only
      const pageIndex = page ? page.index : bitfield.getPageIndex(index)

      const buf = b4a.allocUnsafe(bitfield.getPageByteLength())

      if (page) {
        const src = page.bitfield // Uint32Array
        buf.set(b4a.from(src.buffer, src.byteOffset, src.byteLength), 0)
      } else {
        b4a.fill(buf, 0)
      }

      const last = (pageIndex + 1) * (buf.byteLength << 3)
      const offset = pageIndex * (buf.byteLength << 3)

      let hasValue = false

      while (i < this.ranges.length) {
        const { start, end, value } = this.ranges[i]

        if (!hasValue && value) hasValue = true

        const from = start < index ? index : start
        const to = end < last ? end : last

        quickbit.fill(buf, value, from - offset, to - offset)

        index = to

        if (to === last) break

        i++
      }

      if (page || hasValue) tx.putBitfieldPage(pageIndex, buf)
    }

    return this.ranges
  }
}

function mergeRanges(a, b) {
  const ranges = []
  if (a.start < b.start) ranges.push({ start: a.start, end: b.start, value: a.value })
  ranges.push({ start: b.start, end: b.end, value: b.value })
  if (b.end < a.end) ranges.push({ start: b.end, end: a.end, value: a.value })

  return ranges
}
const BigSparseArray = require('big-sparse-array')
const b4a = require('b4a')
const quickbit = require('./compat').quickbit

const BITS_PER_PAGE = 32768
const BYTES_PER_PAGE = BITS_PER_PAGE / 8
const WORDS_PER_PAGE = BYTES_PER_PAGE / 4
const BITS_PER_SEGMENT = 2097152
const BYTES_PER_SEGMENT = BITS_PER_SEGMENT / 8
const WORDS_PER_SEGMENT = BYTES_PER_SEGMENT / 4
const INITIAL_WORDS_PER_SEGMENT = 1024
const PAGES_PER_SEGMENT = BITS_PER_SEGMENT / BITS_PER_PAGE
const SEGMENT_GROWTH_FACTOR = 4

class BitfieldPage {
  constructor(index, segment) {
    this.index = index
    this.offset = index * BYTES_PER_PAGE - segment.offset
    this.bitfield = null
    this.segment = segment

    segment.add(this)
  }

  get tree() {
    return this.segment.tree
  }

  get(index, dirty) {
    return quickbit.get(this.bitfield, index)
  }

  set(index, val) {
    if (quickbit.set(this.bitfield, index, val)) {
      this.tree.update(this.offset * 8 + index)
    }
  }

  setRange(start, end, val) {
    quickbit.fill(this.bitfield, val, start, end)

    let i = Math.floor(start / 128)
    const n = i + Math.ceil((end - start) / 128)

    while (i <= n) this.tree.update(this.offset * 8 + i++ * 128)
  }

  findFirst(val, position) {
    return quickbit.findFirst(this.bitfield, val, position)
  }

  findLast(val, position) {
    return quickbit.findLast(this.bitfield, val, position)
  }

  count(start, length, val) {
    const end = start + length

    let i = start
    let c = 0

    while (length > 0) {
      const l = this.findFirst(val, i)
      if (l === -1 || l >= end) return c

      const h = this.findFirst(!val, l + 1)
      if (h === -1 || h >= end) return c + end - l

      c += h - l
      length -= h - i
      i = h
    }

    return c
  }
}

class BitfieldSegment {
  constructor(index, bitfield) {
    this.index = index
    this.offset = index * BYTES_PER_SEGMENT
    this.tree = quickbit.Index.from(bitfield, BYTES_PER_SEGMENT)
    this.pages = new Array(PAGES_PER_SEGMENT)
  }

  get bitfield() {
    return this.tree.field
  }

  add(page) {
    const i = page.index - this.index * PAGES_PER_SEGMENT
    this.pages[i] = page

    const start = i * WORDS_PER_PAGE
    const end = start + WORDS_PER_PAGE

    if (end >= this.bitfield.length) this.reallocate(end)

    page.bitfield = this.bitfield.subarray(start, end)
  }

  reallocate(length) {
    let target = this.bitfield.length
    while (target < length) target *= SEGMENT_GROWTH_FACTOR

    const bitfield = new Uint32Array(target)
    bitfield.set(this.bitfield)

    this.tree = quickbit.Index.from(bitfield, BYTES_PER_SEGMENT)

    for (let i = 0; i < this.pages.length; i++) {
      const page = this.pages[i]
      if (!page) continue

      const start = i * WORDS_PER_PAGE
      const end = start + WORDS_PER_PAGE

      page.bitfield = bitfield.subarray(start, end)
    }
  }

  findFirst(val, position) {
    position = this.tree.skipFirst(!val, position)

    let j = position & (BITS_PER_PAGE - 1)
    let i = (position - j) / BITS_PER_PAGE

    if (i >= PAGES_PER_SEGMENT) return -1

    while (i < this.pages.length) {
      const p = this.pages[i]

      let index = -1

      if (p) index = p.findFirst(val, j)
      else if (!val) index = j

      if (index !== -1) return i * BITS_PER_PAGE + index

      j = 0
      i++
    }

    return -1
  }

  findLast(val, position) {
    position = this.tree.skipLast(!val, position)

    let j = position & (BITS_PER_PAGE - 1)
    let i = (position - j) / BITS_PER_PAGE

    if (i >= PAGES_PER_SEGMENT) return -1

    while (i >= 0) {
      const p = this.pages[i]

      let index = -1

      if (p) index = p.findLast(val, j)
      else if (!val) index = j

      if (index !== -1) return i * BITS_PER_PAGE + index

      j = BITS_PER_PAGE - 1
      i--
    }

    return -1
  }
}

module.exports = class Bitfield {
  static BITS_PER_PAGE = BITS_PER_PAGE
  static BYTES_PER_PAGE = BYTES_PER_PAGE

  constructor(buffer) {
    this.resumed = !!(buffer && buffer.byteLength >= 0)

    this._pages = new BigSparseArray()
    this._segments = new BigSparseArray()

    const view = this.resumed
      ? new Uint32Array(buffer.buffer, buffer.byteOffset, Math.floor(buffer.byteLength / 4))
      : new Uint32Array(INITIAL_WORDS_PER_SEGMENT)

    for (let i = 0; i < view.length; i += WORDS_PER_SEGMENT) {
      let bitfield = view.subarray(i, i + WORDS_PER_SEGMENT)
      let length = WORDS_PER_SEGMENT

      if (i === 0) {
        length = INITIAL_WORDS_PER_SEGMENT
        while (length < bitfield.length) length *= SEGMENT_GROWTH_FACTOR
      }

      if (bitfield.length !== length) {
        const copy = new Uint32Array(length)
        copy.set(bitfield, 0)
        bitfield = copy
      }

      const segment = new BitfieldSegment(i / WORDS_PER_SEGMENT, bitfield)
      this._segments.set(segment.index, segment)

      for (let j = 0; j < bitfield.length; j += WORDS_PER_PAGE) {
        const page = new BitfieldPage((i + j) / WORDS_PER_PAGE, segment)
        this._pages.set(page.index, page)
      }
    }
  }

  static from(bitfield) {
    return new Bitfield(bitfield.toBuffer(bitfield._pages.maxLength * BITS_PER_PAGE))
  }

  toBuffer(length) {
    const pages = Math.ceil(length / BITS_PER_PAGE)
    const buffer = b4a.allocUnsafe(pages * BYTES_PER_PAGE)

    for (let i = 0; i < pages; i++) {
      const page = this._pages.get(i)
      const offset = i * BYTES_PER_PAGE

      if (page) {
        const buf = b4a.from(
          page.bitfield.buffer,
          page.bitfield.byteOffset,
          page.bitfield.byteLength
        )

        buffer.set(buf, offset)
      } else {
        buffer.fill(0, offset, offset + BYTES_PER_PAGE)
      }
    }

    return buffer
  }

  getBitfield(index) {
    const i = this.getPageIndex(index)

    const p = this._pages.get(i)
    return p || null
  }

  merge(bitfield, length) {
    let i = 0

    while (i < length) {
      const start = bitfield.firstSet(i)
      if (start === -1) break

      i = bitfield.firstUnset(start)

      if (i === -1 || i > length) i = length

      this.setRange(start, i, true)

      if (i >= length) break
    }
  }

  get(index) {
    const j = index & (BITS_PER_PAGE - 1)
    const i = (index - j) / BITS_PER_PAGE

    const p = this._pages.get(i)

    return p ? p.get(j) : false
  }

  getPageByteLength() {
    return BYTES_PER_PAGE
  }

  getPageIndex(index) {
    const j = index & (BITS_PER_PAGE - 1)
    return (index - j) / BITS_PER_PAGE
  }

  getPage(index, create) {
    const i = this.getPageIndex(index)

    let p = this._pages.get(i)

    if (p) return p

    if (!create) return null

    const k = Math.floor(i / PAGES_PER_SEGMENT)
    const s =
      this._segments.get(k) ||
      this._segments.set(
        k,
        new BitfieldSegment(
          k,
          new Uint32Array(k === 0 ? INITIAL_WORDS_PER_SEGMENT : WORDS_PER_SEGMENT)
        )
      )

    p = this._pages.set(i, new BitfieldPage(i, s))

    return p
  }

  set(index, val) {
    const j = index & (BITS_PER_PAGE - 1)
    const i = (index - j) / BITS_PER_PAGE

    let p = this._pages.get(i)

    if (!p && val) {
      const k = Math.floor(i / PAGES_PER_SEGMENT)
      const s =
        this._segments.get(k) ||
        this._segments.set(
          k,
          new BitfieldSegment(
            k,
            new Uint32Array(k === 0 ? INITIAL_WORDS_PER_SEGMENT : WORDS_PER_SEGMENT)
          )
        )

      p = this._pages.set(i, new BitfieldPage(i, s))
    }

    if (p) p.set(j, val)
  }

  setRange(start, end, val) {
    let j = start & (BITS_PER_PAGE - 1)
    let i = (start - j) / BITS_PER_PAGE

    while (start < end) {
      let p = this._pages.get(i)

      if (!p && val) {
        const k = Math.floor(i / PAGES_PER_SEGMENT)
        const s =
          this._segments.get(k) ||
          this._segments.set(
            k,
            new BitfieldSegment(
              k,
              new Uint32Array(k === 0 ? INITIAL_WORDS_PER_SEGMENT : WORDS_PER_SEGMENT)
            )
          )

        p = this._pages.set(i, new BitfieldPage(i, s))
      }

      const offset = i * BITS_PER_PAGE
      const last = Math.min(end - offset, BITS_PER_PAGE)
      const range = last - j

      if (p) p.setRange(j, last, val)

      j = 0
      i++
      start += range
    }
  }

  findFirst(val, position) {
    let j = position & (BITS_PER_SEGMENT - 1)
    let i = (position - j) / BITS_PER_SEGMENT

    while (i < this._segments.maxLength) {
      const s = this._segments.get(i)

      let index = -1

      if (s) index = s.findFirst(val, j)
      else if (!val) index = j

      if (index !== -1) return i * BITS_PER_SEGMENT + index

      j = 0
      i++
    }

    return val ? -1 : this._segments.maxLength * BITS_PER_SEGMENT
  }

  firstSet(position) {
    return this.findFirst(true, position)
  }

  firstUnset(position) {
    return this.findFirst(false, position)
  }

  findLast(val, position) {
    let j = position & (BITS_PER_SEGMENT - 1)
    let i = (position - j) / BITS_PER_SEGMENT

    while (i >= 0) {
      const s = this._segments.get(i)

      let index = -1

      if (s) index = s.findLast(val, j)
      else if (!val) index = j

      if (index !== -1) return i * BITS_PER_SEGMENT + index

      j = BITS_PER_SEGMENT - 1
      i--
    }

    return -1
  }

  lastSet(position) {
    return this.findLast(true, position)
  }

  lastUnset(position) {
    return this.findLast(false, position)
  }

  hasSet(start, length) {
    const end = start + length

    let j = start & (BITS_PER_SEGMENT - 1)
    let i = (start - j) / BITS_PER_SEGMENT

    while (i < this._segments.maxLength) {
      const s = this._segments.get(i)

      let index = -1

      if (s) index = s.findFirst(true, j)

      if (index !== -1) return i * BITS_PER_SEGMENT + index < end

      j = 0
      i++

      if (i * BITS_PER_SEGMENT >= end) return false
    }

    return false
  }

  count(start, length, val) {
    let j = start & (BITS_PER_PAGE - 1)
    let i = (start - j) / BITS_PER_PAGE
    let c = 0

    while (length > 0) {
      const p = this._pages.get(i)

      const end = Math.min(j + length, BITS_PER_PAGE)
      const range = end - j

      if (p) c += p.count(j, range, val)
      else if (!val) c += range

      j = 0
      i++
      length -= range
    }

    return c
  }

  countSet(start, length) {
    return this.count(start, length, true)
  }

  countUnset(start, length) {
    return this.count(start, length, false)
  }

  *want(start, length) {
    const j = start & (BITS_PER_SEGMENT - 1)
    let i = (start - j) / BITS_PER_SEGMENT

    while (length > 0) {
      const s = this._segments.get(i)

      if (s) {
        // We always send at least 4 KiB worth of bitfield in a want, rounding
        // to the nearest 4 KiB.
        const end = ceilTo(clamp(length / 8, 4096, BYTES_PER_SEGMENT), 4096)

        yield {
          start: i * BITS_PER_SEGMENT,
          bitfield: s.bitfield.subarray(0, end / 4)
        }
      }

      i++
      length -= BITS_PER_SEGMENT
    }
  }

  clear(tx) {
    return tx.deleteBitfieldPageRange(0, -1)
  }

  onupdate(ranges) {
    for (const { start, end, value } of ranges) {
      this.setRange(start, end, value)
    }
  }

  static async open(storage, length) {
    if (length === 0) return new Bitfield(storage, null)

    const pages = Math.ceil(length / BITS_PER_PAGE)
    const buffer = b4a.alloc(pages * BYTES_PER_PAGE)
    const stream = storage.createBitfieldStream({ lt: pages })

    for await (const { index, page } of stream) {
      buffer.set(page, index * BYTES_PER_PAGE)
    }

    return new Bitfield(buffer)
  }
}

function clamp(n, min, max) {
  return Math.min(Math.max(n, min), max)
}

function ceilTo(n, multiple = 1) {
  const remainder = n % multiple
  if (remainder === 0) return n
  return n + multiple - remainder
}
const crypto = require('hypercore-crypto')
const sodium = require('sodium-universal')
const b4a = require('b4a')
const c = require('compact-encoding')

// TODO: rename this to "crypto" and move everything hashing related etc in here
// Also lets move the tree stuff from hypercore-crypto here

const [
  TREE,
  REPLICATE_INITIATOR,
  REPLICATE_RESPONDER,
  MANIFEST,
  DEFAULT_NAMESPACE,
  DEFAULT_ENCRYPTION
] = crypto.namespace('hypercore', 6)

exports.MANIFEST = MANIFEST
exports.DEFAULT_NAMESPACE = DEFAULT_NAMESPACE
exports.DEFAULT_ENCRYPTION = DEFAULT_ENCRYPTION

exports.replicate = function (isInitiator, key, handshakeHash) {
  const out = b4a.allocUnsafe(32)
  sodium.crypto_generichash_batch(
    out,
    [isInitiator ? REPLICATE_INITIATOR : REPLICATE_RESPONDER, key],
    handshakeHash
  )
  return out
}

exports.treeSignable = function (manifestHash, treeHash, length, fork) {
  const state = { start: 0, end: 112, buffer: b4a.allocUnsafe(112) }
  c.fixed32.encode(state, TREE)
  c.fixed32.encode(state, manifestHash)
  c.fixed32.encode(state, treeHash)
  c.uint64.encode(state, length)
  c.uint64.encode(state, fork)
  return state.buffer
}

exports.treeSignableCompat = function (hash, length, fork, noHeader) {
  const end = noHeader ? 48 : 80
  const state = { start: 0, end, buffer: b4a.allocUnsafe(end) }
  if (!noHeader) c.fixed32.encode(state, TREE) // ultra legacy mode, kill in future major
  c.fixed32.encode(state, hash)
  c.uint64.encode(state, length)
  c.uint64.encode(state, fork)
  return state.buffer
}
// Export the appropriate version of `quickbit-universal` as the plain import
// may resolve to an older version in some environments
let quickbit = require('quickbit-universal')
if (
  typeof quickbit.findFirst !== 'function' ||
  typeof quickbit.findLast !== 'function' ||
  typeof quickbit.clear !== 'function'
) {
  // This should always load the fallback from the locally installed version
  quickbit = require('quickbit-universal/fallback')
}
exports.quickbit = quickbit
const crypto = require('hypercore-crypto')
const flat = require('flat-tree')
const b4a = require('b4a')
const quickbit = require('quickbit-universal')
const Bitfield = require('./bitfield')

const MAX_BATCH_USED = 4 * 1024 * 1024
const MIN_BATCH_USED = 512 * 1024

// just in its own file as its a bit involved

module.exports = copyPrologue

async function copyPrologue(src, dst) {
  const prologue = dst.header.manifest.prologue

  if (src.length < prologue.length || prologue.length === 0) return

  const stack = []
  const roots = flat.fullRoots(prologue.length * 2)
  const batch = { roots, first: true, last: false, contig: 0, used: 0, tree: [], blocks: [] }

  for (let i = 0; i < roots.length; i++) {
    const node = roots[i]
    batch.tree.push(node)
    stack.push(node)
  }

  let lastPage = -1
  let lastBlock = -1

  for await (const data of src.storage.createBlockStream({
    gte: 0,
    lt: prologue.length,
    reverse: true
  })) {
    if (walkTree(stack, data.index * 2, batch) === false) {
      throw new Error('Missing block or tree node for ' + data.index)
    }

    batch.contig = data.index + 1 === lastBlock ? batch.contig + 1 : 1
    lastBlock = data.index

    const page = getBitfieldPage(data.index)
    batch.blocks.push(data)

    if (lastPage !== page) batch.used += 4096
    batch.used += Math.max(data.value.byteLength, 128) // 128 is just a sanity number to avoid mega batches

    // always safe to partially flush so we do that ondemand to reduce memory usage...
    if ((batch.used >= MIN_BATCH_USED && page !== lastPage) || batch.used >= MAX_BATCH_USED) {
      await flushBatch(prologue, src, dst, batch)
    }

    lastPage = page
  }

  if (lastBlock !== 0) batch.contig = 0

  batch.last = true
  await flushBatch(prologue, src, dst, batch)
}

async function flushBatch(prologue, src, dst, batch) {
  const nodePromises = []

  const srcReader = src.storage.read()
  for (const index of batch.tree) {
    nodePromises.push(srcReader.getTreeNode(index))
  }
  srcReader.tryFlush()

  const nodes = await Promise.all(nodePromises)

  const pagePromises = []
  const dstReader = dst.storage.read()

  const headPromise = batch.first ? dstReader.getHead() : null
  if (headPromise) headPromise.catch(noop)

  let lastPage = -1
  for (const { index } of batch.blocks) {
    const page = getBitfieldPage(index)
    if (page === lastPage) continue
    lastPage = page
    pagePromises.push(dstReader.getBitfieldPage(page))
  }

  dstReader.tryFlush()

  const pages = await Promise.all(pagePromises)
  const head = headPromise === null ? null : await headPromise
  const userData = []

  // reads done!

  if (batch.first) {
    const roots = nodes.slice(0, batch.roots.length)

    for (const node of roots) {
      if (!node) throw new Error('Missing nodes for prologue hash')
    }

    const treeHash = crypto.tree(roots)
    if (!b4a.equals(treeHash, prologue.hash)) throw new Error('Prologue does not match source')
  }

  if (batch.first) {
    for await (const data of src.storage.createUserDataStream()) userData.push(data)
  }

  for (let i = 0; i < pages.length; i++) {
    if (!pages[i]) pages[i] = b4a.alloc(4096)
  }

  const tx = dst.storage.write()

  for (const node of nodes) tx.putTreeNode(node)

  lastPage = -1
  let pageIndex = -1

  for (const { index, value } of batch.blocks) {
    const page = getBitfieldPage(index)

    if (page !== lastPage) {
      lastPage = page
      pageIndex++
      // queue the page now, we mutate it below but its the same ref
      tx.putBitfieldPage(pageIndex, pages[pageIndex])
    }

    const pageBuffer = pages[pageIndex]
    quickbit.set(pageBuffer, getBitfieldOffset(index), true)
    tx.putBlock(index, value)
  }

  for (const { key, value } of userData) {
    tx.putUserData(key, value)
  }

  let upgraded = batch.first && !head
  if (upgraded) {
    tx.setHead(prologueToTree(prologue))
  }

  await tx.flush()

  if (upgraded) {
    const roots = nodes.slice(0, batch.roots.length)
    dst.state.setRoots(roots)
    dst.header.tree = prologueToTree(prologue)
  }

  if (userData.length > 0) {
    dst.header.userData = userData.concat(dst.header.userData)
  }

  if (batch.contig) {
    // TODO: we need to persist this somehow
    dst.header.hints.contiguousLength = batch.contig
  }

  let start = 0
  let length = 0

  // update in memory bitfield
  for (const { index } of batch.blocks) {
    if (start === 0 || start - 1 === index) {
      length++
    } else {
      if (length > 0) signalReplicator(dst, upgraded, start, length)
      upgraded = false
      length = 1
    }

    start = index
    dst.bitfield.set(index, true)
  }

  if (length > 0) signalReplicator(dst, upgraded, start, length)

  // unlink
  batch.tree = []
  batch.blocks = []
  batch.first = false
  batch.used = 0
}

function signalReplicator(core, upgraded, start, length) {
  if (upgraded) {
    core.replicator.cork()
    core.replicator.onhave(start, length, false)
    core.replicator.onupgrade()
    core.replicator.uncork()
  } else {
    core.replicator.onhave(start, length, false)
  }
}

function prologueToTree(prologue) {
  return {
    fork: 0,
    length: prologue.length,
    rootHash: prologue.hash,
    signature: null
  }
}

function getBitfieldPage(index) {
  return Math.floor(index / Bitfield.BITS_PER_PAGE)
}

function getBitfieldOffset(index) {
  return index & (Bitfield.BITS_PER_PAGE - 1)
}

function walkTree(stack, target, batch) {
  while (stack.length > 0) {
    const node = stack.pop()

    if ((node & 1) === 0) {
      if (node === target) return true
      continue
    }

    const ite = flat.iterator(node)
    if (!ite.contains(target)) continue

    while ((ite.index & 1) !== 0) {
      const left = ite.leftChild()
      const right = ite.sibling() // is right child

      batch.tree.push(left, right)

      if (ite.contains(target)) stack.push(left)
      else ite.sibling()
    }

    if (ite.index === target) return true
  }

  return false
}

function noop() {}
const crypto = require('hypercore-crypto')
const b4a = require('b4a')
const unslab = require('unslab')
const z32 = require('z32')
const Mutex = require('./mutex')
const { MerkleTree, ReorgBatch } = require('./merkle-tree')
const BitInterlude = require('./bit-interlude')
const Bitfield = require('./bitfield')
const RemoteBitfield = require('./remote-bitfield')
const {
  BAD_ARGUMENT,
  STORAGE_EMPTY,
  STORAGE_CONFLICT,
  INVALID_SIGNATURE,
  INVALID_CHECKSUM
} = require('hypercore-errors')
const Verifier = require('./verifier')
const audit = require('./audit')
const copyPrologue = require('./copy-prologue')
const SessionState = require('./session-state')
const Replicator = require('./replicator')

module.exports = class Core {
  constructor(db, opts = {}) {
    this.db = db
    this.storage = null
    this.replicator = new Replicator(this, opts)
    this.sessionStates = []
    this.monitors = []
    this.activeSessions = 0
    this.gc = 0 // corestore uses this to main a gc strike pool

    this.id = opts.key ? z32.encode(opts.key) : null
    this.key = opts.key || null
    this.discoveryKey = opts.discoveryKey || (opts.key && crypto.discoveryKey(opts.key)) || null
    this.manifest = null
    this.opening = null
    this.closing = null
    this.exclusive = null

    this.preupdate = null
    this.header = null
    this.compat = false
    this.bitfield = null
    this.verifier = null
    this.truncating = 0
    this.updating = false
    this.skipBitfield = null
    this.globalCache = opts.globalCache || null
    this.autoClose = opts.autoClose !== false
    this.onidle = noop

    this.state = null
    this.opened = false
    this.destroyed = false
    this.closed = false

    this.hintsChanged = false

    this._bitfield = null
    this._verifies = null
    this._verifiesFlushed = null
    this._legacy = !!opts.legacy

    this.opening = this._open(opts)
    this.opening.catch(noop)
  }

  ready() {
    return this.opening
  }

  addMonitor(s) {
    if (s._monitorIndex >= 0) return
    s._monitorIndex = this.monitors.push(s) - 1
  }

  removeMonitor(s) {
    if (s._monitorIndex < 0) return
    const head = this.monitors.pop()
    if (head !== s) this.monitors[(head._monitorIndex = s._monitorIndex)] = head
    s._monitorIndex = -1
  }

  emitManifest() {
    for (let i = this.monitors.length - 1; i >= 0; i--) {
      this.monitors[i].emit('manifest')
    }
  }

  createUserDataStream(opts, session = this.state) {
    return session.storage.createUserDataStream(opts)
  }

  allSessions() {
    const sessions = []
    for (const state of this.sessionStates) {
      if (state.sessions.length) sessions.push(...state.sessions)
    }
    return sessions
  }

  hasSession() {
    return this.activeSessions !== 0
  }

  compact() {
    const compacting = []
    for (const s of this.sessionStates) {
      compacting.push(s.storage.compact())
    }
    return Promise.all(compacting)
  }

  checkIfIdle() {
    if (!this.opened || this.destroyed === true || this.hasSession() === true) return
    if (this.replicator.idle() === false) return
    if (this.state === null || this.state.mutex.idle() === false) return
    this.onidle()
  }

  async lockExclusive() {
    if (this.exclusive === null) this.exclusive = new Mutex()
    await this.exclusive.lock()
  }

  unlockExclusive() {
    if (this.exclusive !== null) this.exclusive.unlock()
  }

  async _open(opts) {
    try {
      await this._tryOpen(opts)
    } catch (err) {
      this.onidle()
      throw err
    }

    this.opened = true
  }

  async _tryOpen(opts) {
    if (opts.preopen) await opts.preopen // just a hook to allow exclusive access here...

    let storage = await this.db.resumeCore(this.discoveryKey)

    let overwrite = opts.overwrite === true

    const force = opts.force === true
    const createIfMissing = opts.createIfMissing !== false
    // kill this flag soon
    const legacy = !!opts.legacy

    // default to true for now if no manifest is provided
    let compat = opts.compat === true || (opts.compat !== false && !opts.manifest)

    let header = storage ? parseHeader(await getCoreInfo(storage)) : null

    const hadManifest = !!header && !!header.manifest && !!header.key
    const hadKeyPair = !!header && !!header.keyPair

    if (force && opts.key && header && !b4a.equals(header.key, opts.key)) {
      overwrite = true
    }

    if (!header && opts.discoveryKey && !(opts.key || opts.manifest)) {
      throw STORAGE_EMPTY('No Hypercore is stored here', this.discoveryKey)
    }

    if (!header || overwrite) {
      if (!createIfMissing) {
        throw STORAGE_EMPTY('No Hypercore is stored here', this.discoveryKey)
      }

      if (compat) {
        if (opts.key && opts.keyPair && !b4a.equals(opts.key, opts.keyPair.publicKey)) {
          throw BAD_ARGUMENT('Key must match publicKey when in compat mode', this.discoveryKey)
        }
      }

      const keyPair = opts.keyPair || (opts.key ? null : crypto.keyPair())

      const defaultManifest =
        !opts.manifest &&
        (!!opts.compat || !opts.key || !!(keyPair && b4a.equals(opts.key, keyPair.publicKey)))
      const manifest = defaultManifest
        ? Verifier.defaultSignerManifest(opts.key || keyPair.publicKey)
        : Verifier.createManifest(opts.manifest)

      header = {
        key: opts.key || (compat ? manifest.signers[0].publicKey : Verifier.manifestHash(manifest)),
        manifest,
        keyPair: keyPair
          ? { publicKey: keyPair.publicKey, secretKey: keyPair.secretKey || null }
          : null,
        frozen: false,
        tree: {
          fork: 0,
          length: 0,
          rootHash: null,
          signature: null
        },
        hints: {
          reorgs: [],
          contiguousLength: 0,
          remoteContiguousLength: 0
        }
      }

      const discoveryKey = opts.discoveryKey || crypto.discoveryKey(header.key)

      storage = await this.db.createCore({
        key: header.key,
        manifest,
        keyPair,
        frozen: false,
        discoveryKey,
        userData: opts.userData || [],
        alias: opts.alias || null
      })
    }

    // unslab the long lived buffers to avoid keeping the slab alive
    header.key = unslab(header.key)

    if (header.tree) {
      header.tree.rootHash = unslab(header.tree.rootHash)
      header.tree.signature = unslab(header.tree.signature)
    }

    if (header.keyPair) {
      header.keyPair.publicKey = unslab(header.keyPair.publicKey)
      header.keyPair.secretKey = unslab(header.keyPair.secretKey)
    }

    if (header.keyPair) {
      header.keyPair.publicKey = unslab(header.keyPair.publicKey)
      header.keyPair.secretKey = unslab(header.keyPair.secretKey)
    }

    if (opts.manifest) {
      // if we provide a manifest and no key, verify that the stored key is the same
      if (
        !opts.key &&
        !Verifier.isValidManifest(header.key, Verifier.createManifest(opts.manifest))
      ) {
        throw STORAGE_CONFLICT('Manifest does not hash to provided key', this.discoveryKey)
      }

      if (!header.manifest) header.manifest = opts.manifest
    }

    if (opts.key && !b4a.equals(header.key, opts.key)) {
      throw STORAGE_CONFLICT('Another Hypercore is stored here', this.discoveryKey)
    }

    // if we signalled compat, but already now this core isn't disable it
    if (compat && header.manifest && !Verifier.isCompat(header.key, header.manifest)) {
      compat = false
    } else if (!compat && header.manifest && Verifier.isCompat(header.key, header.manifest)) {
      compat = true
    }

    const prologue = header.manifest ? header.manifest.prologue : null

    const bitfield = await Bitfield.open(storage, header.tree.length)

    const treeInfo = {
      fork: header.tree.fork,
      length: header.tree.length,
      signature: header.tree.signature,
      roots: header.tree.length
        ? await MerkleTree.getRootsFromStorage(storage, header.tree.length)
        : [],
      prologue
    }

    if (overwrite) {
      const tx = storage.write()
      tx.deleteTreeNodeRange(0, -1)
      tx.deleteBlockRange(0, -1)
      bitfield.clear(tx)
      await tx.flush()
    }

    const len = bitfield.findFirst(false, header.hints.contiguousLength)
    if (header.hints.contiguousLength !== len && !storage.readOnly) {
      header.hints.contiguousLength = len
      const tx = storage.write()
      tx.setHints(header.hints)
      await tx.flush()
    }

    // to unslab
    if (header.manifest) {
      header.manifest = Verifier.createManifest(header.manifest)
      if (!hadManifest || (header.keyPair && !hadKeyPair)) {
        const tx = storage.write()
        tx.setAuth({
          key: header.key,
          discoveryKey: crypto.discoveryKey(header.key),
          manifest: header.manifest,
          keyPair: header.keyPair
        })
        await tx.flush()
      }
    }

    const verifier = header.manifest
      ? new Verifier(header.key, header.manifest, { crypto, legacy })
      : null

    this.storage = storage
    this.header = header
    this.compat = compat
    this.bitfield = bitfield
    this.verifier = verifier
    this.state = new SessionState(this, null, storage, treeInfo, null)

    if (this.key === null) this.key = this.header.key
    if (this.discoveryKey === null) this.discoveryKey = crypto.discoveryKey(this.key)
    if (this.id === null) this.id = z32.encode(this.key)
    if (this.manifest === null) this.manifest = this.header.manifest
  }

  async audit(opts) {
    await this.state.mutex.lock()

    try {
      return await audit(this, opts)
    } finally {
      this.state._unlock()
    }
  }

  async setManifest(manifest) {
    await this.state.mutex.lock()

    try {
      if (manifest && this.header.manifest === null) {
        if (!Verifier.isValidManifest(this.header.key, manifest)) {
          throw INVALID_CHECKSUM('Manifest hash does not match', this.discoveryKey)
        }

        const tx = this.state.createWriteBatch()
        this._setManifest(tx, Verifier.createManifest(manifest), null)
        await this.state.flush()
      }
    } finally {
      this.state._unlock()
    }
  }

  _setManifest(tx, manifest, keyPair) {
    if (!manifest && b4a.equals(keyPair.publicKey, this.header.key)) {
      manifest = Verifier.defaultSignerManifest(this.header.key)
    }
    if (!manifest) return

    const verifier = new Verifier(this.header.key, manifest, { legacy: this._legacy })

    if (verifier.prologue) this.state.prologue = Object.assign({}, verifier.prologue)

    this.manifest = this.header.manifest = manifest

    tx.setAuth({
      key: this.header.key,
      discoveryKey: this.discoveryKey,
      manifest,
      keyPair: this.header.keyPair
      // TODO: encryptionKey?
    })

    this.compat = verifier.compat
    this.verifier = verifier

    this.replicator.onupgrade()
    this.emitManifest()
  }

  async copyPrologue(src) {
    await this.state.mutex.lock()

    try {
      await src.mutex.lock()
    } catch (err) {
      this.state.mutex.unlock()
      throw err
    }

    try {
      await copyPrologue(src, this)
    } finally {
      src.mutex.unlock()
      this.state.mutex.unlock()
      this.checkIfIdle()
    }
  }

  flushed() {
    return this.state.flushed()
  }

  async _validateCommit(state, treeLength) {
    if (this.state.length > state.length) {
      return false // TODO: partial commit and truncation possible in the future
    }

    if (this.state.length > treeLength) {
      for (const root of this.state.roots) {
        const batchRoot = await MerkleTree.get(state, root.index)
        if (batchRoot.size !== root.size || !b4a.equals(batchRoot.hash, root.hash)) {
          return false
        }
      }
    }

    if (this.verifier === null) {
      return false // easier to assert than upsert
    }

    return true
  }

  _verifyBatchUpgrade(batch, manifest) {
    if (!this.header.manifest) {
      // compat, drop at some point
      if (!manifest) manifest = Verifier.defaultSignerManifest(this.header.key)

      if (
        !manifest ||
        !(
          Verifier.isValidManifest(this.header.key, manifest) ||
          Verifier.isCompat(this.header.key, manifest)
        )
      ) {
        throw INVALID_SIGNATURE('Proof contains an invalid manifest', this.discoveryKey) // TODO: proper error type
      }
    }

    const verifier =
      this.verifier ||
      new Verifier(this.header.key, Verifier.createManifest(manifest), { legacy: this._legacy })

    if (!verifier.verify(batch, batch.signature)) {
      throw INVALID_SIGNATURE('Proof contains an invalid signature', this.discoveryKey)
    }

    return manifest
  }

  async _verifyExclusive({ batch, bitfield, value, manifest }) {
    manifest = this._verifyBatchUpgrade(batch, manifest)

    if (
      !(await this.state._verifyBlock(
        batch,
        bitfield,
        value,
        this.header.manifest ? null : manifest
      ))
    ) {
      return false
    }

    if (!batch.upgraded && bitfield) {
      this.replicator.onhave(bitfield.start, bitfield.length, bitfield.drop)
    }

    return true
  }

  async _verifyShared() {
    if (!this._verifies.length) return false

    await this.state.mutex.lock()

    const tx = this.state.createWriteBatch()

    const verifies = this._verifies
    this._verifies = null
    this._verified = null

    try {
      for (const { batch, bitfield, value } of verifies) {
        if (!batch.commitable()) continue

        if (bitfield) {
          tx.putBlock(bitfield.start, value)
        }
      }

      const bits = new BitInterlude()

      for (let i = 0; i < verifies.length; i++) {
        const { batch, bitfield, manifest } = verifies[i]

        if (!batch.commitable()) {
          verifies[i] = null // signal that we cannot commit this one
          continue
        }

        if (bitfield) {
          bits.setRange(bitfield.start, bitfield.start + 1, true)
        }

        // if we got a manifest AND its strictly a non compat one, lets store it
        if (manifest && this.header.manifest === null) {
          if (!Verifier.isValidManifest(this.header.key, manifest)) {
            throw INVALID_CHECKSUM('Manifest hash does not match', this.discoveryKey)
          }
          this._setManifest(tx, manifest, null)
        }

        if (batch.commitable()) batch.commit(tx)
      }

      const ranges = bits.flush(tx, this.bitfield)

      await this.state.flush()

      for (const { start, end, value } of ranges) {
        this._setBitfieldRanges(start, end, value)
      }

      for (let i = 0; i < verifies.length; i++) {
        const bitfield = verifies[i] && verifies[i].bitfield
        if (bitfield) {
          this.updateContiguousLength(bitfield)
          this.replicator.onhave(bitfield.start, bitfield.length, bitfield.drop)
        }
      }

      if (this.hintsChanged) await this.flushHints()
    } finally {
      this.state._clearActiveBatch()
      this.state.mutex.unlock()
    }

    return verifies[0] !== null
  }

  async flushHints() {
    if (!this.hintsChanged) return
    this.hintsChanged = false // we unset this immediately as a "debounce"

    const tx = this.state.storage.write()

    tx.setHints({
      contiguousLength: this.header.hints.contiguousLength,
      remoteContiguousLength: this.header.hints.remoteContiguousLength
    })

    await tx.flush()
  }

  async checkConflict(proof, from) {
    if (this.state.length < proof.upgrade.length || proof.fork !== this.state.fork) {
      // out of date this proof - ignore for now
      return false
    }

    // sanity check -> no manifest, no way to verify
    if (!this.header.manifest) {
      return false
    }

    const batch = MerkleTree.verifyFullyRemote(this.state, proof)

    try {
      this._verifyBatchUpgrade(batch, proof.manifest)
    } catch {
      return true
    }

    const roots = await MerkleTree.getRootsFromStorage(this.storage, proof.upgrade.length)
    const remoteTreeHash = crypto.tree(proof.upgrade.nodes)
    const localTreeHash = crypto.tree(roots)

    try {
      const rx = this.state.storage.read()
      const treeProofPromise = MerkleTree.proof(this.state, rx, {
        block: null,
        hash: null,
        seek: null,
        upgrade: {
          start: 0,
          length: proof.upgrade.length
        }
      })

      rx.tryFlush()

      const treeProof = await treeProofPromise

      const verifyBatch = MerkleTree.verifyFullyRemote(this.state, await treeProof.settle())
      this._verifyBatchUpgrade(verifyBatch, this.header.manifest)
    } catch {
      return true
    }

    // both proofs are valid, now check if they forked
    if (b4a.equals(localTreeHash, remoteTreeHash)) return false

    await this.state.mutex.lock()

    try {
      const tx = this.state.createWriteBatch()

      this.header.frozen = true

      tx.setAuth({
        key: this.header.key,
        discoveryKey: this.discoveryKey,
        manifest: this.header.manifest,
        keyPair: this.header.keyPair,
        frozen: true
      })

      await this.state.flush()
    } finally {
      this.state.mutex.unlock()
    }

    // tmp log so we can see these
    const id = b4a.toString(this.discoveryKey, 'hex')
    console.log(
      '[hypercore] conflict detected in ' +
        id +
        ' (writable=' +
        !!this.header.keyPair +
        ',quorum=' +
        this.header.manifest.quorum +
        ')'
    )
    await this._onconflict(proof)
    return true
  }

  async verifyReorg(proof) {
    const batch = new ReorgBatch(this.state)
    await MerkleTree.reorg(this.state, proof, batch)
    const manifest = this._verifyBatchUpgrade(batch, proof.manifest)

    if (manifest && !this.header.manifest) {
      await this.state.mutex.lock()
      try {
        if (manifest && this.header.manifest === null) {
          const tx = this.state.createWriteBatch()
          this._setManifest(tx, Verifier.createManifest(manifest), null)
          await this.state.flush()
        }
      } finally {
        this.state._unlock()
      }
    }

    return batch
  }

  async verify(proof, from) {
    // We cannot apply "other forks" atm.
    // We should probably still try and they are likely super similar for non upgrades
    // but this is easy atm (and the above layer will just retry)
    if (proof.fork !== this.state.fork) return false

    const batch = await MerkleTree.verify(this.state, proof)

    if (batch.upgraded && batch.length <= this.state.length) {
      await batch.downgrade()
    }

    if (!batch.commitable()) {
      return false
    }

    const value = (proof.block && proof.block.value) || null
    const op = {
      batch,
      bitfield: value && { drop: false, start: proof.block.index, length: 1 },
      value,
      manifest: proof.manifest,
      from
    }

    if (batch.upgraded) {
      return this._verifyExclusive(op)
    }

    if (this._verifies !== null) {
      const verifies = this._verifies
      const i = verifies.push(op)
      await this._verified
      return verifies[i] !== null
    }

    this._verifies = [op]
    this._verified = this._verifyShared()

    return this._verified
  }

  async reorg(batch) {
    if (!batch.commitable()) return false

    this.truncating++

    try {
      await this.state.reorg(batch)
    } finally {
      this.truncating--
    }

    return true
  }

  openSkipBitfield() {
    if (this.skipBitfield !== null) return this.skipBitfield
    this.skipBitfield = new RemoteBitfield()
    const buf = this.bitfield.toBuffer(this.state.length)
    const bitfield = new Uint32Array(buf.buffer, buf.byteOffset, buf.byteLength / 4)
    this.skipBitfield.insert(0, bitfield)
    return this.skipBitfield
  }

  _setBitfieldRanges(start, end, value) {
    this.bitfield.setRange(start, end, value)
    if (this.skipBitfield !== null) this.skipBitfield.setRange(start, end, value)
  }

  close() {
    if (!this.closing) this.closing = this._close()
    return this.closing
  }

  updateContiguousLength(bitfield) {
    const contig = updateContigBatch(this.header.hints.contiguousLength, bitfield, this.bitfield)

    if (contig.length !== -1 && contig.length !== this.header.hints.contiguousLength) {
      this.header.hints.contiguousLength = contig.length
      this.hintsChanged = true
    }
  }

  updateRemoteContiguousLength(length) {
    this.header.hints.remoteContiguousLength = length
    this.hintsChanged = true

    for (let i = this.monitors.length - 1; i >= 0; i--) {
      this.monitors[i].emit('remote-contiguous-length', length)
    }
  }

  onappend(tree, bitfield) {
    this.header.tree = tree

    if (!bitfield) {
      this.replicator.onupgrade()
      return
    }

    this.replicator.cork()

    const { start, length, drop } = bitfield

    this._setBitfieldRanges(start, start + length, true)
    this.updateContiguousLength({ start, length, drop: false })

    this.replicator.onupgrade()
    this.replicator.onhave(start, length, drop)
    this.replicator.uncork()
  }

  ontruncate(tree, { start, length }) {
    if (tree) this.header.tree = tree

    this.replicator.cork()

    this.replicator.ontruncate(start, length)
    this.replicator.onhave(start, length, true)
    this.replicator.onupgrade()
    this.replicator.uncork()

    for (const sessionState of this.sessionStates) {
      if (start < sessionState.snapshotCompatLength) sessionState.snapshotCompatLength = start
    }

    this._setBitfieldRanges(start, start + length, false)
    this.updateContiguousLength({ start, length, drop: true })
  }

  async _onconflict(proof) {
    await this.replicator.onconflict()

    for (let i = this.monitors.length - 1; i >= 0; i--) {
      const s = this.monitors[i]
      s.emit('conflict', proof.upgrade.length, proof.fork, proof)
    }

    const err = new Error('Two conflicting signatures exist for length ' + proof.upgrade.length)
    await this.closeAllSessions(err)
  }

  async closeAllSessions(err) {
    // this.sessions modifies itself when a session closes
    // This way we ensure we indeed iterate over all sessions
    const sessions = this.allSessions()

    const all = []
    for (const s of sessions) all.push(s.close({ error: err, force: false })) // force false or else infinite recursion
    await Promise.allSettled(all)
  }

  async destroy() {
    if (this.destroyed === true) return
    this.destroyed = true

    if (this.hasSession() === true) throw new Error('Cannot destroy while sessions are open')

    const weakSessions = this.allSessions()

    if (this.replicator) this.replicator.destroy()
    if (this.state) await this.state.close()

    // close all pending weak sessions...
    for (const s of weakSessions) s.close().catch(noop)
  }

  async _close() {
    if (this.opened === false) await this.opening
    if (this.hasSession() === true) throw new Error('Cannot close while sessions are open')

    if (this.replicator) await this.replicator.close()

    await this.destroy()
    if (this.autoClose) await this.storage.store.close()

    this.closed = true
  }
}

function updateContigBatch(start, upd, bitfield) {
  const end = upd.start + upd.length

  let c = start

  if (upd.drop) {
    // If we dropped a block in the current contig range, "downgrade" it
    if (c > upd.start) {
      c = upd.start
    }
  } else {
    if (c <= end && c >= upd.start) {
      c = end
      while (bitfield.get(c)) c++
    }
  }

  if (c === start) {
    return {
      length: -1
    }
  }

  if (c > start) {
    return {
      length: c
    }
  }

  return {
    length: c
  }
}

function getDefaultTree() {
  return {
    fork: 0,
    length: 0,
    rootHash: null,
    signature: null
  }
}

function parseHeader(info) {
  if (!info) return null

  return {
    key: info.key,
    manifest: info.manifest,
    external: null,
    keyPair: info.keyPair,
    tree: info.head || getDefaultTree(),
    hints: {
      reorgs: [],
      contiguousLength: info.hints ? info.hints.contiguousLength : 0,
      remoteContiguousLength: info.hints ? info.hints.remoteContiguousLength : 0
    }
  }
}

function noop() {}

async function getCoreInfo(storage) {
  const r = storage.read()

  const auth = r.getAuth()
  const head = r.getHead()
  const hints = r.getHints()

  r.tryFlush()

  const [authInfo, headInfo, hintsInfo] = await Promise.all([auth, head, hints])
  return {
    ...authInfo,
    head: headInfo,
    hints: hintsInfo
  }
}
const sodium = require('sodium-universal')
const c = require('compact-encoding')
const b4a = require('b4a')
const { DEFAULT_ENCRYPTION } = require('./caps')

const nonce = b4a.alloc(sodium.crypto_stream_NONCEBYTES)

module.exports = class DefaultEncryption {
  static PADDING = 8

  constructor(encryptionKey, hypercoreKey, opts = {}) {
    this.key = encryptionKey
    this.compat = opts.compat === true

    const keys = DefaultEncryption.deriveKeys(encryptionKey, hypercoreKey, opts)

    this.blockKey = keys.block
    this.blindingKey = keys.blinding
  }

  static deriveKeys(encryptionKey, hypercoreKey, { block = false, compat = false } = {}) {
    const subKeys = b4a.alloc(2 * sodium.crypto_stream_KEYBYTES)

    const blockKey = block ? encryptionKey : subKeys.subarray(0, sodium.crypto_stream_KEYBYTES)
    const blindingKey = subKeys.subarray(sodium.crypto_stream_KEYBYTES)

    if (!block) {
      if (compat) {
        sodium.crypto_generichash_batch(blockKey, [encryptionKey], hypercoreKey)
      } else {
        sodium.crypto_generichash_batch(blockKey, [DEFAULT_ENCRYPTION, hypercoreKey, encryptionKey])
      }
    }

    sodium.crypto_generichash(blindingKey, blockKey)

    return {
      blinding: blindingKey,
      block: blockKey
    }
  }

  static blockEncryptionKey(hypercoreKey, encryptionKey) {
    const blockKey = b4a.alloc(sodium.crypto_stream_KEYBYTES)
    sodium.crypto_generichash_batch(blockKey, [DEFAULT_ENCRYPTION, hypercoreKey, encryptionKey])
    return blockKey
  }

  static encrypt(index, block, fork, blockKey, blindingKey) {
    const padding = block.subarray(0, DefaultEncryption.PADDING)
    block = block.subarray(DefaultEncryption.PADDING)

    c.uint64.encode({ start: 0, end: 8, buffer: padding }, fork)
    c.uint64.encode({ start: 0, end: 8, buffer: nonce }, index)

    // Zero out any previous padding.
    nonce.fill(0, 8, 8 + padding.byteLength)

    // Blind the fork ID, possibly risking reusing the nonce on a reorg of the
    // Hypercore. This is fine as the blinding is best-effort and the latest
    // fork ID shared on replication anyway.
    sodium.crypto_stream_xor(padding, padding, nonce, blindingKey)

    nonce.set(padding, 8)

    // The combination of a (blinded) fork ID and a block index is unique for a
    // given Hypercore and is therefore a valid nonce for encrypting the block.
    sodium.crypto_stream_xor(block, block, nonce, blockKey)
  }

  static decrypt(index, block, blockKey) {
    const padding = block.subarray(0, DefaultEncryption.PADDING)
    block = block.subarray(DefaultEncryption.PADDING)

    c.uint64.encode({ start: 0, end: 8, buffer: nonce }, index)

    nonce.set(padding, 8)

    // Decrypt the block using the blinded fork ID.
    sodium.crypto_stream_xor(block, block, nonce, blockKey)
  }

  encrypt(index, block, fork, core) {
    if (core.compat !== this.compat) this._reload(core)
    return DefaultEncryption.encrypt(index, block, fork, this.blockKey, this.blindingKey)
  }

  decrypt(index, block, core) {
    if (core.compat !== this.compat) this._reload(core)
    return DefaultEncryption.decrypt(index, block, this.blockKey)
  }

  padding() {
    return DefaultEncryption.PADDING
  }

  _reload(core) {
    const block = b4a.equals(this.key, this.blockKey)
    const keys = DefaultEncryption.deriveKeys(this.key, core.key, { block, compat: core.compat })

    this.blockKey = keys.blockKey
    this.blindingKey = keys.blindingKey
  }
}
module.exports = class Download {
  constructor(session, range) {
    this.session = session
    this.range = range
    this.request = null
    this.opened = false
    this.opening = this._open()
    this.opening.catch(noop)
  }

  ready() {
    return this.opening
  }

  async _open() {
    if (this.session.opened === false) await this.session.opening
    this._download()
    this.opened = true
  }

  async done() {
    await this.ready()

    try {
      return await this.request.promise
    } catch (err) {
      if (isSessionMoved(err)) return this._download()
      throw err
    }
  }

  _download() {
    const activeRequests = (this.range && this.range.activeRequests) || this.session.activeRequests
    this.request = this.session.core.replicator.addRange(activeRequests, this.range)
    this.request.promise.catch(noop)
    return this.request.promise
  }

  /**
   * Deprecated. Use `range.done()`.
   */
  downloaded() {
    return this.done()
  }

  destroy() {
    this._destroyBackground().catch(noop)
  }

  async _destroyBackground() {
    if (this.opened === false) await this.ready()
    if (this.request.context) this.request.context.detach(this.request)
  }
}

function noop() {}

function isSessionMoved(err) {
  return err.code === 'SESSION_MOVED'
}
const TICKS = 16

module.exports = class HotswapQueue {
  constructor() {
    this.priorities = [[], [], []]
  }

  *pick(peer) {
    for (let i = 0; i < this.priorities.length; i++) {
      // try first one more than second one etc etc
      let ticks = (this.priorities.length - i) * TICKS
      const queue = this.priorities[i]

      for (let j = 0; j < queue.length; j++) {
        const r = j + Math.floor(Math.random() * queue.length - j)
        const a = queue[j]
        const b = queue[r]

        if (r !== j) {
          queue[(b.hotswap.index = j)] = b
          queue[(a.hotswap.index = r)] = a
        }

        if (hasInflight(b, peer)) continue

        yield b

        if (--ticks <= 0) break
      }
    }
  }

  add(block) {
    if (block.hotswap !== null) this.remove(block)
    if (block.inflight.length === 0 || block.inflight.length >= 3) return

    // TODO: also use other stuff to determine queue prio
    const queue = this.priorities[block.inflight.length - 1]

    const index = queue.push(block) - 1
    block.hotswap = { ref: this, queue, index }
  }

  remove(block) {
    const hotswap = block.hotswap
    if (hotswap === null) return

    block.hotswap = null
    const head = hotswap.queue.pop()
    if (head === block) return
    hotswap.queue[(head.hotswap.index = hotswap.index)] = head
  }
}

function hasInflight(block, peer) {
  for (let j = 0; j < block.inflight.length; j++) {
    if (block.inflight[j].peer === peer) return true
  }
  return false
}
module.exports = class Info {
  constructor(opts = {}) {
    this.key = opts.key
    this.discoveryKey = opts.discoveryKey
    this.length = opts.length || 0
    this.contiguousLength = opts.contiguousLength || 0
    this.byteLength = opts.byteLength || 0
    this.fork = opts.fork || 0
    this.padding = opts.padding || 0
    this.storage = opts.storage || null
  }

  static async from(session, opts = {}) {
    return new Info({
      key: session.key,
      discoveryKey: session.discoveryKey,
      length: session.length,
      contiguousLength: session.contiguousLength,
      byteLength: session.byteLength,
      fork: session.fork,
      padding: session.padding,
      storage: opts.storage ? await this.storage(session) : null
    })
  }

  static async storage(session) {
    const { oplog, tree, blocks, bitfield } = session.core
    try {
      return {
        oplog: await Info.bytesUsed(oplog.storage),
        tree: await Info.bytesUsed(tree.storage),
        blocks: await Info.bytesUsed(blocks.storage),
        bitfield: await Info.bytesUsed(bitfield.storage)
      }
    } catch {
      return null
    }
  }

  static bytesUsed(file) {
    return new Promise((resolve, reject) => {
      file.stat((err, st) => {
        if (err) {
          resolve(0) // prob just file not found (TODO, improve)
        } else if (typeof st.blocks !== 'number') {
          reject(new Error('cannot determine bytes used'))
        } else {
          resolve(st.blocks * 512)
        }
      })
    })
  }
}
const b4a = require('b4a')

module.exports = function (core, depth, opts) {
  let indent = ''
  if (typeof opts.indentationLvl === 'number') {
    while (indent.length < opts.indentationLvl) indent += ' '
  }

  let peers = ''
  const min = Math.min(core.peers.length, 5)

  for (let i = 0; i < min; i++) {
    const peer = core.peers[i]

    peers += `${indent}    Peer(\n`
    peers += `${indent}      remotePublicKey: ${opts.stylize(toHex(peer.remotePublicKey), 'string')}\n`
    peers += `${indent}      remoteLength: ${opts.stylize(peer.remoteLength, 'number')}\n`
    peers += `${indent}      remoteFork: ${opts.stylize(peer.remoteFork, 'number')}\n`
    peers += `${indent}      remoteCanUpgrade: ${opts.stylize(peer.remoteCanUpgrade, 'boolean')}\n`
    peers += `${indent}    )\n`
  }

  if (core.peers.length > 5) {
    peers += `${indent}  ... and ${core.peers.length - 5} more\n`
  }

  if (peers) peers = `[\n${peers}${indent}  ]`
  else peers = `[ ${opts.stylize(0, 'number')} ]`

  return (
    `${core.constructor.name}(\n` +
    `${indent}  id: ${opts.stylize(core.id, 'string')}\n` +
    `${indent}  key: ${opts.stylize(toHex(core.key), 'string')}\n` +
    `${indent}  discoveryKey: ${opts.stylize(toHex(core.discoveryKey), 'string')}\n` +
    `${indent}  opened: ${opts.stylize(core.opened, 'boolean')}\n` +
    `${indent}  closed: ${opts.stylize(core.closed, 'boolean')}\n` +
    `${indent}  snapshotted: ${opts.stylize(core.snapshotted, 'boolean')}\n` +
    `${indent}  writable: ${opts.stylize(core.writable, 'boolean')}\n` +
    `${indent}  length: ${opts.stylize(core.length, 'number')}\n` +
    `${indent}  fork: ${opts.stylize(core.fork, 'number')}\n` +
    `${indent}  sessions: [ ${opts.stylize(core.sessions.length, 'number')} ]\n` +
    `${indent}  activeRequests: [ ${opts.stylize(core.activeRequests.length, 'number')} ]\n` +
    `${indent}  peers: ${peers}\n` +
    `${indent})`
  )
}

function toHex(buf) {
  return buf && b4a.toString(buf, 'hex')
}
const flat = require('flat-tree')
const crypto = require('hypercore-crypto')
const b4a = require('b4a')
const unslab = require('unslab')
const caps = require('./caps')
const {
  INVALID_PROOF,
  INVALID_CHECKSUM,
  INVALID_OPERATION,
  BAD_ARGUMENT,
  ASSERTION
} = require('hypercore-errors')

class NodeQueue {
  constructor(nodes, extra = null) {
    this.i = 0
    this.nodes = nodes
    this.extra = extra
    this.length = nodes.length + (this.extra === null ? 0 : 1)
  }

  shift(index) {
    if (this.extra !== null && this.extra.index === index) {
      const node = this.extra
      this.extra = null
      this.length--
      return node
    }

    if (this.i >= this.nodes.length) {
      throw INVALID_OPERATION('Expected node ' + index + ', got (nil)')
    }

    const node = this.nodes[this.i++]
    if (node.index !== index) {
      throw INVALID_OPERATION('Expected node ' + index + ', got node ' + node.index)
    }

    this.length--
    return node
  }
}

class MerkleTreeBatch {
  constructor(session) {
    this.fork = session.fork
    this.roots = [...session.roots]
    this.length = session.length
    this.signature = session.signature
    this.ancestors = session.length
    this.byteLength = session.byteLength
    this.prologue = session.prologue
    this.hashCached = null

    this.committed = false
    this.truncated = false
    this.treeLength = session.length
    this.treeFork = session.fork
    this.storage = session.storage
    this.session = session
    this.nodes = []
    this.upgraded = false
  }

  checkout(length, additionalRoots) {
    const roots = []
    let r = 0

    const head = 2 * length - 2
    const gaps = new Set()
    const all = new Map()

    // additional roots is so the original roots can be passed (we mutate the array in appendRoot)
    if (additionalRoots) {
      for (const node of additionalRoots) all.set(node.index, node)
    }

    for (const node of this.nodes) all.set(node.index, node)

    for (const index of flat.fullRoots(head + 2)) {
      const left = flat.leftSpan(index)
      if (left !== 0) gaps.add(left - 1)

      if (r < this.roots.length && this.roots[r].index === index) {
        roots.push(this.roots[r++])
        continue
      }
      const node = all.get(index)
      if (!node) throw new BAD_ARGUMENT('root missing for given length')
      roots.push(node)
    }

    this.roots = roots
    this.length = length
    this.byteLength = totalSize(roots)
    this.hashCached = null
    this.signature = null

    for (let i = 0; i < this.nodes.length; i++) {
      const index = this.nodes[i].index
      if (index <= head && !gaps.has(index)) continue
      const last = this.nodes.pop()
      if (i < this.nodes.length) this.nodes[i--] = last
    }
  }

  prune(length) {
    if (length === 0) return

    const head = 2 * length - 2
    const gaps = new Set()

    // TODO: make a function for this in flat-tree
    for (const index of flat.fullRoots(head + 2)) {
      const left = flat.leftSpan(index)
      if (left !== 0) gaps.add(left - 1)
    }

    for (let i = 0; i < this.nodes.length; i++) {
      const index = this.nodes[i].index
      if (index > head || gaps.has(index)) continue
      const last = this.nodes.pop()
      if (i < this.nodes.length) this.nodes[i--] = last
    }
  }

  clone() {
    const b = new MerkleTreeBatch(this.session)

    b.fork = this.fork
    b.roots = [...this.roots]
    b.length = this.length
    b.byteLength = this.byteLength
    b.signature = this.signature
    b.treeLength = this.treeLength
    b.treeFork = this.treeFork
    b.tree = this.tree
    b.nodes = [...this.nodes]
    b.upgraded = this.upgraded

    return b
  }

  hash() {
    if (this.hashCached === null) this.hashCached = unslab(crypto.tree(this.roots))
    return this.hashCached
  }

  signable(manifestHash) {
    return caps.treeSignable(manifestHash, this.hash(), this.length, this.fork)
  }

  signableCompat(noHeader) {
    return caps.treeSignableCompat(this.hash(), this.length, this.fork, noHeader)
  }

  get(index) {
    if (index >= this.length * 2) {
      return null
    }

    for (const n of this.nodes) {
      if (n.index === index) return n
    }

    return getTreeNodeFromStorage(this.session.storage, index)
  }

  // deprecated, use sssion proof instead
  proof(batch, { block, hash, seek, upgrade }) {
    return generateProof(this.session, batch, block, hash, seek, upgrade)
  }

  verifyUpgrade(proof) {
    const unverified = verifyTree(proof, this.nodes)

    if (!proof.upgrade) throw INVALID_OPERATION('Expected upgrade proof')

    return verifyUpgrade(proof, unverified, this)
  }

  addNodesUnsafe(nodes) {
    for (let i = 0; i < nodes.length; i++) {
      this.nodes.push(nodes[i])
    }
  }

  append(buf) {
    const head = this.length * 2
    const ite = flat.iterator(head)
    const node = blockNode(head, buf)

    this.appendRoot(node, ite)
  }

  appendRoot(node, ite) {
    node = unslabNode(node)
    this.hashCached = null
    this.upgraded = true
    this.length += ite.factor / 2
    this.byteLength += node.size
    this.roots.push(node)
    this.nodes.push(node)

    while (this.roots.length > 1) {
      const a = this.roots[this.roots.length - 1]
      const b = this.roots[this.roots.length - 2]

      // TODO: just have a peek sibling instead? (pretty sure it's always the left sib as well)
      if (ite.sibling() !== b.index) {
        ite.sibling() // unset so it always points to last root
        break
      }

      const node = unslabNode(parentNode(ite.parent(), a, b))
      this.nodes.push(node)
      this.roots.pop()
      this.roots.pop()
      this.roots.push(node)
    }
  }

  commitable() {
    return (
      this.treeFork === this.session.fork &&
      (this.upgraded
        ? this.treeLength === this.session.length
        : this.treeLength <= this.session.length)
    )
  }

  commit(tx) {
    if (tx === undefined) throw INVALID_OPERATION('No database batch was passed')
    if (!this.commitable()) {
      throw INVALID_OPERATION('Tree was modified during batch, refusing to commit')
    }

    if (this.upgraded) this._commitUpgrade(tx)

    for (let i = 0; i < this.nodes.length; i++) {
      const node = this.nodes[i]
      tx.putTreeNode(node)
    }

    this.committed = true

    return this
  }

  _commitUpgrade(tx) {
    // TODO: If easy to detect, we should refuse an trunc+append here without a fork id
    // change. Will only happen on user error so mostly to prevent that.

    if (this.ancestors < this.treeLength) {
      tx.deleteTreeNodeRange(this.ancestors * 2, this.treeLength * 2)

      if (this.ancestors > 0) {
        const head = this.ancestors * 2
        const ite = flat.iterator(head - 2)

        while (true) {
          if (ite.contains(head) && ite.index < head) {
            tx.deleteTreeNode(ite.index)
          }
          if (ite.offset === 0) break
          ite.parent()
        }

        this.truncated = true
      }
    }
  }

  seek(bytes, padding) {
    return new ByteSeeker(this, this, bytes, padding)
  }

  byteRange(index) {
    const rx = this.storage.read()
    const range = getByteRange(this, index, rx)
    rx.tryFlush()

    return range
  }

  byteOffset(index) {
    if (index === 2 * this.length) return this.byteLength

    const rx = this.storage.read()
    const offset = getByteOffset(this, index, rx)
    rx.tryFlush()

    return offset
  }

  async downgrade() {
    if (!this.upgraded) return true

    const rx = this.storage.read()
    const nodePromises = []

    for (const r of this.roots) {
      nodePromises.push(rx.getTreeNode(r.index))
    }

    rx.tryFlush()

    const nodes = await Promise.all(nodePromises)

    for (let i = 0; i < this.roots.length; i++) {
      const root = this.roots[i]
      const node = nodes[i]

      if (!node) return false
      if (!b4a.equals(node.hash, root.hash)) return false
    }

    this.upgraded = false
    return true
  }

  async restore(length) {
    if (length === this.length) return this

    const roots = unslabNodes(await MerkleTree.getRootsFromStorage(this.storage, length))

    this.roots = roots
    this.length = length
    this.byteLength = totalSize(roots)
    this.ancestors = length

    for (const node of roots) this.byteLength += node.size

    return this
  }
}

class ReorgBatch extends MerkleTreeBatch {
  constructor(session) {
    super(session)

    this.roots = []
    this.length = 0
    this.byteLength = 0
    this.diff = null
    this.ancestors = 0
    // We set upgraded because reorgs are signed so hit will
    // hit the same code paths (like the treeLength check in commit)
    this.upgraded = true
    this.want = {
      nodes: 0,
      start: 0,
      end: 0
    }
  }

  get finished() {
    return this.want === null
  }

  update(proof) {
    if (this.want === null) return true

    const nodes = []
    const root = verifyTree(proof, nodes)

    if (root === null || !b4a.equals(root.hash, this.diff.hash)) return false

    this.nodes.push(...nodes)
    return this._update(nodes)
  }

  async _update(nodes) {
    const n = new Map()
    for (const node of nodes) n.set(node.index, node)

    let diff = null
    const ite = flat.iterator(this.diff.index)
    const startingDiff = this.diff

    while ((ite.index & 1) !== 0) {
      const left = n.get(ite.leftChild())
      if (!left) break

      const existing = await getTreeNodeFromStorage(this.session.storage, left.index)
      if (!existing || !b4a.equals(existing.hash, left.hash)) {
        diff = left
      } else {
        diff = n.get(ite.sibling())
      }
    }

    if ((this.diff.index & 1) === 0) return true
    if (diff === null) return false
    if (startingDiff !== this.diff) return false

    return this._updateDiffRoot(diff)
  }

  _updateDiffRoot(diff) {
    if (this.want === null) return true

    const spans = flat.spans(diff.index)
    const start = spans[0] / 2
    const end = Math.min(this.treeLength, spans[1] / 2 + 1)
    const len = end - start

    this.ancestors = start
    this.diff = diff

    if ((diff.index & 1) === 0 || this.want.start >= this.treeLength || len <= 0) {
      this.want = null
      return true
    }

    this.want.start = start
    this.want.end = end
    this.want.nodes = log2(spans[1] - spans[0] + 2) - 1

    return false
  }
}

class ByteSeeker {
  constructor(session, bytes, padding = 0) {
    this.session = session
    this.bytes = bytes
    this.padding = padding

    const size = session.byteLength - session.length * padding

    this.start = bytes >= size ? session.length : 0
    this.end = bytes < size ? session.length : 0
  }

  async _seek(bytes) {
    if (!bytes) return [0, 0]

    for (const node of this.session.roots) {
      // all async ticks happen once we find the root so safe
      const size = getUnpaddedSize(node, this.padding, null)

      if (bytes === size) return [flat.rightSpan(node.index) + 2, 0]
      if (bytes > size) {
        bytes -= size
        continue
      }

      const ite = flat.iterator(node.index)

      while ((ite.index & 1) !== 0) {
        const l = await getTreeNodeFromStorage(this.session.storage, ite.leftChild())

        if (l) {
          const size = getUnpaddedSize(l, this.padding, ite)

          if (size === bytes) return [ite.rightSpan() + 2, 0]
          if (size > bytes) continue
          bytes -= size
          ite.sibling()
        } else {
          ite.parent()
          return [ite.index, bytes]
        }
      }

      return [ite.index, bytes]
    }

    return null
  }

  async update() {
    // TODO: combine _seek and this, much simpler
    const res = await this._seek(this.bytes)
    if (!res) return null
    if ((res[0] & 1) === 0) return [res[0] / 2, res[1]]

    const span = flat.spans(res[0])
    this.start = span[0] / 2
    this.end = span[1] / 2 + 1

    return null
  }
}

class TreeProof {
  constructor(session, block, hash, seek, upgrade) {
    this.fork = session.fork
    this.signature = session.signature

    this.block = block
    this.hash = hash
    this.seek = seek
    this.upgrade = upgrade

    this.pending = {
      node: null,
      seek: null,
      upgrade: null,
      additionalUpgrade: null
    }
  }

  async settle() {
    const result = {
      fork: this.fork,
      block: null,
      hash: null,
      seek: null,
      upgrade: null,
      manifest: null
    }

    const [pNode, pSeek, pUpgrade, pAdditional] = await settleProof(this.pending)

    if (this.block) {
      if (pNode === null) throw INVALID_OPERATION('Invalid block request')
      result.block = {
        index: this.block.index,
        value: null, // populated upstream, alloc it here for simplicity
        nodes: pNode
      }
    } else if (this.hash) {
      if (pNode === null) throw INVALID_OPERATION('Invalid block request')
      result.hash = {
        index: this.hash.index,
        nodes: pNode
      }
    }

    if (this.seek && pSeek !== null) {
      result.seek = {
        bytes: this.seek.bytes,
        nodes: pSeek
      }
    }

    if (this.upgrade) {
      result.upgrade = {
        start: this.upgrade.start,
        length: this.upgrade.length,
        nodes: pUpgrade,
        additionalNodes: pAdditional || [],
        signature: this.signature
      }
    }

    return result
  }
}

class MerkleTree {
  static hash(s) {
    return unslab(crypto.tree(s.roots))
  }

  static signable(s, namespace) {
    return caps.treeSignable(namespace, MerkleTree.hash(s), s.length, s.fork)
  }

  static size(roots) {
    return totalSize(roots)
  }

  static span(roots) {
    return totalSpan(roots)
  }

  static getRoots(session, length) {
    return MerkleTree.getRootsFromStorage(session.storage, length)
  }

  static getRootsFromStorage(storage, length) {
    const indexes = flat.fullRoots(2 * length)
    const roots = new Array(indexes.length)
    const rx = storage.read()

    for (let i = 0; i < indexes.length; i++) {
      roots[i] = getTreeNodeOrError(rx, indexes[i])
    }

    rx.tryFlush()

    return Promise.all(roots)
  }

  static async upgradeable(session, length) {
    const indexes = flat.fullRoots(2 * length)
    const roots = new Array(indexes.length)
    const rx = session.storage.read()

    for (let i = 0; i < indexes.length; i++) {
      roots[i] = rx.getTreeNode(indexes[i])
    }

    rx.tryFlush()

    for (const node of await Promise.all(roots)) {
      if (node === null) return false
    }

    return true
  }

  static seek(session, bytes, padding) {
    return new ByteSeeker(session, bytes, padding)
  }

  static get(session, index) {
    return getTreeNodeFromStorage(session.storage, index)
  }

  static async truncate(session, length, batch, fork = batch.fork) {
    const head = length * 2
    const fullRoots = flat.fullRoots(head)

    for (let i = 0; i < fullRoots.length; i++) {
      const root = fullRoots[i]
      if (i < batch.roots.length && batch.roots[i].index === root) continue

      while (batch.roots.length > i) batch.roots.pop()
      batch.roots.push(unslabNode(await getTreeNodeFromStorageOrError(session.storage, root)))
    }

    while (batch.roots.length > fullRoots.length) {
      batch.roots.pop()
    }

    batch.fork = fork
    batch.length = length
    batch.ancestors = length
    batch.byteLength = totalSize(batch.roots)
    batch.upgraded = true

    return batch
  }

  static async reorg(session, proof, batch) {
    let unverified = null

    if (proof.block || proof.hash || proof.seek) {
      unverified = verifyTree(proof, batch.nodes)
    }

    if (!verifyUpgrade(proof, unverified, batch)) {
      throw INVALID_PROOF('Fork proof not verifiable')
    }

    for (const root of batch.roots) {
      const existing = await getTreeNodeFromStorage(session.storage, root.index)
      if (existing && b4a.equals(existing.hash, root.hash)) continue
      batch._updateDiffRoot(root)
      break
    }

    if (batch.diff !== null) {
      await batch._update(batch.nodes)
    } else {
      batch.want = null
      batch.ancestors = batch.length
    }

    return batch
  }

  static verifyFullyRemote(session, proof) {
    // TODO: impl this less hackishly
    const batch = new MerkleTreeBatch(session)

    batch.fork = proof.fork
    batch.roots = []
    batch.length = 0
    batch.ancestors = 0
    batch.byteLength = 0

    let unverified = verifyTree(proof, batch.nodes)

    if (proof.upgrade) {
      if (verifyUpgrade(proof, unverified, batch)) {
        unverified = null
      }
    }

    return batch
  }

  static async verify(session, proof) {
    const batch = new MerkleTreeBatch(session)

    let unverified = verifyTree(proof, batch.nodes)

    if (proof.upgrade) {
      if (verifyUpgrade(proof, unverified, batch)) {
        unverified = null
      }
    }

    if (unverified) {
      const verified = await getTreeNodeFromStorageOrError(session.storage, unverified.index)
      if (!b4a.equals(verified.hash, unverified.hash)) {
        throw INVALID_CHECKSUM('Invalid checksum at node ' + unverified.index)
      }
    }

    return batch
  }

  static proof(session, rx, { block, hash, seek, upgrade }) {
    return generateProof(session, rx, block, hash, seek, upgrade)
  }

  static maxMissingNodes(index, length) {
    const head = 2 * length
    const ite = flat.iterator(index)

    // See iterator.rightSpan()
    const iteRightSpan = ite.index + ite.factor / 2 - 1
    // If the index is not in the current tree, we do not know how many missing nodes there are...
    if (iteRightSpan >= head) return 0

    for (const r of flat.fullRoots(head)) {
      if (r === index) return 0
    }

    let cnt = 0
    while (!ite.contains(head)) {
      cnt++
      ite.parent()
    }

    return cnt
  }

  static async missingNodes(session, index, length) {
    const head = 2 * length
    const ite = flat.iterator(index)

    // See iterator.rightSpan()
    const iteRightSpan = ite.index + ite.factor / 2 - 1
    // If the index is not in the current tree, we do not know how many missing nodes there are...
    if (iteRightSpan >= head) return 0

    let cnt = 0
    // TODO: we could prop use a read batch here and do this in blocks of X for perf
    while (!ite.contains(head) && !(await hasTreeNode(session.storage, ite.index))) {
      cnt++
      if (cnt >= 1024) {
        throw ASSERTION('Bad arguments to missingNodes index=' + index + ' at length=' + length)
      }
      ite.parent()
    }

    return cnt
  }

  static byteOffset(session, index) {
    return getByteOffsetSession(session, index, null)
  }

  static byteRange(session, index) {
    const rx = session.storage.read()
    const offset = getByteOffsetSession(session, index, rx)
    const size = getNodeSize(index, rx)
    rx.tryFlush()
    return Promise.all([offset, size])
  }
}

module.exports = {
  MerkleTreeBatch,
  ReorgBatch,
  MerkleTree
}

async function getNodeSize(index, rx) {
  return (await getTreeNodeOrError(rx, index)).size
}

async function getByteOffsetSession(session, index, rx) {
  if (index === 2 * session.length) return session.byteLength

  const treeNodes =
    rx === null
      ? await getByteOffsetBatchFlush(session.roots, index, session.storage.read())
      : await getByteOffsetBatch(session.roots, index, rx)

  let offset = 0
  for (const node of treeNodes) offset += node.size

  return offset
}

async function getByteOffset(tree, index, rx) {
  if (index === 2 * tree.length) return tree.byteLength

  const treeNodes = await getByteOffsetBatch(tree.roots, index, rx)

  let offset = 0
  for (const node of treeNodes) offset += node.size

  return offset
}

function getByteOffsetBatchFlush(roots, index, rx) {
  const treeNodes = getByteOffsetBatch(roots, index, rx)
  rx.tryFlush()
  return treeNodes
}

function getByteOffsetBatch(roots, index, rx) {
  if ((index & 1) === 1) index = flat.leftSpan(index)

  let head = 0
  let cnt = 0

  const promises = []

  for (const node of roots) {
    // all async ticks happen once we find the root so safe
    head += 2 * (node.index - head + 1)

    if (index >= head) {
      promises.push(node.size)
      continue
    }

    const ite = flat.iterator(node.index)

    while (ite.index !== index) {
      if (++cnt >= 1024) throw ASSERTION('Bad getByteOffsetSession index=' + index)

      if (index < ite.index) {
        ite.leftChild()
      } else {
        promises.push(getTreeNodeOrError(rx, ite.leftChild()))
        ite.sibling()
      }
    }

    return Promise.all(promises)
  }

  throw ASSERTION('Failed to find offset')
}

function getByteRange(tree, index, rx) {
  const head = 2 * tree.length
  if (((index & 1) === 0 ? index : flat.rightSpan(index)) >= head) {
    throw BAD_ARGUMENT('Index is out of bounds')
  }

  const offset = getByteOffset(tree, index, rx)
  const size = getNodeSize(index, rx)

  return Promise.all([offset, size])
}

// All the methods needed for proof verification

function verifyTree({ block, hash, seek }, nodes) {
  const untrustedNode = block
    ? { index: 2 * block.index, value: block.value, nodes: block.nodes }
    : hash
      ? { index: hash.index, value: null, nodes: hash.nodes }
      : null

  if (untrustedNode === null && (!seek || !seek.nodes.length)) return null

  let root = null

  if (seek && seek.nodes.length) {
    const ite = flat.iterator(seek.nodes[0].index)
    const q = new NodeQueue(seek.nodes)

    root = q.shift(ite.index)
    nodes.push(root)

    while (q.length > 0) {
      const node = q.shift(ite.sibling())

      root = parentNode(ite.parent(), root, node)
      nodes.push(node)
      nodes.push(root)
    }
  }

  if (untrustedNode === null) return root

  const ite = flat.iterator(untrustedNode.index)
  const blockHash = untrustedNode.value && blockNode(ite.index, untrustedNode.value)

  const q = new NodeQueue(untrustedNode.nodes, root)

  root = blockHash || q.shift(ite.index)
  nodes.push(root)

  while (q.length > 0) {
    const node = q.shift(ite.sibling())

    root = parentNode(ite.parent(), root, node)
    nodes.push(node)
    nodes.push(root)
  }

  return root
}

function verifyUpgrade({ fork, upgrade }, blockRoot, batch) {
  const prologue = batch.prologue

  if (prologue) {
    const { start, length } = upgrade
    if (start < prologue.length && (start !== 0 || length < prologue.length)) {
      throw INVALID_PROOF('Upgrade does not satisfy prologue')
    }
  }

  const q = new NodeQueue(upgrade.nodes, blockRoot)

  let grow = batch.roots.length > 0
  let i = 0

  const to = 2 * (upgrade.start + upgrade.length)
  const ite = flat.iterator(0)

  for (; ite.fullRoot(to); ite.nextTree()) {
    if (i < batch.roots.length && batch.roots[i].index === ite.index) {
      i++
      continue
    }

    if (grow) {
      grow = false
      const root = ite.index
      if (i < batch.roots.length) {
        ite.seek(batch.roots[batch.roots.length - 1].index)
        while (ite.index !== root) {
          batch.appendRoot(q.shift(ite.sibling()), ite)
        }
        continue
      }
    }

    batch.appendRoot(q.shift(ite.index), ite)
  }

  if (prologue && batch.length === prologue.length) {
    if (!b4a.equals(prologue.hash, batch.hash())) {
      throw INVALID_PROOF('Invalid hash')
    }
  }

  const extra = upgrade.additionalNodes

  ite.seek(batch.roots[batch.roots.length - 1].index)
  i = 0

  while (i < extra.length && extra[i].index === ite.sibling()) {
    batch.appendRoot(extra[i++], ite)
  }

  while (i < extra.length) {
    const node = extra[i++]

    while (node.index !== ite.index) {
      if (ite.factor === 2) throw INVALID_OPERATION('Unexpected node: ' + node.index)
      ite.leftChild()
    }

    batch.appendRoot(node, ite)
    ite.sibling()
  }

  batch.signature = unslab(upgrade.signature)
  batch.fork = fork

  return q.extra === null
}

async function seekFromHead(session, head, bytes, padding) {
  const roots = flat.fullRoots(head)

  for (let i = 0; i < roots.length; i++) {
    const root = roots[i]
    const node = await getTreeNodeFromStorage(session.storage, root)
    const size = getUnpaddedSize(node, padding, null)

    if (bytes === size) return root
    if (bytes > size) {
      bytes -= size
      continue
    }

    return seekTrustedTree(session, root, bytes, padding)
  }

  return head
}

// trust that bytes are within the root tree and find the block at bytes

async function seekTrustedTree(session, root, bytes, padding) {
  if (!bytes) return root

  const ite = flat.iterator(root)
  let cnt = 0

  while ((ite.index & 1) !== 0) {
    if (++cnt >= 1024) throw ASSERTION('Bad seekTrusted bytes=' + bytes + ', paddding=' + padding)
    const l = await getTreeNodeFromStorage(session.storage, ite.leftChild())

    if (l) {
      const size = getUnpaddedSize(l, padding, ite)
      if (size === bytes) return ite.index
      if (size > bytes) continue
      bytes -= size
      ite.sibling()
    } else {
      ite.parent()
      return ite.index
    }
  }

  return ite.index
}

// try to find the block at bytes without trusting that is *is* within the root passed

async function seekUntrustedTree(session, root, bytes, padding) {
  const offset =
    (await getByteOffsetSession(session, root, null)) -
    (padding ? (padding * flat.leftSpan(root)) / 2 : 0)

  if (offset > bytes) throw INVALID_OPERATION('Invalid seek')
  if (offset === bytes) return root

  bytes -= offset

  const node = await getTreeNodeFromStorageOrError(session.storage, root)

  if (getUnpaddedSize(node, padding, null) <= bytes) throw INVALID_OPERATION('Invalid seek')

  return seekTrustedTree(session, root, bytes, padding)
}

// Below is proof production, ie, construct proofs to verify a request
// Note, that all these methods are sync as we can statically infer which nodes
// are needed for the remote to verify given they arguments they passed us

function seekProof(session, rx, seekRoot, root, p) {
  const ite = flat.iterator(seekRoot)
  let cnt = 0

  p.seek = []
  p.seek.push(getTreeNodeOrError(rx, ite.index))

  while (ite.index !== root) {
    if (++cnt >= 1024) throw ASSERTION('Bad seekProof seekRoot=' + seekRoot + ', root=' + root)
    ite.sibling()
    p.seek.push(getTreeNodeOrError(rx, ite.index))
    ite.parent()
  }
}

function blockAndSeekProof(session, rx, node, seek, seekRoot, root, p) {
  if (!node) return seekProof(session, rx, seekRoot, root, p)

  const ite = flat.iterator(node.index)
  let cnt = 0

  p.node = []
  if (!node.value) p.node.push(getTreeNodeOrError(rx, ite.index))

  while (ite.index !== root) {
    if (++cnt >= 1024) {
      throw ASSERTION('Bad blockAndSeekProof seekRoot=' + seekRoot + ', root=' + root)
    }
    ite.sibling()

    if (seek && ite.contains(seekRoot) && ite.index !== seekRoot) {
      seekProof(session, rx, seekRoot, ite.index, p)
    } else {
      p.node.push(getTreeNodeOrError(rx, ite.index))
    }

    ite.parent()
  }
}

function upgradeProof(session, rx, node, seek, from, to, subTree, p) {
  if (from === 0) p.upgrade = []

  for (const ite = flat.iterator(0); ite.fullRoot(to); ite.nextTree()) {
    // check if they already have the node
    if (ite.index + ite.factor / 2 < from) continue

    // connect existing tree
    if (p.upgrade === null && ite.contains(from - 2)) {
      p.upgrade = []

      const root = ite.index
      const target = from - 2
      let cnt = 0

      ite.seek(target)

      while (ite.index !== root) {
        if (++cnt >= 1024) throw ASSERTION('Bad upgradeProof target=' + target + ', root=' + root)

        ite.sibling()
        if (ite.index > target) {
          if (p.node === null && p.seek === null && ite.contains(subTree)) {
            blockAndSeekProof(session, rx, node, seek, subTree, ite.index, p)
          } else {
            p.upgrade.push(getTreeNodeOrError(rx, ite.index))
          }
        }
        ite.parent()
      }

      continue
    }

    if (p.upgrade === null) {
      p.upgrade = []
    }

    // if the subtree included is a child of this tree, include that one
    // instead of a dup node
    if (p.node === null && p.seek === null && ite.contains(subTree)) {
      blockAndSeekProof(session, rx, node, seek, subTree, ite.index, p)
      continue
    }

    // add root (can be optimised since the root might be in tree.roots)
    p.upgrade.push(getTreeNodeOrError(rx, ite.index))
  }
}

function additionalUpgradeProof(session, rx, from, to, p) {
  if (from === 0) p.additionalUpgrade = []

  for (const ite = flat.iterator(0); ite.fullRoot(to); ite.nextTree()) {
    // check if they already have the node
    if (ite.index + ite.factor / 2 < from) continue

    // connect existing tree
    if (p.additionalUpgrade === null && ite.contains(from - 2)) {
      p.additionalUpgrade = []

      const root = ite.index
      const target = from - 2
      let cnt = 0

      ite.seek(target)

      while (ite.index !== root) {
        if (++cnt >= 1024) {
          throw ASSERTION(
            'Bad arguments to additionalUpgradeProof root=' + root + ' target=' + target
          )
        }
        ite.sibling()
        if (ite.index > target) {
          p.additionalUpgrade.push(getTreeNodeOrError(rx, ite.index))
        }
        ite.parent()
      }

      continue
    }

    if (p.additionalUpgrade === null) {
      p.additionalUpgrade = []
    }

    // add root (can be optimised since the root is in tree.roots)
    p.additionalUpgrade.push(getTreeNodeOrError(rx, ite.index))
  }
}

function nodesToRoot(index, nodes, head) {
  const ite = flat.iterator(index)

  for (let i = 0; i < nodes; i++) {
    ite.parent()
    if (ite.contains(head)) throw INVALID_OPERATION('Nodes is out of bounds')
  }

  return ite.index
}

function totalSize(nodes) {
  let s = 0
  for (const node of nodes) s += node.size
  return s
}

function totalSpan(nodes) {
  let s = 0
  for (const node of nodes) s += 2 * (node.index - s + 1)
  return s
}

function blockNode(index, value) {
  return { index, size: value.byteLength, hash: crypto.data(value) }
}

function parentNode(index, a, b) {
  return { index, size: a.size + b.size, hash: crypto.parent(a, b) }
}

function log2(n) {
  let res = 1

  while (n > 2) {
    n /= 2
    res++
  }

  return res
}

function normalizeIndexed(block, hash) {
  if (block) {
    return {
      value: true,
      index: block.index * 2,
      nodes: block.nodes,
      lastIndex: block.index
    }
  }
  if (hash) {
    return {
      value: false,
      index: hash.index,
      nodes: hash.nodes,
      lastIndex: flat.rightSpan(hash.index) / 2
    }
  }
  return null
}

async function getTreeNodeOrError(rx, index) {
  const node = await rx.getTreeNode(index)
  if (node === null) {
    throw INVALID_OPERATION('Expected tree node ' + index + ' from storage, got (nil)')
  }
  return node
}

function getTreeNodeFromStorageOrError(storage, index) {
  const rx = storage.read()
  const p = getTreeNodeOrError(rx, index)
  rx.tryFlush()
  return p
}

function getTreeNodeFromStorage(storage, index) {
  const rx = storage.read()
  const node = rx.getTreeNode(index)
  rx.tryFlush()
  return node
}

function hasTreeNode(storage, index) {
  const rx = storage.read()
  const has = rx.hasTreeNode(index)
  rx.tryFlush()
  return has
}

async function settleProof(p) {
  const result = [
    p.node && Promise.all(p.node),
    p.seek && Promise.all(p.seek),
    p.upgrade && Promise.all(p.upgrade),
    p.additionalUpgrade && Promise.all(p.additionalUpgrade)
  ]

  try {
    return await Promise.all(result)
  } catch (err) {
    if (p.node) await Promise.allSettled(p.node)
    if (p.seek) await Promise.allSettled(p.seek)
    if (p.upgrade) await Promise.allSettled(p.upgrade)
    if (p.additionalUpgrade) await Promise.allSettled(p.additionalUpgrade)
    throw err
  }
}

// tree can be either the merkle tree or a merkle tree batch
async function generateProof(session, rx, block, hash, seek, upgrade) {
  // Important that this does not throw inbetween making the promise arrays
  // and finalise being called, otherwise there will be lingering promises in the background

  if (session.prologue && upgrade) {
    upgrade.start = upgrade.start < session.prologue.length ? 0 : upgrade.start
    upgrade.length =
      upgrade.start < session.prologue.length ? session.prologue.length : upgrade.length
  }

  const head = 2 * session.length
  const from = upgrade ? upgrade.start * 2 : 0
  const to = upgrade ? from + upgrade.length * 2 : head
  const node = normalizeIndexed(block, hash)

  // can't do anything as we have no data...
  if (head === 0) return new TreeProof(session, null, null, null, null)

  if (from >= to || to > head) {
    throw INVALID_OPERATION('Invalid upgrade')
  }
  if (seek && upgrade && node !== null && node.index >= from) {
    throw INVALID_OPERATION('Cannot both do a seek and block/hash request when upgrading')
  }

  let subTree = head

  const p = new TreeProof(session, block, hash, seek, upgrade)

  if (node !== null && (!upgrade || node.lastIndex < upgrade.start)) {
    subTree = nodesToRoot(node.index, node.nodes, to)
    const seekRoot = seek
      ? await seekUntrustedTree(session, subTree, seek.bytes, seek.padding)
      : head
    blockAndSeekProof(session, rx, node, seek, seekRoot, subTree, p.pending)
  } else if ((node || seek) && upgrade) {
    subTree = seek ? await seekFromHead(session, to, seek.bytes, seek.padding) : node.index
  }

  if (upgrade) {
    upgradeProof(session, rx, node, seek, from, to, subTree, p.pending)
    if (head > to) additionalUpgradeProof(session, rx, to, head, p.pending)
  }

  return p
}

function getUnpaddedSize(node, padding, ite) {
  return padding === 0
    ? node.size
    : node.size - padding * (ite ? ite.countLeaves() : flat.countLeaves(node.index))
}

function unslabNodes(nodes) {
  for (const node of nodes) unslabNode(node)
  return nodes
}

function unslabNode(node) {
  if (node === null) return node
  node.hash = unslab(node.hash)
  return node
}
const c = require('compact-encoding')
const b4a = require('b4a')
const { DEFAULT_NAMESPACE } = require('./caps')
const { INVALID_OPLOG_VERSION } = require('hypercore-errors')
const unslab = require('unslab')

const EMPTY = b4a.alloc(0)

const MANIFEST_PATCH = 0b00000001
const MANIFEST_PROLOGUE = 0b00000010
const MANIFEST_LINKED = 0b00000100
const MANIFEST_USER_DATA = 0b00001000

const hashes = {
  preencode(state, m) {
    state.end++ // small uint
  },
  encode(state, m) {
    if (m === 'blake2b') {
      c.uint.encode(state, 0)
      return
    }

    throw new Error('Unknown hash: ' + m)
  },
  decode(state) {
    const n = c.uint.decode(state)
    if (n === 0) return 'blake2b'
    throw new Error('Unknown hash id: ' + n)
  }
}

const signatures = {
  preencode(state, m) {
    state.end++ // small uint
  },
  encode(state, m) {
    if (m === 'ed25519') {
      c.uint.encode(state, 0)
      return
    }

    throw new Error('Unknown signature: ' + m)
  },
  decode(state) {
    const n = c.uint.decode(state)
    if (n === 0) return 'ed25519'
    throw new Error('Unknown signature id: ' + n)
  }
}

const signer = {
  preencode(state, m) {
    signatures.preencode(state, m.signature)
    c.fixed32.preencode(state, m.namespace)
    c.fixed32.preencode(state, m.publicKey)
  },
  encode(state, m) {
    signatures.encode(state, m.signature)
    c.fixed32.encode(state, m.namespace)
    c.fixed32.encode(state, m.publicKey)
  },
  decode(state) {
    return {
      signature: signatures.decode(state),
      namespace: c.fixed32.decode(state),
      publicKey: c.fixed32.decode(state)
    }
  }
}

const signerArray = c.array(signer)

const prologue = {
  preencode(state, p) {
    c.fixed32.preencode(state, p.hash)
    c.uint.preencode(state, p.length)
  },
  encode(state, p) {
    c.fixed32.encode(state, p.hash)
    c.uint.encode(state, p.length)
  },
  decode(state) {
    return {
      hash: c.fixed32.decode(state),
      length: c.uint.decode(state)
    }
  }
}

const manifestv0 = {
  preencode(state, m) {
    hashes.preencode(state, m.hash)
    state.end++ // type

    if (m.prologue && m.signers.length === 0) {
      c.fixed32.preencode(state, m.prologue.hash)
      return
    }

    if (m.quorum === 1 && m.signers.length === 1 && !m.allowPatch) {
      signer.preencode(state, m.signers[0])
    } else {
      state.end++ // flags
      c.uint.preencode(state, m.quorum)
      signerArray.preencode(state, m.signers)
    }
  },
  encode(state, m) {
    hashes.encode(state, m.hash)

    if (m.prologue && m.signers.length === 0) {
      c.uint.encode(state, 0)
      c.fixed32.encode(state, m.prologue.hash)
      return
    }

    if (m.quorum === 1 && m.signers.length === 1 && !m.allowPatch) {
      c.uint.encode(state, 1)
      signer.encode(state, m.signers[0])
    } else {
      c.uint.encode(state, 2)
      c.uint.encode(state, m.allowPatch ? 1 : 0)
      c.uint.encode(state, m.quorum)
      signerArray.encode(state, m.signers)
    }
  },
  decode(state) {
    const hash = hashes.decode(state)
    const type = c.uint.decode(state)

    if (type > 2) throw new Error('Unknown type: ' + type)

    if (type === 0) {
      return {
        version: 0,
        hash,
        allowPatch: false,
        quorum: 0,
        signers: [],
        prologue: {
          hash: c.fixed32.decode(state),
          length: 0
        },
        linked: null,
        userData: null
      }
    }

    if (type === 1) {
      return {
        version: 0,
        hash,
        allowPatch: false,
        quorum: 1,
        signers: [signer.decode(state)],
        prologue: null,
        linked: null,
        userData: null
      }
    }

    const flags = c.uint.decode(state)

    return {
      version: 0,
      hash,
      allowPatch: (flags & 1) !== 0,
      quorum: c.uint.decode(state),
      signers: signerArray.decode(state),
      prologue: null,
      linked: null,
      userData: null
    }
  }
}

const fixed32Array = c.array(c.fixed32)

const manifest = (exports.manifest = {
  preencode(state, m) {
    state.end++ // version

    if (m.version === 0) return manifestv0.preencode(state, m)

    state.end++ // flags
    hashes.preencode(state, m.hash)

    c.uint.preencode(state, m.quorum)
    signerArray.preencode(state, m.signers)

    if (m.prologue) prologue.preencode(state, m.prologue)
    if (m.linked) fixed32Array.preencode(state, m.linked)
    if (m.userData) c.buffer.preencode(state, m.userData)
  },
  encode(state, m) {
    c.uint.encode(state, m.version)

    if (m.version === 0) return manifestv0.encode(state, m)

    let flags = 0
    if (m.allowPatch) flags |= MANIFEST_PATCH
    if (m.prologue) flags |= MANIFEST_PROLOGUE
    if (m.linked) flags |= MANIFEST_LINKED
    if (m.userData) flags |= MANIFEST_USER_DATA

    c.uint.encode(state, flags)
    hashes.encode(state, m.hash)

    c.uint.encode(state, m.quorum)
    signerArray.encode(state, m.signers)

    if (m.prologue) prologue.encode(state, m.prologue)
    if (m.linked) fixed32Array.encode(state, m.linked)
    if (m.userData) c.buffer.encode(state, m.userData)
  },
  decode(state) {
    const version = c.uint.decode(state)

    if (version === 0) return manifestv0.decode(state)
    if (version > 2) throw new Error('Unknown version: ' + version)

    const flags = c.uint.decode(state)
    const hash = hashes.decode(state)
    const quorum = c.uint.decode(state)
    const signers = signerArray.decode(state)

    const hasPatch = (flags & MANIFEST_PATCH) !== 0
    const hasPrologue = (flags & MANIFEST_PROLOGUE) !== 0
    const hasLinked = (flags & MANIFEST_LINKED) !== 0
    const hasUserData = (flags & MANIFEST_USER_DATA) !== 0

    return {
      version,
      hash,
      allowPatch: hasPatch,
      quorum,
      signers,
      prologue: hasPrologue ? prologue.decode(state) : null,
      linked: hasLinked ? fixed32Array.decode(state) : null,
      userData: hasUserData ? c.buffer.decode(state) : null
    }
  }
})

const node = {
  preencode(state, n) {
    c.uint.preencode(state, n.index)
    c.uint.preencode(state, n.size)
    c.fixed32.preencode(state, n.hash)
  },
  encode(state, n) {
    c.uint.encode(state, n.index)
    c.uint.encode(state, n.size)
    c.fixed32.encode(state, n.hash)
  },
  decode(state) {
    return {
      index: c.uint.decode(state),
      size: c.uint.decode(state),
      hash: c.fixed32.decode(state)
    }
  }
}

const nodeArray = c.array(node)

const wire = (exports.wire = {})

wire.handshake = {
  preencode(state, m) {
    c.uint.preencode(state, 1)
    c.fixed32.preencode(state, m.capability)
  },
  encode(state, m) {
    c.uint.encode(state, m.seeks ? 1 : 0)
    c.fixed32.encode(state, m.capability)
  },
  decode(state) {
    const flags = c.uint.decode(state)
    return {
      seeks: (flags & 1) !== 0,
      capability: unslab(c.fixed32.decode(state))
    }
  }
}

const requestBlock = {
  preencode(state, b) {
    c.uint.preencode(state, b.index)
    c.uint.preencode(state, b.nodes)
  },
  encode(state, b) {
    c.uint.encode(state, b.index)
    c.uint.encode(state, b.nodes)
  },
  decode(state) {
    return {
      index: c.uint.decode(state),
      nodes: c.uint.decode(state)
    }
  }
}

const requestSeek = {
  preencode(state, s) {
    c.uint.preencode(state, s.bytes)
    c.uint.preencode(state, s.padding)
  },
  encode(state, s) {
    c.uint.encode(state, s.bytes)
    c.uint.encode(state, s.padding)
  },
  decode(state) {
    return {
      bytes: c.uint.decode(state),
      padding: c.uint.decode(state)
    }
  }
}

const requestUpgrade = {
  preencode(state, u) {
    c.uint.preencode(state, u.start)
    c.uint.preencode(state, u.length)
  },
  encode(state, u) {
    c.uint.encode(state, u.start)
    c.uint.encode(state, u.length)
  },
  decode(state) {
    return {
      start: c.uint.decode(state),
      length: c.uint.decode(state)
    }
  }
}

wire.request = {
  preencode(state, m) {
    state.end++ // flags
    c.uint.preencode(state, m.id)
    c.uint.preencode(state, m.fork)

    if (m.block) requestBlock.preencode(state, m.block)
    if (m.hash) requestBlock.preencode(state, m.hash)
    if (m.seek) requestSeek.preencode(state, m.seek)
    if (m.upgrade) requestUpgrade.preencode(state, m.upgrade)
    if (m.priority) c.uint.preencode(state, m.priority)
  },
  encode(state, m) {
    const flags =
      (m.block ? 1 : 0) |
      (m.hash ? 2 : 0) |
      (m.seek ? 4 : 0) |
      (m.upgrade ? 8 : 0) |
      (m.manifest ? 16 : 0) |
      (m.priority ? 32 : 0)

    c.uint.encode(state, flags)
    c.uint.encode(state, m.id)
    c.uint.encode(state, m.fork)

    if (m.block) requestBlock.encode(state, m.block)
    if (m.hash) requestBlock.encode(state, m.hash)
    if (m.seek) requestSeek.encode(state, m.seek)
    if (m.upgrade) requestUpgrade.encode(state, m.upgrade)
    if (m.priority) c.uint.encode(state, m.priority)
  },
  decode(state) {
    const flags = c.uint.decode(state)

    return {
      id: c.uint.decode(state),
      fork: c.uint.decode(state),
      block: flags & 1 ? requestBlock.decode(state) : null,
      hash: flags & 2 ? requestBlock.decode(state) : null,
      seek: flags & 4 ? requestSeek.decode(state) : null,
      upgrade: flags & 8 ? requestUpgrade.decode(state) : null,
      manifest: (flags & 16) !== 0,
      priority: flags & 32 ? c.uint.decode(state) : 0
    }
  }
}

wire.cancel = {
  preencode(state, m) {
    c.uint.preencode(state, m.request)
  },
  encode(state, m) {
    c.uint.encode(state, m.request)
  },
  decode(state, m) {
    return {
      request: c.uint.decode(state)
    }
  }
}

const dataUpgrade = {
  preencode(state, u) {
    c.uint.preencode(state, u.start)
    c.uint.preencode(state, u.length)
    nodeArray.preencode(state, u.nodes)
    nodeArray.preencode(state, u.additionalNodes)
    c.buffer.preencode(state, u.signature)
  },
  encode(state, u) {
    c.uint.encode(state, u.start)
    c.uint.encode(state, u.length)
    nodeArray.encode(state, u.nodes)
    nodeArray.encode(state, u.additionalNodes)
    c.buffer.encode(state, u.signature)
  },
  decode(state) {
    return {
      start: c.uint.decode(state),
      length: c.uint.decode(state),
      nodes: nodeArray.decode(state),
      additionalNodes: nodeArray.decode(state),
      signature: c.buffer.decode(state)
    }
  }
}

const dataSeek = {
  preencode(state, s) {
    c.uint.preencode(state, s.bytes)
    nodeArray.preencode(state, s.nodes)
  },
  encode(state, s) {
    c.uint.encode(state, s.bytes)
    nodeArray.encode(state, s.nodes)
  },
  decode(state) {
    return {
      bytes: c.uint.decode(state),
      nodes: nodeArray.decode(state)
    }
  }
}

const dataBlock = {
  preencode(state, b) {
    c.uint.preencode(state, b.index)
    c.buffer.preencode(state, b.value)
    nodeArray.preencode(state, b.nodes)
  },
  encode(state, b) {
    c.uint.encode(state, b.index)
    c.buffer.encode(state, b.value)
    nodeArray.encode(state, b.nodes)
  },
  decode(state) {
    return {
      index: c.uint.decode(state),
      value: c.buffer.decode(state) || EMPTY,
      nodes: nodeArray.decode(state)
    }
  }
}

const dataHash = {
  preencode(state, b) {
    c.uint.preencode(state, b.index)
    nodeArray.preencode(state, b.nodes)
  },
  encode(state, b) {
    c.uint.encode(state, b.index)
    nodeArray.encode(state, b.nodes)
  },
  decode(state) {
    return {
      index: c.uint.decode(state),
      nodes: nodeArray.decode(state)
    }
  }
}

wire.data = {
  preencode(state, m) {
    state.end++ // flags
    c.uint.preencode(state, m.request)
    c.uint.preencode(state, m.fork)

    if (m.block) dataBlock.preencode(state, m.block)
    if (m.hash) dataHash.preencode(state, m.hash)
    if (m.seek) dataSeek.preencode(state, m.seek)
    if (m.upgrade) dataUpgrade.preencode(state, m.upgrade)
    if (m.manifest) manifest.preencode(state, m.manifest)
  },
  encode(state, m) {
    const flags =
      (m.block ? 1 : 0) |
      (m.hash ? 2 : 0) |
      (m.seek ? 4 : 0) |
      (m.upgrade ? 8 : 0) |
      (m.manifest ? 16 : 0)

    c.uint.encode(state, flags)
    c.uint.encode(state, m.request)
    c.uint.encode(state, m.fork)

    if (m.block) dataBlock.encode(state, m.block)
    if (m.hash) dataHash.encode(state, m.hash)
    if (m.seek) dataSeek.encode(state, m.seek)
    if (m.upgrade) dataUpgrade.encode(state, m.upgrade)
    if (m.manifest) manifest.encode(state, m.manifest)
  },
  decode(state) {
    const flags = c.uint.decode(state)

    return {
      request: c.uint.decode(state),
      fork: c.uint.decode(state),
      block: flags & 1 ? dataBlock.decode(state) : null,
      hash: flags & 2 ? dataHash.decode(state) : null,
      seek: flags & 4 ? dataSeek.decode(state) : null,
      upgrade: flags & 8 ? dataUpgrade.decode(state) : null,
      manifest: flags & 16 ? manifest.decode(state) : null
    }
  }
}

wire.noData = {
  preencode(state, m) {
    c.uint.preencode(state, m.request)
    c.uint.preencode(state, m.reason ? 1 : 0)
    if (m.reason) c.uint.preencode(state, m.reason)
  },
  encode(state, m) {
    c.uint.encode(state, m.request)
    c.uint.encode(state, m.reason ? 1 : 0)
    if (m.reason) c.uint.encode(state, m.reason)
  },
  decode(state, m) {
    const request = c.uint.decode(state)
    const flags = state.start < state.end ? c.uint.decode(state) : 0
    return {
      request,
      reason: flags & 1 ? c.uint.decode(state) : 0
    }
  }
}

wire.want = {
  preencode(state, m) {
    c.uint.preencode(state, m.start)
    c.uint.preencode(state, m.length)
  },
  encode(state, m) {
    c.uint.encode(state, m.start)
    c.uint.encode(state, m.length)
  },
  decode(state) {
    return {
      start: c.uint.decode(state),
      length: c.uint.decode(state)
    }
  }
}

wire.unwant = {
  preencode(state, m) {
    c.uint.preencode(state, m.start)
    c.uint.preencode(state, m.length)
  },
  encode(state, m) {
    c.uint.encode(state, m.start)
    c.uint.encode(state, m.length)
  },
  decode(state, m) {
    return {
      start: c.uint.decode(state),
      length: c.uint.decode(state)
    }
  }
}

wire.range = {
  preencode(state, m) {
    state.end++ // flags
    c.uint.preencode(state, m.start)
    if (m.length !== 1) c.uint.preencode(state, m.length)
  },
  encode(state, m) {
    c.uint.encode(state, (m.drop ? 1 : 0) | (m.length === 1 ? 2 : 0))
    c.uint.encode(state, m.start)
    if (m.length !== 1) c.uint.encode(state, m.length)
  },
  decode(state) {
    const flags = c.uint.decode(state)

    return {
      drop: (flags & 1) !== 0,
      start: c.uint.decode(state),
      length: (flags & 2) !== 0 ? 1 : c.uint.decode(state)
    }
  }
}

wire.bitfield = {
  preencode(state, m) {
    c.uint.preencode(state, m.start)
    c.uint32array.preencode(state, m.bitfield)
  },
  encode(state, m) {
    c.uint.encode(state, m.start)
    c.uint32array.encode(state, m.bitfield)
  },
  decode(state, m) {
    return {
      start: c.uint.decode(state),
      bitfield: c.uint32array.decode(state)
    }
  }
}

wire.sync = {
  preencode(state, m) {
    state.end++ // flags
    c.uint.preencode(state, m.fork)
    c.uint.preencode(state, m.length)
    c.uint.preencode(state, m.remoteLength)
  },
  encode(state, m) {
    c.uint.encode(
      state,
      (m.canUpgrade ? 1 : 0) |
        (m.uploading ? 2 : 0) |
        (m.downloading ? 4 : 0) |
        (m.hasManifest ? 8 : 0) |
        (m.allowPush ? 16 : 0)
    )
    c.uint.encode(state, m.fork)
    c.uint.encode(state, m.length)
    c.uint.encode(state, m.remoteLength)
  },
  decode(state) {
    const flags = c.uint.decode(state)

    return {
      fork: c.uint.decode(state),
      length: c.uint.decode(state),
      remoteLength: c.uint.decode(state),
      canUpgrade: (flags & 1) !== 0,
      uploading: (flags & 2) !== 0,
      downloading: (flags & 4) !== 0,
      hasManifest: (flags & 8) !== 0,
      allowPush: (flags & 16) !== 0
    }
  }
}

wire.reorgHint = {
  preencode(state, m) {
    c.uint.preencode(state, m.from)
    c.uint.preencode(state, m.to)
    c.uint.preencode(state, m.ancestors)
  },
  encode(state, m) {
    c.uint.encode(state, m.from)
    c.uint.encode(state, m.to)
    c.uint.encode(state, m.ancestors)
  },
  decode(state) {
    return {
      from: c.uint.encode(state),
      to: c.uint.encode(state),
      ancestors: c.uint.encode(state)
    }
  }
}

wire.extension = {
  preencode(state, m) {
    c.string.preencode(state, m.name)
    c.raw.preencode(state, m.message)
  },
  encode(state, m) {
    c.string.encode(state, m.name)
    c.raw.encode(state, m.message)
  },
  decode(state) {
    return {
      name: c.string.decode(state),
      message: c.raw.decode(state)
    }
  }
}

const keyValue = {
  preencode(state, p) {
    c.string.preencode(state, p.key)
    c.buffer.preencode(state, p.value)
  },
  encode(state, p) {
    c.string.encode(state, p.key)
    c.buffer.encode(state, p.value)
  },
  decode(state) {
    return {
      key: c.string.decode(state),
      value: c.buffer.decode(state)
    }
  }
}

const treeUpgrade = {
  preencode(state, u) {
    c.uint.preencode(state, u.fork)
    c.uint.preencode(state, u.ancestors)
    c.uint.preencode(state, u.length)
    c.buffer.preencode(state, u.signature)
  },
  encode(state, u) {
    c.uint.encode(state, u.fork)
    c.uint.encode(state, u.ancestors)
    c.uint.encode(state, u.length)
    c.buffer.encode(state, u.signature)
  },
  decode(state) {
    return {
      fork: c.uint.decode(state),
      ancestors: c.uint.decode(state),
      length: c.uint.decode(state),
      signature: c.buffer.decode(state)
    }
  }
}

const bitfieldUpdate = {
  // TODO: can maybe be folded into a HAVE later on with the most recent spec
  preencode(state, b) {
    state.end++ // flags
    c.uint.preencode(state, b.start)
    c.uint.preencode(state, b.length)
  },
  encode(state, b) {
    state.buffer[state.start++] = b.drop ? 1 : 0
    c.uint.encode(state, b.start)
    c.uint.encode(state, b.length)
  },
  decode(state) {
    const flags = c.uint.decode(state)
    return {
      drop: (flags & 1) !== 0,
      start: c.uint.decode(state),
      length: c.uint.decode(state)
    }
  }
}

const oplog = (exports.oplog = {})

oplog.entry = {
  preencode(state, m) {
    state.end++ // flags
    if (m.userData) keyValue.preencode(state, m.userData)
    if (m.treeNodes) nodeArray.preencode(state, m.treeNodes)
    if (m.treeUpgrade) treeUpgrade.preencode(state, m.treeUpgrade)
    if (m.bitfield) bitfieldUpdate.preencode(state, m.bitfield)
  },
  encode(state, m) {
    const s = state.start++
    let flags = 0

    if (m.userData) {
      flags |= 1
      keyValue.encode(state, m.userData)
    }
    if (m.treeNodes) {
      flags |= 2
      nodeArray.encode(state, m.treeNodes)
    }
    if (m.treeUpgrade) {
      flags |= 4
      treeUpgrade.encode(state, m.treeUpgrade)
    }
    if (m.bitfield) {
      flags |= 8
      bitfieldUpdate.encode(state, m.bitfield)
    }

    state.buffer[s] = flags
  },
  decode(state) {
    const flags = c.uint.decode(state)
    return {
      userData: (flags & 1) !== 0 ? keyValue.decode(state) : null,
      treeNodes: (flags & 2) !== 0 ? nodeArray.decode(state) : null,
      treeUpgrade: (flags & 4) !== 0 ? treeUpgrade.decode(state) : null,
      bitfield: (flags & 8) !== 0 ? bitfieldUpdate.decode(state) : null
    }
  }
}

const keyPair = {
  preencode(state, kp) {
    c.buffer.preencode(state, kp.publicKey)
    c.buffer.preencode(state, kp.secretKey)
  },
  encode(state, kp) {
    c.buffer.encode(state, kp.publicKey)
    c.buffer.encode(state, kp.secretKey)
  },
  decode(state) {
    return {
      publicKey: c.buffer.decode(state),
      secretKey: c.buffer.decode(state)
    }
  }
}

const reorgHint = {
  preencode(state, r) {
    c.uint.preencode(state, r.from)
    c.uint.preencode(state, r.to)
    c.uint.preencode(state, r.ancestors)
  },
  encode(state, r) {
    c.uint.encode(state, r.from)
    c.uint.encode(state, r.to)
    c.uint.encode(state, r.ancestors)
  },
  decode(state) {
    return {
      from: c.uint.decode(state),
      to: c.uint.decode(state),
      ancestors: c.uint.decode(state)
    }
  }
}

const reorgHintArray = c.array(reorgHint)

const hints = {
  preencode(state, h) {
    reorgHintArray.preencode(state, h.reorgs)
    c.uint.preencode(state, h.contiguousLength)
  },
  encode(state, h) {
    reorgHintArray.encode(state, h.reorgs)
    c.uint.encode(state, h.contiguousLength)
  },
  decode(state) {
    return {
      reorgs: reorgHintArray.decode(state),
      contiguousLength: state.start < state.end ? c.uint.decode(state) : 0
    }
  }
}

const treeHeader = {
  preencode(state, t) {
    c.uint.preencode(state, t.fork)
    c.uint.preencode(state, t.length)
    c.buffer.preencode(state, t.rootHash)
    c.buffer.preencode(state, t.signature)
  },
  encode(state, t) {
    c.uint.encode(state, t.fork)
    c.uint.encode(state, t.length)
    c.buffer.encode(state, t.rootHash)
    c.buffer.encode(state, t.signature)
  },
  decode(state) {
    return {
      fork: c.uint.decode(state),
      length: c.uint.decode(state),
      rootHash: c.buffer.decode(state),
      signature: c.buffer.decode(state)
    }
  }
}

const types = {
  preencode(state, t) {
    c.string.preencode(state, t.tree)
    c.string.preencode(state, t.bitfield)
    c.string.preencode(state, t.signer)
  },
  encode(state, t) {
    c.string.encode(state, t.tree)
    c.string.encode(state, t.bitfield)
    c.string.encode(state, t.signer)
  },
  decode(state) {
    return {
      tree: c.string.decode(state),
      bitfield: c.string.decode(state),
      signer: c.string.decode(state)
    }
  }
}

const externalHeader = {
  preencode(state, m) {
    c.uint.preencode(state, m.start)
    c.uint.preencode(state, m.length)
  },
  encode(state, m) {
    c.uint.encode(state, m.start)
    c.uint.encode(state, m.length)
  },
  decode(state) {
    return {
      start: c.uint.decode(state),
      length: c.uint.decode(state)
    }
  }
}

const keyValueArray = c.array(keyValue)

oplog.header = {
  preencode(state, h) {
    state.end += 2 // version + flags
    if (h.external) {
      externalHeader.preencode(state, h.external)
      return
    }
    c.fixed32.preencode(state, h.key)
    if (h.manifest) manifest.preencode(state, h.manifest)
    if (h.keyPair) keyPair.preencode(state, h.keyPair)
    keyValueArray.preencode(state, h.userData)
    treeHeader.preencode(state, h.tree)
    hints.preencode(state, h.hints)
  },
  encode(state, h) {
    c.uint.encode(state, 1)
    if (h.external) {
      c.uint.encode(state, 1) // ONLY set the first big for clarity
      externalHeader.encode(state, h.external)
      return
    }
    c.uint.encode(state, (h.manifest ? 2 : 0) | (h.keyPair ? 4 : 0))
    c.fixed32.encode(state, h.key)
    if (h.manifest) manifest.encode(state, h.manifest)
    if (h.keyPair) keyPair.encode(state, h.keyPair)
    keyValueArray.encode(state, h.userData)
    treeHeader.encode(state, h.tree)
    hints.encode(state, h.hints)
  },
  decode(state) {
    const version = c.uint.decode(state)

    if (version > 1) {
      throw INVALID_OPLOG_VERSION('Invalid header version. Expected <= 1, got ' + version)
    }

    if (version === 0) {
      const old = {
        types: types.decode(state),
        userData: keyValueArray.decode(state),
        tree: treeHeader.decode(state),
        signer: keyPair.decode(state),
        hints: hints.decode(state)
      }

      return {
        external: null,
        key: old.signer.publicKey,
        manifest: {
          version: 0,
          hash: old.types.tree,
          allowPatch: false,
          quorum: 1,
          signers: [
            {
              signature: old.types.signer,
              namespace: DEFAULT_NAMESPACE,
              publicKey: old.signer.publicKey
            }
          ],
          prologue: null,
          linked: null,
          userData: null
        },
        keyPair: old.signer.secretKey ? old.signer : null,
        userData: old.userData,
        tree: old.tree,
        hints: old.hints
      }
    }

    const flags = c.uint.decode(state)

    if (flags & 1) {
      return {
        external: externalHeader.decode(state),
        key: null,
        manifest: null,
        keyPair: null,
        userData: null,
        tree: null,
        hints: null
      }
    }

    return {
      external: null,
      key: c.fixed32.decode(state),
      manifest: (flags & 2) !== 0 ? manifest.decode(state) : null,
      keyPair: (flags & 4) !== 0 ? keyPair.decode(state) : null,
      userData: keyValueArray.decode(state),
      tree: treeHeader.decode(state),
      hints: hints.decode(state)
    }
  }
}

const uintArray = c.array(c.uint)

const multisigInput = {
  preencode(state, inp) {
    c.uint.preencode(state, inp.signer)
    c.fixed64.preencode(state, inp.signature)
    c.uint.preencode(state, inp.patch)
  },
  encode(state, inp) {
    c.uint.encode(state, inp.signer)
    c.fixed64.encode(state, inp.signature)
    c.uint.encode(state, inp.patch)
  },
  decode(state) {
    return {
      signer: c.uint.decode(state),
      signature: c.fixed64.decode(state),
      patch: c.uint.decode(state)
    }
  }
}

const patchEncodingv0 = {
  preencode(state, n) {
    c.uint.preencode(state, n.start)
    c.uint.preencode(state, n.length)
    uintArray.preencode(state, n.nodes)
  },
  encode(state, n) {
    c.uint.encode(state, n.start)
    c.uint.encode(state, n.length)
    uintArray.encode(state, n.nodes)
  },
  decode(state) {
    return {
      start: c.uint.decode(state),
      length: c.uint.decode(state),
      nodes: uintArray.decode(state)
    }
  }
}

const multisigInputv0 = {
  preencode(state, n) {
    state.end++
    c.uint.preencode(state, n.signer)
    c.fixed64.preencode(state, n.signature)
    if (n.patch) patchEncodingv0.preencode(state, n.patch)
  },
  encode(state, n) {
    c.uint.encode(state, n.patch ? 1 : 0)
    c.uint.encode(state, n.signer)
    c.fixed64.encode(state, n.signature)
    if (n.patch) patchEncodingv0.encode(state, n.patch)
  },
  decode(state) {
    const flags = c.uint.decode(state)
    return {
      signer: c.uint.decode(state),
      signature: c.fixed64.decode(state),
      patch: flags & 1 ? patchEncodingv0.decode(state) : null
    }
  }
}

const multisigInputArrayv0 = c.array(multisigInputv0)
const multisigInputArray = c.array(multisigInput)

const compactNode = {
  preencode(state, n) {
    c.uint.preencode(state, n.index)
    c.uint.preencode(state, n.size)
    c.fixed32.preencode(state, n.hash)
  },
  encode(state, n) {
    c.uint.encode(state, n.index)
    c.uint.encode(state, n.size)
    c.fixed32.encode(state, n.hash)
  },
  decode(state) {
    return {
      index: c.uint.decode(state),
      size: c.uint.decode(state),
      hash: c.fixed32.decode(state)
    }
  }
}

const compactNodeArray = c.array(compactNode)

exports.multiSignaturev0 = {
  preencode(state, s) {
    multisigInputArrayv0.preencode(state, s.proofs)
    compactNodeArray.preencode(state, s.patch)
  },
  encode(state, s) {
    multisigInputArrayv0.encode(state, s.proofs)
    compactNodeArray.encode(state, s.patch)
  },
  decode(state) {
    return {
      proofs: multisigInputArrayv0.decode(state),
      patch: compactNodeArray.decode(state)
    }
  }
}

exports.multiSignature = {
  preencode(state, s) {
    multisigInputArray.preencode(state, s.proofs)
    compactNodeArray.preencode(state, s.patch)
  },
  encode(state, s) {
    multisigInputArray.encode(state, s.proofs)
    compactNodeArray.encode(state, s.patch)
  },
  decode(state) {
    return {
      proofs: multisigInputArray.decode(state),
      patch: compactNodeArray.decode(state)
    }
  }
}
const c = require('compact-encoding')
const b4a = require('b4a')
const flat = require('flat-tree')
const { MerkleTree } = require('./merkle-tree')
const { multiSignature, multiSignaturev0 } = require('./messages')

module.exports = {
  assemblev0,
  assemble,
  inflatev0,
  inflate,
  partialSignature,
  signableLength
}

function inflatev0(data) {
  return c.decode(multiSignaturev0, data)
}

function inflate(data) {
  return c.decode(multiSignature, data)
}

async function partialSignature(
  core,
  signer,
  from,
  to = core.state.length,
  signature = core.state.signature
) {
  if (from > core.state.length) return null
  const nodes = to <= from ? null : await upgradeNodes(core, from, to)

  if (signature.byteLength !== 64) {
    signature = c.decode(multiSignature, signature).proofs[0].signature
  }

  return {
    signer,
    signature,
    patch: nodes ? to - from : 0,
    nodes
  }
}

async function upgradeNodes(core, from, to) {
  const rx = core.state.storage.read()
  let p = null
  try {
    p = await MerkleTree.proof(core.state, rx, { upgrade: { start: from, length: to - from } })
  } catch (err) {
    rx.destroy()
    throw err
  }
  rx.tryFlush()
  return (await p.settle()).upgrade.nodes
}

function signableLength(lengths, quorum) {
  if (quorum <= 0) quorum = 1
  if (quorum > lengths.length) return 0

  return lengths.sort(cmp)[quorum - 1]
}

function cmp(a, b) {
  return b - a
}

function assemblev0(inputs) {
  const proofs = []
  const patch = []

  for (const u of inputs) {
    proofs.push(compressProof(u, patch))
  }

  return c.encode(multiSignaturev0, { proofs, patch })
}

function assemble(inputs) {
  const proofs = []
  const patch = []
  const seen = new Set()

  for (const u of inputs) {
    if (u.nodes) {
      for (const node of u.nodes) {
        if (seen.has(node.index)) continue
        seen.add(node.index)
        patch.push(node)
      }
    }

    proofs.push({
      signer: u.signer,
      signature: u.signature,
      patch: u.patch
    })
  }

  return c.encode(multiSignature, { proofs, patch })
}

function compareNode(a, b) {
  if (a.index !== b.index) return false
  if (a.size !== b.size) return false
  return b4a.equals(a.hash, b.hash)
}

function compressProof(proof, nodes) {
  return {
    signer: proof.signer,
    signature: proof.signature,
    patch: proof.patch ? compressUpgrade(proof, nodes) : null
  }
}

function compressUpgrade(p, nodes) {
  const u = {
    start: flat.rightSpan(p.nodes[p.nodes.length - 1].index) / 2 + 1,
    length: p.patch,
    nodes: []
  }

  for (const node of p.nodes) {
    let present = false
    for (let i = 0; i < nodes.length; i++) {
      if (!compareNode(nodes[i], node)) continue

      u.nodes.push(i)
      present = true
      break
    }

    if (present) continue
    u.nodes.push(nodes.push(node) - 1)
  }

  return u
}
module.exports = class Mutex {
  constructor() {
    this.locked = false
    this.destroyed = false

    this._destroying = null
    this._destroyError = null
    this._queue = []
    this._enqueue = (resolve, reject) => this._queue.push([resolve, reject])
  }

  idle() {
    return this._queue.length === 0 && this.locked === false
  }

  lock() {
    if (this.destroyed) {
      return Promise.reject(this._destroyError || new Error('Mutex has been destroyed'))
    }
    if (this.locked) return new Promise(this._enqueue)
    this.locked = true
    return Promise.resolve()
  }

  unlock() {
    if (!this._queue.length) {
      this.locked = false
      return
    }
    this._queue.shift()[0]()
  }

  destroy(err) {
    if (!this._destroying) {
      this._destroying = this.locked ? this.lock().catch(() => {}) : Promise.resolve()
    }

    this.destroyed = true
    if (err) this._destroyError = err

    if (err) {
      while (this._queue.length) this._queue.shift()[1](err)
    }

    return this._destroying
  }
}
const FIFO = require('fast-fifo')

module.exports = class ReceiverQueue {
  constructor() {
    this.queue = new FIFO()
    this.priority = []
    this.requests = new Map()
    this.length = 0
  }

  push(req) {
    // TODO: use a heap at some point if we wanna support multiple prios
    if (req.priority > 0) this.priority.push(req)
    else this.queue.push(req)

    this.requests.set(req.id, req)
    this.length++
  }

  shift() {
    while (this.priority.length > 0) {
      const msg = this.priority.pop()
      const req = this._processRequest(msg)
      if (req !== null) return req
    }

    while (this.queue.length > 0) {
      const msg = this.queue.shift()
      const req = this._processRequest(msg)
      if (req !== null) return req
    }

    return null
  }

  _processRequest(req) {
    if (req.block || req.hash || req.seek || req.upgrade || req.manifest) {
      this.requests.delete(req.id)
      this.length--
      return req
    }

    return null
  }

  clear() {
    this.queue.clear()
    this.priority = []
    this.length = 0
    this.requests.clear()
  }

  delete(id) {
    const req = this.requests.get(id)
    if (!req) return

    req.block = null
    req.hash = null
    req.seek = null
    req.upgrade = null
    req.manifest = false

    this.requests.delete(id)
    this.length--

    if (this.length === 0) {
      this.queue.clear()
      this.priority = []
    }
  }
}
const BigSparseArray = require('big-sparse-array')
const quickbit = require('./compat').quickbit

const BITS_PER_PAGE = 32768
const BYTES_PER_PAGE = BITS_PER_PAGE / 8
const WORDS_PER_PAGE = BYTES_PER_PAGE / 4
const BITS_PER_SEGMENT = 2097152
const BYTES_PER_SEGMENT = BITS_PER_SEGMENT / 8
const PAGES_PER_SEGMENT = BITS_PER_SEGMENT / BITS_PER_PAGE

class RemoteBitfieldPage {
  constructor(index, bitfield, segment) {
    this.index = index
    this.offset = index * BYTES_PER_PAGE - segment.offset
    this.bitfield = bitfield
    this.segment = segment

    segment.add(this)
  }

  get tree() {
    return this.segment.tree
  }

  get(index) {
    return quickbit.get(this.bitfield, index)
  }

  set(index, val) {
    if (quickbit.set(this.bitfield, index, val)) {
      this.tree.update(this.offset * 8 + index)
    }
  }

  setRange(start, end, val) {
    quickbit.fill(this.bitfield, val, start, end)

    let i = Math.floor(start / 128)
    const n = i + Math.ceil((end - start) / 128)

    while (i <= n) this.tree.update(this.offset * 8 + i++ * 128)
  }

  findFirst(val, position) {
    return quickbit.findFirst(this.bitfield, val, position)
  }

  findLast(val, position) {
    return quickbit.findLast(this.bitfield, val, position)
  }

  insert(start, bitfield) {
    this.bitfield.set(bitfield, start / 32)
    this.segment.refresh()
  }

  clear(start, bitfield) {
    quickbit.clear(this.bitfield, { field: bitfield, offset: start })
  }
}

class RemoteBitfieldSegment {
  constructor(index) {
    this.index = index
    this.offset = index * BYTES_PER_SEGMENT
    this.tree = quickbit.Index.from([], BYTES_PER_SEGMENT)
    this.pages = new Array(PAGES_PER_SEGMENT)
    this.pagesLength = 0
  }

  get chunks() {
    return this.tree.chunks
  }

  refresh() {
    this.tree = quickbit.Index.from(this.tree.chunks, BYTES_PER_SEGMENT)
  }

  add(page) {
    const pageIndex = page.index - this.index * PAGES_PER_SEGMENT
    if (pageIndex >= this.pagesLength) this.pagesLength = pageIndex + 1

    this.pages[pageIndex] = page

    const chunk = { field: page.bitfield, offset: page.offset }

    this.chunks.push(chunk)

    for (let i = this.chunks.length - 2; i >= 0; i--) {
      const prev = this.chunks[i]
      if (prev.offset <= chunk.offset) break
      this.chunks[i] = chunk
      this.chunks[i + 1] = prev
    }
  }

  findFirst(val, position) {
    position = this.tree.skipFirst(!val, position)

    let j = position & (BITS_PER_PAGE - 1)
    let i = (position - j) / BITS_PER_PAGE

    if (i >= PAGES_PER_SEGMENT) return -1

    while (i < this.pagesLength) {
      const p = this.pages[i]

      let index = -1

      if (p) index = p.findFirst(val, j)
      else if (!val) index = j

      if (index !== -1) return i * BITS_PER_PAGE + index

      j = 0
      i++
    }

    return val || this.pagesLength === PAGES_PER_SEGMENT ? -1 : this.pagesLength * BITS_PER_PAGE
  }

  findLast(val, position) {
    position = this.tree.skipLast(!val, position)

    let j = position & (BITS_PER_PAGE - 1)
    let i = (position - j) / BITS_PER_PAGE

    if (i >= PAGES_PER_SEGMENT) return -1

    while (i >= 0) {
      const p = this.pages[i]

      let index = -1

      if (p) index = p.findLast(val, j)
      else if (!val) index = j

      if (index !== -1) return i * BITS_PER_PAGE + index

      j = BITS_PER_PAGE - 1
      i--
    }

    return -1
  }
}

module.exports = class RemoteBitfield {
  static BITS_PER_PAGE = BITS_PER_PAGE

  constructor() {
    this._pages = new BigSparseArray()
    this._segments = new BigSparseArray()
    this._maxSegments = 0
  }

  getBitfield(index) {
    const j = index & (BITS_PER_PAGE - 1)
    const i = (index - j) / BITS_PER_PAGE

    const p = this._pages.get(i)
    return p || null
  }

  get(index) {
    const j = index & (BITS_PER_PAGE - 1)
    const i = (index - j) / BITS_PER_PAGE

    const p = this._pages.get(i)

    return p ? p.get(j) : false
  }

  set(index, val) {
    const j = index & (BITS_PER_PAGE - 1)
    const i = (index - j) / BITS_PER_PAGE

    let p = this._pages.get(i)

    if (!p && val) {
      const k = Math.floor(i / PAGES_PER_SEGMENT)
      const s = this._segments.get(k) || this._segments.set(k, new RemoteBitfieldSegment(k))
      if (this._maxSegments <= k) this._maxSegments = k + 1

      p = this._pages.set(i, new RemoteBitfieldPage(i, new Uint32Array(WORDS_PER_PAGE), s))
    }

    if (p) p.set(j, val)
  }

  setRange(start, end, val) {
    let j = start & (BITS_PER_PAGE - 1)
    let i = (start - j) / BITS_PER_PAGE

    while (start < end) {
      let p = this._pages.get(i)

      if (!p && val) {
        const k = Math.floor(i / PAGES_PER_SEGMENT)
        const s = this._segments.get(k) || this._segments.set(k, new RemoteBitfieldSegment(k))
        if (this._maxSegments <= k) this._maxSegments = k + 1

        p = this._pages.set(i, new RemoteBitfieldPage(i, new Uint32Array(WORDS_PER_PAGE), s))
      }

      const offset = i * BITS_PER_PAGE
      const last = Math.min(end - offset, BITS_PER_PAGE)
      const range = last - j

      if (p) p.setRange(j, last, val)

      j = 0
      i++
      start += range
    }
  }

  findFirst(val, position) {
    let j = position & (BITS_PER_SEGMENT - 1)
    let i = (position - j) / BITS_PER_SEGMENT

    while (i < this._maxSegments) {
      const s = this._segments.get(i)

      let index = -1

      if (s) index = s.findFirst(val, j)
      else if (!val) index = j

      if (index !== -1) return i * BITS_PER_SEGMENT + index

      j = 0
      i++
    }

    // For the val === false case, we always return at least
    // the 'position', also if nothing was found
    return val ? -1 : Math.max(position, this._maxSegments * BITS_PER_SEGMENT)
  }

  firstSet(position) {
    return this.findFirst(true, position)
  }

  firstUnset(position) {
    return this.findFirst(false, position)
  }

  findLast(val, position) {
    let j = position & (BITS_PER_SEGMENT - 1)
    let i = (position - j) / BITS_PER_SEGMENT

    while (i >= 0) {
      const s = this._segments.get(i)

      let index = -1

      if (s) index = s.findLast(val, j)
      else if (!val) index = j

      if (index !== -1) return i * BITS_PER_SEGMENT + index

      j = BITS_PER_SEGMENT - 1
      i--
    }

    return -1
  }

  lastSet(position) {
    return this.findLast(true, position)
  }

  lastUnset(position) {
    return this.findLast(false, position)
  }

  insert(start, bitfield) {
    if (start % 32 !== 0) return false

    let length = bitfield.byteLength * 8

    let j = start & (BITS_PER_PAGE - 1)
    let i = (start - j) / BITS_PER_PAGE

    while (length > 0) {
      let p = this._pages.get(i)

      if (!p) {
        const k = Math.floor(i / PAGES_PER_SEGMENT)
        const s = this._segments.get(k) || this._segments.set(k, new RemoteBitfieldSegment(k))
        if (this._maxSegments <= k) this._maxSegments = k + 1

        p = this._pages.set(i, new RemoteBitfieldPage(i, new Uint32Array(WORDS_PER_PAGE), s))
      }

      const end = Math.min(j + length, BITS_PER_PAGE)
      const range = end - j

      p.insert(j, bitfield.subarray(0, range / 32))

      bitfield = bitfield.subarray(range / 32)

      j = 0
      i++
      length -= range
    }

    return true
  }

  clear(start, bitfield) {
    if (start % 32 !== 0) return false

    let length = bitfield.byteLength * 8

    let j = start & (BITS_PER_PAGE - 1)
    let i = (start - j) / BITS_PER_PAGE

    while (length > 0) {
      let p = this._pages.get(i)

      if (!p) {
        const k = Math.floor(i / PAGES_PER_SEGMENT)
        const s = this._segments.get(k) || this._segments.set(k, new RemoteBitfieldSegment(k))
        if (this._maxSegments <= k) this._maxSegments = k + 1

        p = this._pages.set(i, new RemoteBitfieldPage(i, new Uint32Array(WORDS_PER_PAGE), s))
      }

      const end = Math.min(j + length, BITS_PER_PAGE)
      const range = end - j

      p.clear(j, bitfield.subarray(0, range / 32))

      bitfield = bitfield.subarray(range / 32)

      j = 0
      i++
      length -= range
    }

    return true
  }
}
/* DEV DOCS
  Every hypercore has one Replicator object managing its connections to other peers.
  There is one Peer object per peer connected to the Hypercore.
  Hypercores do not know about other hypercores, so when a peer is connected to multiple cores, there exists one Peer object per core.

  Hypercore indicates block should be downloaded through methods like Replicator.addRange or Replicator.addBlock
  Hypercore calls Replicator.updateActivity every time a hypercore session opens/closes
  Replicator.updateActivity ensures the Hypercore is downloading blocks as expected
  Replicator keeps track of:
    - Which blocks need to be downloaded (Replicator._blocks)
    - Which blocks currently have inflight requests (Replicator._inflight)

  Blocks are requested from remote peers by Peer objects. The flow is:
    - The replicator's updatePeer method gets called
    - The replicator detects whether the Peer can accept more requests (for example by checking if it's maxed out on inflight blocks)
    - The replicator then tells the Peer what to request (e.g. Peer_requestRange or Peer._requestBlock methods)

  The Peer object is responsible for tracking
    - Which blocks does the Peer have available (tracked in remoteBitfield)
    - Which blocks are you actively looking for from this peer (tracked in missingBlocks)
    - How many blocks are currently inflight (tracked in inflight)
  The Peer uses this information to decide which blocks to request from the peer in response to _requestRange requests and the like.
*/

const b4a = require('b4a')
const safetyCatch = require('safety-catch')
const RandomIterator = require('random-array-iterator')
const flatTree = require('flat-tree')
const Mutex = require('./mutex')
const ReceiverQueue = require('./receiver-queue')
const HotswapQueue = require('./hotswap-queue')
const RemoteBitfield = require('./remote-bitfield')
const { MerkleTree } = require('./merkle-tree')
const {
  REQUEST_CANCELLED,
  REQUEST_TIMEOUT,
  INVALID_CAPABILITY,
  SNAPSHOT_NOT_AVAILABLE,
  ASSERTION
} = require('hypercore-errors')
const m = require('./messages')
const caps = require('./caps')

const DEFAULT_MAX_INFLIGHT = [16, 512]
const SCALE_LATENCY = 50
const DEFAULT_SEGMENT_SIZE = 256 * 1024 * 8 // 256 KiB in bits
const NOT_DOWNLOADING_SLACK = (20000 + Math.random() * 20000) | 0
const MAX_PEERS_UPGRADE = 3
const LAST_BLOCKS = 256
const MAX_REMOTE_SEGMENTS = 2048

const MAX_RANGES = 64

const NOT_AVAILABLE = 1
const INVALID_REQUEST = 2
const MAX_INVALID_REQUESTS = 64
const MAX_BACKOFFS = MAX_INVALID_REQUESTS / 2

const PRIORITY = {
  NORMAL: 0,
  HIGH: 1,
  VERY_HIGH: 2,
  CANCELLED: 255 // reserved to mark cancellation
}

class Attachable {
  constructor(replicator) {
    this.replicator = replicator
    this.resolved = false
    this.processing = false
    this.refs = []
  }

  attach(session) {
    const r = {
      context: this,
      session,
      sindex: 0,
      rindex: 0,
      snapshot: true,
      resolve: null,
      reject: null,
      promise: null,
      timeout: null
    }

    r.sindex = session.push(r) - 1
    r.rindex = this.refs.push(r) - 1
    r.promise = new Promise((resolve, reject) => {
      r.resolve = resolve
      r.reject = reject
    })

    return r
  }

  detach(r, err = null) {
    if (r.context !== this) return false

    this._detach(r)
    this._cancel(r, err)
    this.gc()

    return true
  }

  _detach(r) {
    const rh = this.refs.pop()
    const sh = r.session.pop()

    if (r.rindex < this.refs.length) this.refs[(rh.rindex = r.rindex)] = rh
    if (r.sindex < r.session.length) r.session[(sh.sindex = r.sindex)] = sh

    destroyRequestTimeout(r)
    r.context = null

    return r
  }

  gc() {
    if (this.refs.length === 0 && !this.processing) this._unref()
  }

  processed() {
    this.processing = false
    this.gc()
  }

  _cancel(r, err) {
    r.reject(err || REQUEST_CANCELLED())
  }

  _unref() {
    // overwrite me
  }

  resolve(val) {
    this.resolved = true
    while (this.refs.length > 0) {
      this._detach(this.refs[this.refs.length - 1]).resolve(val)
    }
  }

  reject(err) {
    this.resolved = true
    while (this.refs.length > 0) {
      this._detach(this.refs[this.refs.length - 1]).reject(err)
    }
  }

  setTimeout(r, ms) {
    destroyRequestTimeout(r)
    r.timeout = setTimeout(onrequesttimeout, ms, r)
  }
}

class BlockRequest extends Attachable {
  constructor(replicator, tracker, index, priority) {
    super(replicator)

    this.index = index
    this.priority = priority
    this.inflight = []
    this.queued = false
    this.hotswap = null
    this.tracker = tracker
  }

  _unref() {
    this.queued = false

    for (const req of this.inflight) {
      req.peer._cancelRequest(req)
    }

    this.tracker.remove(this.index)
    removeHotswap(this)
  }
}

class RangeRequest extends Attachable {
  constructor(replicator, ranges, start, end, linear, ifAvailable, blocks) {
    super(replicator)

    this.start = start
    this.end = end
    this.linear = linear
    this.ifAvailable = ifAvailable
    this.blocks = blocks
    this.ranges = ranges
    ranges.push(this)

    if (this.end === -1) {
      this.replicator._alwaysLatestBlock++
    }

    // As passed by the user, immut
    this.userStart = start
    this.userEnd = end
  }

  _unref() {
    const rangeIndex = this.ranges.indexOf(this)
    if (rangeIndex === -1) return

    const h = this.ranges.pop()
    if (h !== this) {
      this.ranges[rangeIndex] = h
    }

    if (this.end === -1) {
      this.replicator._alwaysLatestBlock--
    }
  }

  _cancel(r) {
    r.resolve(false)
  }
}

class UpgradeRequest extends Attachable {
  constructor(replicator, fork, length) {
    super(replicator)

    this.fork = fork
    this.length = length
    this.inflight = []
  }

  _unref() {
    if (this.replicator.eagerUpgrade === true || this.inflight.length > 0) return
    this.replicator._upgrade = null
  }

  _cancel(r) {
    r.resolve(false)
  }
}

class SeekRequest extends Attachable {
  constructor(replicator, seeks, seeker) {
    super(replicator)

    this.seeker = seeker
    this.inflight = []
    this.seeks = seeks
  }

  _unref() {
    if (this.inflight.length > 0) return
    const i = this.seeks.indexOf(this)
    if (i === -1) return
    const h = this.seeks.pop()
    if (i < this.seeks.length) this.seeks[i] = h
  }
}

class InflightTracker {
  constructor() {
    this._requests = []
    this._free = []
    this._active = 0
  }

  get idle() {
    return this._active === 0
  }

  *[Symbol.iterator]() {
    for (const req of this._requests) {
      if (req !== null) yield req
    }
  }

  add(req) {
    const id = this._free.length ? this._free.pop() : this._requests.push(null)
    req.id = id
    this._requests[id - 1] = req
    this._active++
    return req
  }

  get(id) {
    return id <= this._requests.length ? this._requests[id - 1] : null
  }

  remove(id, roundtrip) {
    if (id > this._requests.length) return
    if (this._requests[id - 1]) this._active--
    this._requests[id - 1] = null
    if (roundtrip === true) this._free.push(id)
  }

  reusable(id) {
    this._free.push(id)
  }
}

class BlockTracker {
  constructor(replicator) {
    this._replicator = replicator
    this._map = new Map()
  }

  [Symbol.iterator]() {
    return this._map.values()
  }

  isEmpty() {
    return this._map.size === 0
  }

  has(index) {
    return this._map.has(index)
  }

  get(index) {
    return this._map.get(index) || null
  }

  add(index, priority) {
    let b = this._map.get(index)
    if (b) return b

    b = new BlockRequest(this._replicator, this, index, priority)
    this._map.set(index, b)

    return b
  }

  remove(index) {
    const b = this.get(index)
    this._map.delete(index)
    return b
  }
}

class RoundtripQueue {
  constructor() {
    this.queue = []
    this.tick = 0
  }

  clear() {
    const ids = new Array(this.queue.length)
    for (let i = 0; i < ids.length; i++) {
      ids[i] = this.queue[i][1]
    }

    this.queue = []

    return ids
  }

  add(id) {
    this.queue.push([++this.tick, id])
  }

  flush(tick) {
    let flushed = null

    for (let i = 0; i < this.queue.length; i++) {
      if (this.queue[i][0] > tick) break
      if (flushed === null) flushed = []
      flushed.push(this.queue[i][1])
    }

    if (flushed !== null) this.queue.splice(0, flushed.length)
    return flushed
  }
}

class ProofRequest {
  constructor(msg, proof, block, manifest) {
    this.msg = msg
    this.proof = proof
    this.block = block
    this.manifest = manifest
  }

  async fulfill() {
    if (this.proof === null) return null

    const [proof, block] = await Promise.all([this.proof.settle(), this.block])

    if (this.manifest) proof.manifest = this.manifest
    if (!block && proof.block) return null

    if (block) proof.block.value = block
    return proof
  }
}

class Peer {
  constructor(replicator, protomux, channel, inflightRange) {
    this.core = replicator.core
    this.replicator = replicator
    this.stream = protomux.stream
    this.protomux = protomux
    this.remotePublicKey = this.stream.remotePublicKey
    this.remoteSupportsSeeks = false
    this.inflightRange = inflightRange
    this.remoteSegmentsWanted = new Set()
    this.fullyDownloadedSignaled = false

    this.paused = false
    this.removed = false

    this.channel = channel
    this.channel.userData = this

    this.wireSync = this.channel.messages[0]
    this.wireRequest = this.channel.messages[1]
    this.wireCancel = this.channel.messages[2]
    this.wireData = this.channel.messages[3]
    this.wireNoData = this.channel.messages[4]
    this.wireWant = this.channel.messages[5]
    this.wireUnwant = this.channel.messages[6]
    this.wireBitfield = this.channel.messages[7]
    this.wireRange = this.channel.messages[8]
    this.wireExtension = this.channel.messages[9]

    // Same stats as replicator, but for this specific peer
    this.stats = {
      wireSync: { tx: 0, rx: 0 },
      wireRequest: { tx: 0, rx: 0 },
      wireCancel: { tx: 0, rx: 0 },
      wireData: { tx: 0, rx: 0 },
      wireWant: { tx: 0, rx: 0 },
      wireBitfield: { tx: 0, rx: 0 },
      wireRange: { tx: 0, rx: 0 },
      wireExtension: { tx: 0, rx: 0 },
      hotswaps: 0,
      invalidData: 0,
      invalidRequests: 0,
      backoffs: 0
    }

    this.receiverQueue = new ReceiverQueue()
    this.receiverBusy = false

    // most often not used, so made on demand
    this.roundtripQueue = null

    this.inflight = 0
    this.dataProcessing = 0

    this.canUpgrade = true

    this.needsSync = false
    this.syncsProcessing = 0
    this.lastUpgradableLength = 0
    this.lastUpgradableFork = 0

    this._remoteContiguousLength = 0

    // TODO: tweak pipelining so that data sent BEFORE remoteOpened is not cap verified!
    // we might wanna tweak that with some crypto, ie use the cap to encrypt it...
    // or just be aware of that, to only push non leaky data

    this.remoteOpened = false
    this.remoteBitfield = new RemoteBitfield()
    this.missingBlocks = new RemoteBitfield()

    this.pushedLength = 0

    this.remoteFork = 0
    this.remoteLength = 0
    this.remoteCanUpgrade = false
    this.remoteUploading = true
    this.remoteDownloading = true
    this.remoteSynced = false
    this.remoteHasManifest = false
    this.remoteAllowPush = false
    this.remoteRequests = new Map()

    this.segmentsWanted = new Set()
    this.broadcastedNonSparse = false

    this.lengthAcked = 0

    this.extensions = new Map()
    this.lastExtensionSent = ''
    this.lastExtensionRecv = ''

    replicator._ifAvailable++
    replicator._active++
  }

  get remoteContiguousLength() {
    return this.remoteBitfield.findFirst(false, this._remoteContiguousLength)
  }

  getMaxInflight() {
    const stream = this.stream.rawStream
    if (!stream.udx) return Math.min(this.inflightRange[1], this.inflightRange[0] * 3)

    const scale =
      stream.rtt <= SCALE_LATENCY
        ? 1
        : (stream.rtt / SCALE_LATENCY) * Math.min(1, 2 / this.replicator.peers.length)
    return Math.max(
      this.inflightRange[0],
      Math.round(Math.min(this.inflightRange[1], this.inflightRange[0] * scale))
    )
  }

  getMaxHotswapInflight() {
    const inf = this.getMaxInflight()
    return Math.max(16, inf / 2)
  }

  signalUpgrade() {
    if (this.fullyDownloadedSignaled && !this.replicator.fullyDownloaded()) {
      this.fullyDownloadedSignaled = false
    }
    if (this._shouldUpdateCanUpgrade() === true) this._updateCanUpgradeAndSync()
    else this.sendSync()
  }

  _markInflight(index) {
    this.missingBlocks.set(index, false)
  }

  broadcastRange(start, length, drop) {
    if (!this.isActive()) return

    if (drop) this._unclearLocalRange(start, length)
    else this._clearLocalRange(start, length)

    const i = Math.floor(start / DEFAULT_SEGMENT_SIZE)
    const fullyContig = this.core.header.hints.contiguousLength === this.core.state.length

    if (
      start + LAST_BLOCKS < this.core.state.length &&
      !this.remoteSegmentsWanted.has(i) &&
      !drop &&
      !fullyContig
    ) {
      return
    }

    let force = false
    if (fullyContig && !drop) {
      start = 0
      length = this.core.state.length

      // Always send the broadcast when we switch from sparse to fully contiguous, so remote knows too
      if (!this.fullyDownloadedSignaled) {
        this.fullyDownloadedSignaled = true
        force = true
      }
    }

    // TODO: consider also adding early-returns on the drop===true case
    if (!force && !drop) {
      // No need to broadcast if the remote already has this range

      if (this._remoteContiguousLength >= start + length) return

      if (length === 1) {
        if (this.remoteBitfield.get(start)) return
      } else {
        if (this.remoteBitfield.firstUnset(start) >= start + length) return
      }
    }

    this.wireRange.send({
      drop,
      start,
      length
    })
    incrementTx(this.stats.wireRange, this.replicator.stats.wireRange)
  }

  extension(name, message) {
    this.wireExtension.send({ name: name === this.lastExtensionSent ? '' : name, message })
    incrementTx(this.stats.wireExtension, this.replicator.stats.wireExtension)
    this.lastExtensionSent = name
  }

  onextension(message) {
    const name = message.name || this.lastExtensionRecv
    this.lastExtensionRecv = name
    const ext = this.extensions.get(name)
    if (ext) {
      ext._onmessage({ start: 0, end: message.message.byteLength, buffer: message.message }, this)
    }
  }

  sendSync() {
    if (this.syncsProcessing !== 0) {
      this.needsSync = true
      return
    }

    if (this.core.state.fork !== this.remoteFork) {
      this.canUpgrade = false
    }

    this.needsSync = false

    this.wireSync.send({
      fork: this.core.state.fork,
      length: this.core.state.length,
      remoteLength: this.core.state.fork === this.remoteFork ? this.remoteLength : 0,
      canUpgrade: this.canUpgrade,
      uploading: true,
      downloading: this.replicator.isDownloading(),
      hasManifest: !!this.core.header.manifest && this.core.compat === false,
      allowPush: this.replicator.allowPush
    })
    incrementTx(this.stats.wireSync, this.replicator.stats.wireSync)
  }

  onopen({ seeks, capability }) {
    const expected = caps.replicate(
      this.stream.isInitiator === false,
      this.core.key,
      this.stream.handshakeHash
    )

    if (b4a.equals(capability, expected) !== true) {
      // TODO: change this to a rejection instead, less leakage
      throw INVALID_CAPABILITY('Remote sent an invalid replication capability')
    }

    if (this.remoteOpened === true) return
    this.remoteOpened = true
    this.remoteSupportsSeeks = seeks

    this.protomux.cork()

    this.sendSync()

    const contig = Math.min(this.core.state.length, this.core.header.hints.contiguousLength)
    if (contig > 0) {
      this.broadcastRange(0, contig, false)

      if (contig === this.core.state.length) {
        this.broadcastedNonSparse = true
      }
    }

    this.replicator._ifAvailable--
    this.replicator._addPeer(this)

    this.protomux.uncork()

    this.core.checkIfIdle()
  }

  onclose(isRemote) {
    // we might have signalled to the remote that we are done (ie not downloading) and the remote might agree on that
    // if that happens, the channel might be closed by the remote. if so just renegotiate it.
    // TODO: add a CLOSE_REASON to mux to we can make this cleaner...
    const reopen =
      isRemote === true &&
      this.remoteOpened === true &&
      this.remoteDownloading === false &&
      this.remoteUploading === true &&
      this.replicator.downloading === true

    if (this.remoteOpened === false) {
      this.replicator._ifAvailable--
      this.replicator.updateAll()
      return
    }

    this.remoteOpened = false
    this.removed = true
    this.remoteRequests.clear() // cancel all
    this.receiverQueue.clear()

    if (this.roundtripQueue !== null) {
      for (const id of this.roundtripQueue.clear()) this.replicator._inflight.reusable(id)
    }

    this.replicator._removePeer(this)

    if (reopen) {
      this.replicator._makePeer(this.protomux)
    }
  }

  closeIfIdle() {
    if (this.remoteDownloading === false && this.replicator.isDownloading() === false) {
      // idling, shut it down...
      this.channel.close()
      return true
    }

    return false
  }

  async onsync(msg) {
    const {
      fork,
      length,
      remoteLength,
      canUpgrade,
      uploading,
      downloading,
      hasManifest,
      allowPush
    } = msg

    const lengthChanged = length !== this.remoteLength
    const sameFork = fork === this.core.state.fork

    this.remoteSynced = true
    this.remoteFork = fork
    this.remoteLength = length
    this.remoteCanUpgrade = canUpgrade
    this.remoteUploading = uploading
    this.remoteDownloading = downloading
    this.remoteHasManifest = hasManifest
    this.remoteAllowPush = allowPush

    if (this.closeIfIdle()) return

    this.lengthAcked = sameFork ? remoteLength : 0
    this.syncsProcessing++

    this.replicator._updateFork(this)

    if (this.remoteLength > this.core.state.length && this.lengthAcked === this.core.state.length) {
      if (this.replicator._addUpgradeMaybe() !== null) this._update()
    }

    const upgrade =
      lengthChanged === false || sameFork === false
        ? this.canUpgrade && sameFork
        : await this._canUpgrade(length, fork)

    if (length === this.remoteLength && fork === this.core.state.fork) {
      this.canUpgrade = upgrade
      if (upgrade) {
        this.lastUpgradableFork = fork
        this.lastUpgradableLength = length
      }
    }

    if (--this.syncsProcessing !== 0) return // ie not latest

    if (
      this.needsSync === true ||
      (this.core.state.fork === this.remoteFork && this.core.state.length > this.remoteLength)
    ) {
      this.signalUpgrade()
    }

    this._update()
  }

  _shouldUpdateCanUpgrade() {
    return (
      this.core.state.fork === this.remoteFork &&
      this.core.state.length > this.remoteLength &&
      this.canUpgrade === false &&
      this.syncsProcessing === 0
    )
  }

  async _updateCanUpgradeAndSync() {
    const { length, fork } = this.core.state

    const remoteLength = this.remoteLength
    const remoteFork = this.remoteFork
    const canUpgrade = await this._canUpgrade(this.remoteLength, this.remoteFork)

    if (
      this.syncsProcessing > 0 ||
      length !== this.core.state.length ||
      fork !== this.core.state.fork
    ) {
      return
    }
    if (remoteLength !== this.remoteLength || remoteFork !== this.remoteFork) {
      return
    }
    if (canUpgrade === this.canUpgrade) {
      return
    }

    this.canUpgrade = canUpgrade
    if (canUpgrade) {
      this.lastUpgradableLength = this.remoteLength
      this.lastUpgradableFork = this.remoteFork
    }

    this.sendSync()
  }

  // Safe to call in the background - never fails
  async _canUpgrade(remoteLength, remoteFork) {
    if (remoteFork !== this.core.state.fork) return false

    if (remoteLength === 0) return true
    if (remoteLength >= this.core.state.length) return false

    try {
      // Rely on caching to make sure this is cheap...
      const canUpgrade = await MerkleTree.upgradeable(this.core.state, remoteLength)

      if (remoteFork !== this.core.state.fork) return false

      return canUpgrade
    } catch {
      return false
    }
  }

  async _getProof(batch, msg, pushing) {
    let block = null

    if (msg.block) {
      const index = msg.block.index

      if (!pushing && (msg.fork !== this.core.state.fork || !this.core.bitfield.get(index))) {
        return new ProofRequest(msg, null, null, null)
      }

      block = batch.getBlock(index)
      block.catch(noop)
    }

    const manifest = msg.manifest && !this.core.compat ? this.core.header.manifest : null

    try {
      const proof = await MerkleTree.proof(this.core.state, batch, msg)
      return new ProofRequest(msg, proof, block, manifest)
    } catch (err) {
      batch.destroy()

      this.replicator._oninvalidrequest(err, msg, this)

      if (this.stats.invalidRequests >= MAX_INVALID_REQUESTS) throw err

      await backoff(this.stats.invalidRequests)
      return null
    }
  }

  async onrequest(msg) {
    const size = this.remoteRequests.size
    this.remoteRequests.set(msg.id, msg)

    // if size didnt change -> id overwrite -> old one is deleted, cancel current and re-add
    if (size === this.remoteRequests.size) {
      this._cancel(msg.id)
      this.remoteRequests.set(msg.id, msg)
    }

    if (!this.protomux.drained || this.receiverQueue.length) {
      this.receiverQueue.push(msg)
      return
    }

    if (this.replicator.destroyed) return

    await this._handleRequest(msg)
  }

  oncancel(msg) {
    this._cancel(msg.request)
  }

  _cancel(id) {
    this.remoteRequests.delete(id)
    this.receiverQueue.delete(id)
  }

  ondrain() {
    return this._handleRequests()
  }

  async _handleRequests() {
    if (this.receiverBusy || this.replicator.destroyed) return
    this.receiverBusy = true
    this.protomux.cork()

    while (
      this.remoteOpened &&
      this.protomux.drained &&
      this.receiverQueue.length > 0 &&
      !this.removed
    ) {
      const msg = this.receiverQueue.shift()
      await this._handleRequest(msg)
    }

    this.protomux.uncork()
    this.receiverBusy = false
  }

  async push(index) {
    if (!this.remoteAllowPush) return
    if (this.core.state.fork !== this.remoteFork) return
    if (this.remoteBitfield.get(index)) return

    const msg = {
      id: 0,
      fork: this.core.state.fork,
      block: null,
      hash: null,
      seek: null,
      upgrade: null,
      manifest: this.remoteLength === 0,
      priority: 0
    }

    const remoteLength = Math.max(this.remoteLength, this.pushedLength)

    msg.block = {
      index,
      nodes: MerkleTree.maxMissingNodes(2 * index, remoteLength)
    }

    if (index >= this.remoteLength) {
      msg.upgrade = {
        start: remoteLength,
        length: this.core.state.length - remoteLength
      }
    }

    let req = null
    const batch = this.core.storage.read()
    try {
      req = await this._getProof(batch, msg, true)
    } catch {
      return
    }

    if (req === null) return
    batch.tryFlush()

    await this._fulfillRequest(req, true)
  }

  async _handleRequest(msg) {
    const batch = this.core.storage.read()

    // TODO: could still be answerable if (index, fork) is an ancestor of the current fork
    const req =
      msg.fork === this.core.state.fork
        ? await this._getProof(batch, msg, false)
        : new ProofRequest(msg, null, null, null)

    if (req === null) {
      this.wireNoData.send({ request: msg.id, reason: INVALID_REQUEST })
      return
    }

    batch.tryFlush()

    await this._fulfillRequest(req, false)
  }

  async _fulfillRequest(req, pushing) {
    const proof = await req.fulfill()

    if (!pushing) {
      // if cancelled do not reply
      if (this.remoteRequests.get(req.msg.id) !== req.msg) {
        return
      }

      // sync from now on, so safe to delete from the map
      this.remoteRequests.delete(req.msg.id)
    } else if (!this.remoteAllowPush) {
      // if pushing but remote disabled it, just drop it
      return
    }

    if (!this.isActive() && proof.block !== null) {
      return
    }

    if (proof === null) {
      if (req.msg.manifest && this.core.header.manifest) {
        const manifest = this.core.header.manifest
        this.wireData.send({
          request: req.msg.id,
          fork: this.core.state.fork,
          block: null,
          hash: null,
          seek: null,
          upgrade: null,
          manifest
        })
        incrementTx(this.stats.wireData, this.replicator.stats.wireData)
        return
      }

      this.wireNoData.send({ request: req.msg.id, reason: NOT_AVAILABLE })
      return
    }

    if (proof.block !== null) {
      this.replicator._onupload(proof.block.index, proof.block.value.byteLength, this)
    }

    if (proof.upgrade) {
      const remoteLength = proof.upgrade.start + proof.upgrade.length
      if (remoteLength > this.pushedLength) this.pushedLength = remoteLength
    }

    this.wireData.send({
      request: req.msg.id,
      fork: req.msg.fork,
      block: proof.block,
      hash: proof.hash,
      seek: proof.seek,
      upgrade: proof.upgrade,
      manifest: proof.manifest
    })
    incrementTx(this.stats.wireData, this.replicator.stats.wireData)
  }

  _cancelRequest(req) {
    if (req.priority === PRIORITY.CANCELLED) return
    // mark as cancelled also and avoid re-entry
    req.priority = PRIORITY.CANCELLED

    this.inflight--
    this.replicator._requestDone(req.id, false)

    // clear inflight state
    if (isBlockRequest(req)) this.replicator._unmarkInflight(req.block.index)
    if (isUpgradeRequest(req)) this.replicator._clearInflightUpgrade(req)

    if (this.roundtripQueue === null) this.roundtripQueue = new RoundtripQueue()
    this.roundtripQueue.add(req.id)
    this.wireCancel.send({ request: req.id })
    incrementTx(this.stats.wireCancel, this.replicator.stats.wireCancel)
  }

  _checkIfConflict() {
    this.paused = true

    const length = Math.min(this.core.state.length, this.remoteLength)
    if (length === 0) return // pause and ignore

    this.wireRequest.send({
      id: 0, // TODO: use an more explicit id for this eventually...
      fork: this.remoteFork,
      block: null,
      hash: null,
      seek: null,
      upgrade: {
        start: 0,
        length
      }
    })

    incrementTx(this.stats.wireRequest, this.replicator.stats.wireRequest)
  }

  _unmarkInflightBlockRequest(req, data) {
    if (isBlockRequest(req)) this.replicator._unmarkInflight(req.block.index)
    else if (data.block && !req) this.replicator._unmarkInflight(data.block.index)
  }

  async ondata(data) {
    if (data.request !== 0) return this._handleData(data)

    await this.replicator._pushLock.lock()
    try {
      await this._handleData(data)
    } finally {
      this.replicator._pushLock.unlock()
    }
  }

  async _handleData(data) {
    // always allow a fork conflict proof to be sent
    if (data.request === 0 && data.upgrade && data.upgrade.start === 0) {
      if (await this.core.checkConflict(data, this)) return
      this.paused = false
    }

    const req = data.request > 0 ? this.replicator._inflight.get(data.request) : null
    const reorg = data.fork > this.core.state.fork

    // no push atm, TODO: check if this satisfies another pending request
    // allow reorg pushes tho as those are not written to storage so we'll take all the help we can get
    if (req === null && reorg === false && !this.replicator.allowPush) {
      return
    }

    if (req === null && reorg === false && data.block) {
      // mark this as inflight to avoid parallel requests
      this.replicator._markInflight(data.block.index)
    }

    if (req !== null) {
      if (req.peer !== this) return
      this._onrequestroundtrip(req)
    }

    try {
      if (reorg === true) return await this.replicator._onreorgdata(this, req, data)
    } catch (err) {
      safetyCatch(err)
      this._unmarkInflightBlockRequest(req, data)

      this.paused = true
      this.replicator._oninvaliddata(err, req, data, this)
      return
    }

    this.dataProcessing++
    if (isBlockRequest(req)) this.replicator._markProcessing(req.block.index)

    try {
      if (!this.replicator._matchingRequest(req, data) || !(await this.core.verify(data, this))) {
        this.replicator._onnodata(this, req, 0)
        return
      }
    } catch (err) {
      safetyCatch(err)
      this._unmarkInflightBlockRequest(req, data)

      if (err.code === 'WRITE_FAILED') {
        // For example, we don't want to keep pulling data when storage is full
        // TODO: notify the user somehow
        this.paused = true
        return
      }

      if (this.core.closed && !isCriticalError(err)) return

      if (err.code !== 'INVALID_OPERATION') {
        // might be a fork, verify
        this._checkIfConflict()
      }

      // if push mode, we MIGHT get an occasional bad message
      // no need to mega bail for that. TODO: rate limit instead as a general thing
      if (!this.replicator.allowPush) {
        this.paused = true
      }

      this.replicator._onnodata(this, req, 0)
      this.replicator._oninvaliddata(err, req, data, this)
      return
    } finally {
      if (isBlockRequest(req)) this.replicator._markProcessed(req.block.index)
      this.dataProcessing--
    }

    this.replicator._ondata(this, req, data)

    if (this._shouldUpdateCanUpgrade() === true) {
      this._updateCanUpgradeAndSync()
    }
  }

  onnodata({ request, reason }) {
    const req = request > 0 ? this.replicator._inflight.get(request) : null

    if (req === null || req.peer !== this) {
      this.replicator.updateAll()
      return
    }

    this._onrequestroundtrip(req)
    this.replicator._onnodata(this, req, reason)
  }

  _onrequestroundtrip(req) {
    if (req.priority === PRIORITY.CANCELLED) return
    // to avoid re-entry we also just mark it as cancelled
    req.priority = PRIORITY.CANCELLED

    this.inflight--
    this.replicator._requestDone(req.id, true)
    if (this.roundtripQueue === null) return
    const flushed = this.roundtripQueue.flush(req.rt)
    if (flushed === null) return
    for (const id of flushed) this.replicator._inflight.reusable(id)
  }

  onwant({ start, length }) {
    const i = Math.floor(start / DEFAULT_SEGMENT_SIZE)
    if (this.remoteSegmentsWanted.size >= MAX_REMOTE_SEGMENTS) this.remoteSegmentsWanted.clear() // just in case of abuse
    this.remoteSegmentsWanted.add(i)
    this.replicator._onwant(this, start, length)
  }

  onunwant() {
    // TODO
  }

  onbitfield({ start, bitfield }) {
    if (start < this._remoteContiguousLength) this._remoteContiguousLength = start // bitfield is always the truth
    this.remoteBitfield.insert(start, bitfield)
    this.missingBlocks.insert(start, bitfield)
    this._clearLocalRange(start, bitfield.byteLength * 8)
    this._update()
  }

  _clearLocalRange(start, length) {
    const bitfield = this.core.skipBitfield === null ? this.core.bitfield : this.core.skipBitfield

    if (length === 1) {
      this.missingBlocks.set(start, this._remoteHasBlock(start) && !bitfield.get(start))
      return
    }

    const contig = Math.min(this.core.state.length, this.core.header.hints.contiguousLength)

    if (start + length < contig) {
      this.missingBlocks.setRange(start, contig, false)
      return
    }

    const rem = start & 32767
    if (rem > 0) {
      start -= rem
      length += rem
    }

    const end = start + Math.min(length, this.core.state.length)
    while (start < end) {
      const local = bitfield.getBitfield(start)

      if (local && local.bitfield) {
        this.missingBlocks.clear(start, local.bitfield)
      }

      start += 32768
    }
  }

  _resetMissingBlock(index) {
    const bitfield = this.core.skipBitfield === null ? this.core.bitfield : this.core.skipBitfield
    this.missingBlocks.set(index, this._remoteHasBlock(index) && !bitfield.get(index))
  }

  _unclearLocalRange(start, length) {
    if (length === 1) {
      this._resetMissingBlock(start)
      return
    }

    const rem = start & 2097151
    if (rem > 0) {
      start -= rem
      length += rem
    }

    const fixedStart = start

    const end = start + Math.min(length, this.remoteLength)
    while (start < end) {
      const remote = this.remoteBitfield.getBitfield(start)
      if (remote && remote.bitfield) {
        this.missingBlocks.insert(start, remote.bitfield)
      }

      start += 2097152
    }

    this._clearLocalRange(fixedStart, length)
  }

  async onrange({ drop, start, length }) {
    const has = drop === false

    if (drop === true && start < this._remoteContiguousLength) {
      this._remoteContiguousLength = start
    }

    if (start === 0 && drop === false) {
      if (length > this._remoteContiguousLength) {
        this._remoteContiguousLength = length
        if (
          this.remoteFork === this.core.state.fork &&
          length > this.core.header.hints.remoteContiguousLength
        ) {
          this.core.updateRemoteContiguousLength(length)
        }
      }
    } else if (length === 1) {
      const bitfield = this.core.skipBitfield === null ? this.core.bitfield : this.core.skipBitfield
      this.remoteBitfield.set(start, has)
      this.missingBlocks.set(start, has && !bitfield.get(start))
    } else {
      const rangeStart = this.remoteBitfield.findFirst(!has, start)
      const rangeEnd = length + start

      if (rangeStart !== -1 && rangeStart < rangeEnd) {
        this.remoteBitfield.setRange(rangeStart, rangeEnd, has)
        this.missingBlocks.setRange(rangeStart, rangeEnd, has)
        if (has) this._clearLocalRange(rangeStart, rangeEnd - rangeStart)
      }
    }

    if (this.core.hintsChanged) await this.core.flushHints()
    if (drop === false) this._update()
  }

  onreorghint() {
    // TODO
  }

  _update() {
    // TODO: if this is in a batch or similar it would be better to defer it
    // we could do that with nextTick/microtick mb? (combined with a property on the session to signal read buffer mb)
    this.replicator.updatePeer(this)
  }

  async _onconflict() {
    this.protomux.cork()
    if (this.remoteLength > 0 && this.core.state.fork === this.remoteFork) {
      await this.onrequest({
        id: 0,
        fork: this.core.state.fork,
        block: null,
        hash: null,
        seek: null,
        upgrade: {
          start: 0,
          length: Math.min(this.core.state.length, this.remoteLength)
        }
      })
    }
    this.channel.close()
    this.protomux.uncork()
  }

  _makeRequest(needsUpgrade, priority, minLength) {
    if (needsUpgrade === true && this.replicator._shouldUpgrade(this) === false) {
      return null
    }

    // ensure that the remote has signalled they have the length we request
    if (this.remoteLength < minLength) {
      return null
    }

    if (needsUpgrade === false && this.replicator._autoUpgrade(this) === true) {
      needsUpgrade = true
    }

    return {
      peer: this,
      rt: this.roundtripQueue === null ? 0 : this.roundtripQueue.tick,
      id: 0,
      fork: this.remoteFork,
      block: null,
      hash: null,
      seek: null,
      upgrade:
        needsUpgrade === false
          ? null
          : { start: this.core.state.length, length: this.remoteLength - this.core.state.length },
      // remote manifest check can be removed eventually...
      manifest: this.core.header.manifest === null && this.remoteHasManifest === true,
      priority,
      timestamp: Date.now(),
      elapsed: 0
    }
  }

  _requestManifest() {
    const req = this._makeRequest(false, 0, 0)
    this._send(req)
  }

  _includeLastBlock() {
    if (this.replicator._alwaysLatestBlock === 0) return null

    const index = this.remoteLength - 1

    // only valid if its an OOB get
    if (index < this.core.state.length) return null

    // do the normal checks
    if (!this._remoteHasBlock(index)) return null
    if (!this._canRequest(index)) return null

    // atm we only ever do one upgrade request in parallel, if that changes
    // then we should add some check here for inflights to avoid over requesting
    const b = this.replicator._blocks.add(index, PRIORITY.NORMAL)
    return b
  }

  _requestUpgrade(u) {
    const req = this._makeRequest(true, 0, 0)
    if (req === null) return false

    const b = this._includeLastBlock()

    if (b) this._sendBlockRequest(req, b)
    else this._send(req)

    return true
  }

  _requestSeek(s) {
    // if replicator is updating the seeks etc, bail and wait for it to drain
    if (this.replicator._updatesPending > 0) return false

    const { length, fork } = this.core.state

    if (fork !== this.remoteFork) return false

    if (s.seeker.start >= length) {
      const req = this._makeRequest(true, 0, 0)

      // We need an upgrade for the seek, if non can be provided, skip
      if (req === null) return false

      req.seek = this.remoteSupportsSeeks
        ? { bytes: s.seeker.bytes, padding: s.seeker.padding }
        : null

      s.inflight.push(req)
      this._send(req)

      return true
    }

    const len = s.seeker.end - s.seeker.start
    const off = s.seeker.start + Math.floor(Math.random() * len)

    for (let i = 0; i < len; i++) {
      let index = off + i
      if (index > s.seeker.end) index -= len

      if (this._remoteHasBlock(index) === false) continue
      if (this.core.bitfield.get(index) === true) continue
      if (!this._canRequest(index)) continue

      // Check if this block is currently inflight - if so pick another
      const b = this.replicator._blocks.get(index)
      if (b !== null && b.inflight.length > 0) continue

      // Block is not inflight, but we only want the hash, check if that is inflight
      const h = this.replicator._hashes.add(index, PRIORITY.NORMAL)
      if (h.inflight.length > 0) continue

      const req = this._makeRequest(false, h.priority, index + 1)
      if (req === null) continue

      const nodes = flatTree.depth(s.seeker.start + s.seeker.end - 1)

      req.hash = { index: 2 * index, nodes }
      req.seek = this.remoteSupportsSeeks
        ? { bytes: s.seeker.bytes, padding: s.seeker.padding }
        : null

      s.inflight.push(req)
      h.inflight.push(req)
      this._send(req)

      return true
    }

    this._maybeWant(s.seeker.start, len)
    return false
  }

  _canRequest(index) {
    if (!(index >= 0)) throw ASSERTION('bad index to _canRequest: ' + index)

    if (this.remoteLength >= this.core.state.length) {
      return true
    }

    if (index < this.lastUpgradableLength && this.lastUpgradableFork === this.core.state.fork) {
      return true
    }

    if (this.canUpgrade && this.syncsProcessing === 0) {
      return true
    }

    return false
  }

  _remoteHasBlock(index) {
    return index < this._remoteContiguousLength || this.remoteBitfield.get(index) === true
  }

  _sendBlockRequest(req, b) {
    req.block = { index: b.index, nodes: 0 }
    this.replicator._markInflight(b.index)

    b.inflight.push(req)
    this.replicator.hotswaps.add(b)
    this._send(req)
  }

  _requestBlock(b) {
    const { length, fork } = this.core.state

    if (this._remoteHasBlock(b.index) === false || fork !== this.remoteFork) {
      this._maybeWant(b.index)
      return false
    }

    if (!this._canRequest(b.index)) return false

    const req = this._makeRequest(b.index >= length, b.priority, b.index + 1)
    if (req === null) return false

    this._sendBlockRequest(req, b)

    return true
  }

  _requestRangeBlock(index, length) {
    if (this.core.bitfield.get(index) === true || !this._canRequest(index)) return false

    const b = this.replicator._blocks.add(index, PRIORITY.NORMAL)
    if (b.inflight.length > 0) {
      this.missingBlocks.set(index, false) // in case we missed some states just set them ondemand, nbd
      return false
    }

    const req = this._makeRequest(index >= length, b.priority, index + 1)

    // If the request cannot be satisfied, dealloc the block request if no one is subscribed to it
    if (req === null) {
      b.gc()
      return false
    }

    this._sendBlockRequest(req, b)

    // Don't think this will ever happen, as the pending queue is drained before the range queue
    // but doesn't hurt to check this explicitly here also.
    if (b.queued) b.queued = false
    return true
  }

  _findNext(i) {
    if (i < this._remoteContiguousLength) {
      if (this.core.skipBitfield === null) this.replicator._openSkipBitfield()
      i = this.core.skipBitfield.findFirst(false, i)
      if (i < this._remoteContiguousLength && i > -1) return i
      i = this._remoteContiguousLength
    }

    return this.missingBlocks.findFirst(true, i)
  }

  _requestRange(r) {
    if (this.syncsProcessing > 0) return false

    const { length, fork } = this.core.state

    if (r.blocks) {
      let min = -1
      let max = -1

      for (let i = r.start; i < r.end; i++) {
        const index = r.blocks[i]
        if (min === -1 || index < min) min = index
        if (max === -1 || index > max) max = index
        const has = index < this._remoteContiguousLength || this.missingBlocks.get(index) === true
        if (has === true && this._requestRangeBlock(index, length)) return true
      }

      if (min > -1) this._maybeWant(min, max - min)
      return false
    }

    // if we can upgrade the remote, or the remote is ahead, then all the remotes blocks are valid
    // otherwise truncate to the last length the remote has acked for us
    const maxLocalLength =
      this.canUpgrade || this.remoteLength >= this.core.state.length
        ? this.core.state.length
        : fork === this.lastUpgradableFork
          ? Math.min(this.lastUpgradableLength, this.core.state.length)
          : 0

    const end = Math.min(
      maxLocalLength,
      Math.min(r.end === -1 ? this.remoteLength : r.end, this.remoteLength)
    )
    if (end <= r.start || fork !== this.remoteFork) return false

    const len = end - r.start
    const off = r.start + (r.linear ? 0 : Math.floor(Math.random() * len))

    let i = off
    // should be way less than this, but this is worst case upper bound for the skiplist
    let tries = this.inflight

    do {
      i = this._findNext(i)
      if (i === -1 || i >= end) break

      if (this._requestRangeBlock(i, length)) return true
      i++
    } while (tries-- > 0)

    i = r.start

    do {
      i = this._findNext(i)
      if (i === -1 || i >= off) break

      if (this._requestRangeBlock(i, length)) return true
      i++
    } while (tries-- > 0)

    this._maybeWant(r.start, len)
    return false
  }

  _requestForkProof(f) {
    if (!this.remoteLength) return

    const req = this._makeRequest(false, 0, 0)

    req.upgrade = { start: 0, length: this.remoteLength }
    req.manifest = !this.core.header.manifest

    f.inflight.push(req)
    this._send(req)
  }

  _requestForkRange(f) {
    if (f.fork !== this.remoteFork || f.batch.want === null) return false

    const end = Math.min(f.batch.want.end, this.remoteLength)
    if (end < f.batch.want.start) return false

    const len = end - f.batch.want.start
    const off = f.batch.want.start + Math.floor(Math.random() * len)

    for (let i = 0; i < len; i++) {
      let index = off + i
      if (index >= end) index -= len

      if (this._remoteHasBlock(index) === false) continue

      const req = this._makeRequest(false, 0, 0)

      req.hash = { index: 2 * index, nodes: f.batch.want.nodes }

      f.inflight.push(req)
      this._send(req)

      return true
    }

    this._maybeWant(f.batch.want.start, len)
    return false
  }

  _maybeWant(start, length = 1) {
    if (start + length <= this.remoteContiguousLength) return

    let i = Math.floor(start / DEFAULT_SEGMENT_SIZE)
    const n = Math.ceil((start + length) / DEFAULT_SEGMENT_SIZE)

    for (; i < n; i++) {
      if (this.segmentsWanted.has(i)) continue
      this.segmentsWanted.add(i)

      this.wireWant.send({
        start: i * DEFAULT_SEGMENT_SIZE,
        length: DEFAULT_SEGMENT_SIZE
      })
      incrementTx(this.stats.wireWant, this.replicator.stats.wireWant)
    }
  }

  isActive() {
    if (this.paused || this.removed || this.core.header.frozen) return false
    return true
  }

  async _send(req) {
    const fork = this.core.state.fork

    this.inflight++
    this.replicator._inflight.add(req)

    if (req.upgrade !== null && req.fork === fork) {
      const u = this.replicator._addUpgrade()
      u.inflight.push(req)
    }

    try {
      if (req.block !== null && req.fork === fork) {
        req.block.nodes = await MerkleTree.missingNodes(
          this.core.state,
          2 * req.block.index,
          this.core.state.length
        )
        if (req.priority === PRIORITY.CANCELLED) return
      }
      if (req.hash !== null && req.fork === fork && req.hash.nodes === 0) {
        req.hash.nodes = await MerkleTree.missingNodes(
          this.core.state,
          req.hash.index,
          this.core.state.length
        )
        if (req.priority === PRIORITY.CANCELLED) return

        // nodes === 0, we already have it, bail
        if (req.hash.nodes === 0 && (req.hash.index & 1) === 0) {
          this.inflight--
          this.replicator._resolveHashLocally(this, req)
          return
        }
      }
    } catch (err) {
      this.stream.destroy(err)
      return
    }

    this.wireRequest.send(req)
    incrementTx(this.stats.wireRequest, this.replicator.stats.wireRequest)
  }
}

module.exports = class Replicator {
  static Peer = Peer // hack to be able to access Peer from outside this module

  constructor(
    core,
    {
      notDownloadingLinger = NOT_DOWNLOADING_SLACK,
      eagerUpgrade = true,
      allowFork = true,
      allowPush = false,
      alwaysLatestBlock = false,
      inflightRange = null
    } = {}
  ) {
    this.core = core
    this.eagerUpgrade = eagerUpgrade
    this.allowFork = allowFork
    this.allowPush = allowPush
    this.ondownloading = null // optional external hook for monitoring downloading status
    this.peers = []
    this.findingPeers = 0 // updatable from the outside
    this.destroyed = false
    this.downloading = false
    this.activeSessions = 0

    this.hotswaps = new HotswapQueue()
    this.inflightRange = inflightRange || DEFAULT_MAX_INFLIGHT

    // Note: nodata and unwant not currently tracked
    // tx = transmitted, rx = received
    this.stats = {
      wireSync: { tx: 0, rx: 0 },
      wireRequest: { tx: 0, rx: 0 },
      wireCancel: { tx: 0, rx: 0 },
      wireData: { tx: 0, rx: 0 },
      wireWant: { tx: 0, rx: 0 },
      wireBitfield: { tx: 0, rx: 0 },
      wireRange: { tx: 0, rx: 0 },
      wireExtension: { tx: 0, rx: 0 },
      hotswaps: 0,
      invalidData: 0,
      invalidRequests: 0,
      backoffs: 0
    }

    this._attached = new Set()
    this._inflight = new InflightTracker()
    this._blocks = new BlockTracker(this)
    this._hashes = new BlockTracker(this)

    this._alwaysLatestBlock = alwaysLatestBlock ? 1 : 0
    this._queued = []

    this._seeks = []
    this._upgrade = null
    this._reorgs = []
    this._ranges = []

    this._pushLock = new Mutex()
    this._hadPeers = false
    this._active = 0
    this._ifAvailable = 0
    this._updatesPending = 0
    this._applyingReorg = null
    this._manifestPeer = null
    this._notDownloadingLinger = notDownloadingLinger
    this._downloadingTimer = null

    const self = this
    this._onstreamclose = onstreamclose

    function onstreamclose() {
      self.detachFrom(this.userData)
    }
  }

  updateActivity(inc, session) {
    this.activeSessions += inc
    this.setDownloading(this.activeSessions !== 0, session)
  }

  isDownloading() {
    return this.downloading || !this._inflight.idle
  }

  async push(index) {
    const all = []
    for (const peer of this.peers) {
      all.push(peer.push(index))
    }
    await Promise.all(all)
  }

  setAllowPush(allowPush) {
    if (allowPush === this.allowPush) return
    this.allowPush = allowPush
    for (const peer of this.peers) peer.sendSync()
  }

  setDownloading(downloading) {
    clearTimeout(this._downloadingTimer)

    if (this.destroyed) return
    if (downloading || this._notDownloadingLinger === 0) {
      this.setDownloadingNow(downloading)
      return
    }

    this._downloadingTimer = setTimeout(
      setDownloadingLater,
      this._notDownloadingLinger,
      this,
      downloading
    )
    if (this._downloadingTimer.unref) this._downloadingTimer.unref()
  }

  setDownloadingNow(downloading) {
    this._downloadingTimer = null
    if (this.downloading === downloading) return
    this.downloading = downloading
    if (!downloading && this.isDownloading()) return

    for (const peer of this.peers) peer.signalUpgrade()

    if (downloading) {
      // restart channel if needed...
      for (const protomux of this._attached) {
        if (!protomux.stream.handshakeHash) continue
        if (protomux.opened({ protocol: 'hypercore/alpha', id: this.core.discoveryKey })) continue
        this._makePeer(protomux, true)
      }
    } else {
      for (const peer of this.peers) peer.closeIfIdle()
    }

    if (this.ondownloading !== null && downloading) this.ondownloading()
  }

  cork() {
    for (const peer of this.peers) peer.protomux.cork()
  }

  uncork() {
    for (const peer of this.peers) peer.protomux.uncork()
  }

  // Called externally when a range of new blocks has been processed/removed
  onhave(start, length, drop = false) {
    for (const peer of this.peers) peer.broadcastRange(start, length, drop)
  }

  // Called externally when a truncation upgrade has been processed
  ontruncate(newLength, truncated) {
    const notify = []

    for (const blk of this._blocks) {
      if (blk.index < newLength) continue
      notify.push(blk)
    }

    for (const blk of notify) {
      for (const r of blk.refs) {
        if (r.snapshot === false) continue
        blk.detach(r, SNAPSHOT_NOT_AVAILABLE())
      }
    }

    for (const peer of this.peers) peer._unclearLocalRange(newLength, truncated)
  }

  // Called externally when a upgrade has been processed
  onupgrade() {
    for (const peer of this.peers) peer.signalUpgrade()
    if (this._blocks.isEmpty() === false) this._resolveBlocksLocally()
    if (this._upgrade !== null) this._resolveUpgradeRequest(null)
    if (!this._blocks.isEmpty() || this._ranges.length !== 0 || this._seeks.length !== 0) {
      this._updateNonPrimary(true)
    }
  }

  // Called externally when a conflict has been detected and verified
  async onconflict() {
    const all = []
    for (const peer of this.peers) {
      all.push(peer._onconflict())
    }
    await Promise.allSettled(all)
  }

  async applyPendingReorg() {
    if (this._applyingReorg !== null) {
      await this._applyingReorg
      return true
    }

    for (let i = this._reorgs.length - 1; i >= 0; i--) {
      const f = this._reorgs[i]
      if (f.batch !== null && f.batch.finished) {
        await this._applyReorg(f)
        return true
      }
    }

    return false
  }

  addUpgrade(session) {
    if (this._upgrade !== null) {
      const ref = this._upgrade.attach(session)
      this._checkUpgradeIfAvailable()
      return ref
    }

    const ref = this._addUpgrade().attach(session)

    this.updateAll()

    return ref
  }

  addBlock(session, index) {
    const b = this._blocks.add(index, PRIORITY.HIGH)
    const ref = b.attach(session)

    this._queueBlock(b)
    this.updateAll()

    return ref
  }

  addSeek(session, seeker) {
    const s = new SeekRequest(this, this._seeks, seeker)
    const ref = s.attach(session)

    this._seeks.push(s)
    this.updateAll()

    return ref
  }

  addRange(
    session,
    {
      start = 0,
      end = -1,
      length = toLength(start, end),
      blocks = null,
      linear = false,
      ifAvailable = false
    } = {}
  ) {
    if (blocks !== null) {
      // if using blocks, start, end just acts as frames around the blocks array
      start = 0
      end = length = blocks.length
    }

    const r = new RangeRequest(
      this,
      this._ranges,
      start,
      length === -1 ? -1 : start + length,
      linear,
      ifAvailable,
      blocks
    )

    const ref = r.attach(session)

    // Trigger this to see if this is already resolved...
    // Also auto compresses the range based on local bitfield
    clampRange(this.core, r)

    if (r.end !== -1 && r.start >= r.end) {
      this._resolveRangeRequest(r)
      return ref
    }

    this.updateAll()

    return ref
  }

  cancel(ref) {
    ref.context.detach(ref, null)
  }

  static clearRequests(session, err) {
    if (session.length === 0) return

    const updated = new Set()

    while (session.length > 0) {
      const ref = session[session.length - 1]
      updated.add(ref.context.replicator)
      ref.context.detach(ref, err)
    }

    for (const replicator of updated) {
      replicator.updateAll()
    }
  }

  clearRequests(session, err = null) {
    let cleared = false
    while (session.length > 0) {
      const ref = session[session.length - 1]
      ref.context.detach(ref, err)
      cleared = true
    }

    if (cleared) this.updateAll()
  }

  _matchingRequest(req, data) {
    if (this.allowPush) {
      return true
    }
    if (data.block !== null && (req.block === null || req.block.index !== data.block.index)) {
      return false
    }
    if (data.hash !== null && (req.hash === null || req.hash.index !== data.hash.index)) {
      return false
    }
    if (data.seek !== null && (req.seek === null || req.seek.bytes !== data.seek.bytes)) {
      return false
    }
    if (data.upgrade !== null && req.upgrade === null) {
      return false
    }
    return req.fork === data.fork
  }

  _addUpgradeMaybe() {
    return this.eagerUpgrade === true ? this._addUpgrade() : this._upgrade
  }

  // TODO: this function is OVER called atm, at each updatePeer/updateAll
  // instead its more efficient to only call it when the conditions in here change - ie on sync/add/remove peer
  // Do this when we have more tests.
  _checkUpgradeIfAvailable() {
    if (this._ifAvailable > 0 && this.peers.length < MAX_PEERS_UPGRADE) return
    if (this._upgrade === null || this._upgrade.refs.length === 0) return
    if (this._hadPeers === false && this.findingPeers > 0) return

    const maxPeers = Math.min(this.peers.length, MAX_PEERS_UPGRADE)

    // check if a peer can upgrade us

    for (let i = 0; i < maxPeers; i++) {
      const peer = this.peers[i]

      if (peer.remoteSynced === false) return

      if (this.core.state.length === 0 && peer.remoteLength > 0) return

      if (peer.remoteLength <= this._upgrade.length || peer.remoteFork !== this._upgrade.fork) {
        continue
      }

      if (peer.syncsProcessing > 0) return

      if (peer.lengthAcked !== this.core.state.length && peer.remoteFork === this.core.state.fork) {
        return
      }

      if (peer.remoteCanUpgrade === true) return
    }

    // check if reorgs in progress...

    if (this._applyingReorg !== null) return

    // TODO: we prob should NOT wait for inflight reorgs here, seems better to just resolve the upgrade
    // and then apply the reorg on the next call in case it's slow - needs some testing in practice

    for (let i = 0; i < this._reorgs.length; i++) {
      const r = this._reorgs[i]
      if (r.inflight.length > 0) return
    }

    // if something is inflight, wait for that first
    if (this._upgrade.inflight.length > 0) return

    // nothing to do, indicate no update avail

    const u = this._upgrade
    this._upgrade = null
    u.resolve(false)
  }

  _addUpgrade() {
    if (this._upgrade !== null) return this._upgrade

    // TODO: needs a reorg: true/false flag to indicate if the user requested a reorg
    this._upgrade = new UpgradeRequest(this, this.core.state.fork, this.core.state.length)

    return this._upgrade
  }

  _addReorg(fork, peer) {
    if (this.allowFork === false) return null

    // TODO: eager gc old reorgs from the same peer
    // not super important because they'll get gc'ed when the request finishes
    // but just spam the remote can do ...

    for (const f of this._reorgs) {
      if (f.fork > fork && f.batch !== null) return null
      if (f.fork === fork) return f
    }

    const f = {
      fork,
      inflight: [],
      batch: null
    }

    this._reorgs.push(f)

    // maintain sorted by fork
    let i = this._reorgs.length - 1
    while (i > 0 && this._reorgs[i - 1].fork > fork) {
      this._reorgs[i] = this._reorgs[i - 1]
      this._reorgs[--i] = f
    }

    return f
  }

  _shouldUpgrade(peer) {
    if (this._upgrade !== null && this._upgrade.inflight.length > 0) return false
    return (
      peer.remoteCanUpgrade === true &&
      peer.remoteLength > this.core.state.length &&
      peer.lengthAcked === this.core.state.length
    )
  }

  _autoUpgrade(peer) {
    return (
      this._upgrade !== null &&
      peer.remoteFork === this.core.state.fork &&
      this._shouldUpgrade(peer)
    )
  }

  _addPeer(peer) {
    this._hadPeers = true
    this.peers.push(peer)
    this.updatePeer(peer)
    this._onpeerupdate(true, peer)
  }

  _requestDone(id, roundtrip) {
    this._inflight.remove(id, roundtrip)
    if (this.isDownloading() === true) return
    for (const peer of this.peers) peer.signalUpgrade()
  }

  _removePeer(peer) {
    this.peers.splice(this.peers.indexOf(peer), 1)

    if (this._manifestPeer === peer) this._manifestPeer = null

    for (const req of this._inflight) {
      if (req.peer !== peer) continue
      this._inflight.remove(req.id, true)
      this._clearRequest(peer, req)
    }

    this._onpeerupdate(false, peer)
    this.updateAll()
  }

  _queueBlock(b) {
    if (b.inflight.length > 0 || b.queued === true) return
    b.queued = true
    this._queued.push(b)
  }

  _resolveHashLocally(peer, req) {
    this._requestDone(req.id, false)
    this._resolveBlockRequest(this._hashes, req.hash.index / 2, null, req)
    this.updatePeer(peer)
  }

  // Runs in the background - not allowed to throw
  async _resolveBlocksLocally() {
    // TODO: check if fork compat etc. Requires that we pass down truncation info

    const clear = []
    const blocks = []

    const reader = this.core.storage.read()
    for (const b of this._blocks) {
      if (this.core.bitfield.get(b.index) === false) continue
      blocks.push(this._resolveLocalBlock(b, reader, clear))
    }
    reader.tryFlush()

    await Promise.all(blocks)

    if (!clear.length) return

    // Currently the block tracker does not support deletes during iteration, so we make
    // sure to clear them afterwards.
    for (const b of clear) {
      this._blocks.remove(b.index)
      removeHotswap(b)
    }
  }

  async _resolveLocalBlock(b, reader, resolved) {
    try {
      b.resolve(await reader.getBlock(b.index))
    } catch (err) {
      b.reject(err)
      return
    }

    resolved.push(b)
  }

  _resolveBlockRequest(tracker, index, value, req) {
    const b = tracker.remove(index)
    if (b === null) return false

    removeInflight(b.inflight, req)
    removeHotswap(b)
    b.queued = false

    b.resolve(value)

    if (b.inflight.length > 0) {
      // if anything is still inflight, cancel it
      for (let i = b.inflight.length - 1; i >= 0; i--) {
        const req = b.inflight[i]
        req.peer._cancelRequest(req)
      }
    }

    return true
  }

  _resolveUpgradeRequest(req) {
    if (req !== null) removeInflight(this._upgrade.inflight, req)

    if (
      this.core.state.length === this._upgrade.length &&
      this.core.state.fork === this._upgrade.fork
    ) {
      return false
    }

    const u = this._upgrade
    this._upgrade = null
    u.resolve(true)

    return true
  }

  _resolveRangeRequest(req) {
    req.resolve(true)
    req.gc()
  }

  _clearInflightBlock(tracker, req) {
    const isBlock = tracker === this._blocks
    const index = isBlock === true ? req.block.index : req.hash.index / 2
    const b = tracker.get(index)

    if (b === null || removeInflight(b.inflight, req) === false) return

    if (removeHotswap(b) === true && b.inflight.length > 0) {
      this.hotswaps.add(b)
    }

    if (b.refs.length > 0 && isBlock === true) {
      this._queueBlock(b)
      return
    }

    b.gc()
  }

  _clearInflightUpgrade(req) {
    if (this._upgrade === null || removeInflight(this._upgrade.inflight, req) === false) return
    this._upgrade.gc()
  }

  _clearInflightSeeks(req) {
    for (const s of this._seeks) {
      if (removeInflight(s.inflight, req) === false) continue
      s.gc()
    }
  }

  _clearInflightReorgs(req) {
    for (const r of this._reorgs) {
      removeInflight(r.inflight, req)
    }
  }

  _clearOldReorgs(fork) {
    for (let i = 0; i < this._reorgs.length; i++) {
      const f = this._reorgs[i]
      if (f.fork >= fork) continue
      if (i === this._reorgs.length - 1) this._reorgs.pop()
      else this._reorgs[i] = this._reorgs.pop()
      i--
    }
  }

  // "slow" updates here - async but not allowed to ever throw
  async _updateNonPrimary(updateAll) {
    // Check if running, if so skip it and the running one will issue another update for us (debounce)
    while (++this._updatesPending === 1) {
      let len = Math.min(MAX_RANGES, this._ranges.length)

      for (let i = 0; i < len; i++) {
        const r = this._ranges[i]

        clampRange(this.core, r)

        if (r.end !== -1 && r.start >= r.end) {
          this._resolveRangeRequest(r)
          i--
          if (len > this._ranges.length) len--
          if (this._ranges.length === MAX_RANGES) updateAll = true
        }
      }

      for (let i = 0; i < this._seeks.length; i++) {
        const s = this._seeks[i]

        let err = null
        let res = null

        try {
          res = await s.seeker.update()
        } catch (error) {
          err = error
        }

        if (!res && !err) continue

        if (i < this._seeks.length - 1) this._seeks[i] = this._seeks.pop()
        else this._seeks.pop()

        i--

        if (err) s.reject(err)
        else s.resolve(res)
      }

      // No additional updates scheduled - break
      if (--this._updatesPending === 0) break
      // Debounce the additional updates - continue
      this._updatesPending = 0
    }

    if (this._inflight.idle || updateAll) this.updateAll()
  }

  _maybeResolveIfAvailableRanges() {
    if (this._ifAvailable > 0 || !this._inflight.idle || !this._ranges.length) return

    for (let i = 0; i < this.peers.length; i++) {
      if (this.peers[i].dataProcessing > 0) return
    }

    for (let i = 0; i < this._ranges.length; i++) {
      const r = this._ranges[i]

      if (r.ifAvailable) {
        this._resolveRangeRequest(r)
        i--
      }
    }
  }

  _clearRequest(peer, req) {
    if (req.block !== null) {
      this._clearInflightBlock(this._blocks, req)
      this._unmarkInflight(req.block.index)
    }

    if (req.hash !== null) {
      this._clearInflightBlock(this._hashes, req)
    }

    if (req.upgrade !== null && this._upgrade !== null) {
      this._clearInflightUpgrade(req)
    }

    if (this._seeks.length > 0) {
      this._clearInflightSeeks(req)
    }

    if (this._reorgs.length > 0) {
      this._clearInflightReorgs(req)
    }
  }

  _onnodata(peer, req, reason) {
    if (reason === INVALID_REQUEST) {
      peer.stats.backoffs++
      this.stats.backoffs++

      if (peer.stats.backoffs >= MAX_BACKOFFS) {
        peer.paused = true
      }
    }

    if (req) {
      this._clearRequest(peer, req)
    }

    this.updateAll()
  }

  _openSkipBitfield() {
    // technically the skip bitfield gets bits cleared if .clear() is called
    // also which might be in inflight also, but that just results in that section being overcalled shortly
    // worst case, so ok for now

    const bitfield = this.core.openSkipBitfield()

    for (const req of this._inflight) {
      if (req.block) bitfield.set(req.block.index, true) // skip
    }
  }

  _markProcessing(index) {
    const b = this._blocks.get(index)
    if (b) {
      b.processing = true
      return
    }

    const h = this._hashes.get(index)
    if (h) h.processing = true
  }

  _markProcessed(index) {
    const b = this._blocks.get(index)
    if (b) return b.processed()

    const h = this._hashes.get(index)
    if (h) h.processed()
  }

  _markInflight(index) {
    if (this.core.skipBitfield !== null) this.core.skipBitfield.set(index, true)
    for (const peer of this.peers) peer._markInflight(index)
  }

  _unmarkInflight(index) {
    if (this.core.skipBitfield !== null) {
      this.core.skipBitfield.set(index, this.core.bitfield.get(index))
    }
    for (const peer of this.peers) peer._resetMissingBlock(index)
  }

  fullyDownloaded() {
    if (!this.core.state.length) return false
    return this.core.state.length === this.core.header.hints.contiguousLength
  }

  _ondata(peer, req, data) {
    if (req) {
      req.elapsed = Date.now() - req.timestamp
    }

    if (data.block !== null) {
      this._resolveBlockRequest(this._blocks, data.block.index, data.block.value, req)
      this._ondownload(data.block.index, data.block.value.byteLength, peer, req)
    }

    if (data.hash !== null && (data.hash.index & 1) === 0) {
      this._resolveBlockRequest(this._hashes, data.hash.index / 2, null, req)
    }

    if (this._upgrade !== null) {
      this._resolveUpgradeRequest(req)
    }

    if (this._seeks.length > 0) {
      this._clearInflightSeeks(req)
    }

    if (this._reorgs.length > 0) {
      this._clearInflightReorgs(req)
    }

    if (this._manifestPeer === peer && this.core.header.manifest !== null) {
      this._manifestPeer = null
    }

    if (this._seeks.length > 0 || this._ranges.length > 0) {
      this._updateNonPrimary(this._seeks.length > 0)
    }

    this.updatePeer(peer)
  }

  _onwant(peer, start, length) {
    if (!peer.isActive()) return

    const contig = Math.min(this.core.state.length, this.core.header.hints.contiguousLength)

    if (start + length < contig || this.core.state.length === contig) {
      peer.wireRange.send({
        drop: false,
        start: 0,
        length: contig
      })
      incrementTx(peer.stats.wireRange, this.stats.wireRange)
      return
    }

    length = Math.min(length, this.core.state.length - start)

    peer.protomux.cork()

    for (const msg of this.core.bitfield.want(start, length)) {
      peer.wireBitfield.send(msg)
      incrementTx(peer.stats.wireBitfield, this.stats.wireBitfield)
    }

    peer.protomux.uncork()
  }

  async _onreorgdata(peer, req, data) {
    const newBatch = data.upgrade && (await this.core.verifyReorg(data))
    const f = this._addReorg(data.fork, peer)

    if (f === null) {
      this.updateAll()
      return
    }

    removeInflight(f.inflight, req)

    if (f.batch) {
      await f.batch.update(data)
    } else if (data.upgrade) {
      f.batch = newBatch

      // Remove "older" reorgs in progress as we just verified this one.
      this._clearOldReorgs(f.fork)
    }

    if (f.batch && f.batch.finished) {
      if (this._addUpgradeMaybe() !== null) {
        await this._applyReorg(f)
      }
    }

    this.updateAll()
  }

  // Never throws, allowed to run in the background
  async _applyReorg(f) {
    // TODO: more optimal here to check if potentially a better reorg
    // is available, ie higher fork, and request that one first.
    // This will request that one after this finishes, which is fine, but we
    // should investigate the complexity in going the other way

    const u = this._upgrade

    this._reorgs = [] // clear all as the nodes are against the old tree - easier
    this._applyingReorg = this.core.reorg(f.batch, null) // TODO: null should be the first/last peer?

    try {
      await this._applyingReorg
    } catch (err) {
      this._upgrade = null
      u.reject(err)
    }

    this._applyingReorg = null

    if (this._upgrade !== null) {
      this._resolveUpgradeRequest(null)
    }

    for (const peer of this.peers) this._updateFork(peer)

    // TODO: all the remaining is a tmp workaround until we have a flag/way for ANY_FORK
    for (const r of this._ranges) {
      r.start = r.userStart
      r.end = r.userEnd
    }

    this.updateAll()
  }

  _maybeUpdate() {
    return this._upgrade !== null && this._upgrade.inflight.length === 0
  }

  _maybeRequestManifest() {
    return this.core.header.manifest === null && this._manifestPeer === null
  }

  _updateFork(peer) {
    if (
      this._applyingReorg !== null ||
      this.allowFork === false ||
      peer.remoteFork <= this.core.state.fork
    ) {
      return false
    }

    const f = this._addReorg(peer.remoteFork, peer)

    // TODO: one per peer is better
    if (f !== null && f.batch === null && f.inflight.length === 0) {
      return peer._requestForkProof(f)
    }

    return false
  }

  _updateHotswap(peer) {
    const maxHotswaps = peer.getMaxHotswapInflight()
    if (!peer.isActive() || peer.inflight >= maxHotswaps) return

    for (const b of this.hotswaps.pick(peer)) {
      if (peer._requestBlock(b) === false) continue
      peer.stats.hotswaps++
      peer.replicator.stats.hotswaps++
      if (peer.inflight >= maxHotswaps) break
    }
  }

  _updatePeer(peer) {
    if (!peer.isActive() || peer.inflight >= peer.getMaxInflight() || !peer.remoteUploading) {
      return false
    }

    // Eagerly request the manifest even if the remote length is 0. If not 0 we'll get as part of the upgrade request...
    if (
      this._maybeRequestManifest() === true &&
      peer.remoteLength === 0 &&
      peer.remoteHasManifest === true
    ) {
      this._manifestPeer = peer
      peer._requestManifest()
    }

    for (const s of this._seeks) {
      if (s.inflight.length > 0) continue // TODO: one per peer is better
      if (peer._requestSeek(s) === true) {
        return true
      }
    }

    // Implied that any block in the queue should be requested, no matter how many inflights
    const blks = new RandomIterator(this._queued)

    for (const b of blks) {
      if (b.queued === false || peer._requestBlock(b) === true) {
        b.queued = false
        blks.dequeue()
        return true
      }
    }

    return false
  }

  _updatePeerNonPrimary(peer) {
    if (!peer.isActive() || peer.inflight >= peer.getMaxInflight() || !peer.remoteUploading) {
      return false
    }

    const ranges = new RandomIterator(this._ranges)
    let tried = 0

    for (const r of ranges) {
      if (peer._requestRange(r) === true) {
        return true
      }
      if (++tried >= MAX_RANGES) break
    }

    // Iterate from newest fork to oldest fork...
    for (let i = this._reorgs.length - 1; i >= 0; i--) {
      const f = this._reorgs[i]
      if (f.batch !== null && f.inflight.length === 0 && peer._requestForkRange(f) === true) {
        return true
      }
    }

    if (this._maybeUpdate() === true && peer._requestUpgrade(this._upgrade) === true) {
      return true
    }

    return false
  }

  updatePeer(peer) {
    // Quick shortcut to wait for flushing reorgs - not needed but less waisted requests
    if (this._applyingReorg !== null) return

    while (this._updatePeer(peer) === true);
    while (this._updatePeerNonPrimary(peer) === true);

    if (this.peers.length > 1 && this._blocks.isEmpty() === false) {
      this._updateHotswap(peer)
    }

    this._checkUpgradeIfAvailable()
    this._maybeResolveIfAvailableRanges()
  }

  updateAll() {
    // Quick shortcut to wait for flushing reorgs - not needed but less waisted requests
    if (this._applyingReorg !== null) return

    const peers = new RandomIterator(this.peers)

    for (const peer of peers) {
      if (this._updatePeer(peer) === true) {
        peers.requeue()
      }
    }

    // Check if we can skip the non primary check fully
    if (this._maybeUpdate() === false && this._ranges.length === 0 && this._reorgs.length === 0) {
      this._checkUpgradeIfAvailable()
      return
    }

    for (const peer of peers.restart()) {
      if (this._updatePeerNonPrimary(peer) === true) {
        peers.requeue()
      }
    }

    this._checkUpgradeIfAvailable()
    this._maybeResolveIfAvailableRanges()
  }

  onpeerdestroy() {
    if (--this._active === 0) this.core.checkIfIdle()
  }

  attached(protomux) {
    return this._attached.has(protomux)
  }

  attachTo(protomux) {
    if (this.core.closed) return

    const makePeer = this._makePeer.bind(this, protomux)

    this._attached.add(protomux)
    protomux.pair({ protocol: 'hypercore/alpha', id: this.core.discoveryKey }, makePeer)
    protomux.stream.setMaxListeners(0)
    protomux.stream.on('close', this._onstreamclose)

    this._ifAvailable++
    this._active++

    protomux.stream.opened.then((opened) => {
      this._ifAvailable--
      this._active--

      if (opened && !this.destroyed) makePeer()
      this._checkUpgradeIfAvailable()

      this.core.checkIfIdle()
    })
  }

  detachFrom(protomux) {
    if (this._attached.delete(protomux)) {
      protomux.stream.removeListener('close', this._onstreamclose)
      protomux.unpair({ protocol: 'hypercore/alpha', id: this.core.discoveryKey })
    }
  }

  idle() {
    return this.peers.length === 0 && this._active === 0
  }

  close() {
    const waiting = []

    for (const peer of this.peers) {
      waiting.push(peer.channel.fullyClosed())
    }

    this.destroy()
    return Promise.all(waiting)
  }

  destroy() {
    if (this.destroyed) return
    this.destroyed = true

    if (this._downloadingTimer) {
      clearTimeout(this._downloadingTimer)
      this._downloadingTimer = null
    }

    while (this.peers.length) {
      const peer = this.peers[this.peers.length - 1]
      this.detachFrom(peer.protomux)
      peer.channel.close() // peer is removed from array in onclose
    }

    for (const protomux of this._attached) {
      this.detachFrom(protomux)
    }
  }

  _makePeer(protomux) {
    const replicator = this
    if (protomux.opened({ protocol: 'hypercore/alpha', id: this.core.discoveryKey })) {
      return onnochannel()
    }

    const channel = protomux.createChannel({
      userData: null,
      protocol: 'hypercore/alpha',
      aliases: ['hypercore'],
      id: this.core.discoveryKey,
      handshake: m.wire.handshake,
      messages: [
        { encoding: m.wire.sync, onmessage: onwiresync },
        { encoding: m.wire.request, onmessage: onwirerequest },
        { encoding: m.wire.cancel, onmessage: onwirecancel },
        { encoding: m.wire.data, onmessage: onwiredata },
        { encoding: m.wire.noData, onmessage: onwirenodata },
        { encoding: m.wire.want, onmessage: onwirewant },
        { encoding: m.wire.unwant, onmessage: onwireunwant },
        { encoding: m.wire.bitfield, onmessage: onwirebitfield },
        { encoding: m.wire.range, onmessage: onwirerange },
        { encoding: m.wire.extension, onmessage: onwireextension }
      ],
      onopen: onwireopen,
      onclose: onwireclose,
      ondrain: onwiredrain,
      ondestroy: onwiredestroy
    })

    if (channel === null) return onnochannel()

    const peer = new Peer(replicator, protomux, channel, this.inflightRange)
    const stream = protomux.stream

    peer.channel.open({
      seeks: true,
      capability: caps.replicate(stream.isInitiator, this.core.key, stream.handshakeHash)
    })

    return true

    function onnochannel() {
      return false
    }
  }

  _onpeerupdate(added, peer) {
    const name = added ? 'peer-add' : 'peer-remove'
    const sessions = this.core.monitors

    for (let i = sessions.length - 1; i >= 0; i--) {
      sessions[i].emit(name, peer)

      if (added) {
        for (const ext of sessions[i].extensions.values()) {
          peer.extensions.set(ext.name, ext)
        }
      }
    }
  }

  _ondownload(index, byteLength, from, req) {
    const sessions = this.core.monitors

    for (let i = sessions.length - 1; i >= 0; i--) {
      const s = sessions[i]
      s.emit('download', index, byteLength - s.padding, from, req)
    }
  }

  _onupload(index, byteLength, from) {
    const sessions = this.core.monitors

    for (let i = sessions.length - 1; i >= 0; i--) {
      const s = sessions[i]
      s.emit('upload', index, byteLength - s.padding, from)
    }
  }

  _oninvaliddata(err, req, res, from) {
    const sessions = this.core.monitors

    this.stats.invalidData++
    from.stats.invalidData++
    for (let i = 0; i < sessions.length; i++) {
      sessions[i].emit('verification-error', err, req, res, from)
    }
  }

  _oninvalidrequest(err, req, from) {
    const sessions = this.core.monitors

    from.stats.invalidRequests++
    this.stats.invalidRequests++
    for (let i = 0; i < sessions.length; i++) {
      sessions[i].emit('invalid-request', err, req, from)
    }
  }
}

function removeHotswap(block) {
  if (block.hotswap === null) return false
  block.hotswap.ref.remove(block)
  return true
}

function removeInflight(inf, req) {
  const i = inf.indexOf(req)
  if (i === -1) return false
  if (i < inf.length - 1) inf[i] = inf.pop()
  else inf.pop()
  return true
}

function toLength(start, end) {
  return end === -1 ? -1 : end < start ? 0 : end - start
}

function clampRange(core, r) {
  if (r.blocks === null) {
    const start = core.bitfield.firstUnset(r.start)

    if (r.end === -1) r.start = start === -1 ? core.state.length : start
    else if (start === -1 || start >= r.end) r.start = r.end
    else {
      r.start = start

      const end = core.bitfield.lastUnset(r.end - 1)

      if (end === -1 || start >= end + 1) r.end = r.start
      else r.end = end + 1
    }
  } else {
    while (r.start < r.end && core.bitfield.get(r.blocks[r.start])) r.start++
    while (r.start < r.end && core.bitfield.get(r.blocks[r.end - 1])) r.end--
  }
}

function onrequesttimeout(req) {
  if (req.context) req.context.detach(req, REQUEST_TIMEOUT())
}

function destroyRequestTimeout(req) {
  if (req.timeout !== null) {
    clearTimeout(req.timeout)
    req.timeout = null
  }
}

function isCriticalError(err) {
  // TODO: expose .critical or similar on the hypercore errors that are critical (if all not are)
  return err.name === 'HypercoreError'
}

function onwireopen(m, c) {
  return c.userData.onopen(m)
}

function onwireclose(isRemote, c) {
  return c.userData.onclose(isRemote)
}

function onwiredestroy(c) {
  c.userData.replicator.onpeerdestroy()
}

function onwiredrain(c) {
  return c.userData.ondrain()
}

function onwiresync(m, c) {
  incrementRx(c.userData.stats.wireSync, c.userData.replicator.stats.wireSync)
  return c.userData.onsync(m)
}

function onwirerequest(m, c) {
  incrementRx(c.userData.stats.wireRequest, c.userData.replicator.stats.wireRequest)
  return c.userData.onrequest(m)
}

function onwirecancel(m, c) {
  incrementRx(c.userData.stats.wireCancel, c.userData.replicator.stats.wireCancel)
  return c.userData.oncancel(m)
}

function onwiredata(m, c) {
  incrementRx(c.userData.stats.wireData, c.userData.replicator.stats.wireData)
  return c.userData.ondata(m)
}

function onwirenodata(m, c) {
  return c.userData.onnodata(m)
}

function onwirewant(m, c) {
  incrementRx(c.userData.stats.wireWant, c.userData.replicator.stats.wireWant)
  return c.userData.onwant(m)
}

function onwireunwant(m, c) {
  return c.userData.onunwant(m)
}

function onwirebitfield(m, c) {
  incrementRx(c.userData.stats.wireBitfield, c.userData.replicator.stats.wireBitfield)
  return c.userData.onbitfield(m)
}

function onwirerange(m, c) {
  incrementRx(c.userData.stats.wireRange, c.userData.replicator.stats.wireRange)
  return c.userData.onrange(m)
}

function onwireextension(m, c) {
  incrementRx(c.userData.stats.wireExtension, c.userData.replicator.stats.wireExtension)
  return c.userData.onextension(m)
}

function setDownloadingLater(repl, downloading, session) {
  repl.setDownloadingNow(downloading, session)
}

function isBlockRequest(req) {
  return req !== null && req.block !== null
}

function isUpgradeRequest(req) {
  return req !== null && req.upgrade !== null
}

function incrementTx(stats1, stats2) {
  stats1.tx++
  stats2.tx++
}

function incrementRx(stats1, stats2) {
  stats1.rx++
  stats2.rx++
}

function noop() {}

function backoff(times) {
  const sleep = times < 2 ? 200 : times < 5 ? 500 : times < 40 ? 1000 : 5000
  return new Promise((resolve) => setTimeout(resolve, sleep))
}
const crypto = require('hypercore-crypto')
const b4a = require('b4a')
const assert = require('nanoassert')
const flat = require('flat-tree')
const quickbit = require('quickbit-universal')

const { INVALID_OPERATION, INVALID_SIGNATURE } = require('hypercore-errors')

const Mutex = require('./mutex')
const Bitfield = require('./bitfield')
const { MerkleTree, MerkleTreeBatch } = require('./merkle-tree')

module.exports = class SessionState {
  constructor(core, parent, storage, treeInfo, name) {
    this.core = core
    this.index = this.core.sessionStates.push(this) - 1

    this.storage = storage
    this.name = name
    this.sessions = []

    // small hack to close old storages as late as possible.
    // TODO: add a read lock so we can avoid that
    this.lingers = null

    this.parent = parent
    this.atomized = null
    this.mutex = new Mutex()

    // merkle state
    this.roots = treeInfo.roots.length ? treeInfo.roots : []
    this.fork = treeInfo.fork || 0
    this.length = MerkleTree.span(this.roots) / 2
    this.byteLength = MerkleTree.size(this.roots)
    this.prologue = treeInfo.prologue || null
    this.signature = treeInfo.signature || null

    this.snapshotCompatLength = this.isSnapshot()
      ? Math.min(this.length, this.core.state.length)
      : -1
    this.lastTruncation = null

    this.active = 0

    this._activeTx = null
    this._pendingBitfield = null

    this.ref()
  }

  isSnapshot() {
    return this.storage.snapshotted
  }

  isDefault() {
    return this.core.state === this || this.isAtomicDefault()
  }

  isAtomicDefault() {
    return !!this.storage.atom && !!this.parent && this.parent.isDefault()
  }

  createTreeBatch() {
    return new MerkleTreeBatch(this)
  }

  addSession(s) {
    if (s._stateIndex !== -1) return
    s._stateIndex = this.sessions.push(s) - 1
    if (s.weak === false) this.core.activeSessions++
  }

  removeSession(s) {
    if (s._stateIndex === -1) return
    const head = this.sessions.pop()
    if (head !== s) this.sessions[(head._stateIndex = s._stateIndex)] = head
    s._stateIndex = -1
    if (s.weak === false) this.core.activeSessions--
    this.core.checkIfIdle()
  }

  flushedLength() {
    if (this.isDefault() || this.isSnapshot()) return this.length
    const deps = this.storage.dependencies
    if (deps.length) return deps[deps.length - 1].length
    return 0
  }

  signedLength() {
    const l = Math.min(this.flushedLength(), this.core.state.length)
    return this.isSnapshot() && l > this.snapshotCompatLength ? this.snapshotCompatLength : l
  }

  unref() {
    if (--this.active > 0) return
    this.close().catch(noop) // technically async, but only for the last db session
  }

  ref() {
    this.active++
    return this
  }

  hash() {
    return MerkleTree.hash(this)
  }

  setRoots(roots) {
    this.roots = roots
    this.length = MerkleTree.span(roots) / 2
    this.byteLength = MerkleTree.size(roots)
  }

  get encryptionFork() {
    return this.core.header.tree.fork
  }

  async updateSnapshotStorage(storage) {
    if (!this.atomized || !this.atomized.flushing) return this.treeInfo()
    await this.atomized.flushed()

    let rx = storage.read()
    const headPromise = rx.getHead()
    const authPromise = rx.getAuth()
    const depPromise = rx.getDependency()

    rx.tryFlush()
    const [head, auth, dep] = await Promise.all([headPromise, authPromise, depPromise])

    storage.setDependencyHead(dep)

    const fork = head ? head.fork : 0
    const length = head ? head.length : 0

    rx = storage.read()
    const rootPromises = []
    for (const r of flat.fullRoots(length * 2)) {
      rootPromises.push(rx.getTreeNode(r))
    }

    rx.tryFlush()

    const roots = await Promise.all(rootPromises)

    // dbl check if we are hitting an regression from earler
    for (const root of roots) {
      if (root === null) {
        throw new Error(
          'Bad snapshot from atomized session, id = ' +
            this.core.id +
            ' length = ' +
            length +
            ' fork = ' +
            fork
        )
      }
    }

    return {
      fork,
      roots,
      length,
      prologue: auth.manifest && auth.manifest.prologue,
      signature: head && head.signature
    }
  }

  treeInfo() {
    return {
      fork: this.fork,
      roots: this.roots.slice(),
      length: this.length,
      prologue: this.prologue,
      signature: this.signature
    }
  }

  async close() {
    if (this.index === -1) return

    this.active = 0
    this.mutex.destroy(new Error('Closed')).catch(noop)
    if (this.parent && this.parent.atomized) this.parent.atomized = null

    const closing = this.storage.close()

    const head = this.core.sessionStates.pop()
    if (head !== this) this.core.sessionStates[(head.index = this.index)] = head

    this.index = -1
    this.core.checkIfIdle()

    if (this.lingers !== null) {
      for (const storage of this.lingers) await storage.close()
    }

    return closing
  }

  async snapshot() {
    const storage = this.storage.snapshot()
    const treeInfo = await this.updateSnapshotStorage(storage)

    const s = new SessionState(this.core, null, storage, treeInfo, this.name)

    return s
  }

  updateDependency(tx, length) {
    const dependency = updateDependency(this, length, false)
    if (dependency) tx.setDependency(dependency)
    return dependency
  }

  _clearActiveBatch() {
    this._activeTx = null
  }

  createWriteBatch() {
    assert(!this._activeTx && !this.storage.snapshotted)

    this._activeTx = this.storage.write()
    return this._activeTx
  }

  _unlock() {
    this._clearActiveBatch()
    this.mutex.unlock()
    this.core.checkIfIdle()
  }

  async flush() {
    const tx = this._activeTx
    this._activeTx = null

    try {
      if (!(await tx.flush())) return false
    } finally {
      this._clearActiveBatch()
    }

    this.lastTruncation = null
    return true
  }

  _precommit() {
    this.commiting = true
  }

  async _commit() {
    await this.mutex.lock()

    try {
      const bitfield = this._pendingBitfield
      this._pendingBitfield = null
      this.lastTruncation = null
      await this.parent._oncommit(this, bitfield)
    } finally {
      this.commiting = false
      this.mutex.unlock()
    }
  }

  async _oncommit(src, bitfield) {
    await this.mutex.lock()

    try {
      const currLength = this.length

      // load dependency into memory
      const rx = this.storage.read()
      const dependencyPromise = rx.getDependency()

      rx.tryFlush()

      const dependency = await dependencyPromise

      this.fork = src.fork
      this.length = src.length
      this.byteLength = src.byteLength
      this.roots = src.roots.slice()
      this.signature = src.signature

      const tree = {
        fork: this.fork,
        length: this.length,
        rootHash: this.hash(),
        signature: this.signature
      }

      if (dependency) this.storage.setDependencyHead(dependency)

      const b = bitfield

      if (b && b.truncated && b.start < currLength) {
        this.ontruncate(tree, b.start, currLength, true)
        if (!b || b.appends === 0) return
      }

      const append = b ? { start: b.start, length: b.appends, drop: false } : null

      this.onappend(tree, append, true)

      if (this.core.hintsChanged && this.isDefault()) await this.core.flushHints()
    } finally {
      this.mutex.unlock()
      this.core.checkIfIdle()
    }
  }

  async setUserData(key, value) {
    await this.mutex.lock()

    try {
      const tx = this.createWriteBatch()
      tx.putUserData(key, value)

      return await this.flush()
    } finally {
      this._unlock()
    }
  }

  async _verifyBlock(batch, bitfield, value, manifest, from) {
    await this.mutex.lock()

    try {
      if (batch.upgraded && batch.length <= this.length) {
        await batch.downgrade()
      }

      if (!batch.commitable()) {
        return false
      }

      if (this.core.preupdate !== null) {
        await this.core.preupdate(batch, this.core.header.key)
      }

      const tx = this.createWriteBatch()
      this.updating = true

      if (bitfield) {
        tx.putBlock(bitfield.start, value)
      }

      if (bitfield && this.isDefault()) {
        await storeBitfieldRange(this.storage, tx, bitfield.start, bitfield.start + 1, true)
      }

      if (manifest) this.core._setManifest(tx, manifest, null)

      assert(batch.commitable(), 'Should still be commitable')
      batch.commit(tx)

      const head = {
        fork: batch.fork,
        length: batch.length,
        rootHash: batch.hash(),
        signature: batch.signature
      }

      if (batch.upgraded) tx.setHead(head)

      const flushed = await this.flush()

      if (batch.upgraded) {
        this.roots = batch.roots
        this.length = batch.length
        this.byteLength = batch.byteLength
        this.fork = batch.fork
        this.signature = batch.signature

        this.onappend(head, bitfield, flushed)
      }

      if (this.core.hintsChanged && this.isDefault()) await this.core.flushHints()
    } finally {
      this._clearActiveBatch()
      this.updating = false
      this.mutex.unlock()
    }

    return true
  }

  async truncate(length, fork, { signature, keyPair } = {}) {
    if (!keyPair && this.isDefault()) keyPair = this.core.header.keyPair

    await this.mutex.lock()

    try {
      if (this.prologue && length < this.prologue.length) {
        throw INVALID_OPERATION('Truncation breaks prologue', this.core.discoveryKey)
      }
      if (length > this.length) {
        throw INVALID_OPERATION(
          'Not a truncation, ' + length + ' must be less or equal to ' + this.length,
          this.core.discoveryKey
        )
      }

      const batch = this.createTreeBatch()
      await MerkleTree.truncate(this, length, batch, fork)

      if (!signature && keyPair && length > 0) signature = this.core.verifier.sign(batch, keyPair)
      if (signature) batch.signature = signature

      const tx = this.createWriteBatch()

      // upsert compat manifest
      if (this.core.verifier === null && keyPair) this.core._setManifest(tx, null, keyPair)

      const { dependency, tree, roots } = await this._truncate(tx, batch)

      for (const sessionState of this.core.sessionStates) {
        if (
          sessionState.isSnapshot() &&
          sessionState.name === this.name &&
          length < sessionState.snapshotCompatLength
        ) {
          sessionState.snapshotCompatLength = length
        }
      }

      const flushed = await this.flush()

      this.fork = tree.fork
      this.length = tree.length
      this.byteLength = MerkleTree.size(roots)
      this.roots = roots
      this.signature = tree.signature

      if (dependency) this.storage.setDependencyHead(dependency)

      this.ontruncate(tree, tree.length, batch.treeLength, flushed)

      if (this.core.hintsChanged && this.isDefault()) await this.core.flushHints()
    } finally {
      this._unlock()
    }
  }

  async reorg(batch) {
    await this.mutex.lock()

    const storage = this.createWriteBatch()

    try {
      if (!batch.commitable()) return false

      const { dependency, tree } = await this._truncate(storage, batch)

      const flushed = await this.flush()

      this.fork = batch.fork
      this.length = batch.length
      this.byteLength = batch.byteLength
      this.roots = batch.roots
      this.signature = batch.signature

      if (dependency) this.storage.setDependencyHead(dependency)

      this.ontruncate(tree, batch.ancestors, batch.treeLength, flushed)

      if (this.core.hintsChanged && this.isDefault()) await this.core.flushHints()
    } finally {
      this._unlock()
    }
  }

  async _truncate(storage, batch) {
    storage.deleteBlockRange(batch.ancestors, batch.treeLength)

    assert(batch.commitable(), 'Batch must be commitable')

    const tree = {
      fork: batch.fork,
      length: batch.length,
      rootHash: batch.hash(),
      signature: batch.signature
    }

    storage.setHead(tree)
    batch.commit(storage)

    const truncated = batch.length < this.flushedLength()
    const dependency = truncated ? updateDependency(this, batch.length, true) : null

    if (dependency) storage.setDependency(dependency)

    if (this.isDefault()) {
      await storeBitfieldRange(this.storage, storage, batch.ancestors, batch.treeLength, false)
      if (batch.ancestors < this.core.header.hints.contiguousLength) {
        this.core.header.hints.remoteContiguousLength = Math.min(
          batch.length,
          this.core.header.hints.remoteContiguousLength
        )
        storage.setHints({
          contiguousLength: batch.ancestors,
          remoteContiguousLength: this.core.header.hints.remoteContiguousLength
        })
      }
    }

    return { dependency, tree, roots: batch.roots }
  }

  async clear(start, end, cleared) {
    await this.mutex.lock()

    try {
      const tx = this.createWriteBatch()

      const ite = flat.iterator(start * 2)
      while (!isRootIndex(ite.index, this.roots)) {
        const a = ite.index
        const b = ite.sibling()
        ite.parent()

        const [left, right] = flat.spans(b)
        const s = left / 2
        const e = right / 2 + 1
        const has = s <= start && e <= end ? false : this.core.bitfield.hasSet(s, e - s)
        if (has) break

        tx.deleteTreeNode(a)
        tx.deleteTreeNode(b)
      }

      if (this.isDefault()) {
        await storeBitfieldRange(this.storage, tx, start, end, false)
        if (start < this.core.header.hints.contiguousLength) {
          tx.setHints({
            contiguousLength: start,
            remoteContiguousLength: this.core.header.hints.remoteContiguousLength
          })
        }
      }

      if (end - start === 1) tx.deleteBlock(start)
      else tx.deleteBlockRange(start, end)

      const dependency = start < this.flushedLength() ? updateDependency(this, start, true) : null

      const flushed = await this.flush()

      if (dependency) this.storage.setDependencyHead(dependency)

      // todo: atomic event handle
      if (this.isDefault() && flushed) {
        const length = end - start

        this.core.updateContiguousLength({ start, length, drop: true })
        this.core._setBitfieldRanges(start, end, false)
        this.core.replicator.onhave(start, length, true)

        if (this.core.hintsChanged) await this.core.flushHints()
      }
    } finally {
      this._unlock()
    }
  }

  async append(values, { signature, keyPair, preappend, postappend, maxLength = -1 } = {}) {
    if (!keyPair && this.isDefault()) keyPair = this.core.header.keyPair

    await this.mutex.lock()

    try {
      if (maxLength >= 0 && this.length + values.length > maxLength) {
        return { length: this.length, byteLength: this.byteLength }
      }

      const tx = this.createWriteBatch()

      // upsert compat manifest
      if (this.core.verifier === null && keyPair) this.core._setManifest(tx, null, keyPair)

      if (preappend) await preappend(values)

      if (!values.length) {
        await this.flush()
        return { length: this.length, byteLength: this.byteLength }
      }

      const batch = this.createTreeBatch()
      for (const val of values) batch.append(val)

      // only multisig can have prologue so signature is always present
      if (this.prologue && batch.length < this.prologue.length) {
        throw INVALID_OPERATION('Append is not consistent with prologue', this.core.discoveryKey)
      }

      if (!signature && keyPair) signature = this.core.verifier.sign(batch, keyPair)
      if (signature) batch.signature = signature

      batch.commit(tx)

      const tree = {
        fork: batch.fork,
        length: batch.length,
        rootHash: batch.hash(),
        signature: batch.signature
      }

      tx.setHead(tree)

      if (this.isDefault()) {
        await storeBitfieldRange(this.storage, tx, batch.ancestors, batch.length, true)
        if (this.length === this.core.header.hints.contiguousLength) {
          tx.setHints({
            contiguousLength: this.length + values.length,
            remoteContiguousLength: this.core.header.hints.remoteContiguousLength
          })
        }
      }

      for (let i = 0; i < values.length; i++) {
        tx.putBlock(this.length + i, values[i])
      }

      const bitfield = {
        drop: false,
        start: batch.ancestors,
        length: values.length
      }

      const flushed = await this.flush()

      this.fork = batch.fork
      this.roots = batch.roots
      this.length = batch.length
      this.byteLength = batch.byteLength
      this.signature = batch.signature

      if (postappend) await postappend(values)

      this.onappend(tree, bitfield, flushed)

      if (this.core.hintsChanged && this.isDefault()) await this.core.flushHints()

      return { length: this.length, byteLength: this.byteLength }
    } finally {
      this._unlock()
    }
  }

  onappend(tree, bitfield, flushed) {
    if (!flushed) this._updateBitfield(bitfield)
    else if (this.isDefault()) this.core.onappend(tree, bitfield)

    for (let i = this.sessions.length - 1; i >= 0; i--) {
      this.sessions[i].emit('append')
    }
  }

  ontruncate(tree, to, from, flushed) {
    const bitfield = { start: to, length: from - to, drop: true }

    this.lastTruncation = { from, to }

    if (!flushed) this._updateBitfield(bitfield)
    else if (this.isDefault()) this.core.ontruncate(tree, bitfield)

    for (const sessionState of this.core.sessionStates) {
      if (
        sessionState.isSnapshot() &&
        sessionState.name === this.name &&
        to < sessionState.snapshotCompatLength
      ) {
        sessionState.snapshotCompatLength = to
      }
    }

    for (let i = this.sessions.length - 1; i >= 0; i--) {
      this.sessions[i].emit('truncate', to, tree.fork)
    }
  }

  _updateBitfield(bitfield, flushed) {
    if (!bitfield) return

    const p = this._pendingBitfield
    const b = bitfield

    if (b.drop) {
      // truncation must be from end
      if (p && b.start + b.length !== p.start + p.appends) {
        throw INVALID_OPERATION('Atomic truncations must be contiguous', this.core.discoveryKey)
      }

      // actual truncation
      if (p === null || b.start < p.start) {
        this._pendingBitfield = { truncated: true, start: b.start, appends: 0 }
        return
      }

      // just clearing batch data
      p.appends = b.start - p.start

      // we cleared the current batch
      if (p.appends === 0) this._pendingBitfield = null

      return
    }

    if (p === null) {
      this._pendingBitfield = { truncated: false, start: b.start, appends: b.length }
      return
    }

    if (b.start !== p.start + p.appends) {
      throw INVALID_OPERATION('Atomic operations must be contiguous', this.core.discoveryKey)
    }

    p.appends += b.length
  }

  async catchup(length) {
    assert(!this.isDefault(), 'Cannot catchup signed state') // TODO: make this check better

    await this.mutex.lock()

    try {
      const origLength = this.length

      let sharedLength = 0
      for (let i = this.storage.dependencies.length - 1; i >= 0; i--) {
        const dep = this.storage.dependencies[i]
        if (dep.dataPointer === this.core.state.storage.core.dataPointer) {
          sharedLength = dep.length
          break
        }
      }

      const tx = this.createWriteBatch()
      const rx = this.core.state.storage.read()
      const rootPromises = []

      for (const root of flat.fullRoots(length * 2)) {
        rootPromises.push(rx.getTreeNode(root))
      }

      rx.tryFlush()

      const roots = await Promise.all(rootPromises)
      const truncating = sharedLength < origLength

      for (const node of roots) {
        if (node === null) {
          throw INVALID_OPERATION(
            'Invalid catchup length, tree nodes not available',
            this.core.discoveryKey
          )
        }
      }

      const fork = truncating ? this.fork + 1 : this.fork

      // overwrite it atm, TODO: keep what we can connect to the tree
      tx.deleteBlockRange(0, -1)
      tx.deleteTreeNodeRange(0, -1)
      tx.deleteBitfieldPageRange(0, -1)

      const tree = {
        fork,
        length,
        rootHash: crypto.tree(roots),
        signature: null
      }

      tx.setHead(tree)

      // prop a better way to do this
      const dep = updateDependency(this, sharedLength, true)
      dep.length = length

      tx.setDependency(dep)

      const flushed = await this.flush()

      this.storage.setDependencyHead(dep)

      this.fork = tree.fork
      this.roots = roots
      this.length = tree.length
      this.byteLength = MerkleTree.size(roots)

      if (truncating) this.ontruncate(tree, sharedLength, origLength, flushed)
      if (sharedLength < length) this.onappend(tree, null, flushed)

      if (this.core.hintsChanged && this.isDefault()) await this.core.flushHints()
    } finally {
      this.mutex.unlock()
    }
  }

  async _overwrite(source, fork, length, treeLength, signature) {
    const blockPromises = []
    const treePromises = []
    const rootPromises = []

    const rx = source.storage.read()

    for (const root of flat.fullRoots(length * 2)) {
      rootPromises.push(rx.getTreeNode(root))
    }

    for (const index of flat.patch(treeLength * 2, length * 2)) {
      treePromises.push(rx.getTreeNode(index))
    }

    for (let i = treeLength; i < length; i++) {
      treePromises.push(rx.getTreeNode(i * 2))
      treePromises.push(rx.getTreeNode(i * 2 + 1))
      blockPromises.push(rx.getBlock(i))
    }

    rx.tryFlush()

    const blocks = await Promise.all(blockPromises)
    const nodes = await Promise.all(treePromises)
    const roots = await Promise.all(rootPromises)

    if (this.core.destroyed) throw new Error('Core destroyed')

    if (signature) {
      const batch = this.createTreeBatch()
      batch.roots = roots
      batch.length = length

      if (!this.core.verifier.verify(batch, signature)) {
        throw INVALID_SIGNATURE(
          'Signature is not valid over committed tree',
          this.core.discoveryKey
        )
      }
    }

    const tx = this.createWriteBatch()

    // truncate existing tree
    if (treeLength < this.length) {
      tx.deleteBlockRange(treeLength, this.length)
    }

    for (const root of roots) tx.putTreeNode(root)

    // no nodes will be copied in shallow mode
    for (const node of nodes) {
      if (node !== null) tx.putTreeNode(node)
    }

    for (let i = 0; i < blocks.length; i++) {
      assert(blocks[i] !== null, 'has block')
      tx.putBlock(i + treeLength, blocks[i])
    }

    const totalLength = Math.max(length, this.length)

    if (totalLength > treeLength) {
      const firstPage = getBitfieldPage(treeLength)
      const lastPage = getBitfieldPage(totalLength - 1)

      const srx = this.storage.read()
      const bitfieldPagePromise = srx.getBitfieldPage(firstPage)
      srx.tryFlush()

      const bitfieldPage = await bitfieldPagePromise

      let index = treeLength

      for (let i = firstPage; i <= lastPage; i++) {
        const page = b4a.alloc(Bitfield.BYTES_PER_PAGE)
        tx.putBitfieldPage(i, page)

        // copy existing bits in
        if (i === firstPage && bitfieldPage) page.set(bitfieldPage)

        if (index < length) {
          index = fillBitfieldPage(page, index, length, i, true)
          if (index < length) continue
        }

        if (index < this.length) {
          index = fillBitfieldPage(page, index, this.length, i, false)
        }
      }
    }

    const tree = {
      fork,
      length,
      rootHash: crypto.tree(roots),
      signature
    }

    const upgraded = treeLength < this.length || this.length < length || tree.fork !== this.fork

    if (upgraded) tx.setHead(tree)

    const flushed = await this.flush()

    this.fork = tree.fork
    this.roots = roots
    this.length = length
    this.byteLength = MerkleTree.size(roots)
    this.signature = signature

    return { tree, flushed }
  }

  async commit(state, { signature, keyPair, length = state.length, treeLength = -1 } = {}) {
    assert(
      this.isDefault() || (this.parent && this.parent.isDefault()),
      'Can only commit into default state'
    )

    let srcLocked = false
    await this.mutex.lock()

    try {
      await state.mutex.lock()
      srcLocked = true

      if (treeLength === -1) treeLength = state.flushedLength()

      if (!(await this.core._validateCommit(state, treeLength))) return null
      if (this.length > length) return null

      if (this.length < length && !signature) {
        if (!keyPair) keyPair = this.core.header.keyPair
        const batch = state.createTreeBatch()
        if (length !== batch.length) await batch.restore(length)
        signature = this.core.verifier.sign(batch, keyPair)
      }

      const { tree, flushed } = await this._overwrite(
        state,
        this.fork,
        length,
        treeLength,
        signature
      )

      // gc blocks from source
      if (treeLength < length) {
        const tx = state.createWriteBatch()

        tx.deleteBlockRange(treeLength, length)
        const dependency = state.updateDependency(tx, length)

        await state.flush(tx)

        if (dependency) state.storage.setDependencyHead(dependency)
      }

      const bitfield = { start: treeLength, length: length - treeLength, drop: false }
      this.onappend(tree, bitfield, flushed)

      return {
        length: this.length,
        byteLength: this.byteLength
      }
    } finally {
      this.updating = false
      this.mutex.unlock()

      if (srcLocked) {
        state.mutex.unlock()
        state._clearActiveBatch()
      }

      this.core.checkIfIdle()
    }
  }

  async _getTreeHeadAt(length) {
    if (length === null) return this.treeInfo()

    const head = getDefaultTree()

    head.length = length

    const roots = await MerkleTree.getRootsFromStorage(this.storage, length)
    const rootHash = crypto.tree(roots)

    head.fork = this.fork
    head.rootHash = rootHash

    if (length === this.length) head.signature = this.signature

    return head
  }

  _moveToCore(core, truncated, appended) {
    const head = this.core.sessionStates.pop()
    if (head !== this) this.core.sessionStates[(head.index = this.index)] = head

    this.core = core
    this.index = this.core.sessionStates.push(this) - 1

    for (let i = this.sessions.length - 1; i >= 0; i--) {
      const s = this.sessions[i]
      const manifest = s.manifest
      s.transferSession(this.core)
      if (!manifest && s.manifest) s.emit('manifest')
      if (truncated) s.emit('truncate', truncated.to, truncated.fork)
      if (appended) s.emit('append')
    }
  }

  async moveTo(core, length) {
    const state = core.state

    await this.mutex.lock()

    try {
      // if (state.storage && (await state.storage.resumeSession(this.name)) !== null) {
      //   throw STORAGE_CONFLICT('Batch has already been created')
      // }

      const treeLength = this.length

      let truncated = null
      let appended = false

      if (!this.isSnapshot()) {
        if (this.lingers === null) this.lingers = []
        this.lingers.push(this.storage)

        const resumed = await state.storage.resumeSession(this.name)

        const truncation = length < this.length ? await truncateAndFlush(this, length) : null
        const treeInfo = truncation
          ? truncation.tree
          : resumed
            ? null
            : await state._getTreeHeadAt(this.length)

        const fork = truncation ? this.fork + 1 : this.fork

        // todo: validate treeInfo

        let storage = null

        if (resumed) {
          storage = resumed
        } else {
          treeInfo.fork = fork
          storage = await state.storage.createSession(this.name, treeInfo)
        }

        const roots = await MerkleTree.getRootsFromStorage(storage, length)

        this.storage = storage
        this.prologue = state.prologue
        this.fork = fork
        this.length = length
        this.byteLength = MerkleTree.size(roots)
        this.roots = roots

        if (truncation) {
          const { dependency } = truncation

          if (dependency) this.storage.setDependencyHead(dependency)
          truncated = { to: treeLength, fork }
        }

        if (this.length > treeLength) {
          appended = true
        }
      }

      for (let i = this.core.sessionStates.length - 1; i >= 0; i--) {
        const state = this.core.sessionStates[i]
        if (state === this) continue
        if (state.name === this.name) state._moveToCore(core.core)
      }

      this._moveToCore(core.core, truncated, appended)
    } finally {
      this.mutex.unlock()
    }
  }

  async createSession(name, overwrite, atom) {
    let storage = null
    let treeInfo = null

    if (!atom && !overwrite && this.storage) {
      storage = await this.storage.resumeSession(name)

      if (storage !== null) treeInfo = (await getCoreHead(storage)) || getDefaultTree()
    }

    const length = treeInfo ? treeInfo.length : this.length

    if (storage === null) {
      treeInfo = await this._getTreeHeadAt(length)

      if (atom) {
        storage = await this.storage.createAtomicSession(atom, treeInfo)
      } else {
        storage = await this.storage.createSession(name, treeInfo)
      }
    }

    if (this.atomized && atom) {
      throw new Error('Session already atomized')
    }

    const head = {
      fork: this.fork,
      roots:
        length === this.length
          ? this.roots.slice()
          : await MerkleTree.getRootsFromStorage(storage, length),
      length,
      prologue: this.prologue,
      signature: length === this.length ? this.signature : null
    }

    const state = new SessionState(
      this.core,
      atom ? this : null,
      storage,
      head,
      atom ? this.name : name
    )

    if (atom) {
      this.atomized = atom
      atom.onflush(state._commit.bind(state))
    }

    return state
  }
}

function noop() {}

function getBitfieldPage(index) {
  return Math.floor(index / Bitfield.BITS_PER_PAGE)
}

function fillBitfieldPage(page, start, end, pageIndex, value) {
  const offset = pageIndex * Bitfield.BITS_PER_PAGE
  const max = offset + Bitfield.BITS_PER_PAGE

  const index = max < end ? max : end

  const from = start - offset
  const to = index - offset

  quickbit.fill(page, value, from, to)

  return index
}

async function storeBitfieldRange(storage, tx, from, to, value) {
  if (from >= to) return

  const firstPage = getBitfieldPage(from)
  const lastPage = getBitfieldPage(to - 1)

  let index = from

  const rx = storage.read()

  const promises = []
  for (let i = firstPage; i <= lastPage; i++) {
    promises.push(rx.getBitfieldPage(i))
  }

  rx.tryFlush()

  const pages = await Promise.all(promises)
  const cnt = lastPage - firstPage + 1

  for (let i = 0; i < cnt; i++) {
    const pageIndex = i + firstPage
    if (!pages[i]) pages[i] = b4a.alloc(Bitfield.BYTES_PER_PAGE)

    index = fillBitfieldPage(pages[i], index, to, pageIndex, value)
    tx.putBitfieldPage(pageIndex, pages[i])
  }
}

async function truncateAndFlush(s, length) {
  const batch = s.createTreeBatch()
  await MerkleTree.truncate(s, length, batch, s.fork)
  const tx = s.createWriteBatch()

  const info = await s._truncate(tx, batch)
  const flushed = await s.flush()

  return {
    tree: info.tree,
    roots: info.roots,
    dependency: info.dependency,
    flushed
  }
}

function updateDependency(state, length, truncated) {
  const i = state.storage.findDependencyIndex(length, truncated)
  if (i === -1) return null // skip default state and overlays of default

  return {
    dataPointer: state.storage.dependencies[i].dataPointer,
    length
  }
}

function getDefaultTree() {
  return {
    fork: 0,
    length: 0,
    rootHash: null,
    signature: null
  }
}

function getCoreHead(storage) {
  const b = storage.read()
  const p = b.getHead()
  b.tryFlush()
  return p
}

function isRootIndex(index, roots) {
  for (const node of roots) {
    if (node.index === index) return true
  }

  return false
}
const { Writable, Readable } = require('streamx')

class ReadStream extends Readable {
  constructor(core, opts = {}) {
    super()

    this.core = core
    this.start = opts.start || 0
    this.end = typeof opts.end === 'number' ? opts.end : -1
    this.snapshot = !opts.live && opts.snapshot !== false
    this.live = this.end === -1 ? !!opts.live : false
    this.wait = typeof opts.wait === 'boolean' ? opts.wait : this.core.wait
    this.timeout = opts.timeout || core.timeout
  }

  _open(cb) {
    this._openP().then(cb, cb)
  }

  _read(cb) {
    this._readP().then(cb, cb)
  }

  async _openP() {
    if (this.end === -1) await this.core.update()
    else await this.core.ready()
    if (this.snapshot && this.end === -1) this.end = this.core.length
  }

  async _readP() {
    const end = this.live ? -1 : this.end === -1 ? this.core.length : this.end
    if (end >= 0 && this.start >= end) {
      this.push(null)
      return
    }

    this.push(await this.core.get(this.start++, { wait: this.wait, timeout: this.timeout }))
  }
}

exports.ReadStream = ReadStream

class WriteStream extends Writable {
  constructor(core) {
    super()
    this.core = core
  }

  _writev(batch, cb) {
    this._writevP(batch).then(cb, cb)
  }

  async _writevP(batch) {
    await this.core.append(batch)
  }
}

exports.WriteStream = WriteStream

class ByteStream extends Readable {
  constructor(core, opts = {}) {
    super()

    this._core = core
    this._index = 0
    this._range = null

    this._byteOffset = opts.byteOffset || 0
    this._byteLength = typeof opts.byteLength === 'number' ? opts.byteLength : -1
    this._prefetch = typeof opts.prefetch === 'number' ? opts.prefetch : 32

    this._applyOffset = this._byteOffset > 0
  }

  _open(cb) {
    this._openp().then(cb, cb)
  }

  _read(cb) {
    this._readp().then(cb, cb)
  }

  async _openp() {
    if (this._byteLength === -1) {
      await this._core.update()
      this._byteLength = Math.max(this._core.byteLength - this._byteOffset, 0)
    }
  }

  async _readp() {
    let data = null

    if (this._byteLength === 0) {
      this.push(null)
      return
    }

    let relativeOffset = 0

    if (this._applyOffset) {
      this._applyOffset = false

      const [block, byteOffset] = await this._core.seek(this._byteOffset)

      this._index = block
      relativeOffset = byteOffset
    }

    this._predownload(this._index + 1)
    data = await this._core.get(this._index++, { valueEncoding: 'binary' })

    if (relativeOffset > 0) data = data.subarray(relativeOffset)

    if (data.byteLength > this._byteLength) data = data.subarray(0, this._byteLength)
    this._byteLength -= data.byteLength

    this.push(data)
    if (this._byteLength === 0) this.push(null)
  }

  _predownload(index) {
    if (this._range) this._range.destroy()
    this._range = this._core.download({ start: index, end: index + this._prefetch, linear: true })
  }

  _destroy(cb) {
    if (this._range) this._range.destroy()
    cb(null)
  }
}

exports.ByteStream = ByteStream
const crypto = require('hypercore-crypto')
const b4a = require('b4a')
const c = require('compact-encoding')
const flat = require('flat-tree')
const { BAD_ARGUMENT } = require('hypercore-errors')
const unslab = require('unslab')

const m = require('./messages')
const multisig = require('./multisig')
const caps = require('./caps')

class Signer {
  constructor(
    manifestHash,
    version,
    index,
    { signature = 'ed25519', publicKey, namespace = caps.DEFAULT_NAMESPACE } = {}
  ) {
    if (!publicKey) throw BAD_ARGUMENT('public key is required for a signer')
    if (signature !== 'ed25519') throw BAD_ARGUMENT('Only Ed25519 signatures are supported')

    this.manifestHash = manifestHash
    this.version = version
    this.signer = index
    this.signature = signature
    this.publicKey = publicKey
    this.namespace = namespace
  }

  _ctx() {
    return this.version === 0 ? this.namespace : this.manifestHash
  }

  verify(batch, signature) {
    return crypto.verify(batch.signable(this._ctx()), signature, this.publicKey)
  }

  sign(batch, keyPair) {
    return crypto.sign(batch.signable(this._ctx()), keyPair.secretKey)
  }
}

class CompatSigner extends Signer {
  constructor(index, signer, legacy) {
    super(null, 0, index, signer)
    this.legacy = legacy
  }

  verify(batch, signature) {
    return crypto.verify(batch.signableCompat(this.legacy), signature, this.publicKey)
  }

  sign(batch, keyPair) {
    return crypto.sign(batch.signableCompat(this.legacy), keyPair.secretKey)
  }
}

module.exports = class Verifier {
  constructor(
    manifestHash,
    manifest,
    { compat = isCompat(manifestHash, manifest), legacy = false } = {}
  ) {
    const self = this

    this.manifestHash = manifestHash
    this.compat = compat || manifest === null
    this.version = this.compat ? 0 : typeof manifest.version === 'number' ? manifest.version : 1
    this.hash = manifest.hash || 'blake2b'
    this.allowPatch = !this.compat && !!manifest.allowPatch
    this.quorum = this.compat ? 1 : defaultQuorum(manifest)

    this.signers = manifest.signers ? manifest.signers.map(createSigner) : []
    this.prologue = this.compat ? null : manifest.prologue || null

    function createSigner(signer, index) {
      return self.compat
        ? new CompatSigner(index, signer, legacy)
        : new Signer(manifestHash, self.version, index, signer)
    }
  }

  _verifyCompat(batch, signature) {
    if (!signature) return false

    if (this.compat || (!this.allowPatch && this.signers.length === 1)) {
      return !!signature && this.signers[0].verify(batch, signature)
    }

    return this._verifyMulti(batch, signature)
  }

  _inflate(signature) {
    if (this.version >= 1) return multisig.inflate(signature)
    const { proofs, patch } = multisig.inflatev0(signature)

    return {
      proofs: proofs.map(proofToVersion1),
      patch
    }
  }

  _verifyMulti(batch, signature) {
    if (!signature || this.quorum === 0) return false

    const { proofs, patch } = this._inflate(signature)
    if (proofs.length < this.quorum) return false

    const tried = new Uint8Array(this.signers.length)
    const nodes = this.allowPatch && patch.length ? toMap(patch) : null

    for (let i = 0; i < this.quorum; i++) {
      const inp = proofs[i]

      let tree = batch

      if (inp.patch && this.allowPatch) {
        tree = batch.clone()

        const upgrade = generateUpgrade(nodes, batch.length, inp.patch)
        const proof = {
          fork: tree.fork,
          block: null,
          hash: null,
          seek: null,
          upgrade,
          manifest: null
        }

        try {
          if (!tree.verifyUpgrade(proof)) return false
        } catch {
          return false
        }
      }

      if (inp.signer >= this.signers.length || tried[inp.signer]) return false
      tried[inp.signer] = 1

      const s = this.signers[inp.signer]
      if (!s.verify(tree, inp.signature)) return false
    }

    return true
  }

  verify(batch, signature) {
    if (this.version === 0) {
      return this._verifyCompat(batch, signature)
    }

    if (this.prologue !== null && batch.length <= this.prologue.length) {
      return batch.length === this.prologue.length && b4a.equals(batch.hash(), this.prologue.hash)
    }

    return this._verifyMulti(batch, signature)
  }

  // TODO: better api for this that is more ... multisig-ey
  sign(batch, keyPair) {
    if (!keyPair || !keyPair.secretKey) throw BAD_ARGUMENT('No key pair was passed')

    for (const s of this.signers) {
      if (b4a.equals(s.publicKey, keyPair.publicKey)) {
        const signature = s.sign(batch, keyPair)
        if (this.signers.length !== 1 || this.version === 0) return signature
        return this.assemble([{ signer: 0, signature, patch: 0, nodes: null }])
      }
    }

    throw BAD_ARGUMENT('Public key is not a declared signer')
  }

  assemble(inputs) {
    return this.version === 0 ? multisig.assemblev0(inputs) : multisig.assemble(inputs)
  }

  static manifestHash(manifest) {
    return manifestHash(manifest)
  }

  static encodeManifest(manifest) {
    return c.encode(m.manifest, manifest)
  }

  static decodeManifest(manifest) {
    return c.decode(m.manifest, manifest)
  }

  static defaultSignerManifest(publicKey) {
    return {
      version: 1,
      hash: 'blake2b',
      allowPatch: false,
      quorum: 1,
      signers: [
        {
          signature: 'ed25519',
          namespace: caps.DEFAULT_NAMESPACE,
          publicKey
        }
      ],
      prologue: null,
      linked: null,
      userData: null
    }
  }

  static fromManifest(manifest, opts) {
    const m = this.createManifest(manifest)
    return new this(manifestHash(m), m, opts)
  }

  static createManifest(inp) {
    if (!inp) return null

    const manifest = {
      version: getManifestVersion(inp), // defaults to v1
      hash: 'blake2b',
      allowPatch: !!inp.allowPatch,
      quorum: defaultQuorum(inp),
      signers: inp.signers ? inp.signers.map(parseSigner) : [],
      prologue: null,
      linked: null,
      userData: inp.userData || null
    }

    if (inp.hash && inp.hash !== 'blake2b') throw BAD_ARGUMENT('Only Blake2b hashes are supported')

    if (inp.prologue) {
      if (
        !(b4a.isBuffer(inp.prologue.hash) && inp.prologue.hash.byteLength === 32) ||
        !(inp.prologue.length >= 0)
      ) {
        throw BAD_ARGUMENT('Invalid prologue')
      }

      manifest.prologue = inp.prologue
      manifest.prologue.hash = unslab(manifest.prologue.hash)
    }

    if (manifest.userData !== null && manifest.version < 2) {
      throw BAD_ARGUMENT('Invalid field: userData')
    }

    if (inp.linked && inp.linked.length) {
      if (manifest.version < 2) throw BAD_ARGUMENT('Invalid field: linked')

      for (const key of inp.linked) {
        if (!(b4a.isBuffer(key) && key.byteLength === 32)) {
          throw BAD_ARGUMENT('Invalid key')
        }
      }

      manifest.linked = inp.linked
    }

    return manifest
  }

  static isValidManifest(key, manifest) {
    return b4a.equals(key, manifestHash(manifest))
  }

  static isCompat(key, manifest) {
    return isCompat(key, manifest)
  }

  static sign(manifest, batch, keyPair, opts) {
    return Verifier.fromManifest(manifest, opts).sign(batch, keyPair)
  }
}

function toMap(nodes) {
  const m = new Map()
  for (const node of nodes) m.set(node.index, node)
  return m
}

function isCompat(key, manifest) {
  return !!(
    manifest &&
    manifest.signers.length === 1 &&
    b4a.equals(key, manifest.signers[0].publicKey)
  )
}

function defaultQuorum(man) {
  if (typeof man.quorum === 'number') return man.quorum
  if (!man.signers || !man.signers.length) return 0
  return (man.signers.length >> 1) + 1
}

function generateUpgrade(patch, start, length) {
  const upgrade = { start, length, nodes: null, additionalNodes: [], signature: null }

  const from = start * 2
  const to = from + length * 2

  for (const ite = flat.iterator(0); ite.fullRoot(to); ite.nextTree()) {
    if (ite.index + ite.factor / 2 < from) continue

    if (upgrade.nodes === null && ite.contains(from - 2)) {
      upgrade.nodes = []

      const root = ite.index
      const target = from - 2

      ite.seek(target)

      while (ite.index !== root) {
        ite.sibling()
        if (ite.index > target) upgrade.nodes.push(patch.get(ite.index))
        ite.parent()
      }

      continue
    }

    if (upgrade.nodes === null) upgrade.nodes = []
    upgrade.nodes.push(patch.get(ite.index))
  }

  if (upgrade.nodes === null) upgrade.nodes = []
  return upgrade
}

function parseSigner(signer) {
  validateSigner(signer)
  return {
    signature: 'ed25519',
    namespace: unslab(signer.namespace || caps.DEFAULT_NAMESPACE),
    publicKey: unslab(signer.publicKey)
  }
}

function validateSigner(signer) {
  if (!signer || !signer.publicKey) throw BAD_ARGUMENT('Signer missing public key')
  if (signer.signature && signer.signature !== 'ed25519') {
    throw BAD_ARGUMENT('Only Ed25519 signatures are supported')
  }
}

function manifestHash(manifest) {
  const state = { start: 0, end: 32, buffer: null }
  m.manifest.preencode(state, manifest)
  state.buffer = b4a.allocUnsafe(state.end)
  c.raw.encode(state, caps.MANIFEST)
  m.manifest.encode(state, manifest)
  return crypto.hash(state.buffer)
}

function proofToVersion1(proof) {
  return {
    signer: proof.signer,
    signature: proof.signature,
    patch: proof.patch ? proof.patch.length : 0
  }
}

function getManifestVersion(inp) {
  if (typeof inp.version === 'number') return inp.version
  if (inp.linked && inp.linked.length) return 2
  if (inp.userData) return 2
  return 1
}
{
  "name": "hypercore",
  "version": "11.21.5",
  "description": "Hypercore is a secure, distributed append-only log",
  "main": "index.js",
  "scripts": {
    "format": "prettier --write .",
    "lint": "prettier --check . && lunte",
    "test": "brittle test/all.js",
    "test:bare": "bare test/all.js",
    "test:generate": "brittle -r test/all.js test/*.js"
  },
  "repository": {
    "type": "git",
    "url": "git+https://github.com/holepunchto/hypercore.git"
  },
  "contributors": [
    {
      "name": "Mathias Buus",
      "email": "mathiasbuus@gmail.com",
      "url": "https://mafinto.sh"
    },
    {
      "name": "Andrew Osheroff",
      "email": "andrewosh@gmail.com",
      "url": "https://andrewosh.com"
    }
  ],
  "license": "MIT",
  "bugs": {
    "url": "https://github.com/holepunchto/hypercore/issues"
  },
  "homepage": "https://github.com/holepunchto/hypercore#readme",
  "files": [
    "index.js",
    "errors.js",
    "messages.js",
    "lib/**.js"
  ],
  "imports": {
    "events": {
      "bare": "bare-events",
      "default": "events"
    }
  },
  "dependencies": {
    "@hyperswarm/secret-stream": "^6.0.0",
    "b4a": "^1.1.0",
    "bare-events": "^2.2.0",
    "big-sparse-array": "^1.0.3",
    "compact-encoding": "^2.11.0",
    "fast-fifo": "^1.3.0",
    "flat-tree": "^1.9.0",
    "hypercore-crypto": "^3.2.1",
    "hypercore-errors": "^1.5.0",
    "hypercore-id-encoding": "^1.2.0",
    "hypercore-storage": "^2.0.0",
    "is-options": "^1.0.1",
    "nanoassert": "^2.0.0",
    "protomux": "^3.5.0",
    "quickbit-universal": "^2.2.0",
    "random-array-iterator": "^1.0.0",
    "safety-catch": "^1.0.1",
    "sodium-universal": "^5.0.1",
    "streamx": "^2.12.4",
    "unslab": "^1.3.0",
    "z32": "^1.0.0"
  },
  "devDependencies": {
    "brittle": "^3.0.0",
    "debugging-stream": "^3.1.0",
    "hyperswarm": "^4.3.6",
    "lunte": "^1.3.0",
    "prettier": "^3.6.2",
    "prettier-config-holepunch": "^2.0.0",
    "rache": "^1.0.0",
    "range-parser": "^1.2.1",
    "resolve-reject-promise": "^1.1.0",
    "speedometer": "^1.1.0",
    "test-tmp": "^1.0.2",
    "tiny-byte-size": "^1.1.0",
    "udx-native": "^1.6.1",
    "uncaughts": "^1.1.0"
  }
}
const DHT = require('dht-rpc')
const sodium = require('sodium-universal')
const c = require('compact-encoding')
const b4a = require('b4a')
const safetyCatch = require('safety-catch')
const m = require('./lib/messages')
const SocketPool = require('./lib/socket-pool')
const Persistent = require('./lib/persistent')
const Router = require('./lib/router')
const Cache = require('xache')
const Server = require('./lib/server')
const connect = require('./lib/connect')
const { FIREWALL, BOOTSTRAP_NODES, KNOWN_NODES, COMMANDS } = require('./lib/constants')
const { hash, createKeyPair } = require('./lib/crypto')
const { decode } = require('hypercore-id-encoding')
const RawStreamSet = require('./lib/raw-stream-set')
const ConnectionPool = require('./lib/connection-pool')
const { STREAM_NOT_CONNECTED } = require('./lib/errors')

class HyperDHT extends DHT {
  constructor(opts = {}) {
    const port = opts.port || 49737
    const bootstrap = opts.bootstrap || BOOTSTRAP_NODES
    const nodes = opts.nodes || KNOWN_NODES

    super({ ...opts, port, bootstrap, nodes, filterNode })

    const { router, relayAddresses, persistent } = defaultCacheOpts(opts)

    this.defaultKeyPair = opts.keyPair || createKeyPair(opts.seed)
    this.listening = new Set()
    this.connectionKeepAlive =
      opts.connectionKeepAlive === false ? 0 : opts.connectionKeepAlive || 5000

    // stats is inherited from dht-rpc so fwd the ones from there
    this.stats = {
      punches: { consistent: 0, random: 0, open: 0 },
      relaying: { attempts: 0, successes: 0, aborts: 0 },
      ...this.stats
    }
    this.rawStreams = new RawStreamSet(this)

    this._router = new Router(this, router)
    this._socketPool = new SocketPool(this, opts.host || '0.0.0.0')
    this._persistent = null
    this._validatedLocalAddresses = new Map()
    this._relayAddressesCache = new Cache(relayAddresses)

    this._deferRandomPunch = !!opts.deferRandomPunch
    this._lastRandomPunch = this._deferRandomPunch ? Date.now() : 0
    this._connectable = true
    this._randomPunchInterval = opts.randomPunchInterval || 20000 // min 20s between random punches...
    this._randomPunches = 0
    this._randomPunchLimit = 1 // set to one for extra safety for now

    this.once('persistent', () => {
      this._persistent = new Persistent(this, persistent)
    })

    this.on('network-change', () => {
      for (const server of this.listening) server.refresh()
    })

    this.on('network-update', () => {
      if (!this.online) return
      for (const server of this.listening) server.notifyOnline()
    })
  }

  connect(remotePublicKey, opts) {
    return connect(this, decode(remotePublicKey), opts)
  }

  createServer(opts, onconnection) {
    if (typeof opts === 'function') return this.createServer({}, opts)
    if (opts && opts.onconnection) onconnection = opts.onconnection
    const s = new Server(this, opts)
    if (onconnection) s.on('connection', onconnection)
    return s
  }

  pool() {
    return new ConnectionPool(this)
  }

  async resume({ log = noop } = {}) {
    if (this._deferRandomPunch) this._lastRandomPunch = Date.now()
    await super.resume({ log })
    const resuming = []
    for (const server of this.listening) resuming.push(server.resume())
    log('Resuming hyperdht servers')
    await Promise.allSettled(resuming)
    log('Done, hyperdht fully resumed')
  }

  async suspend({ log = noop } = {}) {
    this._connectable = false // just so nothing gets connected during suspension
    const suspending = []
    for (const server of this.listening) suspending.push(server.suspend())
    log('Suspending all hyperdht servers')
    await Promise.allSettled(suspending)
    log('Done, clearing all raw streams')
    await this.rawStreams.clear()
    log('Done, suspending dht-rpc')
    await super.suspend({ log })
    log('Done, clearing raw streams again')
    await this.rawStreams.clear()
    log('Done, hyperdht fully suspended')
    this._connectable = true
  }

  async destroy({ force = false } = {}) {
    if (!force) {
      const closing = []
      for (const server of this.listening) closing.push(server.close())
      await Promise.allSettled(closing)
    }
    this._router.destroy()
    if (this._persistent) this._persistent.destroy()
    await this.rawStreams.clear()
    await this._socketPool.destroy()
    await super.destroy()
  }

  async validateLocalAddresses(addresses) {
    const list = []
    const socks = []
    const waiting = []

    for (const addr of addresses) {
      const { host } = addr

      if (this._validatedLocalAddresses.has(host)) {
        if (await this._validatedLocalAddresses.get(host)) {
          list.push(addr)
        }
        continue
      }

      const sock = this.udx.createSocket()
      try {
        sock.bind(0, host)
      } catch {
        this._validatedLocalAddresses.set(host, Promise.resolve(false))
        continue
      }

      socks.push(sock)

      // semi terrible heuristic until we proper fix local connections by racing them to the remote...
      const promise = new Promise((resolve) => {
        sock.on('message', () => resolve(true))
        setTimeout(() => resolve(false), 500)
        sock.trySend(b4a.alloc(1), sock.address().port, addr.host)
      })

      this._validatedLocalAddresses.set(host, promise)
      waiting.push(addr)
    }

    for (const addr of waiting) {
      const { host } = addr
      if (this._validatedLocalAddresses.has(host)) {
        if (await this._validatedLocalAddresses.get(host)) {
          list.push(addr)
        }
        continue
      }
    }

    for (const sock of socks) await sock.close()

    return list
  }

  findPeer(publicKey, opts = {}) {
    const target = opts.hash === false ? publicKey : hash(publicKey)
    opts = { ...opts, map: mapFindPeer }
    return this.query({ target, command: COMMANDS.FIND_PEER, value: null }, opts)
  }

  lookup(target, opts = {}) {
    opts = { ...opts, map: mapLookup }
    return this.query({ target, command: COMMANDS.LOOKUP, value: null }, opts)
  }

  lookupAndUnannounce(target, keyPair, opts = {}) {
    const unannounces = []
    const dht = this
    const userCommit = opts.commit || noop
    const signUnannounce = opts.signUnannounce || Persistent.signUnannounce

    if (this._persistent !== null) {
      // unlink self
      this._persistent.unannounce(target, keyPair.publicKey)
    }

    opts = { ...opts, map, commit }
    return this.query({ target, command: COMMANDS.LOOKUP, value: null }, opts)

    async function commit(reply, dht, query) {
      await Promise.all(unannounces) // can never fail, caught below
      return userCommit(reply, dht, query)
    }

    function map(reply) {
      const data = mapLookup(reply)

      if (!data || !data.token) return data

      let found = data.peers.length >= 20
      for (let i = 0; !found && i < data.peers.length; i++) {
        found = b4a.equals(data.peers[i].publicKey, keyPair.publicKey)
      }

      if (!found) return data

      if (!data.from.id) return data

      unannounces.push(
        dht
          ._requestUnannounce(keyPair, dht, target, data.token, data.from, signUnannounce)
          .catch(safetyCatch)
      )

      return data
    }
  }

  unannounce(target, keyPair, opts = {}) {
    return this.lookupAndUnannounce(target, keyPair, opts).finished()
  }

  announce(target, keyPair, relayAddresses, opts = {}) {
    const signAnnounce = opts.signAnnounce || Persistent.signAnnounce
    const bump = opts.bump || 0

    opts = { ...opts, commit }

    return opts.clear ? this.lookupAndUnannounce(target, keyPair, opts) : this.lookup(target, opts)

    function commit(reply, dht) {
      return dht._requestAnnounce(
        keyPair,
        dht,
        target,
        reply.token,
        reply.from,
        relayAddresses,
        signAnnounce,
        bump
      )
    }
  }

  async immutableGet(target, opts = {}) {
    opts = { ...opts, map: mapImmutable }

    const query = this.query({ target, command: COMMANDS.IMMUTABLE_GET, value: null }, opts)
    const check = b4a.allocUnsafe(32)

    for await (const node of query) {
      const { value } = node
      sodium.crypto_generichash(check, value)
      if (b4a.equals(check, target)) return node
    }

    return null
  }

  async immutablePut(value, opts = {}) {
    const target = b4a.allocUnsafe(32)
    sodium.crypto_generichash(target, value)

    opts = {
      ...opts,
      map: mapImmutable,
      commit(reply, dht) {
        return dht.request(
          { token: reply.token, target, command: COMMANDS.IMMUTABLE_PUT, value },
          reply.from
        )
      }
    }

    const query = this.query({ target, command: COMMANDS.IMMUTABLE_GET, value: null }, opts)
    await query.finished()

    return { hash: target, closestNodes: query.closestNodes }
  }

  async mutableGet(publicKey, opts = {}) {
    let refresh = opts.refresh || null
    let signed = null
    let result = null

    opts = { ...opts, map: mapMutable, commit: refresh ? commit : null }

    const target = b4a.allocUnsafe(32)
    sodium.crypto_generichash(target, publicKey)

    const userSeq = opts.seq || 0
    const query = this.query(
      { target, command: COMMANDS.MUTABLE_GET, value: c.encode(c.uint, userSeq) },
      opts
    )
    const latest = opts.latest !== false

    for await (const node of query) {
      if (result && node.seq <= result.seq) continue
      if (
        node.seq < userSeq ||
        !Persistent.verifyMutable(node.signature, node.seq, node.value, publicKey)
      )
        continue
      if (!latest) return node
      if (!result || node.seq > result.seq) result = node
    }

    return result

    function commit(reply, dht) {
      if (!signed && result && refresh) {
        if (refresh(result)) {
          signed = c.encode(m.mutablePutRequest, {
            publicKey,
            seq: result.seq,
            value: result.value,
            signature: result.signature
          })
        } else {
          refresh = null
        }
      }

      return signed
        ? dht.request(
            { token: reply.token, target, command: COMMANDS.MUTABLE_PUT, value: signed },
            reply.from
          )
        : Promise.resolve(null)
    }
  }

  async mutablePut(keyPair, value, opts = {}) {
    const signMutable = opts.signMutable || Persistent.signMutable

    const target = b4a.allocUnsafe(32)
    sodium.crypto_generichash(target, keyPair.publicKey)

    const seq = opts.seq || 0
    const signature = await signMutable(seq, value, keyPair)

    const signed = c.encode(m.mutablePutRequest, {
      publicKey: keyPair.publicKey,
      seq,
      value,
      signature
    })

    opts = {
      ...opts,
      map: mapMutable,
      commit(reply, dht) {
        return dht.request(
          { token: reply.token, target, command: COMMANDS.MUTABLE_PUT, value: signed },
          reply.from
        )
      }
    }

    // use seq = 0, for the query part here, as we don't care about the actual values
    const query = this.query(
      { target, command: COMMANDS.MUTABLE_GET, value: c.encode(c.uint, 0) },
      opts
    )
    await query.finished()

    return { publicKey: keyPair.publicKey, closestNodes: query.closestNodes, seq, signature }
  }

  onrequest(req) {
    switch (req.command) {
      case COMMANDS.PEER_HANDSHAKE: {
        this._router.onpeerhandshake(req)
        return true
      }
      case COMMANDS.PEER_HOLEPUNCH: {
        this._router.onpeerholepunch(req)
        return true
      }
    }

    if (this._persistent === null) return false

    switch (req.command) {
      case COMMANDS.FIND_PEER: {
        this._persistent.onfindpeer(req)
        return true
      }
      case COMMANDS.LOOKUP: {
        this._persistent.onlookup(req)
        return true
      }
      case COMMANDS.ANNOUNCE: {
        this._persistent.onannounce(req)
        return true
      }
      case COMMANDS.UNANNOUNCE: {
        this._persistent.onunannounce(req)
        return true
      }
      case COMMANDS.MUTABLE_PUT: {
        this._persistent.onmutableput(req)
        return true
      }
      case COMMANDS.MUTABLE_GET: {
        this._persistent.onmutableget(req)
        return true
      }
      case COMMANDS.IMMUTABLE_PUT: {
        this._persistent.onimmutableput(req)
        return true
      }
      case COMMANDS.IMMUTABLE_GET: {
        this._persistent.onimmutableget(req)
        return true
      }
    }

    return false
  }

  static keyPair(seed) {
    return createKeyPair(seed)
  }

  static hash(data) {
    return hash(data)
  }

  static connectRawStream(encryptedStream, rawStream, remoteId) {
    const stream = encryptedStream.rawStream

    if (!stream.connected) throw STREAM_NOT_CONNECTED()

    rawStream.connect(stream.socket, remoteId, stream.remotePort, stream.remoteHost)
  }

  createRawStream(opts) {
    return this.rawStreams.add(opts)
  }

  async _requestAnnounce(keyPair, dht, target, token, from, relayAddresses, sign, bump) {
    const ann = {
      peer: {
        publicKey: keyPair.publicKey,
        relayAddresses: relayAddresses || []
      },
      refresh: null,
      signature: null,
      bump
    }

    ann.signature = await sign(target, token, from.id, ann, keyPair)

    const value = c.encode(m.announce, ann)

    return dht.request(
      {
        token,
        target,
        command: COMMANDS.ANNOUNCE,
        value
      },
      from
    )
  }

  async _requestUnannounce(keyPair, dht, target, token, from, sign) {
    const unann = {
      peer: {
        publicKey: keyPair.publicKey,
        relayAddresses: []
      },
      signature: null
    }

    unann.signature = await sign(target, token, from.id, unann, keyPair)

    const value = c.encode(m.announce, unann)

    return dht.request(
      {
        token,
        target,
        command: COMMANDS.UNANNOUNCE,
        value
      },
      from
    )
  }
}

HyperDHT.BOOTSTRAP = BOOTSTRAP_NODES
HyperDHT.FIREWALL = FIREWALL

module.exports = HyperDHT

function mapLookup(node) {
  if (!node.value) return null

  const l = c.decode(m.lookupRawReply, node.value)

  try {
    return {
      token: node.token,
      from: node.from,
      to: node.to,
      peers: l.peers,
      bump: l.bump
    }
  } catch {
    return null
  }
}

function mapFindPeer(node) {
  if (!node.value) return null

  try {
    return {
      token: node.token,
      from: node.from,
      to: node.to,
      peer: c.decode(m.peer, node.value)
    }
  } catch {
    return null
  }
}

function mapImmutable(node) {
  if (!node.value) return null

  return {
    token: node.token,
    from: node.from,
    to: node.to,
    value: node.value
  }
}

function mapMutable(node) {
  if (!node.value) return null

  try {
    const { seq, value, signature } = c.decode(m.mutableGetResponse, node.value)

    return {
      token: node.token,
      from: node.from,
      to: node.to,
      seq,
      value,
      signature
    }
  } catch {
    return null
  }
}

function noop() {}

function filterNode(node) {
  // always skip these testnet nodes that got mixed in by accident, until they get updated
  return (
    !(node.port === 49738 && (node.host === '134.209.28.98' || node.host === '167.99.142.185')) &&
    !(node.port === 9400 && node.host === '35.233.47.252') &&
    !(node.host === '150.136.142.116')
  )
}

const defaultMaxSize = 65536
const defaultMaxAge = 20 * 60 * 1000 // 20 minutes

function defaultCacheOpts(opts) {
  const maxSize = opts.maxSize || defaultMaxSize
  const maxAge = opts.maxAge || defaultMaxAge

  return {
    router: {
      forwards: { maxSize, maxAge }
    },
    relayAddresses: { maxSize: Math.min(maxSize, 512), maxAge: 0 },
    persistent: {
      records: { maxSize, maxAge },
      refreshes: { maxSize, maxAge },
      mutables: {
        maxSize: (maxSize / 2) | 0,
        maxAge: opts.maxAge || 48 * 60 * 60 * 1000 // 48 hours
      },
      immutables: {
        maxSize: (maxSize / 2) | 0,
        maxAge: opts.maxAge || 48 * 60 * 60 * 1000 // 48 hours
      },
      bumps: { maxSize, maxAge }
    }
  }
}
const safetyCatch = require('safety-catch')
const c = require('compact-encoding')
const Signal = require('signal-promise')
const { encodeUnslab } = require('./encode')
const Sleeper = require('./sleeper')
const m = require('./messages')
const Persistent = require('./persistent')
const { COMMANDS } = require('./constants')

const MIN_ACTIVE = 3

module.exports = class Announcer {
  constructor(dht, keyPair, target, opts = {}) {
    this.dht = dht
    this.keyPair = keyPair
    this.target = target
    this.relays = []
    this.relayAddresses = []
    this.stopped = false
    this.suspended = false
    this.record = encodeUnslab(m.peer, { publicKey: keyPair.publicKey, relayAddresses: [] })
    this.online = new Signal()

    this._refreshing = false
    this._closestNodes = null
    this._active = null
    this._sleeper = new Sleeper()
    this._resumed = new Signal()
    this._signAnnounce = opts.signAnnounce || Persistent.signAnnounce
    this._signUnannounce = opts.signUnannounce || Persistent.signUnannounce
    this._updating = null
    this._activeQuery = null
    this._unannouncing = null

    this._serverRelays = [new Map(), new Map(), new Map()]
  }

  isRelay(addr) {
    const id = addr.host + ':' + addr.port
    const [a, b, c] = this._serverRelays
    return a.has(id) || b.has(id) || c.has(id)
  }

  async suspend({ log = noop } = {}) {
    if (this.suspended) return
    this.suspended = true

    log('Suspending announcer')

    // Suspend has its own sleep logic
    // so we don't want to hang on this one
    this.online.notify()

    if (this._activeQuery) this._activeQuery.destroy()

    this._sleeper.resume()
    if (this._updating) await this._updating
    log('Suspending announcer (post update)')

    if (this.suspended === false || this.stopped) return

    log('Suspending announcer (pre unannounce)')
    await this._unannounceCurrent()
    log('Suspending announcer (post unannounce)')
  }

  resume() {
    if (!this.suspended) return
    this.suspended = false

    this.refresh()
    this._sleeper.resume()
    this._resumed.notify()
  }

  refresh() {
    if (this.stopped) return
    this._refreshing = true
  }

  async start() {
    if (this.stopped) return
    this._active = this._runUpdate()
    await this._active
    if (this.stopped) return
    this._active = this._background()
  }

  async stop() {
    this.stopped = true
    this.online.notify() // Break out of the _background loop if we're offline
    this._sleeper.resume()
    this._resumed.notify()
    await this._active
    await this._unannounceCurrent()
  }

  async _unannounceCurrent() {
    while (this._unannouncing !== null) await this._unannouncing
    const un = (this._unannouncing = this._unannounceAll(this._serverRelays[2].values()))
    await this._unannouncing
    if (un === this._unannouncing) this._unannouncing = null
  }

  async _background() {
    while (!this.dht.destroyed && !this.stopped) {
      try {
        this._refreshing = false

        // ~5min +-
        for (let i = 0; i < 100 && !this.stopped && !this._refreshing && !this.suspended; i++) {
          const pings = []

          for (const node of this._serverRelays[2].values()) {
            pings.push(this.dht.ping(node))
          }

          const active = await resolved(pings)
          if (active < Math.min(pings.length, MIN_ACTIVE)) {
            this.refresh() // we lost too many relay nodes, retry all
          }

          if (this.stopped) return

          if (!this.suspended && !this._refreshing) await this._sleeper.pause(3000)
        }

        while (!this.stopped && this.suspended) await this._resumed.wait()

        if (!this.stopped) await this._runUpdate()

        while (!this.dht.online && !this.stopped && !this.suspended) {
          // Being offline can make _background repeat very quickly
          // So wait until we're back online
          await this.online.wait()
        }
      } catch (err) {
        safetyCatch(err)
      }
    }
  }

  async _runUpdate() {
    this._updating = this._update()
    await this._updating
    this._updating = null
  }

  async _update() {
    while (this._unannouncing) await this._unannouncing

    this._cycle()

    const q = (this._activeQuery = this.dht.findPeer(this.target, {
      hash: false,
      nodes: this._closestNodes
    }))

    try {
      await q.finished()
    } catch {
      // ignore failures...
    }

    this._activeQuery = null

    if (this.stopped || this.suspended) return

    const ann = []
    const replies = pickBest(q.closestReplies)

    const relays = []
    const relayAddresses = []

    if (!this.dht.firewalled) {
      const addr = this.dht.remoteAddress()
      if (addr) relayAddresses.push(addr)
    }

    for (const msg of replies) {
      ann.push(this._commit(msg, relays, relayAddresses))
    }

    await Promise.allSettled(ann)
    if (this.stopped || this.suspended) return

    this._closestNodes = q.closestNodes
    this.relays = relays
    this.relayAddresses = relayAddresses

    const removed = []
    for (const [key, value] of this._serverRelays[1]) {
      if (!this._serverRelays[2].has(key)) removed.push(value)
    }

    await this._unannounceAll(removed)
  }

  _unannounceAll(relays) {
    const unann = []
    for (const r of relays) unann.push(this._unannounce(r))
    return Promise.allSettled(unann)
  }

  async _unannounce(to) {
    const unann = {
      peer: {
        publicKey: this.keyPair.publicKey,
        relayAddresses: []
      },
      refresh: null,
      signature: null
    }

    const { from, token, value } = await this.dht.request(
      {
        token: null,
        command: COMMANDS.FIND_PEER,
        target: this.target,
        value: null
      },
      to
    )

    if (!token || !from.id || !value) return

    unann.signature = await this._signUnannounce(this.target, token, from.id, unann, this.keyPair)

    await this.dht.request(
      {
        token,
        command: COMMANDS.UNANNOUNCE,
        target: this.target,
        value: c.encode(m.announce, unann)
      },
      to
    )
  }

  async _commit(msg, relays, relayAddresses) {
    const ann = {
      peer: {
        publicKey: this.keyPair.publicKey,
        relayAddresses: []
      },
      refresh: null,
      signature: null
    }

    ann.signature = await this._signAnnounce(this.target, msg.token, msg.from.id, ann, this.keyPair)

    const res = await this.dht.request(
      {
        token: msg.token,
        command: COMMANDS.ANNOUNCE,
        target: this.target,
        value: c.encode(m.announce, ann)
      },
      msg.from
    )

    if (res.error !== 0) return

    if (relayAddresses.length < 3) relayAddresses.push({ host: msg.from.host, port: msg.from.port })
    relays.push({ relayAddress: msg.from, peerAddress: msg.to })

    this._serverRelays[2].set(msg.from.host + ':' + msg.from.port, msg.from)
  }

  _cycle() {
    const tmp = this._serverRelays[0]
    this._serverRelays[0] = this._serverRelays[1]
    this._serverRelays[1] = this._serverRelays[2]
    this._serverRelays[2] = tmp
    tmp.clear()
  }
}

function resolved(ps) {
  let replied = 0
  let ticks = ps.length + 1

  return new Promise((resolve) => {
    for (const p of ps) p.then(push, tick)
    tick()

    function push(v) {
      replied++
      tick()
    }

    function tick() {
      if (--ticks === 0) resolve(replied)
    }
  })
}

function pickBest(replies) {
  // TODO: pick the ones closest to us RTT wise
  return replies.slice(0, 3)
}

function noop() {}
const NoiseSecretStream = require('@hyperswarm/secret-stream')
const b4a = require('b4a')
const relay = require('blind-relay')
const { isPrivate, isBogon } = require('bogon')
const safetyCatch = require('safety-catch')
const unslab = require('unslab')
const Semaphore = require('./semaphore')
const NoiseWrap = require('./noise-wrap')
const SecurePayload = require('./secure-payload')
const Holepuncher = require('./holepuncher')
const Sleeper = require('./sleeper')
const { FIREWALL, ERROR } = require('./constants')
const { unslabbedHash } = require('./crypto')
const {
  CANNOT_HOLEPUNCH,
  HANDSHAKE_INVALID,
  HOLEPUNCH_ABORTED,
  HOLEPUNCH_INVALID,
  HOLEPUNCH_PROBE_TIMEOUT,
  HOLEPUNCH_DOUBLE_RANDOMIZED_NATS,
  PEER_CONNECTION_FAILED,
  PEER_NOT_FOUND,
  REMOTE_ABORTED,
  REMOTE_NOT_HOLEPUNCHABLE,
  REMOTE_NOT_HOLEPUNCHING,
  SERVER_ERROR,
  SERVER_INCOMPATIBLE,
  RELAY_ABORTED,
  SUSPENDED
} = require('./errors')

module.exports = function connect(dht, publicKey, opts = {}) {
  const pool = opts.pool || null

  if (pool && pool.has(publicKey)) return pool.get(publicKey)

  publicKey = unslab(publicKey)

  const keyPair = opts.keyPair || dht.defaultKeyPair
  const relayThrough = selectRelay(opts.relayThrough || null)
  const encryptedSocket = (opts.createSecretStream || defaultCreateSecretStream)(true, null, {
    publicKey: keyPair.publicKey,
    remotePublicKey: publicKey,
    autoStart: false,
    keepAlive: dht.connectionKeepAlive
  })

  // in case a socket is made during suspended state, destroy it immediately
  if (dht.suspended || !dht._connectable) {
    encryptedSocket.destroy(SUSPENDED())
    return encryptedSocket
  }

  if (pool) pool._attachStream(encryptedSocket, false)

  const id = b4a.toString(publicKey, 'hex')
  const c = {
    id,
    dht,
    session: dht.session(),
    relayAddresses: opts.relayAddresses || [],
    remoteRelayAddresses: [],
    pool,
    round: 0,
    target: unslabbedHash(publicKey),
    remotePublicKey: publicKey,
    reusableSocket: !!opts.reusableSocket,
    handshake: (opts.createHandshake || defaultCreateHandshake)(keyPair, publicKey),
    request: null,
    requesting: false,
    lan: opts.localConnection !== false,
    firewall: FIREWALL.UNKNOWN,
    rawStream: dht.createRawStream({ framed: true, firewall }),
    connect: null,
    query: null,
    puncher: null,
    payload: null,
    passiveConnectTimeout: null,
    serverSocket: null,
    serverAddress: null,
    onsocket: null,
    sleeper: new Sleeper(),
    encryptedSocket,

    // Relay state
    relayTimeout: null,
    relayThrough,
    relayToken: relayThrough ? relay.token() : null,
    relaySocket: null,
    relayClient: null,
    relayPaired: false,
    relayKeepAlive: opts.relayKeepAlive || 5000
  }

  // If the raw stream receives an error signal pre connect (ie from the firewall hook), make sure
  // to forward that to the encrypted socket for proper teardown
  c.rawStream.on('error', autoDestroy)
  c.rawStream.once('connect', () => {
    c.rawStream.removeListener('error', autoDestroy)
  })

  encryptedSocket.on('close', function () {
    if (c.passiveConnectTimeout) clearPassiveConnectTimeout(c)
    if (c.query) c.query.destroy()
    if (c.puncher) c.puncher.destroy()
    if (c.rawStream) c.rawStream.destroy()
    c.session.destroy()
    c.sleeper.resume()
  })

  // Safe to run in the background - never throws
  if (dht.suspended) encryptedSocket.destroy(SUSPENDED())
  else connectAndHolepunch(c, opts)

  return encryptedSocket

  function autoDestroy(err) {
    maybeDestroyEncryptedSocket(c, err)
  }

  function firewall(socket, port, host) {
    // Check if the traffic originated from the socket on which we're expecting relay traffic. If so,
    // we haven't hole punched yet and the other side is just sending us traffic through the relay.
    if (c.relaySocket && isRelay(c.relaySocket, socket, port, host)) {
      return false
    }

    if (c.onsocket) {
      c.onsocket(socket, port, host)
    } else {
      c.serverSocket = socket
      c.serverAddress = { port, host }
    }
    return false
  }
}

function isDone(c) {
  // we are destroying or the puncher is connected - done
  if (c.encryptedSocket.destroying || !!(c.puncher && c.puncher.connected)) {
    return true
  }
  // not destroying, but no raw stream - def not done
  if (c.encryptedSocket.rawStream === null) {
    return false
  }
  // we are relayed, but the puncher is not done yet
  if (c.relaySocket && !!(c.puncher && !c.puncher.connected && !c.puncher.destroyed)) {
    return false
  }
  // we are done
  return true
}

async function retryRoute(c, route) {
  const ref = c.dht._socketPool.lookup(route.socket)

  if (!ref) {
    if (route.socket === c.dht.socket) {
      await connectThroughNode(c, route.address, c.dht.socket)
    }
    return
  }

  ref.active()

  try {
    await connectThroughNode(c, route.address, route.socket)
  } catch {
    // if error, just ignore, and continue through the existing strat
  }

  ref.inactive()
}

async function connectAndHolepunch(c, opts) {
  const route = c.reusableSocket ? c.dht._socketPool.routes.get(c.remotePublicKey) : null

  if (route) {
    await retryRoute(c, route)
    if (isDone(c)) return
  }

  await findAndConnect(c, opts)
  if (isDone(c)) return

  if (!c.connect) {
    // TODO: just a quick fix for now, should retry prob
    maybeDestroyEncryptedSocket(c, HANDSHAKE_INVALID())
    return
  }

  await holepunch(c, opts)
}

function getFirstRemoteAddress(addrs, serverAddress) {
  for (const addr of addrs) {
    if (isBogon(addr.host)) continue
    return addr
  }

  return serverAddress
}

async function holepunch(c, opts) {
  let { relayAddress, serverAddress, clientAddress, payload } = c.connect

  const remoteHolepunchable = !!(payload.holepunch && payload.holepunch.relays.length)

  const relayed = diffAddress(serverAddress, relayAddress)

  if (payload.firewall === FIREWALL.OPEN || (relayed && !remoteHolepunchable)) {
    const addr = getFirstRemoteAddress(payload.addresses4, serverAddress)
    if (addr) {
      const socket = c.dht.socket
      c.dht.stats.punches.open++
      c.onsocket(socket, addr.port, addr.host)
      return
    }
    // TODO: check all addresses also obvs
  }

  const onabort = () => {
    c.session.destroy()
    maybeDestroyEncryptedSocket(c, HOLEPUNCH_ABORTED())
  }

  if (c.firewall === FIREWALL.OPEN) {
    c.passiveConnectTimeout = setTimeout(onabort, 10000)
    return
  }

  // TODO: would be better to just try local addrs in the background whilst continuing with other strategies...
  if (c.lan && relayed && clientAddress.host === serverAddress.host) {
    const serverAddresses = payload.addresses4.filter(onlyPrivateHosts)

    if (serverAddresses.length > 0) {
      const myAddresses = Holepuncher.localAddresses(c.dht.io.serverSocket)
      const addr = Holepuncher.matchAddress(myAddresses, serverAddresses) || serverAddresses[0]

      const socket = c.dht.io.serverSocket
      try {
        await c.dht.ping(addr)
      } catch {
        maybeDestroyEncryptedSocket(c, HOLEPUNCH_ABORTED())
        return
      }
      c.onsocket(socket, addr.port, addr.host)
      return
    }
  }

  if (!remoteHolepunchable) {
    maybeDestroyEncryptedSocket(c, CANNOT_HOLEPUNCH())
    return
  }

  c.puncher = new Holepuncher(c.dht, c.session, true, payload.firewall)

  c.puncher.onconnect = c.onsocket
  c.puncher.onabort = onabort

  const serverRelay = pickServerRelay(payload.holepunch.relays, relayAddress)

  // Begin holepunching!

  let probe
  try {
    probe = await probeRound(c, opts.fastOpen === false ? null : serverAddress, serverRelay, true)
  } catch (err) {
    destroyPuncher(c)
    // TODO: we should retry here with some of the other relays, bail for now
    maybeDestroyEncryptedSocket(c, err)
    return
  }

  if (isDone(c) || !probe) return
  const { token, peerAddress } = probe

  // If the relay the server picked is the same as the relay the client picked,
  // then we can use the peerAddress that round one indicates the server wants to use.
  // This shaves off a roundtrip if the server chose to reroll its socket due to some NAT
  // issue with the first one it picked (ie mobile nat inconsistencies...).
  // If the relays were different, then the server would not have a UDP session open on this address
  // to the client relay, which round2 uses.
  if (
    !diffAddress(serverRelay.relayAddress, relayAddress) &&
    diffAddress(serverAddress, peerAddress)
  ) {
    serverAddress = peerAddress
    await c.puncher.openSession(serverAddress)
    if (isDone(c)) return
  }

  // TODO: still continue here if a local connection might work, but then do not holepunch...
  if (
    opts.holepunch &&
    !opts.holepunch(
      c.puncher.remoteFirewall,
      c.puncher.nat.firewall,
      c.puncher.remoteAddresses,
      c.puncher.nat.addresses
    )
  ) {
    await abort(c, serverRelay, HOLEPUNCH_ABORTED('Client aborted holepunch'))
    return
  }

  try {
    await roundPunch(c, serverAddress, token, relayAddress, serverRelay, false)
  } catch (err) {
    destroyPuncher(c)
    // TODO: retry with another relay?
    maybeDestroyEncryptedSocket(c, err)
  }
}

async function findAndConnect(c, opts) {
  let attempts = 0
  let closestNodes = opts.relayAddresses && opts.relayAddresses.length ? opts.relayAddresses : null

  if (!closestNodes) {
    const cachedRelayAddresses = c.dht._relayAddressesCache.get(c.id)
    if (cachedRelayAddresses) closestNodes = cachedRelayAddresses
  }

  if (c.dht._persistent) {
    // check if we know the route ourself...
    const route = c.dht._router.get(c.target)
    if (route && route.relay !== null) {
      closestNodes = [{ host: route.relay.host, port: route.relay.port }]
    }
  }

  // 2 is how many parallel connect attempts we want to do, we can make this configurable
  const sem = new Semaphore(2)
  const signal = sem.signal.bind(sem)
  const tries = closestNodes !== null ? 2 : 1

  try {
    for (let i = 0; i < tries && !isDone(c) && !c.connect; i++) {
      c.query = c.dht.findPeer(c.target, {
        hash: false,
        session: c.session,
        closestNodes,
        onlyClosestNodes: closestNodes !== null,
        retries: closestNodes ? 1 : 3
      })

      for await (const data of c.query) {
        await sem.wait()
        if (isDone(c)) return

        if (c.connect) {
          sem.signal()
          break
        }

        c.remoteRelayAddresses.push(data.from)
        attempts++
        connectThroughNode(c, data.from, null).then(signal, signal)
      }

      closestNodes = null

      if (attempts > 0) await sem.flush()
    }

    c.query = null
    if (isDone(c)) return

    // flush the semaphore
    await sem.flush()
    if (isDone(c)) return
  } catch (err) {
    c.query = null
    maybeDestroyEncryptedSocket(c, err)
    return
  }

  if (!c.connect) {
    maybeDestroyEncryptedSocket(c, attempts ? PEER_CONNECTION_FAILED() : PEER_NOT_FOUND())
  }
}

async function connectThroughNode(c, address, socket) {
  if (!c.requesting) {
    // If we have a stable server address, send it over now
    const addr = c.dht.remoteAddress()
    const localAddrs = c.lan ? Holepuncher.localAddresses(c.dht.io.serverSocket) : null
    const addresses4 = []

    if (addr) addresses4.push(addr)
    if (localAddrs) addresses4.push(...localAddrs)

    c.firewall = addr ? FIREWALL.OPEN : FIREWALL.UNKNOWN
    c.requesting = true
    c.request = await c.handshake.send({
      error: ERROR.NONE,
      firewall: c.firewall,
      holepunch: null,
      addresses4,
      addresses6: [],
      udx: {
        reusableSocket: c.reusableSocket,
        id: c.rawStream.id,
        seq: 0
      },
      secretStream: {},
      relayThrough: c.relayThrough ? { publicKey: c.relayThrough, token: c.relayToken } : null
    })
    if (isDone(c)) return
  }

  const { serverAddress, clientAddress, relayed, noise } = await c.dht._router.peerHandshake(
    c.target,
    { noise: c.request, socket, session: c.session },
    address
  )
  if (isDone(c) || c.connect) return

  const payload = await c.handshake.recv(noise)
  if (isDone(c) || !payload) return

  if (payload.version !== 1) {
    maybeDestroyEncryptedSocket(c, SERVER_INCOMPATIBLE())
    return
  }
  if (payload.error !== ERROR.NONE) {
    maybeDestroyEncryptedSocket(c, SERVER_ERROR())
    return
  }
  if (!payload.udx) {
    maybeDestroyEncryptedSocket(c, SERVER_ERROR('Server did not send UDX data'))
    return
  }

  const hs = c.handshake.final()

  c.handshake = null
  c.request = null
  c.requesting = false
  c.connect = {
    relayed,
    relayAddress: address,
    clientAddress,
    serverAddress,
    payload
  }

  c.payload = new SecurePayload(hs.holepunchSecret)

  c.onsocket = function (socket, port, host) {
    if (c.rawStream === null) return // Already hole punched

    if (c.rawStream.connected) {
      const remoteChanging = c.rawStream.changeRemote(socket, c.connect.payload.udx.id, port, host)

      if (remoteChanging) remoteChanging.catch(safetyCatch)
    } else {
      // cache the relay addrs for a future reconnect, we prefer the remote one so they
      // can give us the correct ones from their pov
      if (payload.relayAddresses && payload.relayAddresses.length) {
        c.dht._relayAddressesCache.set(c.id, payload.relayAddresses)
      } else if (c.remoteRelayAddresses.length) {
        c.dht._relayAddressesCache.set(c.id, c.remoteRelayAddresses)
      }

      c.rawStream.connect(socket, c.connect.payload.udx.id, port, host)
      c.encryptedSocket.start(c.rawStream, { handshake: hs })
    }

    if (c.reusableSocket && payload.udx.reusableSocket) {
      c.dht._socketPool.routes.add(c.remotePublicKey, c.rawStream)
    }

    if (c.puncher) {
      c.puncher.onabort = noop
      c.puncher.destroy()
    }

    if (c.passiveConnectTimeout) {
      clearPassiveConnectTimeout(c)
    }

    c.rawStream = null
  }

  if (payload.relayThrough || c.relayThrough) {
    relayConnection(c, c.relayThrough, payload, hs)
  }

  if (c.serverSocket) {
    c.onsocket(c.serverSocket, c.serverAddress.port, c.serverAddress.host)
    return
  }

  if (!relayed) {
    c.onsocket(socket || c.dht.socket, address.port, address.host)
  }

  c.session.destroy()
}

async function updateHolepunch(c, peerAddress, relayAddr, payload) {
  const holepunch = await c.dht._router.peerHolepunch(
    c.target,
    {
      id: c.connect.payload.holepunch.id,
      payload: c.payload.encrypt(payload),
      peerAddress,
      socket: c.puncher.socket,
      session: c.session
    },
    relayAddr
  )

  if (isDone(c)) return null

  const remotePayload = c.payload.decrypt(holepunch.payload)
  if (!remotePayload) {
    throw HOLEPUNCH_INVALID()
  }

  const { error, firewall, punching, addresses, remoteToken } = remotePayload

  if (error === ERROR.TRY_LATER && c.relayToken && payload.punching) {
    return {
      tryLater: true,
      ...holepunch,
      payload: remotePayload
    }
  }

  if (error !== ERROR.NONE) {
    throw REMOTE_ABORTED('Remote aborted with error code ' + error)
  }

  const echoed = !!(remoteToken && payload.token && b4a.equals(remoteToken, payload.token))

  c.puncher.updateRemote({
    punching,
    firewall,
    addresses,
    verified: echoed ? peerAddress.host : null
  })

  return {
    tryLater: false,
    ...holepunch,
    payload: remotePayload
  }
}

async function probeRound(c, serverAddress, serverRelay, retry) {
  // Open a quick low ttl session against what we think is the server
  if (serverAddress) await c.puncher.openSession(serverAddress)

  if (isDone(c)) return null

  const reply = await updateHolepunch(c, serverRelay.peerAddress, serverRelay.relayAddress, {
    error: ERROR.NONE,
    firewall: c.puncher.nat.firewall,
    round: c.round++,
    connected: false,
    punching: false,
    addresses: c.puncher.nat.addresses,
    remoteAddress: serverAddress,
    token: null,
    remoteToken: null
  })

  if (isDone(c) || !reply) return null

  const { peerAddress } = reply
  const { address, token } = reply.payload

  c.puncher.nat.add(reply.to, reply.from)

  // Open another quick low ttl session against what the server says their address is,
  // if they haven't said they are random yet
  if (
    c.puncher.remoteFirewall < FIREWALL.RANDOM &&
    address &&
    address.host &&
    address.port &&
    diffAddress(address, serverAddress)
  ) {
    await c.puncher.openSession(address)
    if (isDone(c)) return null
  }

  // If the remote told us they didn't know their nat firewall yet, give them a chance to figure it out
  // They might say this to see if the "fast mode" punch comes through first.
  if (c.puncher.remoteFirewall === FIREWALL.UNKNOWN) {
    await c.sleeper.pause(1000)
    if (isDone(c)) return null
  }

  let stable = await c.puncher.analyze(false)
  if (isDone(c)) return null

  // If the socket seems unstable, try to make it stable by setting the "allowReopen" flag
  // Mostly relevant for mobile networks
  if (!stable) {
    stable = await c.puncher.analyze(true)
    if (isDone(c)) return null
    if (stable) return probeRound(c, serverAddress, serverRelay, false)
  }

  if ((c.puncher.remoteFirewall === FIREWALL.UNKNOWN || !token) && retry) {
    return probeRound(c, serverAddress, serverRelay, false)
  }

  if (
    c.puncher.remoteFirewall === FIREWALL.UNKNOWN ||
    c.puncher.nat.firewall === FIREWALL.UNKNOWN
  ) {
    await abort(c, serverRelay, HOLEPUNCH_PROBE_TIMEOUT())
    return null
  }

  if (c.puncher.remoteFirewall >= FIREWALL.RANDOM && c.puncher.nat.firewall >= FIREWALL.RANDOM) {
    await abort(c, serverRelay, HOLEPUNCH_DOUBLE_RANDOMIZED_NATS())
    return null
  }

  return { token, peerAddress }
}

async function roundPunch(c, serverAddress, remoteToken, clientRelay, serverRelay, delayed) {
  // We are gossiping our final NAT status to the other peer now
  // so make sure we don't update our local view for now as that can make things weird
  c.puncher.nat.freeze()

  const isRandom =
    c.puncher.remoteFirewall >= FIREWALL.RANDOM || c.puncher.nat.firewall >= FIREWALL.RANDOM
  if (isRandom) {
    while (
      c.dht._randomPunches >= c.dht._randomPunchLimit ||
      Date.now() - c.dht._lastRandomPunch < c.dht._randomPunchInterval
    ) {
      // if no relay can help, bail
      if (!c.relayToken) throw HOLEPUNCH_ABORTED()

      if (!delayed) {
        delayed = true
        await updateHolepunch(c, serverAddress, clientRelay, {
          error: ERROR.NONE,
          firewall: c.puncher.nat.firewall,
          round: c.round++,
          connected: false,
          punching: false,
          addresses: c.puncher.nat.addresses,
          remoteAddress: null,
          token: c.payload.token(serverAddress),
          remoteToken
        })
        if (isDone(c)) return
      }

      await tryLater(c)
      if (isDone(c)) return
    }
  }

  // increment now, so we can commit to punching
  if (isRandom) c.dht._randomPunches++

  let reply

  try {
    // if delayed switch to the servers chosen relay - we validated anyway
    reply = await updateHolepunch(
      c,
      delayed ? serverRelay.peerAddress : serverAddress,
      delayed ? serverRelay.relayAddress : clientRelay,
      {
        error: ERROR.NONE,
        firewall: c.puncher.nat.firewall,
        round: c.round++,
        connected: false,
        punching: true,
        addresses: c.puncher.nat.addresses,
        remoteAddress: null,
        token: delayed ? null : c.payload.token(serverAddress),
        remoteToken
      }
    )
  } finally {
    // decrement as punch increments for us
    if (isRandom) c.dht._randomPunches--
  }

  if (isDone(c)) return
  if (!reply) return

  if (reply.tryLater) {
    await tryLater(c)
    if (isDone(c)) return
    return roundPunch(c, serverAddress, remoteToken, clientRelay, serverRelay, true)
  }

  if (!c.puncher.remoteHolepunching) {
    throw REMOTE_NOT_HOLEPUNCHING()
  }

  if (!(await c.puncher.punch())) {
    throw REMOTE_NOT_HOLEPUNCHABLE()
  }
}

async function tryLater(c) {
  if (!c.relayToken) throw HOLEPUNCH_ABORTED()
  await c.sleeper.pause(10000 + Math.round(Math.random() * 10000))
}

function maybeDestroyEncryptedSocket(c, err) {
  if (isDone(c)) return
  if (c.encryptedSocket.rawStream) return
  if (c.relaySocket) return // waiting for the relay
  if (c.puncher && !c.puncher.destroyed) return // waiting for the puncher
  c.session.destroy()
  c.encryptedSocket.destroy(err)
}

async function abort(c, { peerAddress, relayAddress }, err) {
  try {
    await updateHolepunch(peerAddress, relayAddress, {
      error: ERROR.ABORTED,
      firewall: FIREWALL.UNKNOWN,
      round: c.round++,
      connected: false,
      punching: false,
      addresses: null,
      remoteAddress: null,
      token: null,
      remoteToken: null
    })
  } catch {}

  destroyPuncher(c)
  maybeDestroyEncryptedSocket(c, err)
}

function relayConnection(c, relayThrough, payload, hs) {
  let isInitiator
  let publicKey
  let token

  if (payload.relayThrough) {
    isInitiator = false
    publicKey = payload.relayThrough.publicKey
    token = payload.relayThrough.token
  } else {
    isInitiator = true
    publicKey = relayThrough
    token = c.relayToken
  }

  c.relayToken = token
  c.relaySocket = c.dht.connect(publicKey)
  c.relaySocket.setKeepAlive(c.relayKeepAlive)
  c.relayClient = relay.Client.from(c.relaySocket, { id: c.relaySocket.publicKey })
  c.relayTimeout = setTimeout(onabort, 15000, null)

  c.relayClient.pair(isInitiator, token, c.rawStream).on('error', onabort).on('data', ondata)

  function ondata(remoteId) {
    if (c.relayTimeout) clearRelayTimeout(c)
    if (c.rawStream === null) {
      onabort(null)
      return
    }

    c.relayPaired = true

    const { remotePort, remoteHost, socket } = c.relaySocket.rawStream

    c.rawStream
      .on('close', () => c.relaySocket.destroy())
      .connect(socket, remoteId, remotePort, remoteHost)

    c.encryptedSocket.start(c.rawStream, { handshake: hs })
  }

  function onabort(err) {
    if (c.relayTimeout) clearRelayTimeout(c)
    const socket = c.relaySocket
    c.relayToken = null
    c.relaySocket = null
    if (socket) socket.destroy()
    maybeDestroyEncryptedSocket(c, err || RELAY_ABORTED())
  }
}

function clearPassiveConnectTimeout(c) {
  clearTimeout(c.passiveConnectTimeout)
  c.passiveConnectTimeout = null
}

function clearRelayTimeout(c) {
  clearTimeout(c.relayTimeout)
  c.relayTimeout = null
}

function destroyPuncher(c) {
  if (c.puncher) c.puncher.destroy()
  c.session.destroy()
}

function pickServerRelay(relays, clientRelay) {
  for (const r of relays) {
    if (!diffAddress(r.relayAddress, clientRelay)) return r
  }
  return relays[0]
}

function diffAddress(a, b) {
  return a.host !== b.host || a.port !== b.port
}

function defaultCreateHandshake(keyPair, remotePublicKey) {
  return new NoiseWrap(keyPair, remotePublicKey)
}

function defaultCreateSecretStream(isInitiator, rawStream, opts) {
  return new NoiseSecretStream(isInitiator, rawStream, opts)
}

function onlyPrivateHosts(addr) {
  return isPrivate(addr.host)
}

function isRelay(relaySocket, socket, port, host) {
  const stream = relaySocket.rawStream
  if (!stream) return false
  if (stream.socket !== socket) return false
  return port === stream.remotePort && host === stream.remoteHost
}

function selectRelay(relayThrough) {
  if (typeof relayThrough === 'function') relayThrough = relayThrough()
  if (relayThrough === null) return null
  if (Array.isArray(relayThrough))
    return relayThrough[Math.floor(Math.random() * relayThrough.length)]
  return relayThrough
}

function noop() {}
const EventEmitter = require('events')
const b4a = require('b4a')
const errors = require('./errors')

module.exports = class ConnectionPool extends EventEmitter {
  constructor(dht) {
    super()

    this._dht = dht
    this._servers = new Map()
    this._connecting = new Map()
    this._connections = new Map()
  }

  _attachServer(server) {
    const keyString = b4a.toString(server.publicKey, 'hex')

    this._servers.set(keyString, server)

    server
      .on('close', () => {
        this._servers.delete(keyString)
      })
      .on('connection', (socket) => {
        this._attachStream(socket, true)
      })
  }

  _attachStream(stream, opened) {
    const existing = this.get(stream.remotePublicKey)

    if (existing) {
      const keepNew =
        stream.isInitiator === existing.isInitiator ||
        b4a.compare(stream.publicKey, stream.remotePublicKey) > 0

      if (keepNew) {
        let closed = false

        const onclose = () => {
          closed = true
        }

        existing
          .on('error', noop)
          .on('close', () => {
            if (closed) return

            stream.off('error', noop).off('close', onclose)

            this._attachStream(stream, opened)
          })
          .destroy(errors.DUPLICATE_CONNECTION())

        stream.on('error', noop).on('close', onclose)
      } else {
        stream.on('error', noop).destroy(errors.DUPLICATE_CONNECTION())
      }

      return
    }

    const session = new ConnectionRef(this, stream)

    const keyString = b4a.toString(stream.remotePublicKey, 'hex')

    if (opened) {
      this._connections.set(keyString, session)

      stream.on('close', () => {
        this._connections.delete(keyString)
      })

      this.emit('connection', stream, session)
    } else {
      this._connecting.set(keyString, session)

      stream
        .on('error', noop)
        .on('close', () => {
          if (opened) this._connections.delete(keyString)
          else this._connecting.delete(keyString)
        })
        .on('open', () => {
          opened = true

          this._connecting.delete(keyString)
          this._connections.set(keyString, session)

          stream.off('error', noop)

          this.emit('connection', stream, session)
        })
    }

    return session
  }

  get connecting() {
    return this._connecting.size
  }

  get connections() {
    return this._connections.values()
  }

  has(publicKey) {
    const keyString = b4a.toString(publicKey, 'hex')

    return this._connections.has(keyString) || this._connecting.has(keyString)
  }

  get(publicKey) {
    const keyString = b4a.toString(publicKey, 'hex')

    const existing = this._connections.get(keyString) || this._connecting.get(keyString)

    return existing?._stream || null
  }
}

class ConnectionRef {
  constructor(pool, stream) {
    this._pool = pool
    this._stream = stream
    this._refs = 0
  }

  active() {
    this._refs++
  }

  inactive() {
    this._refs--
  }

  release() {
    this._stream.destroy()
  }
}

function noop() {}
const crypto = require('hypercore-crypto')

const COMMANDS = (exports.COMMANDS = {
  PEER_HANDSHAKE: 0,
  PEER_HOLEPUNCH: 1,
  FIND_PEER: 2,
  LOOKUP: 3,
  ANNOUNCE: 4,
  UNANNOUNCE: 5,
  MUTABLE_PUT: 6,
  MUTABLE_GET: 7,
  IMMUTABLE_PUT: 8,
  IMMUTABLE_GET: 9
})

exports.BOOTSTRAP_NODES = global.Pear?.config.dht?.bootstrap || [
  '88.99.3.86@node1.hyperdht.org:49737',
  '142.93.90.113@node2.hyperdht.org:49737',
  '138.68.147.8@node3.hyperdht.org:49737'
]

exports.KNOWN_NODES = global.Pear?.config.dht?.nodes || []

exports.FIREWALL = {
  UNKNOWN: 0,
  OPEN: 1,
  CONSISTENT: 2,
  RANDOM: 3
}

exports.ERROR = {
  // noise / connection related
  NONE: 0,
  ABORTED: 1,
  VERSION_MISMATCH: 2,
  TRY_LATER: 3,
  // dht related
  SEQ_REUSED: 16,
  SEQ_TOO_LOW: 17
}

const [NS_ANNOUNCE, NS_UNANNOUNCE, NS_MUTABLE_PUT, NS_PEER_HANDSHAKE, NS_PEER_HOLEPUNCH] =
  crypto.namespace('hyperswarm/dht', [
    COMMANDS.ANNOUNCE,
    COMMANDS.UNANNOUNCE,
    COMMANDS.MUTABLE_PUT,
    COMMANDS.PEER_HANDSHAKE,
    COMMANDS.PEER_HOLEPUNCH
  ])

exports.NS = {
  ANNOUNCE: NS_ANNOUNCE,
  UNANNOUNCE: NS_UNANNOUNCE,
  MUTABLE_PUT: NS_MUTABLE_PUT,
  PEER_HANDSHAKE: NS_PEER_HANDSHAKE,
  PEER_HOLEPUNCH: NS_PEER_HOLEPUNCH
}
const sodium = require('sodium-universal')
const b4a = require('b4a')

function hash(data) {
  const out = b4a.allocUnsafe(32)
  sodium.crypto_generichash(out, data)
  return out
}

function unslabbedHash(data) {
  const out = b4a.allocUnsafeSlow(32)
  sodium.crypto_generichash(out, data)
  return out
}

function createKeyPair(seed) {
  const publicKey = b4a.alloc(32)
  const secretKey = b4a.alloc(64)
  if (seed) sodium.crypto_sign_seed_keypair(publicKey, secretKey, seed)
  else sodium.crypto_sign_keypair(publicKey, secretKey)
  return { publicKey, secretKey }
}

module.exports = {
  hash,
  unslabbedHash,
  createKeyPair
}
const b4a = require('b4a')
const cenc = require('compact-encoding')

function encodeUnslab(enc, m) {
  // Faster than unslab(c.encode(enc, data)) because it avoids the mem copy.
  // Makes sense to put in compact-encoding when we need it in other modules too
  const state = cenc.state()
  enc.preencode(state, m)
  state.buffer = b4a.allocUnsafeSlow(state.end)
  enc.encode(state, m)
  return state.buffer
}

module.exports = {
  encodeUnslab
}
module.exports = class DHTError extends Error {
  constructor(msg, code, fn = DHTError) {
    super(`${code}: ${msg}`)
    this.code = code

    if (Error.captureStackTrace) {
      Error.captureStackTrace(this, fn)
    }
  }

  get name() {
    return 'DHTError'
  }

  static BAD_HANDSHAKE_REPLY(msg = 'Bad handshake reply') {
    return new DHTError(msg, 'BAD_HANDSHAKE_REPLY', DHTError.BAD_HANDSHAKE_REPLY)
  }

  static BAD_HOLEPUNCH_REPLY(msg = 'Bad holepunch reply') {
    return new DHTError(msg, 'BAD_HOLEPUNCH_REPLY', DHTError.BAD_HOLEPUNCH_REPLY)
  }

  static HOLEPUNCH_ABORTED(msg = 'Holepunch aborted') {
    return new DHTError(msg, 'HOLEPUNCH_ABORTED', DHTError.HOLEPUNCH_ABORTED)
  }

  static HOLEPUNCH_INVALID(msg = 'Invalid holepunch payload') {
    return new DHTError(msg, 'HOLEPUNCH_INVALID', DHTError.HOLEPUNCH_INVALID)
  }

  static HOLEPUNCH_PROBE_TIMEOUT(msg = 'Holepunching probe did not finish in time') {
    return new DHTError(msg, 'HOLEPUNCH_PROBE_TIMEOUT', DHTError.HOLEPUNCH_PROBE_TIMEOUT)
  }

  static HOLEPUNCH_DOUBLE_RANDOMIZED_NATS(msg = 'Both remote and local NATs are randomized') {
    return new DHTError(
      msg,
      'HOLEPUNCH_DOUBLE_RANDOMIZED_NATS',
      DHTError.HOLEPUNCH_DOUBLE_RANDOMIZED_NATS
    )
  }

  static CANNOT_HOLEPUNCH(msg = 'Cannot holepunch to remote') {
    return new DHTError(msg, 'CANNOT_HOLEPUNCH', DHTError.CANNOT_HOLEPUNCH)
  }

  static REMOTE_NOT_HOLEPUNCHING(msg = 'Remote is not holepunching') {
    return new DHTError(msg, 'REMOTE_NOT_HOLEPUNCHING', DHTError.REMOTE_NOT_HOLEPUNCHING)
  }

  static REMOTE_NOT_HOLEPUNCHABLE(msg = 'Remote is not holepunchable') {
    return new DHTError(msg, 'REMOTE_NOT_HOLEPUNCHABLE', DHTError.REMOTE_NOT_HOLEPUNCHABLE)
  }

  static REMOTE_ABORTED(msg = 'Remote aborted') {
    return new DHTError(msg, 'REMOTE_ABORTED', DHTError.REMOTE_ABORTED)
  }

  static HANDSHAKE_UNFINISHED(msg = 'Handshake did not finish') {
    return new DHTError(msg, 'HANDSHAKE_UNFINISHED', DHTError.HANDSHAKE_UNFINISHED)
  }

  static HANDSHAKE_INVALID(msg = 'Received invalid handshake') {
    return new DHTError(msg, 'HANDSHAKE_INVALID', DHTError.HANDSHAKE_INVALID)
  }

  static ALREADY_LISTENING(msg = 'Already listening') {
    return new DHTError(msg, 'ALREADY_LISTENING', DHTError.ALREADY_LISTENING)
  }

  static KEYPAIR_ALREADY_USED(msg = 'Keypair already used') {
    return new DHTError(msg, 'KEYPAIR_ALREADY_USED', DHTError.KEYPAIR_ALREADY_USED)
  }

  static NODE_DESTROYED(msg = 'Node destroyed') {
    return new DHTError(msg, 'NODE_DESTROYED', DHTError.NODE_DESTROYED)
  }

  static PEER_CONNECTION_FAILED(msg = 'Could not connect to peer') {
    return new DHTError(msg, 'PEER_CONNECTION_FAILED', DHTError.PEER_CONNECTION_FAILED)
  }

  static PEER_NOT_FOUND(msg = 'Peer not found') {
    return new DHTError(msg, 'PEER_NOT_FOUND', DHTError.PEER_NOT_FOUND)
  }

  static STREAM_NOT_CONNECTED(msg = 'Stream is not connected') {
    return new DHTError(msg, 'STREAM_NOT_CONNECTED', DHTError.STREAM_DISCONNECTED)
  }

  static SERVER_INCOMPATIBLE(msg = 'Server is using an incompatible version') {
    return new DHTError(msg, 'SERVER_INCOMPATIBLE', DHTError.SERVER_INCOMPATIBLE)
  }

  static SERVER_ERROR(msg = 'Server returned an error') {
    return new DHTError(msg, 'SERVER_ERROR', DHTError.SERVER_ERROR)
  }

  static DUPLICATE_CONNECTION(msg = 'Duplicate connection') {
    return new DHTError(msg, 'DUPLICATE_CONNECTION', DHTError.DUPLICATE_CONNECTION)
  }

  static RELAY_ABORTED(msg = 'Relay aborted') {
    return new DHTError(msg, 'RELAY_ABORTED', DHTError.RELAY_ABORTED)
  }

  static SUSPENDED(msg = 'Suspended') {
    return new DHTError(msg, 'SUSPENDED', DHTError.SUSPENDED)
  }
}
const b4a = require('b4a')
const Nat = require('./nat')
const Sleeper = require('./sleeper')
const { FIREWALL } = require('./constants')

const BIRTHDAY_SOCKETS = 256
const HOLEPUNCH = b4a.from([0])
const HOLEPUNCH_TTL = 5
const DEFAULT_TTL = 64
const MAX_REOPENS = 3

module.exports = class Holepuncher {
  constructor(dht, session, isInitiator, remoteFirewall = FIREWALL.UNKNOWN) {
    const holder = dht._socketPool.acquire()

    this.dht = dht
    this.session = session

    this.nat = new Nat(dht, session, holder.socket)
    this.nat.autoSample()

    this.isInitiator = isInitiator

    // events
    this.onconnect = noop
    this.onabort = noop

    this.punching = false
    this.connected = false
    this.destroyed = false
    this.randomized = false

    // track remote state
    this.remoteFirewall = remoteFirewall
    this.remoteAddresses = []
    this.remoteHolepunching = false

    this._sleeper = new Sleeper()
    this._reopening = null
    this._timeout = null
    this._punching = null
    this._allHolders = []
    this._holder = this._addRef(holder)
  }

  get socket() {
    return this._holder.socket
  }

  updateRemote({ punching, firewall, addresses, verified }) {
    const remoteAddresses = []

    if (addresses) {
      for (const addr of addresses) {
        remoteAddresses.push({
          host: addr.host,
          port: addr.port,
          verified: verified === addr.host || this._isVerified(addr.host)
        })
      }
    }

    this.remoteFirewall = firewall
    this.remoteAddresses = remoteAddresses
    this.remoteHolepunching = punching
  }

  _isVerified(host) {
    for (const addr of this.remoteAddresses) {
      if (addr.verified && addr.host === host) {
        return true
      }
    }
    return false
  }

  ping(addr, socket = this._holder.socket) {
    return holepunch(socket, addr, false)
  }

  openSession(addr, socket = this._holder.socket) {
    return holepunch(socket, addr, true)
  }

  async analyze(allowReopen) {
    await this.nat.analyzing
    if (this._unstable()) {
      if (!allowReopen) return false
      if (!this._reopening) this._reopening = this._reopen()
      return this._reopening
    }
    return true
  }

  _unstable() {
    // TODO!!: We need an additional heuristic here... If we were NOT random in the past we should also do this.
    const firewall = this.nat.firewall
    return (
      (this.remoteFirewall >= FIREWALL.RANDOM && firewall >= FIREWALL.RANDOM) ||
      firewall === FIREWALL.UNKNOWN
    )
  }

  _reset() {
    const prev = this._holder

    this._allHolders.pop()
    this._holder = this._addRef(this.dht._socketPool.acquire())

    prev.release()
    this.nat.destroy()

    this.nat = new Nat(this.dht, this.session, this._holder.socket)
    // TODO: maybe make auto sampling configurable somehow?
    this.nat.autoSample()
  }

  _addRef(ref) {
    this._allHolders.push(ref)
    ref.onholepunchmessage = (msg, rinfo) => this._onholepunchmessage(msg, rinfo, ref)
    return ref
  }

  _onholepunchmessage(_, addr, ref) {
    if (!this.isInitiator) {
      // TODO: we don't need this if we had a way to connect a socket to many hosts
      holepunch(ref.socket, addr, false) // never fails
      return
    }

    if (this.connected) return

    this.connected = true
    this.punching = false

    for (const r of this._allHolders) {
      if (r === ref) continue
      r.release()
    }

    this._allHolders[0] = ref
    while (this._allHolders.length > 1) this._allHolders.pop()

    this._decrementRandomized()
    this.onconnect(ref.socket, addr.port, addr.host)
  }

  _done() {
    return this.destroyed || this.connected
  }

  async _reopen() {
    for (let i = 0; this._unstable() && i < MAX_REOPENS && !this._done() && !this.punching; i++) {
      this._reset()
      await this.nat.analyzing
    }

    return coerceFirewall(this.nat.firewall) === FIREWALL.CONSISTENT
  }

  punch() {
    if (!this._punching) this._punching = this._punch()
    return this._punching
  }

  async _punch() {
    if (this._done() || !this.remoteAddresses.length) return false

    this.punching = true

    // Coerce into consistency for now. Obvs we could make this this more efficient if we use that info
    // but that's seldomly used since those will just use tcp most of the time.

    const local = coerceFirewall(this.nat.firewall)
    const remote = coerceFirewall(this.remoteFirewall)

    // Note that most of these async functions are meant to run in the background
    // which is why we don't await them here and why they are not allowed to throw

    let remoteVerifiedAddress = null
    for (const addr of this.remoteAddresses) {
      if (addr.verified) {
        remoteVerifiedAddress = addr
        break
      }
    }

    if (local === FIREWALL.CONSISTENT && remote === FIREWALL.CONSISTENT) {
      this.dht.stats.punches.consistent++
      this._consistentProbe()
      return true
    }

    if (!remoteVerifiedAddress) return false

    if (local === FIREWALL.CONSISTENT && remote >= FIREWALL.RANDOM) {
      this.dht.stats.punches.random++
      this._incrementRandomized()
      this._randomProbes(remoteVerifiedAddress)
      return true
    }

    if (local >= FIREWALL.RANDOM && remote === FIREWALL.CONSISTENT) {
      this.dht.stats.punches.random++
      this._incrementRandomized()
      await this._openBirthdaySockets(remoteVerifiedAddress)
      if (this.punching) this._keepAliveRandomNat(remoteVerifiedAddress)
      return true
    }

    return false
  }

  // Note that this never throws so it is safe to run in the background
  async _consistentProbe() {
    // Here we do the sleep first because the "fast open" mode in the server just fired a ping
    if (!this.isInitiator) await this._sleeper.pause(1000)

    let tries = 0

    while (this.punching && tries++ < 10) {
      for (const addr of this.remoteAddresses) {
        // only try unverified addresses every 4 ticks
        if (!addr.verified && (tries & 3) !== 0) continue
        await holepunch(this._holder.socket, addr, false)
      }
      if (this.punching) await this._sleeper.pause(1000)
    }

    this._autoDestroy()
  }

  // Note that this never throws so it is safe to run in the background
  async _randomProbes(remoteAddr) {
    let tries = 1750 // ~35s

    while (this.punching && tries-- > 0) {
      const addr = { host: remoteAddr.host, port: randomPort() }
      await holepunch(this._holder.socket, addr, false)
      if (this.punching) await this._sleeper.pause(20)
    }

    this._autoDestroy()
  }

  // Note that this never throws so it is safe to run in the background
  async _keepAliveRandomNat(remoteAddr) {
    let i = 0
    let lowTTLRounds = 1

    // TODO: experiment with this here. We just bursted all the messages in
    // openOtherSockets to ensure the sockets are open, so it's potentially
    // a good idea to slow down for a bit.
    await this._sleeper.pause(100)

    let tries = 1750 // ~35s

    while (this.punching && tries-- > 0) {
      if (i === this._allHolders.length) {
        i = 0
        if (lowTTLRounds > 0) lowTTLRounds--
      }

      await holepunch(this._allHolders[i++].socket, remoteAddr, lowTTLRounds > 0)
      if (this.punching) await this._sleeper.pause(20)
    }

    this._autoDestroy()
  }

  async _openBirthdaySockets(remoteAddr) {
    while (this.punching && this._allHolders.length < BIRTHDAY_SOCKETS) {
      const ref = this._addRef(this.dht._socketPool.acquire())
      await holepunch(ref.socket, remoteAddr, HOLEPUNCH_TTL)
    }
  }

  _autoDestroy() {
    if (!this.connected) this.destroy()
  }

  _incrementRandomized() {
    if (!this.randomized) {
      this.randomized = true
      this.dht._randomPunches++
    }
  }

  _decrementRandomized() {
    if (this.randomized) {
      this.dht._lastRandomPunch = Date.now()
      this.randomized = false
      this.dht._randomPunches--
    }
  }

  destroy() {
    if (this.destroyed) return
    this.destroyed = true
    this.punching = false

    for (const ref of this._allHolders) ref.release()
    this._allHolders = []
    this.nat.destroy()

    if (!this.connected) {
      this._decrementRandomized()
      this.onabort()
    }
  }

  static ping(socket, addr) {
    return holepunch(socket, addr, false)
  }

  static localAddresses(socket) {
    return localAddresses(socket)
  }

  static matchAddress(myAddresses, externalAddresses) {
    return matchAddress(myAddresses, externalAddresses)
  }
}

function holepunch(socket, addr, lowTTL) {
  return socket.send(HOLEPUNCH, addr.port, addr.host, lowTTL ? HOLEPUNCH_TTL : DEFAULT_TTL)
}

function randomPort() {
  return (1000 + Math.random() * 64536) | 0
}

function coerceFirewall(fw) {
  return fw === FIREWALL.OPEN ? FIREWALL.CONSISTENT : fw
}

function localAddresses(socket) {
  const addrs = []
  const { host, port } = socket.address()

  if (host === '127.0.0.1') return [{ host, port }]

  for (const n of socket.udx.networkInterfaces()) {
    if (n.family !== 4 || n.internal) continue

    addrs.push({ host: n.host, port })
  }

  if (addrs.length === 0) {
    addrs.push({ host: '127.0.0.1', port })
  }

  return addrs
}

function matchAddress(localAddresses, remoteLocalAddresses) {
  if (remoteLocalAddresses.length === 0) return null

  let best = { segment: 1, addr: null }

  for (const localAddress of localAddresses) {
    // => 192.168.122.238
    const a = localAddress.host.split('.')

    for (const remoteAddress of remoteLocalAddresses) {
      // => 192.168.0.23
      // => 192.168.122.1
      const b = remoteAddress.host.split('.')

      // Matches 192.*.*.*
      if (a[0] === b[0]) {
        if (best.segment === 1) best = { segment: 2, addr: remoteAddress }

        // Matches 192.168.*.*
        if (a[1] === b[1]) {
          if (best.segment === 2) best = { segment: 3, addr: remoteAddress }

          // Matches 192.168.122.*
          if (a[2] === b[2]) return remoteAddress
        }
      }
    }
  }

  return best.addr
}

function noop() {}
const c = require('compact-encoding')
const net = require('compact-encoding-net')

const ipv4 = {
  ...net.ipv4Address,
  decode(state) {
    const ip = net.ipv4Address.decode(state)
    return {
      host: ip.host,
      port: ip.port
    }
  }
}

const ipv4Array = c.array(ipv4)

const ipv6 = {
  ...net.ipv6Address,
  decode(state) {
    const ip = net.ipv6Address.decode(state)
    return {
      host: ip.host,
      port: ip.port
    }
  }
}

const ipv6Array = c.array(ipv6)

exports.handshake = {
  preencode(state, m) {
    state.end += 1 + 1 + (m.peerAddress ? 6 : 0) + (m.relayAddress ? 6 : 0)
    c.buffer.preencode(state, m.noise)
  },
  encode(state, m) {
    const flags = (m.peerAddress ? 1 : 0) | (m.relayAddress ? 2 : 0)

    c.uint.encode(state, flags)
    c.uint.encode(state, m.mode)
    c.buffer.encode(state, m.noise)

    if (m.peerAddress) ipv4.encode(state, m.peerAddress)
    if (m.relayAddress) ipv4.encode(state, m.relayAddress)
  },
  decode(state) {
    const flags = c.uint.decode(state)

    return {
      mode: c.uint.decode(state),
      noise: c.buffer.decode(state),
      peerAddress: flags & 1 ? ipv4.decode(state) : null,
      relayAddress: flags & 2 ? ipv4.decode(state) : null
    }
  }
}

const relayInfo = {
  preencode(state, m) {
    state.end += 12
  },
  encode(state, m) {
    ipv4.encode(state, m.relayAddress)
    ipv4.encode(state, m.peerAddress)
  },
  decode(state) {
    return {
      relayAddress: ipv4.decode(state),
      peerAddress: ipv4.decode(state)
    }
  }
}

const relayInfoArray = c.array(relayInfo)

const holepunchInfo = {
  preencode(state, m) {
    c.uint.preencode(state, m.id)
    relayInfoArray.preencode(state, m.relays)
  },
  encode(state, m) {
    c.uint.encode(state, m.id)
    relayInfoArray.encode(state, m.relays)
  },
  decode(state) {
    return {
      id: c.uint.decode(state),
      relays: relayInfoArray.decode(state)
    }
  }
}

const udxInfo = {
  preencode(state, m) {
    state.end += 2 // version + features
    c.uint.preencode(state, m.id)
    c.uint.preencode(state, m.seq)
  },
  encode(state, m) {
    c.uint.encode(state, 1)
    c.uint.encode(state, m.reusableSocket ? 1 : 0)
    c.uint.encode(state, m.id)
    c.uint.encode(state, m.seq)
  },
  decode(state) {
    const version = c.uint.decode(state)
    const features = c.uint.decode(state)

    return {
      version,
      reusableSocket: (features & 1) !== 0,
      id: c.uint.decode(state),
      seq: c.uint.decode(state)
    }
  }
}

const secretStreamInfo = {
  preencode(state, m) {
    c.uint.preencode(state, 1)
  },
  encode(state, m) {
    c.uint.encode(state, 1)
  },
  decode(state) {
    return {
      version: c.uint.decode(state)
    }
  }
}

const relayThroughInfo = {
  preencode(state, m) {
    c.uint.preencode(state, 1) // version
    c.uint.preencode(state, 0) // flags
    c.fixed32.preencode(state, m.publicKey)
    c.fixed32.preencode(state, m.token)
  },
  encode(state, m) {
    c.uint.encode(state, 1)
    c.uint.encode(state, 0)
    c.fixed32.encode(state, m.publicKey)
    c.fixed32.encode(state, m.token)
  },
  decode(state) {
    const version = c.uint.decode(state)
    c.uint.decode(state)

    return {
      version,
      publicKey: c.fixed32.decode(state),
      token: c.fixed32.decode(state)
    }
  }
}

exports.noisePayload = {
  preencode(state, m) {
    state.end += 4 // version + flags + error + firewall
    if (m.holepunch) holepunchInfo.preencode(state, m.holepunch)
    if (m.addresses4 && m.addresses4.length) ipv4Array.preencode(state, m.addresses4)
    if (m.addresses6 && m.addresses6.length) ipv6Array.preencode(state, m.addresses6)
    if (m.udx) udxInfo.preencode(state, m.udx)
    if (m.secretStream) secretStreamInfo.preencode(state, m.secretStream)
    if (m.relayThrough) relayThroughInfo.preencode(state, m.relayThrough)
    if (m.relayAddresses) ipv4Array.preencode(state, m.relayAddresses)
  },
  encode(state, m) {
    let flags = 0

    if (m.holepunch) flags |= 1
    if (m.addresses4 && m.addresses4.length) flags |= 2
    if (m.addresses6 && m.addresses6.length) flags |= 4
    if (m.udx) flags |= 8
    if (m.secretStream) flags |= 16
    if (m.relayThrough) flags |= 32
    if (m.relayAddresses) flags |= 64

    c.uint.encode(state, 1) // version
    c.uint.encode(state, flags)
    c.uint.encode(state, m.error)
    c.uint.encode(state, m.firewall)

    if (m.holepunch) holepunchInfo.encode(state, m.holepunch)
    if (m.addresses4 && m.addresses4.length) ipv4Array.encode(state, m.addresses4)
    if (m.addresses6 && m.addresses6.length) ipv6Array.encode(state, m.addresses6)
    if (m.udx) udxInfo.encode(state, m.udx)
    if (m.secretStream) secretStreamInfo.encode(state, m.secretStream)
    if (m.relayThrough) relayThroughInfo.encode(state, m.relayThrough)
    if (m.relayAddresses) ipv4Array.encode(state, m.relayAddresses)
  },
  decode(state) {
    const version = c.uint.decode(state)

    if (version !== 1) {
      // Do not attempt to decode but return this back to the user so they can
      // actually handle it
      return {
        version,
        error: 0,
        firewall: 0,
        holepunch: null,
        addresses4: [],
        addresses6: [],
        udx: null,
        secretStream: null,
        relayThrough: null,
        relayAddresses: null
      }
    }

    const flags = c.uint.decode(state)

    return {
      version,
      error: c.uint.decode(state),
      firewall: c.uint.decode(state),
      holepunch: (flags & 1) !== 0 ? holepunchInfo.decode(state) : null,
      addresses4: (flags & 2) !== 0 ? ipv4Array.decode(state) : [],
      addresses6: (flags & 4) !== 0 ? ipv6Array.decode(state) : [],
      udx: (flags & 8) !== 0 ? udxInfo.decode(state) : null,
      secretStream: (flags & 16) !== 0 ? secretStreamInfo.decode(state) : null,
      relayThrough: (flags & 32) !== 0 ? relayThroughInfo.decode(state) : null,
      relayAddresses: (flags & 64) !== 0 ? ipv4Array.decode(state) : null
    }
  }
}

exports.holepunch = {
  preencode(state, m) {
    state.end += 2
    c.uint.preencode(state, m.id)
    c.buffer.preencode(state, m.payload)
    if (m.peerAddress) ipv4.preencode(state, m.peerAddress)
  },
  encode(state, m) {
    const flags = m.peerAddress ? 1 : 0
    c.uint.encode(state, flags)
    c.uint.encode(state, m.mode)
    c.uint.encode(state, m.id)
    c.buffer.encode(state, m.payload)
    if (m.peerAddress) ipv4.encode(state, m.peerAddress)
  },
  decode(state) {
    const flags = c.uint.decode(state)
    return {
      mode: c.uint.decode(state),
      id: c.uint.decode(state),
      payload: c.buffer.decode(state),
      peerAddress: flags & 1 ? ipv4.decode(state) : null
    }
  }
}

exports.holepunchPayload = {
  preencode(state, m) {
    state.end += 4 // flags + error + firewall + round
    if (m.addresses) ipv4Array.preencode(state, m.addresses)
    if (m.remoteAddress) state.end += 6
    if (m.token) state.end += 32
    if (m.remoteToken) state.end += 32
  },
  encode(state, m) {
    const flags =
      (m.connected ? 1 : 0) |
      (m.punching ? 2 : 0) |
      (m.addresses ? 4 : 0) |
      (m.remoteAddress ? 8 : 0) |
      (m.token ? 16 : 0) |
      (m.remoteToken ? 32 : 0)

    c.uint.encode(state, flags)
    c.uint.encode(state, m.error)
    c.uint.encode(state, m.firewall)
    c.uint.encode(state, m.round)

    if (m.addresses) ipv4Array.encode(state, m.addresses)
    if (m.remoteAddress) ipv4.encode(state, m.remoteAddress)
    if (m.token) c.fixed32.encode(state, m.token)
    if (m.remoteToken) c.fixed32.encode(state, m.remoteToken)
  },
  decode(state) {
    const flags = c.uint.decode(state)

    return {
      error: c.uint.decode(state),
      firewall: c.uint.decode(state),
      round: c.uint.decode(state),
      connected: (flags & 1) !== 0,
      punching: (flags & 2) !== 0,
      addresses: (flags & 4) !== 0 ? ipv4Array.decode(state) : null,
      remoteAddress: (flags & 8) !== 0 ? ipv4.decode(state) : null,
      token: (flags & 16) !== 0 ? c.fixed32.decode(state) : null,
      remoteToken: (flags & 32) !== 0 ? c.fixed32.decode(state) : null
    }
  }
}

const peer = (exports.peer = {
  preencode(state, m) {
    state.end += 32
    ipv4Array.preencode(state, m.relayAddresses)
  },
  encode(state, m) {
    c.fixed32.encode(state, m.publicKey)
    ipv4Array.encode(state, m.relayAddresses)
  },
  decode(state) {
    return {
      publicKey: c.fixed32.decode(state),
      relayAddresses: ipv4Array.decode(state)
    }
  }
})

const peers = (exports.peers = c.array(peer))

const rawPeers = c.array(c.raw)

exports.lookupRawReply = {
  preencode(state, m) {
    rawPeers.preencode(state, m.peers)
    c.uint.preencode(state, m.bump)
  },
  encode(state, m) {
    rawPeers.encode(state, m.peers)
    c.uint.encode(state, m.bump)
  },
  decode(state) {
    return {
      peers: peers.decode(state),
      bump: state.start < state.end ? c.uint.decode(state) : 0
    }
  }
}

exports.announce = {
  preencode(state, m) {
    state.end++ // flags
    if (m.peer) peer.preencode(state, m.peer)
    if (m.refresh) state.end += 32
    if (m.signature) state.end += 64
    if (m.bump) c.uint.preencode(state, m.bump)
  },
  encode(state, m) {
    const flags = (m.peer ? 1 : 0) | (m.refresh ? 2 : 0) | (m.signature ? 4 : 0) | (m.bump ? 8 : 0)
    c.uint.encode(state, flags)
    if (m.peer) peer.encode(state, m.peer)
    if (m.refresh) c.fixed32.encode(state, m.refresh)
    if (m.signature) c.fixed64.encode(state, m.signature)
    if (m.bump) c.uint.encode(state, m.bump)
  },
  decode(state) {
    const flags = c.uint.decode(state)

    return {
      peer: (flags & 1) !== 0 ? peer.decode(state) : null,
      refresh: (flags & 2) !== 0 ? c.fixed32.decode(state) : null,
      signature: (flags & 4) !== 0 ? c.fixed64.decode(state) : null,
      bump: (flags & 8) !== 0 ? c.uint.decode(state) : 0
    }
  }
}

exports.mutableSignable = {
  preencode(state, m) {
    c.uint.preencode(state, m.seq)
    c.buffer.preencode(state, m.value)
  },
  encode(state, m) {
    c.uint.encode(state, m.seq)
    c.buffer.encode(state, m.value)
  },
  decode(state) {
    return {
      seq: c.uint.decode(state),
      value: c.buffer.decode(state)
    }
  }
}

exports.mutablePutRequest = {
  preencode(state, m) {
    c.fixed32.preencode(state, m.publicKey)
    c.uint.preencode(state, m.seq)
    c.buffer.preencode(state, m.value)
    c.fixed64.preencode(state, m.signature)
  },
  encode(state, m) {
    c.fixed32.encode(state, m.publicKey)
    c.uint.encode(state, m.seq)
    c.buffer.encode(state, m.value)
    c.fixed64.encode(state, m.signature)
  },
  decode(state) {
    return {
      publicKey: c.fixed32.decode(state),
      seq: c.uint.decode(state),
      value: c.buffer.decode(state),
      signature: c.fixed64.decode(state)
    }
  }
}

exports.mutableGetResponse = {
  preencode(state, m) {
    c.uint.preencode(state, m.seq)
    c.buffer.preencode(state, m.value)
    c.fixed64.preencode(state, m.signature)
  },
  encode(state, m) {
    c.uint.encode(state, m.seq)
    c.buffer.encode(state, m.value)
    c.fixed64.encode(state, m.signature)
  },
  decode(state) {
    return {
      seq: c.uint.decode(state),
      value: c.buffer.decode(state),
      signature: c.fixed64.decode(state)
    }
  }
}
const { FIREWALL } = require('../lib/constants')

module.exports = class Nat {
  constructor(dht, session, socket) {
    this._samplesHost = []
    this._samplesFull = []
    this._visited = new Map()
    this._resolve = null
    this._minSamples = 4
    this._autoSampling = false

    this.dht = dht
    this.session = session
    this.socket = socket

    this.sampled = 0
    this.firewall = dht.firewalled ? FIREWALL.UNKNOWN : FIREWALL.OPEN
    this.addresses = null

    this.analyzing = new Promise((resolve) => {
      this._resolve = resolve
    })
  }

  autoSample(retry = true) {
    if (this._autoSampling) return
    this._autoSampling = true

    const self = this
    const socket = this.socket
    const maxPings = this._minSamples

    let skip = this.dht.nodes.length >= 8 ? 5 : 0
    let pending = 0

    // TODO: it would be best to pick the nodes to help us based on latency to us
    // That should reduce connect latency in general. We should investigate tracking that later on.

    // TODO 2: try to pick nodes with different IPs as well, as that'll help multi IP cell connections...
    // If we expose this from the nat sampler then the DHT should be able to help us filter out scams as well...

    for (
      let node = this.dht.nodes.latest;
      node && this.sampled + pending < maxPings;
      node = node.prev
    ) {
      if (skip > 0) {
        skip--
        continue
      }

      const ref = node.host + ':' + node.port

      if (this._visited.has(ref)) continue
      this._visited.set(ref, 1)

      pending++
      this.session.ping(node, { socket, retry: false }).then(onpong, onskip)
    }

    pending++
    onskip()

    function onpong(res) {
      self.add(res.to, res.from)
      onskip()
    }

    function onskip() {
      if (--pending === 0 && self.sampled < self._minSamples) {
        if (retry) {
          self._autoSampling = false
          self.autoSample(false)
          return
        }
        self._resolve()
      }
    }
  }

  destroy() {
    this._autoSampling = true
    this._minSamples = 0
    this._resolve()
  }

  unfreeze() {
    this.frozen = false
    this._updateFirewall()
    this._updateAddresses()
  }

  freeze() {
    this.frozen = true
  }

  _updateFirewall() {
    if (!this.dht.firewalled) {
      this.firewall = FIREWALL.OPEN
      return
    }

    if (this.sampled < 3) return

    const max = this._samplesFull[0].hits

    if (max >= 3) {
      this.firewall = FIREWALL.CONSISTENT
      return
    }

    if (max === 1) {
      this.firewall = FIREWALL.RANDOM
      return
    }

    // else max === 2

    // 1 host, >= 4 total samples ie, 2 bad ones -> random
    if (this._samplesHost.length === 1 && this.sampled > 3) {
      this.firewall = FIREWALL.RANDOM
      return
    }

    // double hit on two different ips -> assume consistent
    if (this._samplesHost.length > 1 && this._samplesFull[1].hits > 1) {
      this.firewall = FIREWALL.CONSISTENT
      return
    }

    // (4 is just means - all the samples we expect) - no decision - assume random
    if (this.sampled > 4) {
      this.firewall = FIREWALL.RANDOM
    }
  }

  _updateAddresses() {
    if (this.firewall === FIREWALL.UNKNOWN) {
      this.addresses = null
      return
    }

    if (this.firewall === FIREWALL.RANDOM) {
      this.addresses = [this._samplesHost[0]]
      return
    }

    if (this.firewall === FIREWALL.CONSISTENT) {
      this.addresses = []
      for (const addr of this._samplesFull) {
        if (addr.hits >= 2 || this.addresses.length < 2) this.addresses.push(addr)
      }
    }
  }

  update() {
    if (this.dht.firewalled && this.firewall === FIREWALL.OPEN) {
      this.firewall = FIREWALL.UNKNOWN
    }
    this._updateFirewall()
    this._updateAddresses()
  }

  add(addr, from) {
    const ref = from.host + ':' + from.port

    if (this._visited.get(ref) === 2) return
    this._visited.set(ref, 2)

    addSample(this._samplesHost, addr.host, 0)
    addSample(this._samplesFull, addr.host, addr.port)

    if ((++this.sampled >= 3 || !this.dht.firewalled) && !this.frozen) {
      this.update()
    }

    if (this.firewall === FIREWALL.CONSISTENT || this.firewall === FIREWALL.OPEN) {
      this._resolve()
    } else if (this.sampled >= this._minSamples) {
      this._resolve()
    }
  }
}

function addSample(samples, host, port) {
  for (let i = 0; i < samples.length; i++) {
    const s = samples[i]

    if (s.port !== port || s.host !== host) continue
    s.hits++

    for (; i > 0; i--) {
      const prev = samples[i - 1]
      if (prev.hits >= s.hits) return
      samples[i - 1] = s
      samples[i] = prev
    }

    return
  }

  samples.push({
    host,
    port,
    hits: 1
  })
}
const NoiseSecretStream = require('@hyperswarm/secret-stream')
const NoiseHandshake = require('noise-handshake')
const curve = require('noise-curve-ed')
const c = require('compact-encoding')
const b4a = require('b4a')
const sodium = require('sodium-universal')
const m = require('./messages')
const { NS } = require('./constants')
const { HANDSHAKE_UNFINISHED } = require('./errors')

const NOISE_PROLOUGE = NS.PEER_HANDSHAKE

module.exports = class NoiseWrap {
  constructor(keyPair, remotePublicKey) {
    this.isInitiator = !!remotePublicKey
    this.remotePublicKey = remotePublicKey
    this.keyPair = keyPair
    this.handshake = new NoiseHandshake('IK', this.isInitiator, keyPair, { curve })
    this.handshake.initialise(NOISE_PROLOUGE, remotePublicKey)
  }

  send(payload) {
    const buf = c.encode(m.noisePayload, payload)
    return this.handshake.send(buf)
  }

  recv(buf) {
    const payload = c.decode(m.noisePayload, this.handshake.recv(buf))
    this.remotePublicKey = b4a.toBuffer(this.handshake.rs)
    return payload
  }

  final() {
    if (!this.handshake.complete) throw HANDSHAKE_UNFINISHED()

    const holepunchSecret = b4a.allocUnsafe(32)

    sodium.crypto_generichash(holepunchSecret, NS.PEER_HOLEPUNCH, this.handshake.hash)

    return {
      isInitiator: this.isInitiator,
      publicKey: this.keyPair.publicKey,
      streamId: this.streamId,
      remotePublicKey: this.remotePublicKey,
      remoteId: NoiseSecretStream.id(this.handshake.hash, !this.isInitiator),
      holepunchSecret,
      hash: b4a.toBuffer(this.handshake.hash),
      rx: b4a.toBuffer(this.handshake.rx),
      tx: b4a.toBuffer(this.handshake.tx)
    }
  }
}
const c = require('compact-encoding')
const sodium = require('sodium-universal')
const RecordCache = require('record-cache')
const Cache = require('xache')
const b4a = require('b4a')
const unslab = require('unslab')

const { encodeUnslab } = require('./encode')
const m = require('./messages')
const { NS, ERROR } = require('./constants')

const EMPTY = b4a.alloc(0)
const TMP = b4a.allocUnsafe(32)
const MAX_BUMP_DRIFT = 60_000

module.exports = class Persistent {
  constructor(dht, opts) {
    this.dht = dht
    this.records = new RecordCache(opts.records)
    this.bumps = new Cache(opts.bumps)
    this.refreshes = new Cache(opts.refreshes)
    this.mutables = new Cache(opts.mutables)
    this.immutables = new Cache(opts.immutables)
  }

  onlookup(req) {
    if (!req.target) return

    const k = b4a.toString(req.target, 'hex')
    const records = this.records.get(k, 20)
    const bump = this.bumps.get(k) || 0
    const fwd = this.dht._router.get(k)

    if (fwd && records.length < 20) records.push(fwd.record)

    req.reply(records.length ? c.encode(m.lookupRawReply, { peers: records, bump }) : null)
  }

  onfindpeer(req) {
    if (!req.target) return
    const fwd = this.dht._router.get(req.target)
    req.reply(fwd ? fwd.record : null)
  }

  unannounce(target, publicKey) {
    const k = b4a.toString(target, 'hex')
    sodium.crypto_generichash(TMP, publicKey)

    if (b4a.equals(TMP, target)) this.dht._router.delete(k)
    this.records.remove(k, publicKey)
  }

  onunannounce(req) {
    if (!req.target || !req.token) return

    const unann = decode(m.announce, req.value)
    if (unann === null) return

    const { peer, signature } = unann
    if (!peer || !signature) return

    const signable = annSignable(req.target, req.token, this.dht.id, unann, NS.UNANNOUNCE)

    if (!sodium.crypto_sign_verify_detached(signature, signable, peer.publicKey)) {
      return
    }

    this.unannounce(req.target, peer.publicKey)
    req.reply(null, { token: false, closerNodes: false })
  }

  _onrefresh(token, req) {
    sodium.crypto_generichash(TMP, token)
    const activeRefresh = b4a.toString(TMP, 'hex')

    const r = this.refreshes.get(activeRefresh)
    if (!r) return

    const { announceSelf, k, record } = r
    const publicKey = record.subarray(0, 32)

    if (announceSelf) {
      this.dht._router.set(k, {
        relay: req.from,
        record,
        onconnect: null,
        onholepunch: null
      })
      this.records.remove(k, publicKey)
    } else {
      this.records.add(k, publicKey, record)
    }

    this.refreshes.delete(activeRefresh)
    this.refreshes.set(b4a.toString(token, 'hex'), r)

    req.reply(null, { token: false, closerNodes: false })
  }

  onannounce(req) {
    if (!req.target || !req.token || !this.dht.id) return

    const ann = decode(m.announce, req.value)
    if (ann === null) return

    const signable = annSignable(req.target, req.token, this.dht.id, ann, NS.ANNOUNCE)
    const { peer, refresh, signature, bump } = ann

    if (!peer) {
      if (!refresh) return
      this._onrefresh(refresh, req)
      return
    }

    if (!signature || !sodium.crypto_sign_verify_detached(signature, signable, peer.publicKey)) {
      return
    }

    // TODO: it would be potentially be more optimal to allow more than 3 addresses here for a findPeer response
    // and only use max 3 for a lookup reply
    if (peer.relayAddresses.length > 3) {
      peer.relayAddresses = peer.relayAddresses.slice(0, 3)
    }

    sodium.crypto_generichash(TMP, peer.publicKey)

    const k = b4a.toString(req.target, 'hex')
    const announceSelf = b4a.equals(TMP, req.target)
    const record = encodeUnslab(m.peer, peer)

    if (announceSelf) {
      this.dht._router.set(k, {
        relay: req.from,
        record,
        onconnect: null,
        onholepunch: null
      })
      this.records.remove(k, peer.publicKey)
    } else {
      const currentBump = this.bumps.get(k) || 0
      if (bump > currentBump && bump <= Date.now() + MAX_BUMP_DRIFT) this.bumps.set(k, bump)
      this.records.add(k, peer.publicKey, record)
    }

    if (refresh) {
      this.refreshes.set(b4a.toString(refresh, 'hex'), { k, record, announceSelf })
    }

    req.reply(null, { token: false, closerNodes: false })
  }

  onmutableget(req) {
    if (!req.target || !req.value) return

    let seq = 0
    try {
      seq = c.decode(c.uint, req.value)
    } catch {
      return
    }

    const k = b4a.toString(req.target, 'hex')
    const value = this.mutables.get(k)

    if (!value) {
      req.reply(null)
      return
    }

    const localSeq = c.decode(c.uint, value)
    req.reply(localSeq < seq ? null : value)
  }

  onmutableput(req) {
    if (!req.target || !req.token || !req.value) return

    const p = decode(m.mutablePutRequest, req.value)
    if (!p) return

    const { publicKey, seq, value, signature } = p

    const hash = b4a.allocUnsafe(32)
    sodium.crypto_generichash(hash, publicKey)
    if (!b4a.equals(hash, req.target)) return

    if (!value || !verifyMutable(signature, seq, value, publicKey)) return

    const k = b4a.toString(hash, 'hex')
    const local = this.mutables.get(k)

    if (local) {
      const existing = c.decode(m.mutableGetResponse, local)
      if (existing.value && existing.seq === seq && b4a.compare(value, existing.value) !== 0) {
        req.error(ERROR.SEQ_REUSED)
        return
      }
      if (seq < existing.seq) {
        req.error(ERROR.SEQ_TOO_LOW)
        return
      }
    }

    this.mutables.set(k, encodeUnslab(m.mutableGetResponse, { seq, value, signature }))
    req.reply(null)
  }

  onimmutableget(req) {
    if (!req.target) return

    const k = b4a.toString(req.target, 'hex')
    const value = this.immutables.get(k)

    req.reply(value || null)
  }

  onimmutableput(req) {
    if (!req.target || !req.token || !req.value) return

    const hash = b4a.alloc(32)
    sodium.crypto_generichash(hash, req.value)
    if (!b4a.equals(hash, req.target)) return

    const k = b4a.toString(hash, 'hex')
    this.immutables.set(k, unslab(req.value))

    req.reply(null)
  }

  destroy() {
    this.records.destroy()
    this.refreshes.destroy()
    this.mutables.destroy()
    this.immutables.destroy()
  }

  static signMutable(seq, value, keyPair) {
    const signable = b4a.allocUnsafe(32 + 32)
    const hash = signable.subarray(32)

    signable.set(NS.MUTABLE_PUT, 0)

    sodium.crypto_generichash(hash, c.encode(m.mutableSignable, { seq, value }))
    return sign(signable, keyPair)
  }

  static verifyMutable(signature, seq, value, publicKey) {
    return verifyMutable(signature, seq, value, publicKey)
  }

  static signAnnounce(target, token, id, ann, keyPair) {
    return sign(annSignable(target, token, id, ann, NS.ANNOUNCE), keyPair)
  }

  static signUnannounce(target, token, id, ann, keyPair) {
    return sign(annSignable(target, token, id, ann, NS.UNANNOUNCE), keyPair)
  }
}

function verifyMutable(signature, seq, value, publicKey) {
  const signable = b4a.allocUnsafe(32 + 32)
  const hash = signable.subarray(32)

  signable.set(NS.MUTABLE_PUT, 0)

  sodium.crypto_generichash(hash, c.encode(m.mutableSignable, { seq, value }))
  return sodium.crypto_sign_verify_detached(signature, signable, publicKey)
}

function annSignable(target, token, id, ann, ns) {
  const signable = b4a.allocUnsafe(32 + 32)
  const hash = signable.subarray(32)

  signable.set(ns, 0)

  sodium.crypto_generichash_batch(hash, [
    target,
    id,
    token,
    c.encode(m.peer, ann.peer), // note that this is the partial encoding of the announce message so we could just use that for perf
    ann.refresh || EMPTY
  ])

  return signable
}

function sign(signable, keyPair) {
  if (keyPair.sign) {
    return keyPair.sign(signable)
  }
  const secretKey = keyPair.secretKey ? keyPair.secretKey : keyPair
  const signature = b4a.allocUnsafe(64)
  sodium.crypto_sign_detached(signature, signable, secretKey)
  return signature
}

function decode(enc, val) {
  try {
    return val && c.decode(enc, val)
  } catch (err) {
    return null
  }
}
module.exports = class RawStreamSet {
  constructor(dht) {
    this._dht = dht

    this._prefix = 16 - 1 // 16 is the default stream-set side in udx
    this._streams = new Map()
  }

  get size() {
    return this._streams.size
  }

  [Symbol.iterator]() {
    return this._streams.values()
  }

  add(opts) {
    const self = this

    // TODO: we should prob have a udx helper for id generation, given the slight complexity
    // of the below. requires a PRNG in udx tho.

    let id = 0

    while (true) {
      id = (Math.random() * 0x100000000) >>> 0

      if (this._streams.has(id & this._prefix)) continue
      break
    }

    // always have ~50% change of rolling a free one
    if (2 * this._streams.size >= this._prefix) {
      // ie 0b11111 = 0b1111 + 1 + 0b1111
      this._prefix = 2 * this._prefix + 1

      // move the prefixes over
      const next = new Map()
      for (const stream of this._streams.values()) {
        next.set(stream.id & this._prefix, stream)
      }
      this._streams = next
    }

    const stream = this._dht.udx.createStream(id, opts)
    this._streams.set(id & this._prefix, stream)

    stream.on('close', onclose)

    return stream

    function onclose() {
      self._streams.delete(id & self._prefix)
    }
  }

  async clear() {
    const destroying = []

    for (const stream of this._streams.values()) {
      destroying.push(new Promise((resolve) => stream.once('close', resolve).destroy()))
    }

    await Promise.allSettled(destroying)
  }
}
const c = require('compact-encoding')
const Cache = require('xache')
const safetyCatch = require('safety-catch')
const b4a = require('b4a')
const { handshake, holepunch } = require('./messages')
const { COMMANDS } = require('./constants')
const { BAD_HANDSHAKE_REPLY, BAD_HOLEPUNCH_REPLY } = require('./errors')

const FROM_CLIENT = 0
const FROM_SERVER = 1
const FROM_RELAY = 2
const FROM_SECOND_RELAY = 3
const REPLY = 4

// TODO: While the current design is very trustless in regards to clients/servers trusting the DHT,
// we should add a bunch of rate limits everywhere, especially including here to avoid bad users
// using a DHT node to relay traffic indiscriminately using the connect/holepunch messages.
// That's mostly from an abuse POV as none of the messsages do amplication.

module.exports = class Router {
  constructor(dht, opts) {
    this.dht = dht
    this.forwards = new Cache(opts.forwards)
  }

  set(target, state) {
    if (state.onpeerhandshake) {
      this.forwards.retain(toString(target), state)
    } else {
      this.forwards.set(toString(target), state)
    }
  }

  get(target) {
    return this.forwards.get(toString(target))
  }

  delete(target) {
    this.forwards.delete(toString(target))
  }

  destroy() {
    this.forwards.destroy()
  }

  async peerHandshake(target, { noise, peerAddress, relayAddress, socket, session }, to) {
    const dht = this.dht

    const requestValue = c.encode(handshake, {
      mode: FROM_CLIENT,
      noise,
      peerAddress,
      relayAddress
    })

    const res = await dht.request(
      { command: COMMANDS.PEER_HANDSHAKE, target, value: requestValue },
      to,
      { socket, session }
    )

    const hs = decode(handshake, res.value)
    if (
      !hs ||
      hs.mode !== REPLY ||
      to.host !== res.from.host ||
      to.port !== res.from.port ||
      !hs.noise
    ) {
      throw BAD_HANDSHAKE_REPLY()
    }

    return {
      noise: hs.noise,
      relayed: !!hs.peerAddress,
      serverAddress: hs.peerAddress || to,
      clientAddress: res.to
    }
  }

  async onpeerhandshake(req) {
    const hs = req.value && decode(handshake, req.value)
    if (!hs) return

    const { mode, noise, peerAddress, relayAddress } = hs

    const state = req.target && this.get(req.target)
    const isServer = !!(state && state.onpeerhandshake)
    const relay = state && state.relay

    if (isServer) {
      let reply = null
      try {
        reply = noise && (await state.onpeerhandshake({ noise, peerAddress }, req))
      } catch (e) {
        safetyCatch(e)
        return
      }
      if (!reply || !reply.noise) return
      const opts = { socket: reply.socket, closerNodes: false, token: false }

      switch (mode) {
        case FROM_CLIENT: {
          req.reply(
            c.encode(handshake, { mode: REPLY, noise: reply.noise, peerAddress: null }),
            opts
          )
          return
        }
        case FROM_RELAY: {
          req.relay(
            c.encode(handshake, { mode: FROM_SERVER, noise: reply.noise, peerAddress }),
            req.from,
            opts
          )
          return
        }
        case FROM_SECOND_RELAY: {
          if (!relayAddress) return
          req.relay(
            c.encode(handshake, { mode: FROM_SERVER, noise: reply.noise, peerAddress }),
            relayAddress,
            opts
          )
          return // eslint-disable-line
        }
      }
    } else {
      switch (mode) {
        case FROM_CLIENT: {
          // TODO: if no relay is known route closer to the target instead of timing out
          if (!noise) return
          if (!relay && !relayAddress) {
            // help the user route
            req.reply(null, { token: false, closerNodes: true })
            return
          }
          req.relay(
            c.encode(handshake, {
              mode: FROM_RELAY,
              noise,
              peerAddress: req.from,
              relayAddress: null
            }),
            relayAddress || relay
          )
          return
        }
        case FROM_RELAY: {
          if (!relay || !noise) return
          req.relay(
            c.encode(handshake, {
              mode: FROM_SECOND_RELAY,
              noise,
              peerAddress,
              relayAddress: req.from
            }),
            relay
          )
          return
        }
        case FROM_SERVER: {
          if (!peerAddress || !noise) return
          req.reply(
            c.encode(handshake, { mode: REPLY, noise, peerAddress: req.from, relayAddress: null }),
            { to: peerAddress, closerNodes: false, token: false }
          )
          return // eslint-disable-line
        }
      }
    }
  }

  async peerHolepunch(target, { id, payload, peerAddress, socket, session }, to) {
    const dht = this.dht
    const requestValue = c.encode(holepunch, {
      mode: FROM_CLIENT,
      id,
      payload,
      peerAddress
    })

    const res = await dht.request(
      { command: COMMANDS.PEER_HOLEPUNCH, target, value: requestValue },
      to,
      { socket, session }
    )

    const hp = decode(holepunch, res.value)
    if (!hp || hp.mode !== REPLY || to.host !== res.from.host || to.port !== res.from.port) {
      throw BAD_HOLEPUNCH_REPLY()
    }

    return {
      from: res.from,
      to: res.to,
      payload: hp.payload,
      peerAddress: hp.peerAddress || to
    }
  }

  async onpeerholepunch(req) {
    const hp = req.value && decode(holepunch, req.value)
    if (!hp) return

    const { mode, id, payload, peerAddress } = hp

    const state = req.target && this.get(req.target)
    const isServer = !!(state && state.onpeerholepunch)
    const relay = state && state.relay

    switch (mode) {
      case FROM_CLIENT: {
        if (!peerAddress && !relay) return
        req.relay(
          c.encode(holepunch, { mode: FROM_RELAY, id, payload, peerAddress: req.from }),
          peerAddress || relay
        )
        return
      }
      case FROM_RELAY: {
        if (!isServer || !peerAddress) return
        let reply = null
        try {
          reply = await state.onpeerholepunch({ id, payload, peerAddress }, req)
        } catch (e) {
          safetyCatch(e)
          return
        }
        if (!reply) return
        const opts = { socket: reply.socket, closerNodes: false, token: false }
        req.relay(
          c.encode(holepunch, { mode: FROM_SERVER, id: 0, payload: reply.payload, peerAddress }),
          req.from,
          opts
        )
        return
      }
      case FROM_SERVER: {
        req.reply(c.encode(holepunch, { mode: REPLY, id, payload, peerAddress: req.from }), {
          to: peerAddress,
          closerNodes: false,
          token: false
        })
        return // eslint-disable-line
      }
    }
  }
}

function decode(enc, val) {
  try {
    return c.decode(enc, val)
  } catch {
    return null
  }
}

function toString(t) {
  return typeof t === 'string' ? t : b4a.toString(t, 'hex')
}
const sodium = require('sodium-universal')
const b4a = require('b4a')
const { holepunchPayload } = require('./messages')

module.exports = class HolepunchPayload {
  constructor(holepunchSecret) {
    this._sharedSecret = holepunchSecret
    this._localSecret = b4a.allocUnsafe(32)

    sodium.randombytes_buf(this._localSecret)
  }

  decrypt(buffer) {
    const state = { start: 24, end: buffer.byteLength - 16, buffer }

    if (state.end <= state.start) return null

    const nonce = buffer.subarray(0, 24)
    const msg = state.buffer.subarray(state.start, state.end)
    const cipher = state.buffer.subarray(state.start)

    if (!sodium.crypto_secretbox_open_easy(msg, cipher, nonce, this._sharedSecret)) return null

    try {
      return holepunchPayload.decode(state)
    } catch {
      return null
    }
  }

  encrypt(payload) {
    const state = { start: 24, end: 24, buffer: null }
    holepunchPayload.preencode(state, payload)
    state.buffer = b4a.allocUnsafe(state.end + 16)

    const nonce = state.buffer.subarray(0, 24)
    const msg = state.buffer.subarray(state.start, state.end)
    const cipher = state.buffer.subarray(state.start)

    holepunchPayload.encode(state, payload)
    sodium.randombytes_buf(nonce)
    sodium.crypto_secretbox_easy(cipher, msg, nonce, this._sharedSecret)

    return state.buffer
  }

  token(addr) {
    const out = b4a.allocUnsafe(32)
    sodium.crypto_generichash(out, b4a.from(addr.host), this._localSecret)
    return out
  }
}
const DONE = Promise.resolve(true)
const DESTROYED = Promise.resolve(false)

module.exports = class Semaphore {
  constructor(limit = 1) {
    this.limit = limit
    this.active = 0
    this.waiting = []

    this.flushedPromise = null
    this.flushedResolve = null

    this.destroyed = false

    this._onwait = this._queueWaiting.bind(this)
    this._onflush = this._queueFlushed.bind(this)
  }

  _queueWaiting(resolve) {
    this.waiting.push(resolve)
  }

  _queueFlushed(resolve) {
    this.flushedResolve = resolve
  }

  wait() {
    if (this.destroyed === true) return DESTROYED

    if (this.active < this.limit && this.waiting.length === 0) {
      this.active++
      return DONE
    }

    return new Promise(this._onwait)
  }

  signal() {
    if (this.destroyed === true) return

    this.active--
    while (this.active < this.limit && this.waiting.length > 0 && this.destroyed === false) {
      this.active++
      this.waiting.shift()(true)
    }

    if (this.active === 0 && this.flushedResolve) {
      const resolve = this.flushedResolve
      this.flushedResolve = null
      this.flushedPromise = null
      resolve(true)
    }
  }

  async flush() {
    if (this.destroyed === true) return
    if (this.active === 0) return
    if (this.flushedPromise) return this.flushedPromise
    this.flushedPromise = new Promise(this._onflush)
    return this.flushedPromise
  }

  destroy() {
    this.destroyed = true
    this.active = 0
    while (this.waiting.length) this.waiting.pop()(false)
    if (this.flushedResolve) this.flushedResolve(false)
  }
}
const { EventEmitter } = require('events')
const safetyCatch = require('safety-catch')
const NoiseSecretStream = require('@hyperswarm/secret-stream')
const b4a = require('b4a')
const relay = require('blind-relay')
const NoiseWrap = require('./noise-wrap')
const Announcer = require('./announcer')
const { FIREWALL, ERROR } = require('./constants')
const { unslabbedHash } = require('./crypto')
const SecurePayload = require('./secure-payload')
const Holepuncher = require('./holepuncher')
const { isPrivate } = require('bogon')
const { ALREADY_LISTENING, NODE_DESTROYED, KEYPAIR_ALREADY_USED } = require('./errors')

const HANDSHAKE_CLEAR_WAIT = 10000
const HANDSHAKE_INITIAL_TIMEOUT = 10000

module.exports = class Server extends EventEmitter {
  constructor(dht, opts = {}) {
    super()

    this.dht = dht
    this.target = null

    this.closed = false
    this.firewall = opts.firewall || (() => false)
    this.holepunch = opts.holepunch || (() => true)
    this.relayThrough = opts.relayThrough || null
    this.relayKeepAlive = opts.relayKeepAlive || 5000
    this.pool = opts.pool || null
    this.createHandshake = opts.createHandshake || defaultCreateHandshake
    this.createSecretStream = opts.createSecretStream || defaultCreateSecretStream
    this.suspended = false
    this.handshakeClearWait = opts.handshakeClearWait || HANDSHAKE_CLEAR_WAIT

    this._shareLocalAddress = opts.shareLocalAddress !== false
    this._reusableSocket = !!opts.reusableSocket
    this._neverPunch = opts.holepunch === false // useful for fully disabling punching
    this._keyPair = null
    this._announcer = null
    this._connects = new Map()
    this._holepunches = []
    this._listening = null
    this._closing = null
  }

  get listening() {
    return this._listening !== null
  }

  get publicKey() {
    return this._keyPair && this._keyPair.publicKey
  }

  get relayAddresses() {
    return this._announcer ? this._announcer.relayAddresses : []
  }

  onconnection(encryptedSocket) {
    this.emit('connection', encryptedSocket)
  }

  async suspend({ log = noop } = {}) {
    log('Suspending hyperdht server')
    if (this._listening !== null) await this._listening
    log('Suspending hyperdht server (post listening)')
    this.suspended = true
    this._clearAll()
    return this._announcer ? this._announcer.suspend({ log }) : Promise.resolve()
  }

  async resume() {
    if (this._listening !== null) await this._listening
    this.suspended = false
    return this._announcer ? this._announcer.resume() : Promise.resolve()
  }

  address() {
    if (!this._keyPair) return null

    return {
      publicKey: this._keyPair.publicKey,
      host: this.dht.host,
      port: this.dht.port
    }
  }

  close() {
    if (this._closing) return this._closing
    this._closing = this._close()
    return this._closing
  }

  _gc() {
    this.dht.listening.delete(this)
    if (this.target) this.dht._router.delete(this.target)
  }

  async _stopListening() {
    try {
      if (this._announcer) await this._announcer.stop()
    } catch {
      // ignore
    }

    this._announcer = null
    this._listening = null
    this._keyPair = null
  }

  async _close() {
    if (this._listening === null) {
      this.closed = true
      this.emit('close')
      return
    }

    try {
      await this._listening
    } catch {}

    this._gc()
    this._clearAll()

    await this._stopListening()

    this.closed = true
    this.emit('close')
  }

  _clearAll() {
    while (this._holepunches.length > 0) {
      const h = this._holepunches.pop()
      if (h && h.puncher) h.puncher.destroy()
      if (h && h.clearing) clearTimeout(h.clearing)
      if (h && h.prepunching) clearTimeout(h.prepunching)
      if (h && h.rawStream) h.rawStream.destroy()
    }

    this._connects.clear()
  }

  async listen(keyPair = this.dht.defaultKeyPair, opts = {}) {
    if (this._listening !== null) throw ALREADY_LISTENING()
    if (this.dht.destroyed) throw NODE_DESTROYED()

    this._listening = this._listen(keyPair, opts)
    await this._listening
    return this
  }

  async _listen(keyPair, opts) {
    // From now on, the DHT object which created me is responsible for closing me
    this.dht.listening.add(this)

    try {
      await this.dht.bind()
      if (this._closing) return

      for (const s of this.dht.listening) {
        if (s._keyPair && b4a.equals(s._keyPair.publicKey, keyPair.publicKey)) {
          throw KEYPAIR_ALREADY_USED()
        }
      }

      this.target = unslabbedHash(keyPair.publicKey)
      this._keyPair = keyPair
      this._announcer = new Announcer(this.dht, keyPair, this.target, opts)

      this.dht._router.set(this.target, {
        relay: null,
        record: this._announcer.record,
        onpeerhandshake: this._onpeerhandshake.bind(this),
        onpeerholepunch: this._onpeerholepunch.bind(this)
      })

      // warm it up for now
      this._localAddresses().catch(safetyCatch)

      await this._announcer.start()
    } catch (err) {
      await this._stopListening()
      this._gc()
      throw err
    }

    if (this._closing) return
    if (this.suspended) await this._announcer.suspend()

    if (this._closing) return
    if (this.dht.destroyed) throw NODE_DESTROYED()

    if (this.pool) this.pool._attachServer(this)

    this.emit('listening')
  }

  refresh() {
    if (this._announcer && !this.suspended) this._announcer.refresh()
  }

  notifyOnline() {
    if (this._announcer) this._announcer.online.notify()
  }

  _localAddresses() {
    return this.dht.validateLocalAddresses(Holepuncher.localAddresses(this.dht.io.serverSocket))
  }

  async _addHandshake(k, noise, clientAddress, { from, to: serverAddress, socket }, direct) {
    let id = this._holepunches.indexOf(null)
    if (id === -1) id = this._holepunches.push(null) - 1

    const hs = {
      round: 0,
      reply: null,
      puncher: null,
      payload: null,
      rawStream: null,
      encryptedSocket: null,
      prepunching: null,
      firewalled: true,
      clearing: null,
      onsocket: null,
      aborted: false,

      // Relay state
      relayTimeout: null,
      relayToken: null,
      relaySocket: null,
      relayClient: null,
      relayPaired: false
    }

    this._holepunches[id] = hs

    const handshake = this.createHandshake(this._keyPair, null)

    let remotePayload
    try {
      remotePayload = await handshake.recv(noise)
    } catch (err) {
      safetyCatch(err)
      this._clearLater(hs, id, k)
      return null
    }

    if (this._closing || this.suspended) return null

    try {
      hs.firewalled = await this.firewall(handshake.remotePublicKey, remotePayload, clientAddress)
    } catch (err) {
      safetyCatch(err)
    }

    if (this._closing || this.suspended) return null

    if (hs.firewalled) {
      this._clearLater(hs, id, k)
      return null
    }

    const error =
      remotePayload.version === 1
        ? remotePayload.udx
          ? ERROR.NONE
          : ERROR.ABORTED
        : ERROR.VERSION_MISMATCH

    const addresses = []
    const ourRemoteAddr = this.dht.remoteAddress()
    const ourLocalAddrs = this._shareLocalAddress ? await this._localAddresses() : null

    if (this._closing || this.suspended) return null

    if (ourRemoteAddr) addresses.push(ourRemoteAddr)
    if (ourLocalAddrs) addresses.push(...ourLocalAddrs)

    if (error === ERROR.NONE) {
      hs.rawStream = this.dht.createRawStream({
        framed: true,
        firewall(socket, port, host) {
          // Check if the traffic originated from the socket on which we're expecting relay traffic. If so,
          // we haven't hole punched yet and the other side is just sending us traffic through the relay.
          if (hs.relaySocket && isRelay(hs.relaySocket, socket, port, host)) {
            return false
          }

          hs.onsocket(socket, port, host)
          return false
        }
      })

      hs.rawStream.on('error', autoDestroy)

      // Handles the case where onsocket is never called, but the stream got setup
      // This can happen on a relayed connection which never connects directly
      // (onsocket is called there only when the direct connection is established)
      const onrawstreamclose = () => {
        if (this._closing) return
        this._clearLater(hs, id, k)
      }
      hs.rawStream.on('close', onrawstreamclose)

      hs.onsocket = (socket, port, host) => {
        if (hs.rawStream === null) return // Already hole punched

        this._clearLater(hs, id, k)

        if (hs.prepunching) {
          clearTimeout(hs.prepunching)
          hs.prepunching = null
        }

        if (this._reusableSocket && remotePayload.udx.reusableSocket) {
          this.dht._socketPool.routes.add(handshake.remotePublicKey, hs.rawStream)
        }

        hs.rawStream.removeListener('error', autoDestroy)
        hs.rawStream.removeListener('close', onrawstreamclose)

        if (hs.rawStream.connected) {
          const remoteChanging = hs.rawStream.changeRemote(socket, remotePayload.udx.id, port, host)

          if (remoteChanging) remoteChanging.catch(safetyCatch)
        } else {
          hs.rawStream.connect(socket, remotePayload.udx.id, port, host)
          hs.encryptedSocket = this.createSecretStream(false, hs.rawStream, {
            handshake: h,
            keepAlive: this.dht.connectionKeepAlive
          })

          this.onconnection(hs.encryptedSocket)
        }

        if (hs.puncher) {
          hs.puncher.onabort = noop
          hs.puncher.destroy()
        }

        hs.rawStream = null
      }

      function autoDestroy() {
        if (hs.puncher) hs.puncher.destroy()
      }
    }

    const relayAddresses = this.relayAddresses
    const relayThrough = selectRelay(this.relayThrough)

    if (relayThrough) hs.relayToken = relay.token()

    try {
      hs.reply = await handshake.send({
        error,
        firewall: ourRemoteAddr ? FIREWALL.OPEN : FIREWALL.UNKNOWN,
        holepunch: ourRemoteAddr ? null : { id, relays: this._announcer.relays },
        addresses4: addresses,
        addresses6: null,
        udx: {
          reusableSocket: this._reusableSocket,
          id: hs.rawStream ? hs.rawStream.id : 0,
          seq: 0
        },
        secretStream: {},
        relayThrough: relayThrough ? { publicKey: relayThrough, token: hs.relayToken } : null,
        relayAddresses: relayAddresses.length ? relayAddresses : null
      })
    } catch (err) {
      safetyCatch(err)
      hs.rawStream.destroy()
      this._clearLater(hs, id, k)
      return null
    }

    if (this._closing || this.suspended) {
      hs.rawStream.destroy()
      return null
    }

    const h = handshake.final()

    if (error !== ERROR.NONE) {
      hs.rawStream.destroy()
      this._clearLater(hs, id, k)
      return hs
    }

    if (remotePayload.firewall === FIREWALL.OPEN || direct) {
      const sock = direct ? socket : this.dht.socket
      this.dht.stats.punches.open++
      hs.onsocket(sock, clientAddress.port, clientAddress.host)
      return hs
    }

    if (relayThrough || remotePayload.relayThrough) {
      this._relayConnection(hs, relayThrough, remotePayload, h)
    }

    const onabort = () => {
      hs.aborted = true
      if (hs.prepunching) clearTimeout(hs.prepunching)
      hs.prepunching = null
      if (hs.rawStream.destroyed) {
        this._clearLater(hs, id, k)
        return
      }

      hs.rawStream.on('close', () => this._clearLater(hs, id, k))
      if (hs.relayToken === null) hs.rawStream.destroy()
    }

    if (!direct && clientAddress.host === serverAddress.host) {
      const clientAddresses = remotePayload.addresses4.filter(onlyPrivateHosts)

      if (clientAddresses.length > 0 && this._shareLocalAddress) {
        const myAddresses = await this._localAddresses()
        const addr = Holepuncher.matchAddress(myAddresses, clientAddresses)

        if (addr) {
          hs.prepunching = setTimeout(onabort, HANDSHAKE_INITIAL_TIMEOUT)
          return hs
        }
      }
    }

    if (this._closing || this.suspended) return null

    if (ourRemoteAddr || this._neverPunch) {
      hs.prepunching = setTimeout(onabort, HANDSHAKE_INITIAL_TIMEOUT)
      return hs
    }

    hs.payload = new SecurePayload(h.holepunchSecret)
    hs.puncher = new Holepuncher(this.dht, this.dht.session(), false, remotePayload.firewall)

    hs.puncher.onconnect = hs.onsocket
    hs.puncher.onabort = onabort
    hs.prepunching = setTimeout(hs.puncher.destroy.bind(hs.puncher), HANDSHAKE_INITIAL_TIMEOUT)

    return hs
  }

  _clearLater(hs, id, k) {
    if (hs.clearing) return
    hs.clearing = setTimeout(() => this._clear(hs, id, k), this.handshakeClearWait)
  }

  _clear(hs, id, k) {
    if (id >= this._holepunches.length || this._holepunches[id] !== hs) return
    if (hs.clearing) clearTimeout(hs.clearing)

    this._holepunches[id] = null
    while (
      this._holepunches.length > 0 &&
      this._holepunches[this._holepunches.length - 1] === null
    ) {
      this._holepunches.pop()
    }
    this._connects.delete(k)
  }

  async _onpeerhandshake({ noise, peerAddress }, req) {
    const k = b4a.toString(noise, 'hex')

    // The next couple of statements MUST run within the same tick to prevent
    // a malicious peer from flooding us with handshakes.
    let p = this._connects.get(k)
    if (!p) {
      p = this._addHandshake(k, noise, peerAddress || req.from, req, !peerAddress)
      this._connects.set(k, p)
    }

    const h = await p
    if (!h) return null

    if (this._closing !== null || this.suspended) return null

    return { socket: h.puncher && h.puncher.socket, noise: h.reply }
  }

  async _onpeerholepunch({ id, peerAddress, payload }, req) {
    const h = id < this._holepunches.length ? this._holepunches[id] : null
    if (!h) return null

    if (!peerAddress || this._closing !== null || this.suspended) return null

    const p = h.puncher
    if (!p || !p.socket) return this._abort(h) // not opened

    const remotePayload = h.payload.decrypt(payload)
    if (!remotePayload) return null

    const isServerRelay = this._announcer.isRelay(req.from)
    const { error, firewall, round, punching, addresses, remoteAddress, remoteToken } =
      remotePayload

    if (error !== ERROR.NONE) {
      // We actually do not need to set the round here, but just do it for consistency.
      if (round >= h.round) h.round = round
      return this._abort(h)
    }

    const token = h.payload.token(peerAddress)
    const echoed = isServerRelay && !!remoteToken && b4a.equals(token, remoteToken)

    // Update our heuristics here
    if (req.socket === p.socket) {
      p.nat.add(req.to, req.from)
    }

    if (round >= h.round) {
      h.round = round
      p.updateRemote({ punching, firewall, addresses, verified: echoed ? peerAddress.host : null })
    }

    // Wait for the analyzer to reach a conclusion...
    let stable = await p.analyze(false)
    if (p.destroyed) return null

    if (!p.remoteHolepunching && !stable) {
      stable = await p.analyze(true)
      if (p.destroyed) return null
      if (!stable) return this._abort(h)
    }

    // Fast mode! If we are consistent and the remote has opened a session to us (remoteAddress)
    // then fire a quick punch back. Note the await here just waits for the udp socket to flush.
    if (
      isConsistent(p.nat.firewall) &&
      remoteAddress &&
      hasSameAddr(p.nat.addresses, remoteAddress)
    ) {
      await p.ping(peerAddress)
      if (p.destroyed) return null
    }

    // Remote said they are punching (or willing to), so we will punch as well.
    // Note that this returns when the punching has STARTED, so no guarantee
    // we will have a connection after this promise etc.
    if (p.remoteHolepunching) {
      // TODO: still continue here if a local connection might work, but then do not holepunch...
      if (!this.holepunch(p.remoteFirewall, p.nat.firewall, p.remoteAddresses, p.nat.addresses)) {
        return p.destroyed ? null : this._abort(h)
      }

      if (h.prepunching) {
        clearTimeout(h.prepunching)
        h.prepunching = null
      }

      if (p.remoteFirewall >= FIREWALL.RANDOM || p.nat.firewall >= FIREWALL.RANDOM) {
        if (
          this.dht._randomPunches >= this.dht._randomPunchLimit ||
          Date.now() - this.dht._lastRandomPunch < this.dht._randomPunchInterval
        ) {
          if (!h.relayToken) return this._abort(h, ERROR.TRY_LATER)
          return {
            socket: p.socket,
            payload: h.payload.encrypt({
              error: ERROR.TRY_LATER,
              firewall: p.nat.firewall,
              round: h.round,
              connected: p.connected,
              punching: p.punching,
              addresses: p.nat.addresses,
              remoteAddress: null,
              token: isServerRelay ? token : null,
              remoteToken: remotePayload.token
            })
          }
        }
      }

      const punching = await p.punch()
      if (p.destroyed) return null
      if (!punching) return this._abort(h)
    }

    // Freeze that analysis as soon as we have a result we are giving to the other peer
    if (p.nat.firewall !== FIREWALL.UNKNOWN) {
      p.nat.freeze()
    }

    return {
      socket: p.socket,
      payload: h.payload.encrypt({
        error: ERROR.NONE,
        firewall: p.nat.firewall,
        round: h.round,
        connected: p.connected,
        punching: p.punching,
        addresses: p.nat.addresses,
        remoteAddress: null,
        token: isServerRelay ? token : null,
        remoteToken: remotePayload.token
      })
    }
  }

  _abort(h, error = ERROR.ABORTED) {
    if (!h.payload) {
      if (h.puncher) h.puncher.destroy()
      return null
    }

    const payload = h.payload.encrypt({
      error,
      firewall: FIREWALL.UNKNOWN,
      round: h.round,
      connected: false,
      punching: false,
      addresses: null,
      remoteAddress: null,
      token: null,
      remoteToken: null
    })

    h.puncher.destroy()

    return { socket: this.dht.socket, payload }
  }

  _relayConnection(hs, relayThrough, remotePayload, h) {
    this.dht.stats.relaying.attempts++

    let isInitiator
    let publicKey
    let token

    if (relayThrough) {
      isInitiator = true
      publicKey = relayThrough
      token = hs.relayToken
    } else {
      isInitiator = false
      publicKey = remotePayload.relayThrough.publicKey
      token = remotePayload.relayThrough.token
    }

    hs.relayToken = token
    hs.relaySocket = this.dht.connect(publicKey)
    hs.relaySocket.setKeepAlive(this.relayKeepAlive)
    hs.relayClient = relay.Client.from(hs.relaySocket, { id: hs.relaySocket.publicKey })
    hs.relayTimeout = setTimeout(onabort, 15000)

    hs.relayClient
      .pair(isInitiator, token, hs.rawStream)
      .on('error', onabort)
      .on('data', (remoteId) => {
        if (hs.relayTimeout) clearRelayTimeout(hs)
        if (hs.rawStream === null) {
          onabort(null)
          return
        }

        hs.relayPaired = true
        this.dht.stats.relaying.successes++

        if (hs.prepunching) clearTimeout(hs.prepunching)
        hs.prepunching = null

        const { remotePort, remoteHost, socket } = hs.relaySocket.rawStream

        hs.rawStream
          .on('close', () => hs.relaySocket.destroy())
          .connect(socket, remoteId, remotePort, remoteHost)

        hs.encryptedSocket = this.createSecretStream(false, hs.rawStream, { handshake: h })

        this.onconnection(hs.encryptedSocket)
      })

    const dht = this.dht
    function onabort() {
      if (!hs.relayPaired) dht.stats.relaying.aborts++
      if (hs.relayTimeout) clearRelayTimeout(hs)
      const socket = hs.relaySocket
      hs.relayToken = null
      hs.relaySocket = null
      if (socket) socket.destroy()
      if (hs.aborted && hs.rawStream) hs.rawStream.destroy()
    }
  }
}

function clearRelayTimeout(hs) {
  clearTimeout(hs.relayTimeout)
  hs.relayTimeout = null
}

function isConsistent(fw) {
  return fw === FIREWALL.OPEN || fw === FIREWALL.CONSISTENT
}

function hasSameAddr(addrs, other) {
  if (addrs === null) return false

  for (const addr of addrs) {
    if (addr.port === other.port && addr.host === other.host) return true
  }
  return false
}

function defaultCreateHandshake(keyPair, remotePublicKey) {
  return new NoiseWrap(keyPair, remotePublicKey)
}

function defaultCreateSecretStream(isInitiator, rawStream, opts) {
  return new NoiseSecretStream(isInitiator, rawStream, opts)
}

function onlyPrivateHosts(addr) {
  return isPrivate(addr.host)
}

function isRelay(relaySocket, socket, port, host) {
  const stream = relaySocket.rawStream
  if (!stream) return false
  if (stream.socket !== socket) return false
  return port === stream.remotePort && host === stream.remoteHost
}

function selectRelay(relayThrough) {
  if (typeof relayThrough === 'function') relayThrough = relayThrough()
  if (relayThrough === null) return null
  if (Array.isArray(relayThrough)) {
    return relayThrough[Math.floor(Math.random() * relayThrough.length)]
  }
  return relayThrough
}

function noop() {}
module.exports = class Sleeper {
  constructor() {
    this._timeout = null
    this._resolve = null

    this._start = (resolve) => {
      this._resolve = resolve
    }

    this._trigger = () => {
      if (this._resolve === null) return
      const resolve = this._resolve
      this._timeout = null
      this._resolve = null
      resolve()
    }
  }

  pause(ms) {
    const p = new Promise(this._start)
    if (this._timeout !== null) {
      clearTimeout(this._timeout)
      this._trigger()
    }
    this._timeout = setTimeout(this._trigger, ms)
    return p
  }

  resume() {
    if (this._timeout !== null) {
      clearTimeout(this._timeout)
      this._trigger()
    }
  }
}
const b4a = require('b4a')

const LINGER_TIME = 3000

module.exports = class SocketPool {
  constructor(dht, host) {
    this._dht = dht
    this._sockets = new Map()
    this._lingering = new Set() // updated by the ref
    this._host = host

    this.routes = new SocketRoutes(this)
  }

  _onmessage(ref, data, address) {
    this._dht.onmessage(ref.socket, data, address)
  }

  _add(ref) {
    this._sockets.set(ref.socket, ref)
  }

  _remove(ref) {
    this._sockets.delete(ref.socket)
    this._lingering.delete(ref)
  }

  lookup(socket) {
    return this._sockets.get(socket) || null
  }

  setReusable(socket, bool) {
    const ref = this.lookup(socket)
    if (ref) ref.reusable = bool
  }

  acquire() {
    // TODO: Enable socket reuse
    return new SocketRef(this)
  }

  async destroy() {
    const closing = []

    for (const ref of this._sockets.values()) {
      ref._unlinger()
      closing.push(ref.socket.close())
    }

    await Promise.allSettled(closing)
  }
}

class SocketRoutes {
  constructor(pool) {
    this._pool = pool
    this._routes = new Map()
  }

  add(publicKey, rawStream) {
    if (rawStream.socket) this._onconnect(publicKey, rawStream)
    else rawStream.on('connect', this._onconnect.bind(this, publicKey, rawStream))
  }

  get(publicKey) {
    const id = b4a.toString(publicKey, 'hex')
    const route = this._routes.get(id)
    if (!route) return null
    return route
  }

  _onconnect(publicKey, rawStream) {
    const id = b4a.toString(publicKey, 'hex')
    const socket = rawStream.socket

    let route = this._routes.get(id)

    if (!route) {
      const gc = () => {
        if (this._routes.get(id) === route) this._routes.delete(id)
        socket.removeListener('close', gc)
      }

      route = {
        socket,
        address: { host: rawStream.remoteHost, port: rawStream.remotePort },
        gc
      }

      this._routes.set(id, route)
      socket.on('close', gc)
    }

    this._pool.setReusable(socket, true)

    rawStream.on('error', () => {
      this._pool.setReusable(socket, false)
      if (!route) route = this._routes.get(id)
      if (route && route.socket === socket) route.gc()
    })
  }
}

// TODO: we should just make some "user data" object on udx to allow to attach this info
class SocketRef {
  constructor(pool) {
    this._pool = pool

    // Events
    this.onholepunchmessage = noop

    // Whether it should teardown immediately or wait a bit
    this.reusable = false

    this.socket = pool._dht.udx.createSocket()
    this.socket
      .on('close', this._onclose.bind(this))
      .on('message', this._onmessage.bind(this))
      .on('idle', this._onidle.bind(this))
      .on('busy', this._onbusy.bind(this))
      .bind(0, this._pool._host)

    this._refs = 1
    this._released = false
    this._closed = false

    this._timeout = null
    this._wasBusy = false

    this._pool._add(this)
  }

  _onclose() {
    this._pool._remove(this)
  }

  _onmessage(data, address) {
    if (data.byteLength > 1) {
      this._pool._onmessage(this, data, address)
    } else {
      this.onholepunchmessage(data, address, this)
    }
  }

  _onidle() {
    this._closeMaybe()
  }

  _onbusy() {
    this._wasBusy = true
    this._unlinger()
  }

  _reset() {
    this.onholepunchmessage = noop
  }

  _closeMaybe() {
    if (this._refs === 0 && this.socket.idle && !this._timeout) this._close()
  }

  _lingeringClose() {
    this._pool._lingering.delete(this)
    this._timeout = null
    this._closeMaybe()
  }

  _close() {
    this._unlinger()

    if (this.reusable && this._wasBusy) {
      this._wasBusy = false
      this._pool._lingering.add(this)
      this._timeout = setTimeout(this._lingeringClose.bind(this), LINGER_TIME)
      return
    }

    this._closed = true
    this.socket.close()
  }

  _unlinger() {
    if (this._timeout !== null) {
      clearTimeout(this._timeout)
      this._pool._lingering.delete(this)
      this._timeout = null
    }
  }

  get free() {
    return this._refs === 0
  }

  active() {
    this._refs++
    this._unlinger()
  }

  inactive() {
    this._refs--
    this._closeMaybe()
  }

  address() {
    return this.socket.address()
  }

  release() {
    if (this._released) return

    this._released = true
    this._reset()

    this._refs--
    this._closeMaybe()
  }
}

function noop() {}
{
  "name": "hyperdht",
  "version": "6.27.1",
  "description": "The DHT powering Hyperswarm",
  "main": "index.js",
  "browser": "browser.js",
  "bin": {
    "hyperdht": "./bin.js"
  },
  "files": [
    "index.js",
    "browser.js",
    "testnet.js",
    "bin.js",
    "lib/**.js"
  ],
  "imports": {
    "events": {
      "bare": "bare-events",
      "default": "events"
    },
    "child_process": {
      "bare": "bare-node-child-process",
      "default": "child_process"
    }
  },
  "dependencies": {
    "@hyperswarm/secret-stream": "^6.6.2",
    "b4a": "^1.3.1",
    "bare-events": "^2.2.0",
    "blind-relay": "^1.3.0",
    "bogon": "^1.0.0",
    "compact-encoding": "^2.4.1",
    "compact-encoding-net": "^1.0.1",
    "dht-rpc": "^6.15.1",
    "hypercore-crypto": "^3.3.0",
    "hypercore-id-encoding": "^1.2.0",
    "noise-curve-ed": "^2.0.0",
    "noise-handshake": "^4.0.0",
    "record-cache": "^1.1.1",
    "safety-catch": "^1.0.1",
    "signal-promise": "^1.0.3",
    "sodium-universal": "^5.0.1",
    "streamx": "^2.16.1",
    "unslab": "^1.3.0",
    "xache": "^1.1.0"
  },
  "devDependencies": {
    "bare-node-child-process": "^1.0.1",
    "brittle": "^3.0.0",
    "graceful-goodbye": "^1.3.0",
    "newline-decoder": "^1.0.2",
    "prettier": "^3.6.2",
    "prettier-config-holepunch": "^2.0.0"
  },
  "scripts": {
    "format": "prettier --write .",
    "test": "prettier --check . && node test/all.js",
    "test:bare": "bare test/all.js",
    "test:generate": "brittle -r test/all.js test/*.js",
    "integration": "brittle test/integration/*.js",
    "end-to-end": "brittle test/end-to-end/*.js"
  },
  "author": "Mathias Buus (@mafintosh)",
  "license": "MIT",
  "directories": {
    "lib": "lib",
    "test": "test"
  },
  "repository": {
    "type": "git",
    "url": "git+https://github.com/holepunchto/hyperdht.git"
  },
  "keywords": [],
  "bugs": {
    "url": "https://github.com/holepunchto/hyperdht/issues"
  },
  "homepage": "https://github.com/holepunchto/hyperdht#readme"
}
{
  "name": "hyperschema",
  "version": "1.18.0",
  "description": "Create registries of declarative compact-encoding schemas",
  "files": [
    "lib/*.js",
    "builder.mjs",
    "builder.cjs",
    "runtime.mjs",
    "runtime.cjs",
    "primitives.mjs",
    "primitives.cjs"
  ],
  "exports": {
    "./package": "./package.json",
    ".": {
      "import": "./builder.mjs",
      "default": "./builder.cjs"
    },
    "./runtime": {
      "import": "./runtime.mjs",
      "default": "./runtime.cjs"
    },
    "./primitives": {
      "import": "./primitives.mjs",
      "default": "./primitives.cjs"
    }
  },
  "imports": {
    "fs": {
      "bare": "bare-fs",
      "default": "fs"
    },
    "path": {
      "bare": "bare-path",
      "default": "path"
    }
  },
  "scripts": {
    "format": "prettier . --write",
    "test": "prettier . --check && brittle test/index.js",
    "test:bare": "prettier . --check && bare test/index.js"
  },
  "repository": {
    "type": "git",
    "url": "git+https://github.com/holepunchto/hyperschema.git"
  },
  "author": "Holepunch",
  "license": "Apache-2.0",
  "bugs": {
    "url": "https://github.com/holepunchto/hyperschema/issues"
  },
  "homepage": "https://github.com/holepunchto/hyperschema#readme",
  "dependencies": {
    "bare-fs": "^4.0.1",
    "compact-encoding": "^2.15.0",
    "generate-object-property": "^2.0.0",
    "generate-string": "^1.0.1"
  },
  "devDependencies": {
    "brittle": "^3.7.0",
    "prettier": "^3.6.2",
    "prettier-config-holepunch": "^1.0.0",
    "test-tmp": "^1.3.0"
  }
}
module.exports = {
  c: require('compact-encoding')
}
const { EventEmitter } = require('events')
const { getStreamError } = require('streamx')
const DHT = require('hyperdht')
const spq = require('shuffled-priority-queue')
const b4a = require('b4a')
const unslab = require('unslab')

const PeerInfo = require('./lib/peer-info')
const RetryTimer = require('./lib/retry-timer')
const ConnectionSet = require('./lib/connection-set')
const PeerDiscovery = require('./lib/peer-discovery')

const MAX_PEERS = 64
const MAX_PARALLEL = 3
const MAX_CLIENT_CONNECTIONS = Infinity // TODO: Change
const MAX_SERVER_CONNECTIONS = Infinity

const ERR_MISSING_TOPIC = 'Topic is required and must be a 32-byte buffer'
const ERR_DESTROYED = 'Swarm has been destroyed'
const ERR_DUPLICATE = 'Duplicate connection'
const ERR_FIREWALL = 'Peer is firewalled'

module.exports = class Hyperswarm extends EventEmitter {
  constructor(opts = {}) {
    super()
    const {
      seed,
      relayThrough,
      keyPair = DHT.keyPair(seed),
      maxPeers = MAX_PEERS,
      maxClientConnections = MAX_CLIENT_CONNECTIONS,
      maxServerConnections = MAX_SERVER_CONNECTIONS,
      maxParallel = MAX_PARALLEL,
      firewall = allowAll
    } = opts
    this.keyPair = keyPair

    this.dht =
      opts.dht ||
      new DHT({
        bootstrap: opts.bootstrap,
        nodes: opts.nodes,
        port: opts.port,
        deferRandomPunch: opts.deferRandomPunch,
        randomPunchInterval: opts.randomPunchInterval
      })
    this.server = this.dht.createServer(
      {
        firewall: this._handleFirewall.bind(this),
        relayThrough: this._maybeRelayConnection.bind(this),
        handshakeClearWait: opts.handshakeClearWait
      },
      this._handleServerConnection.bind(this)
    )

    this.destroyed = false
    this.suspended = false
    this.maxPeers = maxPeers
    this.maxClientConnections = maxClientConnections
    this.maxServerConnections = maxServerConnections
    this.maxParallel = maxParallel
    this.relayThrough = relayThrough ? toRelayFunction(relayThrough) : null

    this.connecting = 0
    this.connections = new Set()
    this.peers = new Map()
    this.explicitPeers = new Set()
    this.listening = null
    this.stats = {
      updates: 0,
      connects: {
        client: {
          opened: 0,
          closed: 0,
          attempted: 0
        },
        server: {
          // Note: there is no notion of 'attempts' for server connections
          opened: 0,
          closed: 0
        }
      },
      bannedPeers: 0
    }

    this._discovery = new Map()
    this._timer = new RetryTimer(this._requeue.bind(this), {
      backoffs: opts.backoffs,
      jitter: opts.jitter
    })
    this._queue = spq()

    this._allConnections = new ConnectionSet()
    this._pendingFlushes = []
    this._flushTick = 0

    this._drainingQueue = false
    this._clientConnections = 0
    this._serverConnections = 0
    this._firewall = firewall

    this.dht.on('network-change', this._handleNetworkChange.bind(this))
    this.dht.on('network-update', this._handleNetworkUpdate.bind(this))
    this.on('update', this._handleUpdate)
  }

  _maybeRelayConnection(force) {
    if (!this.relayThrough) return null
    return this.relayThrough(force, this)
  }

  _enqueue(peerInfo) {
    if (peerInfo.queued) return
    peerInfo.queued = true
    peerInfo._flushTick = this._flushTick
    this._queue.add(peerInfo)

    this._attemptClientConnections()
  }

  _requeue(batch) {
    if (this.suspended) return
    for (const peerInfo of batch) {
      peerInfo.waiting = false

      if (
        peerInfo._updatePriority() === false ||
        this._allConnections.has(peerInfo.publicKey) ||
        peerInfo.queued
      )
        continue
      peerInfo.queued = true
      peerInfo._flushTick = this._flushTick
      this._queue.add(peerInfo)
    }

    this._attemptClientConnections()
  }

  _flushMaybe(peerInfo) {
    for (let i = 0; i < this._pendingFlushes.length; i++) {
      const flush = this._pendingFlushes[i]
      if (peerInfo._flushTick > flush.tick) continue
      if (--flush.missing > 0) continue
      flush.onflush(true)
      this._pendingFlushes.splice(i--, 1)
    }
  }

  _flushAllMaybe() {
    if (
      this.connecting > 0 ||
      (this._allConnections.size < this.maxPeers &&
        this._clientConnections < this.maxClientConnections)
    ) {
      return false
    }

    while (this._pendingFlushes.length) {
      const flush = this._pendingFlushes.pop()
      flush.onflush(true)
    }

    return true
  }

  _shouldConnectExplicit() {
    return !this.destroyed && !this.suspended && this.connecting < this.maxParallel
  }

  _shouldConnect() {
    return (
      !this.destroyed &&
      !this.suspended &&
      this.connecting < this.maxParallel &&
      this._allConnections.size < this.maxPeers &&
      this._clientConnections < this.maxClientConnections
    )
  }

  _shouldRequeue(peerInfo) {
    if (this.suspended) return false
    if (peerInfo.explicit) return true
    for (const topic of peerInfo.topics) {
      if (this._discovery.has(b4a.toString(topic, 'hex')) && !this.destroyed) {
        return true
      }
    }
    return false
  }

  _connect(peerInfo, queued) {
    if (peerInfo.banned || this._allConnections.has(peerInfo.publicKey)) {
      if (queued) this._flushMaybe(peerInfo)
      return
    }

    // TODO: Support async firewalling at some point.
    if (this._handleFirewall(peerInfo.publicKey, null)) {
      if (queued) this._flushMaybe(peerInfo)
      return
    }

    const relayThrough = this._maybeRelayConnection(peerInfo.forceRelaying)
    const conn = this.dht.connect(peerInfo.publicKey, {
      relayAddresses: peerInfo.relayAddresses,
      keyPair: this.keyPair,
      relayThrough
    })
    this._allConnections.add(conn)

    this.stats.connects.client.attempted++

    this.connecting++
    this._clientConnections++
    let opened = false

    const onerror = (err) => {
      if (this.relayThrough && shouldForceRelaying(err.code)) {
        peerInfo.forceRelaying = true
        // Reset the attempts in order to fast connect to relay
        peerInfo.attempts = 0
      }
    }

    // Removed once a connection is opened
    conn.on('error', onerror)

    conn.on('open', () => {
      opened = true
      this.stats.connects.client.opened++

      this._connectDone()
      this.connections.add(conn)
      conn.removeListener('error', onerror)
      peerInfo._connected()
      peerInfo.client = true
      this.emit('connection', conn, peerInfo)
      if (queued) this._flushMaybe(peerInfo)

      this.emit('update')
    })
    conn.on('close', () => {
      if (!opened) this._connectDone()
      this.stats.connects.client.closed++

      const err = getStreamError(conn)
      if (shouldBan(err)) {
        this._banPeer(peerInfo, true, err)
      }

      this.connections.delete(conn)
      this._allConnections.delete(conn)
      this._clientConnections--
      peerInfo._disconnected()

      peerInfo.waiting = this._shouldRequeue(peerInfo) && this._timer.add(peerInfo)
      this._maybeDeletePeer(peerInfo)

      if (!opened && queued) this._flushMaybe(peerInfo)

      this._attemptClientConnections()

      this.emit('update')
    })

    this.emit('update')
  }

  _connectDone() {
    this.connecting--

    if (this.connecting < this.maxParallel) this._attemptClientConnections()
    if (this.connecting === 0) this._flushAllMaybe()
  }

  // Called when the PeerQueue indicates a connection should be attempted.
  _attemptClientConnections() {
    // Guard against re-entries - unsure if it still needed but doesn't hurt
    if (this._drainingQueue || this.suspended) return
    this._drainingQueue = true

    for (const peerInfo of this.explicitPeers) {
      if (!this._shouldConnectExplicit()) break
      if (
        peerInfo.attempts >= 5 ||
        Date.now() - peerInfo.disconnectedTime < peerInfo.attempts * 1000
      )
        continue
      this._connect(peerInfo, false)
    }

    while (this._queue.length && this._shouldConnect()) {
      const peerInfo = this._queue.shift()
      peerInfo.queued = false
      this._connect(peerInfo, true)
    }
    this._drainingQueue = false
    if (this.connecting === 0) this._flushAllMaybe()
  }

  _handleFirewall(remotePublicKey, payload) {
    if (b4a.equals(remotePublicKey, this.keyPair.publicKey)) return true

    let peerInfo = this.peers.get(b4a.toString(remotePublicKey, 'hex'))
    if (peerInfo && peerInfo.banned) return true

    const firewalled = this._firewall(remotePublicKey, payload)
    if (firewalled) {
      if (!peerInfo) peerInfo = this._upsertPeer(remotePublicKey)
      this._banPeer(peerInfo, true, new Error(ERR_FIREWALL))
    }

    return firewalled
  }

  _handleServerConnectionSwap(existing, conn) {
    let closed = false

    existing.on('close', () => {
      if (closed) return

      conn.removeListener('error', noop)
      conn.removeListener('close', onclose)

      this._handleServerConnection(conn)
    })

    conn.on('error', noop)
    conn.on('close', onclose)

    function onclose() {
      closed = true
    }
  }

  // Called when the DHT receives a new server connection.
  _handleServerConnection(conn) {
    if (this.destroyed || this.suspended) {
      // TODO: Investigate why a final server connection can be received after close
      conn.on('error', noop)
      return conn.destroy(ERR_DESTROYED)
    }

    const existing = this._allConnections.get(conn.remotePublicKey)

    if (existing) {
      // If both connections are from the same peer,
      // - pick the new one if the existing stream is already established (has sent and received bytes),
      //   because the other client must have lost that connection and be reconnecting
      // - otherwise, pick the one thats expected to initiate in a tie break
      const existingIsOutdated = existing.rawBytesRead > 0 && existing.rawBytesWritten > 0
      const expectedInitiator = b4a.compare(conn.publicKey, conn.remotePublicKey) > 0
      const keepNew = existingIsOutdated || expectedInitiator === conn.isInitiator

      if (keepNew === false) {
        existing.sendKeepAlive()
        conn.on('error', noop)
        conn.destroy(new Error(ERR_DUPLICATE))
        return
      }

      existing.on('error', noop)
      existing.destroy(new Error(ERR_DUPLICATE))
      this._handleServerConnectionSwap(existing, conn)
      return
    }

    // When reaching here, the connection will always be 'opened' next tick
    this.stats.connects.server.opened++

    const peerInfo = this._upsertPeer(conn.remotePublicKey, null)

    this.connections.add(conn)
    this._allConnections.add(conn)
    this._serverConnections++

    conn.on('close', () => {
      const err = getStreamError(conn)
      if (shouldBan(err)) {
        this._banPeer(peerInfo, true, err)
      }

      this.connections.delete(conn)
      this._allConnections.delete(conn)
      this._serverConnections--
      this.stats.connects.server.closed++

      this._maybeDeletePeer(peerInfo)

      this._attemptClientConnections()

      this.emit('update')
    })
    peerInfo.client = false
    this.emit('connection', conn, peerInfo)

    this.emit('update')
  }

  _upsertPeer(publicKey, relayAddresses) {
    if (b4a.equals(publicKey, this.keyPair.publicKey)) return null
    const keyString = b4a.toString(publicKey, 'hex')
    let peerInfo = this.peers.get(keyString)

    if (peerInfo) {
      peerInfo.relayAddresses = relayAddresses // new is always better
      return peerInfo
    }

    peerInfo = new PeerInfo({
      publicKey,
      relayAddresses
    })

    this.peers.set(keyString, peerInfo)
    return peerInfo
  }

  _handleUpdate() {
    this.stats.updates++
  }

  _maybeDeletePeer(peerInfo) {
    if (!peerInfo.shouldGC()) return

    const hasActiveConn = this._allConnections.has(peerInfo.publicKey)
    if (hasActiveConn) return

    const keyString = b4a.toString(peerInfo.publicKey, 'hex')
    this.peers.delete(keyString)
  }

  /*
   * Called when a peer is actively discovered during a lookup.
   *
   * Three conditions:
   *  1. Not a known peer -- insert into queue
   *  2. A known peer with normal priority -- do nothing
   *  3. A known peer with low priority -- bump priority, because it's been rediscovered
   */
  _handlePeer(peer, topic) {
    const peerInfo = this._upsertPeer(peer.publicKey, peer.relayAddresses)
    if (peerInfo) peerInfo._topic(topic)
    if (!peerInfo || this._allConnections.has(peer.publicKey)) return
    if (!peerInfo.prioritized || peerInfo.server) peerInfo._reset()
    if (peerInfo._updatePriority()) {
      this._enqueue(peerInfo)
    }
  }

  async _handleNetworkUpdate() {
    if (!this.online) return
    this._handleNetworkChange()
  }

  async _handleNetworkChange() {
    if (this.suspended) return

    // prioritize figuring out if existing connections are dead
    for (const conn of this._allConnections) {
      conn.sendKeepAlive()
    }

    const refreshes = []

    for (const discovery of this._discovery.values()) {
      refreshes.push(discovery.refresh())
    }

    await Promise.allSettled(refreshes)
  }

  _banPeer(peerInfo, banned, err) {
    peerInfo.ban(banned)
    this.stats.bannedPeers++
    this.emit('ban', peerInfo, err)
  }

  status(key) {
    return this._discovery.get(b4a.toString(key, 'hex')) || null
  }

  listen() {
    if (!this.listening) {
      if (this.destroyed) throw new Error('Swarm destroyed')
      this.listening = this.server.listen(this.keyPair)
    }
    return this.listening
  }

  // Object that exposes a cancellation method (destroy)
  // TODO: When you rejoin, it should reannounce + bump lookup priority
  join(topic, opts = {}) {
    if (this.destroyed) throw new Error('Swarm destroyed')
    if (!topic) throw new Error(ERR_MISSING_TOPIC)
    topic = unslab(topic)

    const topicString = b4a.toString(topic, 'hex')

    let discovery = this._discovery.get(topicString)

    if (discovery && !discovery.destroyed) {
      return discovery.session(opts)
    }

    discovery = new PeerDiscovery(this, topic, {
      limit: opts.limit,
      wait: discovery ? discovery.destroy() : null,
      suspended: this.suspended,
      onpeer: (peer) => this._handlePeer(peer, topic)
    })
    this._discovery.set(topicString, discovery)
    return discovery.session(opts)
  }

  // Returns a promise
  async leave(topic) {
    if (!topic) throw new Error(ERR_MISSING_TOPIC)
    const topicString = b4a.toString(topic, 'hex')
    if (!this._discovery.has(topicString)) return Promise.resolve()

    const discovery = this._discovery.get(topicString)

    try {
      await discovery.destroy()
    } catch {
      // ignore, prop network
    }

    if (this._discovery.get(topicString) === discovery) {
      this._discovery.delete(topicString)
    }
  }

  joinPeer(publicKey) {
    const peerInfo = this._upsertPeer(publicKey, null)
    if (!peerInfo) return
    if (!this.explicitPeers.has(peerInfo)) {
      peerInfo.explicit = true
      this.explicitPeers.add(peerInfo)
    }
    if (this._allConnections.has(publicKey)) return
    if (peerInfo._updatePriority()) {
      this._enqueue(peerInfo)
    }
  }

  leavePeer(publicKey) {
    const keyString = b4a.toString(publicKey, 'hex')
    if (!this.peers.has(keyString)) return

    const peerInfo = this.peers.get(keyString)
    peerInfo.explicit = false
    this.explicitPeers.delete(peerInfo)
    this._maybeDeletePeer(peerInfo)
  }

  // Returns a promise
  async flush() {
    const allFlushed = [...this._discovery.values()].map((v) => v.flushed())
    await Promise.all(allFlushed)
    if (this._flushAllMaybe()) return true
    const pendingSize = this._allConnections.size - this.connections.size
    if (!this._queue.length && !pendingSize) return true
    return new Promise((resolve) => {
      this._pendingFlushes.push({
        onflush: resolve,
        missing: this._queue.length + pendingSize,
        tick: this._flushTick++
      })
    })
  }

  async clear() {
    const cleared = Promise.allSettled([...this._discovery.values()].map((d) => d.destroy()))
    this._discovery.clear()
    return cleared
  }

  async destroy({ force } = {}) {
    if (this.destroyed && !force) return
    this.destroyed = true

    this._timer.destroy()

    if (!force) await this.clear()

    await this.server.close()

    while (this._pendingFlushes.length) {
      const flush = this._pendingFlushes.pop()
      flush.onflush(false)
    }

    await this.dht.destroy({ force })
  }

  async suspend({ log = noop } = {}) {
    if (this.suspended) return

    const promises = []

    promises.push(this.server.suspend({ log }))

    for (const discovery of this._discovery.values()) {
      promises.push(discovery.suspend({ log }))
    }

    const pending = []
    for (const connection of this._allConnections) {
      connection.destroy()
      pending.push(new Promise((resolve) => connection.on('close', resolve)))
    }

    this.suspended = true

    log('Suspending server and discovery... (' + promises.length + ')')
    await Promise.allSettled(promises)
    log('Done, suspending the dht...')
    await this.dht.suspend({ log })
    log('Done, swarm fully suspended')

    await Promise.all(pending)

    // reset queue
    this._timer.destroy()
    this._timer = new RetryTimer(this._requeue.bind(this), {
      backoffs: this._timer.backoffs,
      jitter: this._timer.jitter
    })
    this._queue = spq()
  }

  async resume({ log = noop } = {}) {
    if (!this.suspended) return

    log('Resuming the dht')
    await this.dht.resume()
    log('Done, resuming the server')
    await this.server.resume()
    log('Done, all discovery')

    for (const discovery of this._discovery.values()) {
      discovery.resume()
    }

    this.suspended = false
    this._attemptClientConnections()
  }

  topics() {
    return this._discovery.values()
  }
}

function noop() {}

function allowAll() {
  return false
}

function shouldForceRelaying(code) {
  return (
    code === 'HOLEPUNCH_ABORTED' ||
    code === 'HOLEPUNCH_DOUBLE_RANDOMIZED_NATS' ||
    code === 'REMOTE_NOT_HOLEPUNCHABLE'
  )
}

function shouldBan() {
  // return !!err && err.name === 'HypercoreError' && err.code === 'INVALID_OPERATION'
  return false
}

function toRelayFunction(relayThrough) {
  return typeof relayThrough === 'function'
    ? relayThrough
    : (force, swarm) => (force || swarm.dht.randomized ? relayThrough : null)
}
module.exports = class BulkTimer {
  constructor(time, fn) {
    this._time = time
    this._fn = fn
    this._interval = null
    this._next = []
    this._pending = []
    this._destroyed = false
  }

  destroy() {
    if (this._destroyed) return
    this._destroyed = true
    clearInterval(this._interval)
    this._interval = null
  }

  _ontick() {
    if (!this._next.length && !this._pending.length) return
    if (this._next.length) this._fn(this._next)
    this._next = this._pending
    this._pending = []
  }

  add(info) {
    if (this._destroyed) return
    if (!this._interval) {
      this._interval = setInterval(this._ontick.bind(this), Math.floor(this._time * 0.66))
    }

    this._pending.push(info)
  }
}
const b4a = require('b4a')

module.exports = class ConnectionSet {
  constructor() {
    this._byPublicKey = new Map()
  }

  [Symbol.iterator]() {
    return this._byPublicKey.values()
  }

  get size() {
    return this._byPublicKey.size
  }

  has(publicKey) {
    return this._byPublicKey.has(toHex(publicKey))
  }

  get(publicKey) {
    return this._byPublicKey.get(toHex(publicKey))
  }

  add(connection) {
    this._byPublicKey.set(b4a.toString(connection.remotePublicKey, 'hex'), connection)
  }

  delete(connection) {
    const keyString = b4a.toString(connection.remotePublicKey, 'hex')
    const existing = this._byPublicKey.get(keyString)
    if (existing !== connection) return
    this._byPublicKey.delete(keyString)
  }
}

function toHex(b) {
  return typeof b === 'string' ? b : b4a.toString(b, 'hex')
}
const safetyCatch = require('safety-catch')
const b4a = require('b4a')

const REFRESH_INTERVAL = 1000 * 60 * 10 // 10 min
const RANDOM_JITTER = 1000 * 60 * 2 // 2 min
const DELAY_GRACE_PERIOD = 1000 * 30 // 30s

const MAX_DISCOVERY_CACHE = 64

module.exports = class PeerDiscovery {
  constructor(
    swarm,
    topic,
    { limit = Infinity, wait = null, suspended = false, onpeer = noop, onerror = safetyCatch }
  ) {
    this.limit = limit
    this.swarm = swarm
    this.topic = topic
    this.isClient = false
    this.isServer = false
    this.destroyed = false
    this.destroying = null
    this.suspended = suspended

    this._sessions = []
    this._clientSessions = 0
    this._serverSessions = 0

    this._onpeer = onpeer
    this._onerror = onerror

    this._discovered = new Set()
    this._activeQuery = null
    this._timer = null
    this._currentRefresh = null
    this._closestNodes = null
    this._firstAnnounce = true
    this._needsUnannounce = false
    this._refreshes = 0
    this._wait = wait
  }

  session({ server = true, client = true, limit = Infinity, onerror = safetyCatch }) {
    if (this.destroyed) throw new Error('PeerDiscovery is destroyed')
    const session = new PeerDiscoverySession(this)
    session.refresh({ server, client, limit }).catch(onerror)
    this._sessions.push(session)
    return session
  }

  _refreshLater(eager) {
    const jitter = Math.round(Math.random() * RANDOM_JITTER)
    const delay = !eager ? REFRESH_INTERVAL + jitter : jitter

    if (this._timer) clearTimeout(this._timer)

    const startTime = Date.now()
    this._timer = setTimeout(() => {
      // If your laptop went to sleep, and is coming back online...
      const overdue = Date.now() - startTime > delay + DELAY_GRACE_PERIOD
      if (overdue) this._refreshLater(true)
      else this.refresh().catch(this._onerror)
    }, delay)
  }

  _isActive() {
    return !this.destroyed && !this.suspended
  }

  // TODO: Allow announce to be an argument to this
  // TODO: Maybe announce should be a setter?
  async _refresh() {
    if (this.suspended) return
    const clock = ++this._refreshes

    if (this._wait) {
      await this._wait
      this._wait = null
      if (clock !== this._refreshes || !this._isActive()) return
    }

    const clear = this.isServer && this._firstAnnounce
    if (clear) this._firstAnnounce = false

    const opts = {
      clear,
      closestNodes: this._closestNodes
    }

    if (this.isServer) {
      await this.swarm.listen()
      // if a parallel refresh is happening, yield to the new one
      if (clock !== this._refreshes || !this._isActive()) return
      this._needsUnannounce = true
    }

    let limit = this.limit

    if (limit < Infinity && limit > 0) {
      for (const id of this._discovered) {
        if (!this.swarm.connections.has(id)) continue
        if (--limit === 0) break
      }
    }

    this._discovered.clear()

    const announcing = this.isServer
    const query = (this._activeQuery = announcing
      ? this.swarm.dht.announce(
          this.topic,
          this.swarm.keyPair,
          this.swarm.server.relayAddresses,
          opts
        )
      : this._needsUnannounce
        ? this.swarm.dht.lookupAndUnannounce(this.topic, this.swarm.keyPair, opts)
        : this.swarm.dht.lookup(this.topic, opts))

    try {
      for await (const data of this._activeQuery) {
        if (!this.isClient || !this._isActive()) continue
        for (const peer of data.peers) {
          if (limit < Infinity) {
            const id = b4a.toString(peer.publicKey, 'hex')

            if (this._discovered.size < MAX_DISCOVERY_CACHE) {
              this._discovered.add(id)
            }

            if (limit === 0) continue

            // there is a chance it has updated a connection during discovery and we go over
            // the limit - thats acceptable to avoid a complexity spiral here.
            if (!this.swarm.connections.has(id)) limit--
          }

          this._onpeer(peer, data)
        }
      }
    } catch (err) {
      if (this._isActive()) throw err
    } finally {
      if (this._activeQuery === query) {
        this._activeQuery = null
        if (!this.destroyed && !this.suspended) this._refreshLater(false)
      }
    }

    // This is set at the very end, when the query completes successfully.
    this._closestNodes = query.closestNodes

    if (clock !== this._refreshes) return

    // In this is the latest query, unannounce has been fulfilled as well
    if (!announcing) this._needsUnannounce = false
  }

  async refresh() {
    if (this.destroyed) throw new Error('PeerDiscovery is destroyed')

    const server = this._serverSessions > 0
    const client = this._clientSessions > 0

    if (this.suspended) return

    if (server === this.isServer && client === this.isClient) {
      if (this._currentRefresh) return this._currentRefresh
      this._currentRefresh = this._refresh()
    } else {
      if (this._activeQuery) this._activeQuery.destroy()
      this.isServer = server
      this.isClient = client
      this._currentRefresh = this._refresh()
    }

    const refresh = this._currentRefresh
    try {
      await refresh
    } catch {
      return false
    } finally {
      if (refresh === this._currentRefresh) {
        this._currentRefresh = null
      }
    }

    return true
  }

  async flushed() {
    if (this.swarm.listening) await this.swarm.listening

    try {
      await this._currentRefresh
      return true
    } catch {
      return false
    }
  }

  async _destroyMaybe() {
    if (this.destroyed) return

    try {
      if (this._sessions.length === 0) await this.swarm.leave(this.topic)
      else if (this._serverSessions === 0 && this._needsUnannounce) await this.refresh()
    } catch (err) {
      // ignore network failures here, as we are tearing down
      safetyCatch(err)
    }
  }

  destroy() {
    if (this.destroying) return this.destroying
    this.destroying = this._destroy()
    return this.destroying
  }

  async _abort(log) {
    const id = log === noop ? '' : b4a.toString(this.topic, 'hex')

    log('Aborting discovery', id)
    if (this._wait) await this._wait
    log('Aborting discovery (post wait)', id)

    if (this._activeQuery) {
      this._activeQuery.destroy()
      this._activeQuery = null
    }
    if (this._timer) {
      clearTimeout(this._timer)
      this._timer = null
    }

    let nodes = this._closestNodes

    if (this._currentRefresh) {
      try {
        await this._currentRefresh
      } catch {
        // If the destroy causes the refresh to fail, suppress it.
      }
    }

    log('Aborting discovery (post refresh)', id)
    if (this._isActive()) return

    if (!nodes) nodes = this._closestNodes
    else if (this._closestNodes !== nodes) {
      const len = nodes.length
      for (const newer of this._closestNodes) {
        if (newer.id && !hasNode(nodes, len, newer)) nodes.push(newer)
      }
    }

    if (this._needsUnannounce) {
      log('Unannouncing discovery', id)
      if (nodes && nodes.length)
        await this.swarm.dht.unannounce(this.topic, this.swarm.keyPair, {
          closestNodes: nodes,
          onlyClosestNodes: true,
          force: true
        })
      this._needsUnannounce = false
      log('Unannouncing discovery (done)', id)
    }
  }

  _destroy() {
    if (this.destroyed) return
    this.destroyed = true
    return this._abort(noop)
  }

  async suspend({ log = noop } = {}) {
    if (this.suspended) return
    this.suspended = true
    try {
      await this._abort(log)
    } catch {
      // ignore
    }
  }

  resume() {
    if (!this.suspended) return
    this.suspended = false
    this.refresh().catch(noop)
  }
}

class PeerDiscoverySession {
  constructor(discovery) {
    this.discovery = discovery
    this.isClient = false
    this.isServer = false
    this.destroyed = false
  }

  get swarm() {
    return this.discovery.swarm
  }

  get topic() {
    return this.discovery.topic
  }

  async refresh({ client = this.isClient, server = this.isServer, limit = Infinity } = {}) {
    if (this.destroyed) throw new Error('PeerDiscovery is destroyed')
    if (!client && !server) throw new Error('Cannot refresh with neither client nor server option')

    if (client !== this.isClient) {
      this.isClient = client
      this.discovery._clientSessions += client ? 1 : -1
    }

    if (server !== this.isServer) {
      this.isServer = server
      this.discovery._serverSessions += server ? 1 : -1
    }

    this.discovery.limit = limit

    return this.discovery.refresh()
  }

  async flushed() {
    return this.discovery.flushed()
  }

  async destroy() {
    if (this.destroyed) return
    this.destroyed = true

    if (this.isClient) this.discovery._clientSessions--
    if (this.isServer) this.discovery._serverSessions--

    const index = this.discovery._sessions.indexOf(this)
    const head = this.discovery._sessions.pop()

    if (head !== this) this.discovery._sessions[index] = head

    return this.discovery._destroyMaybe()
  }
}

function hasNode(nodes, len, node) {
  for (let i = 0; i < len; i++) {
    const existing = nodes[i]
    if (existing.id && b4a.equals(existing.id, node.id)) return true
  }

  return false
}

function noop() {}
const { EventEmitter } = require('events')
const b4a = require('b4a')
const unslab = require('unslab')

const MIN_CONNECTION_TIME = 15000

const VERY_LOW_PRIORITY = 0
const LOW_PRIORITY = 1
const NORMAL_PRIORITY = 2
const HIGH_PRIORITY = 3
const VERY_HIGH_PRIORITY = 4

module.exports = class PeerInfo extends EventEmitter {
  constructor({ publicKey, relayAddresses }) {
    super()

    this.publicKey = unslab(publicKey)
    this.relayAddresses = relayAddresses

    this.reconnecting = true
    this.proven = false
    this.connectedTime = -1
    this.disconnectedTime = 0
    this.banned = false
    this.tried = false
    this.explicit = false
    this.waiting = false
    this.forceRelaying = false

    // Set by the Swarm
    this.queued = false
    this.client = false
    this.topics = [] // TODO: remove on next major (check with mafintosh for context)

    this.attempts = 0
    this.priority = NORMAL_PRIORITY

    // Used by shuffled-priority-queue
    this._index = 0

    // Used for flush management
    this._flushTick = 0

    // Used for topic multiplexing
    this._seenTopics = new Set()
  }

  get server() {
    return !this.client
  }

  get prioritized() {
    return this.priority >= NORMAL_PRIORITY
  }

  _getPriority() {
    const peerIsStale = this.tried && !this.proven
    if (peerIsStale || this.attempts > 3) return VERY_LOW_PRIORITY
    if (this.attempts === 3) return LOW_PRIORITY
    if (this.attempts === 2) return HIGH_PRIORITY
    if (this.attempts === 1) return VERY_HIGH_PRIORITY
    return NORMAL_PRIORITY
  }

  _connected() {
    this.proven = true
    this.connectedTime = Date.now()
  }

  _disconnected() {
    this.disconnectedTime = Date.now()
    if (this.connectedTime > -1) {
      if (this.disconnectedTime - this.connectedTime >= MIN_CONNECTION_TIME) this.attempts = 0 // fast retry
      this.connectedTime = -1
    }
    this.attempts++
  }

  _deprioritize() {
    this.attempts = 3
  }

  _reset() {
    this.client = false
    this.proven = false
    this.tried = false
    this.attempts = 0
  }

  _updatePriority() {
    if (this.explicit && this.attempts > 3) this._deprioritize()
    if (this.banned || this.queued || this.attempts > 3) return false
    this.priority = this._getPriority()
    return true
  }

  _topic(topic) {
    const topicString = b4a.toString(topic, 'hex')
    if (this._seenTopics.has(topicString)) return
    this._seenTopics.add(topicString)
    this.topics.push(topic)
    this.emit('topic', topic)
  }

  reconnect(val) {
    this.reconnecting = !!val
  }

  ban(val) {
    this.banned = !!val
  }

  shouldGC() {
    return !(this.banned || this.queued || this.explicit || this.waiting)
  }
}
const BulkTimer = require('./bulk-timer')

const BACKOFF_JITTER = 500
const BACKOFF_S = 1000 + Math.round(BACKOFF_JITTER * Math.random())
const BACKOFF_M = 5000 + Math.round(2 * BACKOFF_JITTER * Math.random())
const BACKOFF_L = 15000 + Math.round(4 * BACKOFF_JITTER * Math.random())
const BACKOFF_X = 1000 * 60 * 10 + Math.round(240 * BACKOFF_JITTER * Math.random())

module.exports = class RetryTimer {
  constructor(
    push,
    { backoffs = [BACKOFF_S, BACKOFF_M, BACKOFF_L, BACKOFF_X], jitter = BACKOFF_JITTER } = {}
  ) {
    this.jitter = jitter
    this.backoffs = backoffs

    this._sTimer = new BulkTimer(backoffs[0] + Math.round(jitter * Math.random()), push)
    this._mTimer = new BulkTimer(backoffs[1] + Math.round(jitter * Math.random()), push)
    this._lTimer = new BulkTimer(backoffs[2] + Math.round(jitter * Math.random()), push)
    this._xTimer = new BulkTimer(backoffs[3] + Math.round(jitter * Math.random()), push)
  }

  _selectRetryTimer(peerInfo) {
    if (peerInfo.banned || !peerInfo.reconnecting) return null

    if (peerInfo.attempts > 3) {
      return peerInfo.explicit ? this._xTimer : null
    }

    if (peerInfo.attempts === 0) return this._sTimer
    if (peerInfo.proven) {
      switch (peerInfo.attempts) {
        case 1:
          return this._sTimer
        case 2:
          return this._mTimer
        case 3:
          return this._lTimer
      }
    } else {
      switch (peerInfo.attempts) {
        case 1:
          return this._mTimer
        case 2:
          return this._lTimer
        case 3:
          return this._lTimer
      }
    }

    return null
  }

  add(peerInfo) {
    const timer = this._selectRetryTimer(peerInfo)
    if (!timer) return false

    timer.add(peerInfo)
    return true
  }

  destroy() {
    this._sTimer.destroy()
    this._mTimer.destroy()
    this._lTimer.destroy()
    this._xTimer.destroy()
  }
}
{
  "name": "hyperswarm",
  "version": "4.16.0",
  "description": "A distributed networking stack for connecting peers",
  "files": [
    "index.js",
    "lib/**.js"
  ],
  "imports": {
    "events": {
      "bare": "bare-events",
      "default": "events"
    }
  },
  "dependencies": {
    "b4a": "^1.3.1",
    "bare-events": "^2.2.0",
    "hyperdht": "^6.21.0",
    "safety-catch": "^1.0.2",
    "shuffled-priority-queue": "^2.1.0",
    "streamx": "^2.22.1",
    "unslab": "^1.3.0"
  },
  "devDependencies": {
    "brittle": "^3.0.2",
    "hypercore-crypto": "^3.4.0",
    "prettier": "^3.6.2",
    "prettier-config-holepunch": "^2.0.0"
  },
  "scripts": {
    "format": "prettier --write .",
    "lint": "prettier --check .",
    "test": "node test/all.js",
    "test:generate": "brittle -r test/all.js test/*.js"
  },
  "repository": {
    "type": "git",
    "url": "https://github.com/holepunchto/hyperswarm.git"
  },
  "author": "Mathias Buus (@mafintosh)",
  "contributors": [
    "David Mark Clements (@davidmarkclem)",
    "Andrew Osheroff (@andrewosh)"
  ],
  "license": "MIT",
  "bugs": {
    "url": "https://github.com/holepunchto/hyperswarm/issues"
  },
  "homepage": "https://github.com/holepunchto/hyperswarm"
}
const b4a = require('b4a')

const EMPTY = b4a.alloc(0)
const MAX = b4a.from([0xff])
const BUFFER = {}

BUFFER.preencode = function (state, buf) {
  if (buf === null) buf = EMPTY

  let i = 0
  let extra = 3

  while ((i = b4a.indexOf(buf, 0x00, i)) > -1) {
    i++
    extra++
  }

  state.end += buf.byteLength + extra
}

BUFFER.encode = function (state, buf) {
  if (buf === null) buf = EMPTY

  state.buffer[state.start++] = 0x00

  let prev = 0
  let i = 0

  while ((i = b4a.indexOf(buf, 0x00, i)) > -1) {
    const slice = buf.subarray(prev, ++i)

    state.buffer.set(slice, state.start)
    state.start += slice.byteLength
    state.buffer[state.start++] = 0x02
    prev = i
  }

  const slice = buf.subarray(prev)

  state.buffer.set(slice, state.start)
  state.start += slice.byteLength

  state.buffer[state.start++] = 0x00
  state.buffer[state.start++] = 0x01
}

BUFFER.decode = function (state) {
  if (state.start >= state.end) throw new Error('Out of bounds')
  if (state.buffer[state.start++] !== 0x00) throw new Error('Invalid start of string')

  let escaped = null

  let prev = state.start
  let i = state.start

  while ((i = b4a.indexOf(state.buffer, 0x00, i)) > -1) {
    const next = ++i < state.end ? state.buffer[i] : 0x00

    i++

    if (next === 0x01) {
      break
    }

    if (next === 0x02) {
      if (escaped === null) escaped = []
      escaped.push(state.buffer.subarray(prev, i - 1))
      prev = i
      continue
    }

    throw new Error('Unknown value in terminator')
  }

  if (i === -1) {
    throw new Error('No terminator found')
  }

  state.start = i
  const last = state.buffer.subarray(prev, i - 2)
  if (escaped === null) return last

  escaped.push(last)
  return b4a.concat(escaped)
}

// TODO: can be optimised a lot

const STRING = {}

STRING.preencode = function (state, str) {
  BUFFER.preencode(state, b4a.from(str || ''))
}

STRING.encode = function (state, str) {
  BUFFER.encode(state, b4a.from(str || ''))
}

STRING.decode = function (state, str) {
  return b4a.toString(BUFFER.decode(state))
}

const UINT = {}

UINT.preencode = function (state, n) {
  state.end += n <= 0xfb ? 1 : n <= 0xffff ? 3 : n <= 0xffffffff ? 5 : (n === Infinity ? 1 : 9)
}

UINT.encode = function (state, n) {
  if (n === Infinity) {
    state.buffer[state.start++] = 0xff
    return
  }

  if (n <= 0xfb) {
    state.buffer[state.start++] = n
    return
  }

  if (n <= 0xffff) {
    state.buffer[state.start++] = 0xfc
    state.buffer[state.start++] = n >>> 8
    state.buffer[state.start++] = n
    return
  }

  if (n <= 0xffffffff) {
    state.buffer[state.start++] = 0xfd
    encodeUint32(state, n)
    return
  }

  if (Number.isSafeInteger(n)) {
    state.buffer[state.start++] = 0xfe

    const r = Math.floor(n / 0x100000000)
    encodeUint32(state, r)
    encodeUint32(state, n)
    return
  }

  throw new Error('Invalid number ' + n)
}

UINT.decode = function (state) {
  if (state.start >= state.end) throw new Error('Out of bounds')

  const a = state.buffer[state.start++]

  if (a <= 0xfb) return a

  if (a === 0xfc) {
    if (state.end - state.start < 2) throw new Error('Out of bounds')
    return (
      state.buffer[state.start++] * 0x100 +
      state.buffer[state.start++]
    )
  }

  if (a === 0xfd) {
    return decodeUint32(state)
  }

  if (a === 0xfe) {
    return (
      decodeUint32(state) * 0x100000000 +
      decodeUint32(state)
    )
  }

  return Infinity
}

const BOOL = {}

BOOL.preencode = (state, b) => UINT.preencode(state, b ? 1 : 0)
BOOL.encode = (state, b) => UINT.encode(state, b ? 1 : 0)
BOOL.decode = (state, b) => !!UINT.decode(state)

module.exports = class IndexEncoder {
  constructor (encodings, { prefix = -1 } = {}) {
    this.encodings = encodings
    this.prefix = prefix
  }

  static BUFFER = BUFFER
  static STRING = STRING
  static UINT = UINT
  static BOOL = BOOL

  static lookup (c) {
    switch (c) {
      case 'uint': return UINT
      case 'uint8': return UINT
      case 'uint16': return UINT
      case 'uint24': return UINT
      case 'uint32': return UINT
      case 'uint40': return UINT
      case 'uint48': return UINT
      case 'uint56': return UINT
      case 'uint64': return UINT
      case 'string': return STRING
      case 'utf8': return STRING
      case 'ascii': return STRING
      case 'hex': return STRING
      case 'base64': return STRING
      case 'fixed32': return BUFFER
      case 'fixed64': return BUFFER
      case 'buffer': return BUFFER
      case 'bool': return BOOL
    }

    throw new Error('Unknown type')
  }

  encode (keys) {
    return this._encode(keys, false)
  }

  _encode (keys, terminate) {
    if (b4a.isBuffer(keys)) return keys

    const state = { start: 0, end: 0, buffer: null }

    if (this.prefix !== -1) UINT.preencode(state, this.prefix)
    for (let i = 0; i < keys.length; i++) {
      this.encodings[i].preencode(state, keys[i])
    }

    if (terminate && keys.length < this.encodings.length) {
      state.end++
    }

    state.buffer = b4a.allocUnsafe(state.end)

    if (this.prefix !== -1) UINT.encode(state, this.prefix)
    for (let i = 0; i < keys.length; i++) {
      this.encodings[i].encode(state, keys[i])
    }

    if (terminate && keys.length < this.encodings.length) {
      state.buffer[state.start++] = MAX[0]
    }

    return state.buffer
  }

  decode (buffer) {
    const state = { start: 0, end: buffer.byteLength, buffer }
    const result = []

    if (this.prefix !== -1) UINT.decode(state)
    for (const enc of this.encodings) {
      const key = state.start < state.end ? enc.decode(state) : (enc === UINT ? 0 : null)
      result.push(key)
    }

    return result
  }

  encodeRange ({ gt, gte, lt, lte }) {
    const range = {
      gt: gt && this._encode(gt, true),
      gte: gte && this._encode(gte, false),
      lt: lt && this._encode(lt, false),
      lte: lte && this._encode(lte, true)
    }

    if (this.prefix !== -1) {
      if (!gt && !gte) range.gte = encodeUint(this.prefix)
      if (!lt && !lte) range.lt = encodeUint(this.prefix + 1)
    }

    return range
  }
}

function encodeUint (n) {
  const state = { start: 0, end: 0, buffer: null }
  UINT.preencode(state, n)
  state.buffer = b4a.allocUnsafe(state.end)
  UINT.encode(state, n)
  return state.buffer
}

function encodeUint32 (state, n) {
  state.buffer[state.start++] = n >>> 24
  state.buffer[state.start++] = n >>> 16
  state.buffer[state.start++] = n >>> 8
  state.buffer[state.start++] = n
}

function decodeUint32 (state, n) {
  if (state.end - state.start < 4) throw new Error('Out of bounds')
  return (
    state.buffer[state.start++] * 0x1000000 +
    state.buffer[state.start++] * 0x10000 +
    state.buffer[state.start++] * 0x100 +
    state.buffer[state.start++]
  )
}
{
  "name": "index-encoder",
  "version": "3.4.0",
  "description": "Encode multiple values into sorted keys",
  "main": "index.js",
  "dependencies": {
    "b4a": "^1.6.4"
  },
  "devDependencies": {
    "brittle": "^3.2.1",
    "hyperbee": "^2.10.5",
    "hypercore": "^10.9.2",
    "random-access-memory": "^6.2.0",
    "standard": "^17.0.0"
  },
  "scripts": {
    "test": "standard && brittle test.js"
  },
  "repository": {
    "type": "git",
    "url": "https://github.com/holepunchto/index-encoder.git"
  },
  "author": "Mathias Buus (@mafintosh)",
  "license": "Apache-2.0",
  "bugs": {
    "url": "https://github.com/holepunchto/index-encoder/issues"
  },
  "homepage": "https://github.com/holepunchto/index-encoder"
}
const b4a = require('b4a')

module.exports = function isOptions (opts) {
  return typeof opts === 'object' && opts && !b4a.isBuffer(opts)
}
{
  "name": "is-options",
  "version": "1.0.2",
  "description": "Easily check if input is an options map",
  "main": "index.js",
  "dependencies": {
    "b4a": "^1.1.1"
  },
  "devDependencies": {
    "standard": "^11.0.1",
    "tape": "^4.9.0"
  },
  "scripts": {
    "test": "standard && tape test.js"
  },
  "repository": {
    "type": "git",
    "url": "https://github.com/mafintosh/is-options.git"
  },
  "author": "Mathias Buus (@mafintosh)",
  "license": "MIT",
  "bugs": {
    "url": "https://github.com/mafintosh/is-options/issues"
  },
  "homepage": "https://github.com/mafintosh/is-options"
}
const { EventEmitter } = require('events')

module.exports = class RoutingTable extends EventEmitter {
  constructor (id, opts) {
    if (!opts) opts = {}

    super()

    this.id = id
    this.k = opts.k || 20
    this.size = 0
    this.rows = new Array(id.length * 8)
  }

  add (node) {
    const i = this._diff(node.id)

    let row = this.rows[i]

    if (!row) {
      row = this.rows[i] = new Row(this, i)
      this.emit('row', row)
    }

    const len = row.nodes.length
    if (!row.add(node, this.k)) return false

    this.size += row.nodes.length - len
    return true
  }

  remove (id) {
    const i = this._diff(id)
    const row = this.rows[i]
    if (!row) return false
    if (!row.remove(id)) return false
    this.size--
    return true
  }

  get (id) {
    const i = this._diff(id)
    const row = this.rows[i]
    if (!row) return null
    return row.get(id)
  }

  has (id) {
    return this.get(id) !== null
  }

  random () {
    let n = (Math.random() * this.size) | 0

    for (let i = 0; i < this.rows.length; i++) {
      const r = this.rows[i]
      if (!r) continue
      if (n < r.nodes.length) return r.nodes[n]
      n -= r.nodes.length
    }

    return null
  }

  closest (id, k) {
    if (!k) k = this.k

    const result = []
    const d = this._diff(id)

    // push close nodes
    for (let i = d; i >= 0 && result.length < k; i--) this._pushNodes(i, k, result)

    // if we don't have enough close nodes, populate from other rows, re the paper
    for (let i = d + 1; i < this.rows.length && result.length < k; i++) this._pushNodes(i, k, result)

    return result
  }

  _pushNodes (i, k, result) {
    const row = this.rows[i]
    if (!row) return

    const missing = Math.min(k - result.length, row.nodes.length)
    for (let j = 0; j < missing; j++) result.push(row.nodes[j])
  }

  toArray () {
    return this.closest(this.id, Infinity)
  }

  _diff (id) {
    for (let i = 0; i < id.length; i++) {
      const a = id[i]
      const b = this.id[i]

      if (a !== b) return i * 8 + Math.clz32(a ^ b) - 24
    }

    return this.rows.length - 1
  }
}

class Row extends EventEmitter {
  constructor (table, index) {
    super()

    this.data = null // can be used be upstream for whatevs
    this.byteOffset = index >> 3
    this.index = index
    this.table = table
    this.nodes = []
  }

  add (node) {
    const id = node.id

    let l = 0
    let r = this.nodes.length - 1

    while (l <= r) {
      const m = (l + r) >> 1
      const c = this.compare(id, this.nodes[m].id)

      if (c === 0) {
        this.nodes[m] = node
        return true
      }

      if (c < 0) r = m - 1
      else l = m + 1
    }

    if (this.nodes.length >= this.table.k) {
      this.emit('full', node)
      return false
    }

    this.insert(l, node)
    return true
  }

  remove (id) {
    let l = 0
    let r = this.nodes.length - 1

    while (l <= r) {
      const m = (l + r) >> 1
      const c = this.compare(id, this.nodes[m].id)

      if (c === 0) {
        this.splice(m)
        return true
      }

      if (c < 0) r = m - 1
      else l = m + 1
    }

    return false
  }

  get (id) {
    let l = 0
    let r = this.nodes.length - 1

    while (l <= r) {
      const m = (l + r) >> 1
      const node = this.nodes[m]
      const c = this.compare(id, node.id)

      if (c === 0) return node
      if (c < 0) r = m - 1
      else l = m + 1
    }

    return null
  }

  insert (i, node) {
    this.nodes.push(node) // push node or null or whatevs, just trying to not be polymorphic
    for (let j = this.nodes.length - 1; j > i; j--) this.nodes[j] = this.nodes[j - 1]
    this.nodes[i] = node
    this.emit('add', node)
  }

  splice (i) {
    for (; i < this.nodes.length - 1; i++) this.nodes[i] = this.nodes[i + 1]
    this.emit('remove', this.nodes.pop())
  }

  // very likely they diverge after a couple of bytes so a simple impl, like this is prop fastest vs Buffer.compare
  compare (a, b) {
    for (let i = this.byteOffset; i < a.length; i++) {
      const ai = a[i]
      const bi = b[i]
      if (ai === bi) continue
      return ai < bi ? -1 : 1
    }
    return 0
  }
}
{
  "name": "kademlia-routing-table",
  "version": "1.0.6",
  "description": "XOR based routing table used for P2P networks such as a Kademlia DHT.",
  "main": "index.js",
  "files": [
    "index.js"
  ],
  "imports": {
    "events": {
      "bare": "bare-events",
      "default": "events"
    }
  },
  "dependencies": {
    "bare-events": "^2.2.0"
  },
  "devDependencies": {
    "brittle": "^3.3.2",
    "standard": "^17.1.0"
  },
  "scripts": {
    "test": "standard && brittle test.js"
  },
  "keywords": [
    "kademlia",
    "p2p",
    "k-bucket",
    "k-buckets",
    "xor",
    "routing",
    "distributed",
    "systems"
  ],
  "repository": {
    "type": "git",
    "url": "https://github.com/mafintosh/kademlia-routing-table.git"
  },
  "author": "Mathias Buus (@mafintosh)",
  "license": "MIT",
  "bugs": {
    "url": "https://github.com/mafintosh/kademlia-routing-table/issues"
  },
  "homepage": "https://github.com/mafintosh/kademlia-routing-table"
}
var queueTick = require('queue-tick')

var mutexify = function () {
  var queue = []
  var used = null

  var call = function () {
    used(release)
  }

  var acquire = function (fn) {
    if (used) return queue.push(fn)
    used = fn
    acquire.locked = true
    queueTick(call)
    return 0
  }

  acquire.locked = false

  var release = function (fn, err, value) {
    used = null
    acquire.locked = false
    if (queue.length) acquire(queue.shift())
    if (fn) fn(err, value)
  }

  return acquire
}

module.exports = mutexify
{
  "name": "mutexify",
  "version": "1.4.0",
  "description": "mutex lock for javascript",
  "main": "index.js",
  "dependencies": {
    "queue-tick": "^1.0.0"
  },
  "devDependencies": {
    "standard": "^14.3.3",
    "tape": "^3.0.2"
  },
  "scripts": {
    "test": "tape test.js",
    "posttest": "npm run lint",
    "lint": "standard"
  },
  "repository": {
    "type": "git",
    "url": "https://github.com/mafintosh/mutexify.git"
  },
  "author": "Mathias Buus (@mafintosh)",
  "license": "MIT",
  "bugs": {
    "url": "https://github.com/mafintosh/mutexify/issues"
  },
  "keywords": [
    "mutex",
    "lock"
  ],
  "homepage": "https://github.com/mafintosh/mutexify"
}
var mutexify = require('.')

var mutexifyPromise = function () {
  var lock = mutexify()

  var acquire = function () {
    return new Promise(lock)
  }

  Object.defineProperty(acquire, 'locked', {
    get: function () { return lock.locked },
    enumerable: true
  })

  return acquire
}

module.exports = mutexifyPromise
module.exports = assert

class AssertionError extends Error {}
AssertionError.prototype.name = 'AssertionError'

/**
 * Minimal assert function
 * @param  {any} t Value to check if falsy
 * @param  {string=} m Optional assertion error message
 * @throws {AssertionError}
 */
function assert (t, m) {
  if (!t) {
    var err = new AssertionError(m)
    if (Error.captureStackTrace) Error.captureStackTrace(err, assert)
    throw err
  }
}
{
  "name": "nanoassert",
  "version": "2.0.0",
  "description": "Nanoscale assertion module",
  "main": "index.js",
  "dependencies": {},
  "devDependencies": {
    "tape": "^4.9.1"
  },
  "scripts": {
    "test": "tape test.js"
  },
  "repository": {
    "type": "git",
    "url": "git+https://github.com/emilbayes/nanoassert.git"
  },
  "keywords": [
    "assert",
    "unassert",
    "power-assert",
    "tiny",
    "nano",
    "pico"
  ],
  "author": "Emil Bay <github@tixz.dk>",
  "license": "ISC",
  "bugs": {
    "url": "https://github.com/emilbayes/nanoassert/issues"
  },
  "homepage": "https://github.com/emilbayes/nanoassert#readme"
}
module.exports = class NatSampler {
  constructor () {
    this.host = null
    this.port = 0
    this.size = 0

    this._a = null
    this._b = null
    this._threshold = 0
    this._top = 0
    this._samples = []
  }

  add (host, port) {
    const a = this._bump(host, port, 2)
    const b = this._bump(host, 0, 1)

    if (this._samples.length < 32) {
      this.size++
      this._threshold = this.size - (this.size < 4 ? 0 : this.size < 8 ? 1 : this.size < 12 ? 2 : 3)
      this._samples.push(a, b)
      this._top += 2
    } else {
      if (this._top === 32) this._top = 0

      const oa = this._samples[this._top]
      this._samples[this._top++] = a
      oa.hits--

      const ob = this._samples[this._top]
      this._samples[this._top++] = b
      ob.hits--
    }

    if (this._a === null || this._a.hits < a.hits) this._a = a
    if (this._b === null || this._b.hits < b.hits) this._b = b

    if (this._a.hits >= this._threshold) {
      this.host = this._a.host
      this.port = this._a.port
    } else if (this._b.hits >= this._threshold) {
      this.host = this._b.host
      this.port = 0
    } else {
      this.host = null
      this.port = 0
    }

    return a.hits
  }

  _bump (host, port, inc) {
    for (let i = 0; i < 4; i++) {
      const j = (this._top - inc - (2 * i)) & 31
      if (j >= this._samples.length) return { host, port, hits: 1 }
      const s = this._samples[j]
      if (s.port === port && s.host === host) {
        s.hits++
        return s
      }
    }
    return { host, port, hits: 1 }
  }
}
{
  "name": "nat-sampler",
  "version": "1.0.1",
  "description": "Sample addresses to figure out if a host + port is consistent",
  "main": "index.js",
  "dependencies": {},
  "devDependencies": {
    "standard": "^16.0.3",
    "tape": "^5.2.2"
  },
  "repository": {
    "type": "git",
    "url": "https://github.com/mafintosh/nat-sampler.git"
  },
  "scripts": {
    "test": "standard && tape test.js"
  },
  "author": "Mathias Buus (@mafintosh)",
  "license": "MIT",
  "bugs": {
    "url": "https://github.com/mafintosh/nat-sampler/issues"
  },
  "homepage": "https://github.com/mafintosh/nat-sampler"
}
/* eslint-disable camelcase */
const sodium = require('sodium-universal')
const assert = require('nanoassert')
const b4a = require('b4a')

const DHLEN = sodium.crypto_scalarmult_ed25519_BYTES
const PKLEN = sodium.crypto_scalarmult_ed25519_BYTES
const SCALARLEN = sodium.crypto_scalarmult_ed25519_BYTES
const SKLEN = sodium.crypto_sign_SECRETKEYBYTES
const ALG = 'Ed25519'

module.exports = {
  DHLEN,
  PKLEN,
  SCALARLEN,
  SKLEN,
  ALG,
  name: ALG,
  generateKeyPair,
  dh
}

function generateKeyPair (privKey) {
  if (privKey) return generateSeedKeyPair(privKey.subarray(0, 32))

  const keyPair = {}
  keyPair.secretKey = b4a.alloc(SKLEN)
  keyPair.publicKey = b4a.alloc(PKLEN)

  sodium.crypto_sign_keypair(keyPair.publicKey, keyPair.secretKey)
  return keyPair
}

function generateSeedKeyPair (seed) {
  const keyPair = {}
  keyPair.secretKey = b4a.alloc(SKLEN)
  keyPair.publicKey = b4a.alloc(PKLEN)

  sodium.crypto_sign_seed_keypair(keyPair.publicKey, keyPair.secretKey, seed)
  return keyPair
}

function dh (publicKey, { scalar, secretKey }) {
  // tweaked keys expose scalar directly
  if (!scalar) {
    assert(secretKey.byteLength === SKLEN)

    // libsodium stores seed not actual scalar
    const sk = b4a.alloc(64)
    sodium.crypto_hash_sha512(sk, secretKey.subarray(0, 32))
    sk[0] &= 248
    sk[31] &= 127
    sk[31] |= 64

    scalar = sk.subarray(0, 32)
  }

  assert(scalar.byteLength === SCALARLEN)
  assert(publicKey.byteLength === PKLEN)

  const output = b4a.alloc(DHLEN)

  // we clamp if necessary above
  sodium.crypto_scalarmult_ed25519_noclamp(
    output,
    scalar,
    publicKey
  )

  return output
}
{
  "name": "noise-curve-ed",
  "version": "2.1.0",
  "description": "Ed25519 elliptic curve operations for [`noise-handshake`](https://github.com/chm-diederichs/noise-handshake)",
  "main": "index.js",
  "scripts": {
    "test": "npx standard && tape test.js"
  },
  "repository": {
    "type": "git",
    "url": "git+https://github.com/chm-diederichs/noise-curve-ed.git"
  },
  "keywords": [],
  "author": "",
  "license": "ISC",
  "bugs": {
    "url": "https://github.com/chm-diederichs/noise-curve-ed/issues"
  },
  "homepage": "https://github.com/chm-diederichs/noise-curve-ed#readme",
  "dependencies": {
    "b4a": "^1.1.0",
    "nanoassert": "^2.0.0",
    "sodium-universal": "^5.0.0"
  },
  "devDependencies": {
    "noise-handshake": "^3.0.0",
    "standard": "^16.0.3",
    "tape": "^5.2.2"
  }
}
const sodium = require('sodium-universal')
const b4a = require('b4a')

module.exports = class CipherState {
  constructor (key) {
    this.key = key || null
    this.nonce = 0
    this.CIPHER_ALG = 'ChaChaPoly'
  }

  initialiseKey (key) {
    this.key = key
    this.nonce = 0
  }

  setNonce (nonce) {
    this.nonce = nonce
  }

  encrypt (plaintext, ad) {
    if (!this.hasKey) return plaintext
    if (!ad) ad = b4a.alloc(0)

    const ciphertext = encryptWithAD(this.key, this.nonce, ad, plaintext)
    if (ciphertext.length > 65535) throw new Error(`ciphertext length of ${ciphertext.length} exceeds maximum Noise message length of 65535`)
    this.nonce++

    return ciphertext
  }

  decrypt (ciphertext, ad) {
    if (!this.hasKey) return ciphertext
    if (!ad) ad = b4a.alloc(0)
    if (ciphertext.length > 65535) throw new Error(`ciphertext length of ${ciphertext.length} exceeds maximum Noise message length of 65535`)

    const plaintext = decryptWithAD(this.key, this.nonce, ad, ciphertext)
    this.nonce++

    return plaintext
  }

  get hasKey () {
    return this.key !== null
  }

  _clear () {
    sodium.sodium_memzero(this.key)
    this.key = null
    this.nonce = null
  }

  static get MACBYTES () {
    return 16
  }

  static get NONCEBYTES () {
    return 8
  }

  static get KEYBYTES () {
    return 32
  }
}

function encryptWithAD (key, counter, additionalData, plaintext) {
  // for our purposes, additionalData will always be a pubkey so we encode from hex
  if (!b4a.isBuffer(additionalData)) additionalData = b4a.from(additionalData, 'hex')
  if (!b4a.isBuffer(plaintext)) plaintext = b4a.from(plaintext, 'hex')

  const nonce = b4a.alloc(sodium.crypto_aead_chacha20poly1305_ietf_NPUBBYTES)
  const view = new DataView(nonce.buffer, nonce.byteOffset, nonce.byteLength)
  view.setUint32(4, counter, true)

  const ciphertext = b4a.alloc(plaintext.byteLength + sodium.crypto_aead_chacha20poly1305_ietf_ABYTES)

  sodium.crypto_aead_chacha20poly1305_ietf_encrypt(ciphertext, plaintext, additionalData, null, nonce, key)
  return ciphertext
}

function decryptWithAD (key, counter, additionalData, ciphertext) {
  // for our purposes, additionalData will always be a pubkey so we encode from hex
  if (!b4a.isBuffer(additionalData)) additionalData = b4a.from(additionalData, 'hex')
  if (!b4a.isBuffer(ciphertext)) ciphertext = b4a.from(ciphertext, 'hex')

  const nonce = b4a.alloc(sodium.crypto_aead_chacha20poly1305_ietf_NPUBBYTES)
  const view = new DataView(nonce.buffer, nonce.byteOffset, nonce.byteLength)
  view.setUint32(4, counter, true)

  const plaintext = b4a.alloc(ciphertext.byteLength - sodium.crypto_aead_chacha20poly1305_ietf_ABYTES)

  sodium.crypto_aead_chacha20poly1305_ietf_decrypt(plaintext, null, ciphertext, additionalData, nonce, key)
  return plaintext
}
/* eslint-disable camelcase */
const {
  crypto_kx_SEEDBYTES,
  crypto_kx_keypair,
  crypto_kx_seed_keypair,
  crypto_scalarmult_BYTES,
  crypto_scalarmult_SCALARBYTES,
  crypto_scalarmult,
  crypto_scalarmult_base
} = require('sodium-universal')

const assert = require('nanoassert')
const b4a = require('b4a')

const DHLEN = crypto_scalarmult_BYTES
const PKLEN = crypto_scalarmult_BYTES
const SKLEN = crypto_scalarmult_SCALARBYTES
const SEEDLEN = crypto_kx_SEEDBYTES
const ALG = '25519'

module.exports = {
  DHLEN,
  PKLEN,
  SKLEN,
  SEEDLEN,
  ALG,
  generateKeyPair,
  generateSeedKeyPair,
  dh
}

function generateKeyPair (privKey) {
  const keyPair = {}

  keyPair.secretKey = privKey || b4a.alloc(SKLEN)
  keyPair.publicKey = b4a.alloc(PKLEN)

  if (privKey) {
    crypto_scalarmult_base(keyPair.publicKey, keyPair.secretKey)
  } else {
    crypto_kx_keypair(keyPair.publicKey, keyPair.secretKey)
  }

  return keyPair
}

function generateSeedKeyPair (seed) {
  assert(seed.byteLength === SKLEN)

  const keyPair = {}
  keyPair.secretKey = b4a.alloc(SKLEN)
  keyPair.publicKey = b4a.alloc(PKLEN)

  crypto_kx_seed_keypair(keyPair.publicKey, keyPair.secretKey, seed)
  return keyPair
}

function dh (publicKey, { secretKey }) {
  assert(secretKey.byteLength === SKLEN)
  assert(publicKey.byteLength === PKLEN)

  const output = b4a.alloc(DHLEN)

  crypto_scalarmult(
    output,
    secretKey,
    publicKey
  )

  return output
}
const hmacBlake2b = require('./hmac')
const b4a = require('b4a')

const HASHLEN = 64

module.exports = {
  hkdf,
  HASHLEN
}

// HMAC-based Extract-and-Expand KDF
// https://www.ietf.org/rfc/rfc5869.txt

function hkdf (salt, inputKeyMaterial, info = '', length = 2 * HASHLEN) {
  const pseudoRandomKey = hkdfExtract(salt, inputKeyMaterial)
  return hkdfExpand(pseudoRandomKey, info, length)
}

function hkdfExtract (salt, inputKeyMaterial) {
  const hmac = b4a.alloc(HASHLEN)
  return hmacDigest(hmac, salt, inputKeyMaterial)
}

function hkdfExpand (key, info, length) {
  // Put in dedicated slab to avoid keeping shared slab from being gc'ed
  const buffer = b4a.allocUnsafeSlow(length)

  const infoBuf = b4a.from(info)
  let prev = infoBuf

  const result = []
  for (let i = 0; i < length; i += HASHLEN) {
    const pos = b4a.from([(i / HASHLEN) + 1])

    const out = buffer.subarray(i, i + HASHLEN)
    result.push(out)

    prev = hmacDigest(out, key, [prev, infoBuf, pos])
  }

  return result
}

function hmacDigest (out, key, input) {
  hmacBlake2b(out, input, key)
  return out
}
/* eslint-disable camelcase */
const b4a = require('b4a')
const { sodium_memzero, crypto_generichash, crypto_generichash_batch } = require('sodium-universal')

const HASHLEN = 64
const BLOCKLEN = 128
const scratch = b4a.alloc(BLOCKLEN * 3)
const HMACKey = scratch.subarray(BLOCKLEN * 0, BLOCKLEN * 1)
const OuterKeyPad = scratch.subarray(BLOCKLEN * 1, BLOCKLEN * 2)
const InnerKeyPad = scratch.subarray(BLOCKLEN * 2, BLOCKLEN * 3)

// Post-fill is done in the cases where someone caught an exception that
// happened before we were able to clear data at the end

module.exports = function hmac (out, batch, key) {
  if (key.byteLength > BLOCKLEN) {
    crypto_generichash(HMACKey.subarray(0, HASHLEN), key)
    sodium_memzero(HMACKey.subarray(HASHLEN))
  } else {
    // Covers key <= BLOCKLEN
    HMACKey.set(key)
    sodium_memzero(HMACKey.subarray(key.byteLength))
  }

  for (let i = 0; i < HMACKey.byteLength; i++) {
    OuterKeyPad[i] = 0x5c ^ HMACKey[i]
    InnerKeyPad[i] = 0x36 ^ HMACKey[i]
  }
  sodium_memzero(HMACKey)

  crypto_generichash_batch(out, [InnerKeyPad].concat(batch))
  sodium_memzero(InnerKeyPad)
  crypto_generichash_batch(out, [OuterKeyPad, out])
  sodium_memzero(OuterKeyPad)
}

module.exports.BYTES = HASHLEN
module.exports.KEYBYTES = BLOCKLEN
const assert = require('nanoassert')
const b4a = require('b4a')

const SymmetricState = require('./symmetric-state')
const { HASHLEN } = require('./hkdf')

const PRESHARE_IS = Symbol('initiator static key preshared')
const PRESHARE_RS = Symbol('responder static key preshared')

const TOK_PSK = Symbol('psk')

const TOK_S = Symbol('s')
const TOK_E = Symbol('e')

const TOK_ES = Symbol('es')
const TOK_SE = Symbol('se')
const TOK_EE = Symbol('ee')
const TOK_SS = Symbol('ss')

const HANDSHAKES = Object.freeze({
  NN: [
    [TOK_E],
    [TOK_E, TOK_EE]
  ],
  NNpsk0: [
    [TOK_PSK, TOK_E],
    [TOK_E, TOK_EE]
  ],
  XX: [
    [TOK_E],
    [TOK_E, TOK_EE, TOK_S, TOK_ES],
    [TOK_S, TOK_SE]
  ],
  XXpsk0: [
    [TOK_PSK, TOK_E],
    [TOK_E, TOK_EE, TOK_S, TOK_ES],
    [TOK_S, TOK_SE]
  ],
  IK: [
    PRESHARE_RS,
    [TOK_E, TOK_ES, TOK_S, TOK_SS],
    [TOK_E, TOK_EE, TOK_SE]
  ],
  XK: [
    PRESHARE_RS,
    [TOK_E, TOK_ES],
    [TOK_E, TOK_EE],
    [TOK_S, TOK_SE]
  ]
})

class Writer {
  constructor () {
    this.size = 0
    this.buffers = []
  }

  push (b) {
    this.size += b.byteLength
    this.buffers.push(b)
  }

  end () {
    const all = b4a.alloc(this.size)
    let offset = 0
    for (const b of this.buffers) {
      all.set(b, offset)
      offset += b.byteLength
    }
    return all
  }
}

class Reader {
  constructor (buf) {
    this.offset = 0
    this.buffer = buf
  }

  shift (n) {
    const start = this.offset
    const end = this.offset += n
    if (end > this.buffer.byteLength) throw new Error('Insufficient bytes')
    return this.buffer.subarray(start, end)
  }

  end () {
    return this.shift(this.buffer.byteLength - this.offset)
  }
}

module.exports = class NoiseState extends SymmetricState {
  constructor (pattern, initiator, staticKeypair, opts = {}) {
    super(opts)

    this.s = staticKeypair || this.curve.generateKeyPair()
    this.e = null

    this.psk = null
    if (opts && opts.psk) this.psk = opts.psk

    this.re = null
    this.rs = null

    this.pattern = pattern
    this.handshake = HANDSHAKES[this.pattern].slice()

    this.isPskHandshake = !!this.psk && hasPskToken(this.handshake)

    this.protocol = b4a.from([
      'Noise',
      this.pattern,
      this.DH_ALG,
      this.CIPHER_ALG,
      'BLAKE2b'
    ].join('_'))

    this.initiator = initiator
    this.complete = false

    this.rx = null
    this.tx = null
    this.hash = null
  }

  initialise (prologue, remoteStatic) {
    if (this.protocol.byteLength <= HASHLEN) this.digest.set(this.protocol)
    else this.mixHash(this.protocol)

    this.chainingKey = b4a.from(this.digest)

    this.mixHash(prologue)

    while (!Array.isArray(this.handshake[0])) {
      const message = this.handshake.shift()

      // handshake steps should be as arrays, only
      // preshare tokens are provided otherwise
      assert(message === PRESHARE_RS || message === PRESHARE_IS,
        'Unexpected pattern')

      const takeRemoteKey = this.initiator
        ? message === PRESHARE_RS
        : message === PRESHARE_IS

      if (takeRemoteKey) this.rs = remoteStatic

      const key = takeRemoteKey ? this.rs : this.s.publicKey
      assert(key != null, 'Remote pubkey required')

      this.mixHash(key)
    }
  }

  final () {
    const [k1, k2] = this.split()

    this.tx = this.initiator ? k1 : k2
    this.rx = this.initiator ? k2 : k1

    this.complete = true
    this.hash = this.getHandshakeHash()

    this._clear()
  }

  recv (buf) {
    const r = new Reader(buf)

    for (const pattern of this.handshake.shift()) {
      switch (pattern) {
        case TOK_PSK :
          this.mixKeyAndHash(this.psk)
          break

        case TOK_E :
          this.re = r.shift(this.curve.PKLEN)
          this.mixHash(this.re)
          if (this.isPskHandshake) this.mixKeyNormal(this.re)
          break

        case TOK_S : {
          const klen = this.hasKey ? this.curve.PKLEN + 16 : this.curve.PKLEN
          this.rs = this.decryptAndHash(r.shift(klen))
          break
        }

        case TOK_EE :
        case TOK_ES :
        case TOK_SE :
        case TOK_SS : {
          const useStatic = keyPattern(pattern, this.initiator)

          const localKey = useStatic.local ? this.s : this.e
          const remoteKey = useStatic.remote ? this.rs : this.re

          this.mixKey(remoteKey, localKey)
          break
        }

        default :
          throw new Error('Unexpected message')
      }
    }

    const payload = this.decryptAndHash(r.end())

    if (!this.handshake.length) this.final()
    return payload
  }

  send (payload = b4a.alloc(0)) {
    const w = new Writer()

    for (const pattern of this.handshake.shift()) {
      switch (pattern) {
        case TOK_PSK :
          this.mixKeyAndHash(this.psk)
          break

        case TOK_E :
          if (this.e === null) this.e = this.curve.generateKeyPair()
          this.mixHash(this.e.publicKey)
          if (this.isPskHandshake) this.mixKeyNormal(this.e.publicKey)
          w.push(this.e.publicKey)
          break

        case TOK_S :
          w.push(this.encryptAndHash(this.s.publicKey))
          break

        case TOK_ES :
        case TOK_SE :
        case TOK_EE :
        case TOK_SS : {
          const useStatic = keyPattern(pattern, this.initiator)

          const localKey = useStatic.local ? this.s : this.e
          const remoteKey = useStatic.remote ? this.rs : this.re

          this.mixKey(remoteKey, localKey)
          break
        }

        default :
          throw new Error('Unexpected message')
      }
    }

    w.push(this.encryptAndHash(payload))
    const response = w.end()

    if (!this.handshake.length) this.final()
    return response
  }

  _clear () {
    super._clear()

    this.e.secretKey.fill(0)
    this.e.publicKey.fill(0)

    this.re.fill(0)

    this.e = null
    this.re = null
  }
}

function keyPattern (pattern, initiator) {
  const ret = {
    local: false,
    remote: false
  }

  switch (pattern) {
    case TOK_EE:
      return ret

    case TOK_ES:
      ret.local ^= !initiator
      ret.remote ^= initiator
      return ret

    case TOK_SE:
      ret.local ^= initiator
      ret.remote ^= !initiator
      return ret

    case TOK_SS:
      ret.local ^= 1
      ret.remote ^= 1
      return ret
  }
}

function hasPskToken (handshake) {
  return handshake.some(x => {
    return Array.isArray(x) && x.indexOf(TOK_PSK) !== -1
  })
}
{
  "name": "noise-handshake",
  "version": "4.2.0",
  "description": "Noise protocol handshake",
  "main": "noise.js",
  "files": [
    "*.js"
  ],
  "scripts": {
    "test": "standard && brittle test/*.js"
  },
  "author": "Holepunch",
  "license": "Apache-2.0",
  "repository": {
    "type": "git",
    "url": "git+https://github.com/holepunchto/noise-handshake.git"
  },
  "dependencies": {
    "b4a": "^1.1.0",
    "nanoassert": "^2.0.0",
    "sodium-universal": "^5.0.0"
  },
  "devDependencies": {
    "brittle": "^3.3.2",
    "noise-protocol": "chm-diederichs/noise-protocol.git#xx-ephemeral-key",
    "standard": "^16.0.3"
  }
}
const sodium = require('sodium-universal')
const assert = require('nanoassert')
const b4a = require('b4a')
const CipherState = require('./cipher')
const curve = require('./dh')
const { HASHLEN, hkdf } = require('./hkdf')

module.exports = class SymmetricState extends CipherState {
  constructor (opts = {}) {
    super()

    this.curve = opts.curve || curve
    this.digest = b4a.alloc(HASHLEN)
    this.chainingKey = null
    this.offset = 0

    this.DH_ALG = this.curve.ALG
  }

  mixHash (data) {
    accumulateDigest(this.digest, data)
  }

  mixKeyAndHash (key) {
    const [ck, tempH, tempK] = hkdf(this.chainingKey, key, '', 3 * HASHLEN)
    this.chainingKey = ck
    this.mixHash(tempH)
    this.initialiseKey(tempK.subarray(0, 32))
  }

  mixKeyNormal (key) {
    const [ck, tempK] = hkdf(this.chainingKey, key)
    this.chainingKey = ck
    this.initialiseKey(tempK.subarray(0, 32))
  }

  mixKey (remoteKey, localKey) {
    const dh = this.curve.dh(remoteKey, localKey)
    const hkdfResult = hkdf(this.chainingKey, dh)
    this.chainingKey = hkdfResult[0]
    this.initialiseKey(hkdfResult[1].subarray(0, 32))
  }

  encryptAndHash (plaintext) {
    const ciphertext = this.encrypt(plaintext, this.digest)
    accumulateDigest(this.digest, ciphertext)
    return ciphertext
  }

  decryptAndHash (ciphertext) {
    const plaintext = this.decrypt(ciphertext, this.digest)
    accumulateDigest(this.digest, ciphertext)
    return plaintext
  }

  getHandshakeHash (out) {
    if (!out) return this.getHandshakeHash(b4a.alloc(HASHLEN))
    assert(out.byteLength === HASHLEN, `output must be ${HASHLEN} bytes`)

    out.set(this.digest)
    return out
  }

  split () {
    const res = hkdf(this.chainingKey, b4a.alloc(0))
    return res.map(k => k.subarray(0, 32))
  }

  _clear () {
    super._clear()

    sodium.sodium_memzero(this.digest)
    sodium.sodium_memzero(this.chainingKey)

    this.digest = null
    this.chainingKey = null
    this.offset = null

    this.curve = null
  }

  static get alg () {
    return CipherState.alg + '_BLAKE2b'
  }
}

function accumulateDigest (digest, input) {
  const toHash = b4a.concat([digest, input])
  sodium.crypto_generichash(digest, toHash)
}
var varint = require('varint')
var svarint = require('signed-varint')
var b4a = require('b4a')

exports.make = encoder

exports.name = function (enc) {
  var keys = Object.keys(exports)
  for (var i = 0; i < keys.length; i++) {
    if (exports[keys[i]] === enc) return keys[i]
  }
  return null
}

exports.skip = function (type, buffer, offset) {
  switch (type) {
    case 0:
      varint.decode(buffer, offset)
      return offset + varint.decode.bytes

    case 1:
      return offset + 8

    case 2:
      var len = varint.decode(buffer, offset)
      return offset + varint.decode.bytes + len

    case 3:
    case 4:
      throw new Error('Groups are not supported')

    case 5:
      return offset + 4
  }

  throw new Error('Unknown wire type: ' + type)
}

exports.bytes = encoder(2,
  function encode (val, buffer, offset) {
    var oldOffset = offset
    var len = bufferLength(val)

    varint.encode(len, buffer, offset)
    offset += varint.encode.bytes

    if (b4a.isBuffer(val)) b4a.copy(val, buffer, offset)
    else b4a.write(buffer, val, offset, len)
    offset += len

    encode.bytes = offset - oldOffset
    return buffer
  },
  function decode (buffer, offset) {
    var oldOffset = offset

    var len = varint.decode(buffer, offset)
    offset += varint.decode.bytes

    var val = buffer.subarray(offset, offset + len)
    offset += val.length

    decode.bytes = offset - oldOffset
    return val
  },
  function encodingLength (val) {
    var len = bufferLength(val)
    return varint.encodingLength(len) + len
  }
)

exports.string = encoder(2,
  function encode (val, buffer, offset) {
    var oldOffset = offset
    var len = b4a.byteLength(val)

    varint.encode(len, buffer, offset, 'utf-8')
    offset += varint.encode.bytes

    b4a.write(buffer, val, offset, len)
    offset += len

    encode.bytes = offset - oldOffset
    return buffer
  },
  function decode (buffer, offset) {
    var oldOffset = offset

    var len = varint.decode(buffer, offset)
    offset += varint.decode.bytes

    var val = b4a.toString(buffer, 'utf-8', offset, offset + len)
    offset += len

    decode.bytes = offset - oldOffset
    return val
  },
  function encodingLength (val) {
    var len = b4a.byteLength(val)
    return varint.encodingLength(len) + len
  }
)

exports.bool = encoder(0,
  function encode (val, buffer, offset) {
    buffer[offset] = val ? 1 : 0
    encode.bytes = 1
    return buffer
  },
  function decode (buffer, offset) {
    var bool = buffer[offset] > 0
    decode.bytes = 1
    return bool
  },
  function encodingLength () {
    return 1
  }
)

exports.int32 = encoder(0,
  function encode (val, buffer, offset) {
    varint.encode(val < 0 ? val + 4294967296 : val, buffer, offset)
    encode.bytes = varint.encode.bytes
    return buffer
  },
  function decode (buffer, offset) {
    var val = varint.decode(buffer, offset)
    decode.bytes = varint.decode.bytes
    return val > 2147483647 ? val - 4294967296 : val
  },
  function encodingLength (val) {
    return varint.encodingLength(val < 0 ? val + 4294967296 : val)
  }
)

exports.int64 = encoder(0,
  function encode (val, buffer, offset) {
    if (val < 0) {
      var last = offset + 9
      varint.encode(val * -1, buffer, offset)
      offset += varint.encode.bytes - 1
      buffer[offset] = buffer[offset] | 0x80
      while (offset < last - 1) {
        offset++
        buffer[offset] = 0xff
      }
      buffer[last] = 0x01
      encode.bytes = 10
    } else {
      varint.encode(val, buffer, offset)
      encode.bytes = varint.encode.bytes
    }
    return buffer
  },
  function decode (buffer, offset) {
    var val = varint.decode(buffer, offset)
    if (val >= Math.pow(2, 63)) {
      var limit = 9
      while (buffer[offset + limit - 1] === 0xff) limit--
      limit = limit || 9
      var subset = b4a.allocUnsafe(limit)
      b4a.copy(buffer, subset, 0, offset, offset + limit)
      subset[limit - 1] = subset[limit - 1] & 0x7f
      val = -1 * varint.decode(subset, 0)
      decode.bytes = 10
    } else {
      decode.bytes = varint.decode.bytes
    }
    return val
  },
  function encodingLength (val) {
    return val < 0 ? 10 : varint.encodingLength(val)
  }
)

exports.sint32 =
exports.sint64 = encoder(0,
  svarint.encode,
  svarint.decode,
  svarint.encodingLength
)

exports.uint32 =
exports.uint64 =
exports.enum =
exports.varint = encoder(0,
  varint.encode,
  varint.decode,
  varint.encodingLength
)

// we cannot represent these in javascript so we just use buffers
exports.fixed64 =
exports.sfixed64 = encoder(1,
  function encode (val, buffer, offset) {
    b4a.copy(val, buffer, offset)
    encode.bytes = 8
    return buffer
  },
  function decode (buffer, offset) {
    var val = buffer.subarray(offset, offset + 8)
    decode.bytes = 8
    return val
  },
  function encodingLength () {
    return 8
  }
)

exports.double = encoder(1,
  function encode (val, buffer, offset) {
    b4a.writeDoubleLE(buffer, val, offset)
    encode.bytes = 8
    return buffer
  },
  function decode (buffer, offset) {
    var val = b4a.readDoubleLE(buffer, offset)
    decode.bytes = 8
    return val
  },
  function encodingLength () {
    return 8
  }
)

exports.fixed32 = encoder(5,
  function encode (val, buffer, offset) {
    b4a.writeUInt32LE(buffer, val, offset)
    encode.bytes = 4
    return buffer
  },
  function decode (buffer, offset) {
    var val = b4a.readUInt32LE(buffer, offset)
    decode.bytes = 4
    return val
  },
  function encodingLength () {
    return 4
  }
)

exports.sfixed32 = encoder(5,
  function encode (val, buffer, offset) {
    b4a.writeInt32LE(buffer, val, offset)
    encode.bytes = 4
    return buffer
  },
  function decode (buffer, offset) {
    var val = b4a.readInt32LE(buffer, offset)
    decode.bytes = 4
    return val
  },
  function encodingLength () {
    return 4
  }
)

exports.float = encoder(5,
  function encode (val, buffer, offset) {
    b4a.writeFloatLE(buffer, val, offset)
    encode.bytes = 4
    return buffer
  },
  function decode (buffer, offset) {
    var val = b4a.readFloatLE(buffer, offset)
    decode.bytes = 4
    return val
  },
  function encodingLength () {
    return 4
  }
)

function encoder (type, encode, decode, encodingLength) {
  encode.bytes = decode.bytes = 0

  return {
    type: type,
    encode: encode,
    decode: decode,
    encodingLength: encodingLength
  }
}

function bufferLength (val) {
  return b4a.isBuffer(val) ? val.length : b4a.byteLength(val)
}
{
  "name": "protocol-buffers-encodings",
  "version": "1.2.0",
  "description": "Base encodings for protocol-buffers",
  "main": "index.js",
  "dependencies": {
    "b4a": "^1.6.0",
    "signed-varint": "^2.0.1",
    "varint": "5.0.0"
  },
  "devDependencies": {
    "standard": "^14.3.4",
    "tape": "^5.0.1"
  },
  "scripts": {
    "test": "standard && tape test.js"
  },
  "repository": {
    "type": "git",
    "url": "https://github.com/mafintosh/protocol-buffers-encodings.git"
  },
  "author": "Mathias Buus (@mafintosh)",
  "license": "MIT",
  "bugs": {
    "url": "https://github.com/mafintosh/protocol-buffers-encodings/issues"
  },
  "homepage": "https://github.com/mafintosh/protocol-buffers-encodings"
}
const crypto = require('hypercore-crypto')
const Protomux = require('protomux')
const b4a = require('b4a')
const schema = require('./spec/hyperschema')

const [NS_INITATOR, NS_RESPONDER] = crypto.namespace('wakeup', 2)

const Handshake = schema.getEncoding('@wakeup/handshake')
const Announce = schema.getEncoding('@wakeup/announce')
const Lookup = schema.getEncoding('@wakeup/lookup')
const Info = schema.getEncoding('@wakeup/info')

module.exports = class WakeupSwarm {
  constructor(onwakeup = noop, { gcTickTime = 2000 } = {}) {
    this.gcTickTime = gcTickTime

    this.topics = new Map()
    this.topicsGC = new Set()
    this.muxers = new Set()
    this.stats = {
      sessionsOpened: 0,
      sessionsClosed: 0,
      topicsAdded: 0,
      topicsGcd: 0,
      peersAdded: 0,
      peersRemoved: 0,
      wireAnnounce: { rx: 0, tx: 0 },
      wireLookup: { rx: 0, tx: 0 },
      wireInfo: { rx: 0, tx: 0 }
    }

    this.onwakeup = onwakeup

    this._gcInterval = null
    this._gcBound = this._gc.bind(this)
  }

  session(capability, handlers = {}) {
    const id = handlers.discoveryKey || crypto.discoveryKey(capability)
    const active = handlers.active !== false
    const hex = b4a.toString(id, 'hex')

    let w = this.topics.get(hex)

    if (w) return w.addSession(handlers)

    w = new WakeupTopic(this, id, capability, active)

    this.topics.set(hex, w)

    for (const muxer of this.muxers) {
      w._onopen(muxer, true)
    }

    return w.addSession(handlers)
  }

  getSessions(capability, handlers = {}) {
    const id = handlers.discoveryKey || crypto.discoveryKey(capability)
    const hex = b4a.toString(id, 'hex')
    const w = this.topics.get(hex)
    return w ? w.sessions : []
  }

  hasStream(stream, capability, handlers = {}) {
    if (!capability) {
      const noiseStream = stream.noiseStream || stream
      return this.muxers.has(getMuxer(noiseStream))
    }

    const id = handlers.discoveryKey || crypto.discoveryKey(capability)
    const hex = b4a.toString(id, 'hex')
    const t = this.topics.get(hex)

    return t ? t.peersByStream.has(stream) : false
  }

  addStream(stream) {
    const noiseStream = stream.noiseStream || stream

    if (!noiseStream.connected) {
      noiseStream.once('open', this.addStream.bind(this, noiseStream))
      return
    }

    const muxer = getMuxer(noiseStream)
    muxer.pair({ protocol: 'wakeup' }, (id) => this._onpair(id, muxer))

    this.muxers.add(muxer)
    noiseStream.on('close', () => this.muxers.delete(muxer))

    for (const w of this.topics.values()) {
      if (!w.isActive) continue
      w._onopen(muxer, true)
    }
  }

  _onActive(w) {
    for (const m of this.muxers) {
      w._onopen(m, false)
    }
  }

  _addGC(topic) {
    if (topic.destroyed) return
    this.topicsGC.add(topic)
    if (this._gcInterval === null) {
      this._gcInterval = setInterval(this._gcBound, this.gcTickTime)
    }
  }

  _removeGC(topic) {
    this.topicsGC.delete(topic)
    if (this.topicsGC.size === 0 && this._gcInterval) {
      clearInterval(this._gcInterval)
      this._gcInterval = null
    }
  }

  _gc() {
    const destroy = []
    for (const w of this.topicsGC) {
      w.idleTicks++
      if (w.idleTicks >= 5) destroy.push(w)
    }
    for (const w of destroy) w.teardown()
  }

  destroy() {
    if (this._gcInterval) clearInterval(this._gcInterval)
    this._gcInterval = null

    for (const w of this.topics.values()) w.teardown()
  }

  async _onpair(id, stream) {
    const hex = b4a.toString(id, 'hex')
    const w = this.topics.get(hex)
    if (!w || !w.sessions.length) return this.onwakeup(id, stream)
    w._onopen(getMuxer(stream), false)
  }

  registerMetrics(promClient) {
    const self = this
    new promClient.Gauge({
      // eslint-disable-line no-new
      name: 'protomux_wakeup_sessions_opened',
      help: 'The amount of sessions opened by protomux wakeup',
      collect() {
        this.set(self.stats.sessionsOpened)
      }
    })

    new promClient.Gauge({
      // eslint-disable-line no-new
      name: 'protomux_wakeup_sessions_closed',
      help: 'The amount of sessions closed by protomux wakeup',
      collect() {
        this.set(self.stats.sessionsClosed)
      }
    })

    new promClient.Gauge({
      // eslint-disable-line no-new
      name: 'protomux_wakeup_topics_added',
      help: 'The amount of topics added to protomux wakeup',
      collect() {
        this.set(self.stats.topicsAdded)
      }
    })

    new promClient.Gauge({
      // eslint-disable-line no-new
      name: 'protomux_wakeup_topics_gcd',
      help: 'The amount of topics that got garbage collected by protomux wakeup',
      collect() {
        this.set(self.stats.topicsGcd)
      }
    })

    new promClient.Gauge({
      // eslint-disable-line no-new
      name: 'protomux_wakeup_peers_added',
      help: 'The amount of peers added by protomux wakeup, across all topics',
      collect() {
        this.set(self.stats.peersAdded)
      }
    })

    new promClient.Gauge({
      // eslint-disable-line no-new
      name: 'protomux_wakeup_peers_removed',
      help: 'The amount of peers removed by protomux wakeup, across all topics',
      collect() {
        this.set(self.stats.peersRemoved)
      }
    })

    new promClient.Gauge({
      // eslint-disable-line no-new
      name: 'protomux_wakeup_wire_announce_rx',
      help: 'The amount of wire announce messages received by protomux wakeup',
      collect() {
        this.set(self.stats.wireAnnounce.rx)
      }
    })
    new promClient.Gauge({
      // eslint-disable-line no-new
      name: 'protomux_wakeup_wire_announce_tx',
      help: 'The amount of wire announce messages transmitted by protomux wakeup',
      collect() {
        this.set(self.stats.wireAnnounce.tx)
      }
    })
    new promClient.Gauge({
      // eslint-disable-line no-new
      name: 'protomux_wakeup_wire_lookup_rx',
      help: 'The amount of wire lookup messages received by protomux wakeup',
      collect() {
        this.set(self.stats.wireLookup.rx)
      }
    })
    new promClient.Gauge({
      // eslint-disable-line no-new
      name: 'protomux_wakeup_wire_lookup_tx',
      help: 'The amount of wire lookup messages transmitted by protomux wakeup',
      collect() {
        this.set(self.stats.wireLookup.tx)
      }
    })
    new promClient.Gauge({
      // eslint-disable-line no-new
      name: 'protomux_wakeup_wire_info_rx',
      help: 'The amount of wire info messages received by protomux wakeup',
      collect() {
        this.set(self.stats.wireInfo.rx)
      }
    })
    new promClient.Gauge({
      // eslint-disable-line no-new
      name: 'protomux_wakeup_wire_info_tx',
      help: 'The amount of wire info messages transmitted by protomux wakeup',
      collect() {
        this.set(self.stats.wireInfo.rx)
      }
    })
  }
}

class WakeupPeer {
  constructor(topic) {
    this.index = 0
    this.userData = null // for the user
    this.clock = 0 // for the user, v useful to reduce traffic
    this.pending = true
    this.removed = false
    this.topic = topic
    this.channel = null
    this.stream = null
    this.wireLookup = null
    this.wireAnnounce = null
    this.wireInfo = null
    this.active = false
  }

  unlink(list) {
    // note that since we pop here we can iterate in reverse safely in case a peer is removed in the same tick
    const head = list.pop()
    if (head === this) return
    head.index = this.index
    list[head.index] = head
  }
}

class WakeupSession {
  constructor(topic, handlers) {
    this.index = 0
    this.topic = topic
    this.handlers = handlers
    this.isActive = handlers.active !== false
    this.destroyed = false
  }

  get peers() {
    return this.topic.peers
  }

  hasStream(stream) {
    return !!this.getPeer(stream)
  }

  addStream(stream) {
    this.topic.addStream(stream)
  }

  getPeer(stream) {
    return this.topic.peersByStream.get(stream) || null
  }

  broadcastLookup(req) {
    for (const peer of this.topic.pendingPeers) {
      this.lookup(peer, req)
    }
    for (const peer of this.topic.peers) {
      this.lookup(peer, req)
    }
  }

  lookupByStream(stream, req) {
    const peer = this.topic.peersByStream.get(stream)
    if (peer) this.lookup(peer, req)
  }

  lookup(peer, req) {
    peer.topic.state.stats.wireLookup.tx++
    peer.wireLookup.send(req || { hash: null })
  }

  announceByStream(stream, wakeup) {
    const peer = this.topic.peersByStream.get(stream)
    if (peer && !peer.pending) this.announce(peer, wakeup)
  }

  announce(peer, wakeup) {
    peer.topic.state.stats.wireAnnounce.tx++
    peer.wireAnnounce.send(wakeup)
  }

  active() {
    this.isActive = true
    this.topic._bumpActivity()
  }

  inactive() {
    this.isActive = false
    this.topic._bumpActivity()
  }

  destroy() {
    if (this.destroyed) return
    this.destroyed = true
    this.topic.removeSession(this)
  }
}

class WakeupTopic {
  constructor(state, id, capability, active) {
    this.state = state
    this.sessions = []
    this.id = id
    this.capability = capability
    this.peers = []
    this.pendingPeers = []
    this.peersByStream = new Map()
    this.activePeers = 0
    this.isActive = active
    this.idleTicks = 0
    this.gcing = false
    this.destroyed = false

    this.state.stats.topicsAdded++
  }

  addSession(handlers) {
    this.state.stats.sessionsOpened++
    const session = new WakeupSession(this, handlers)
    session.index = this.sessions.length
    this.sessions.push(session)
    this._bumpActivity()
    this._checkGC()
    return session
  }

  removeSession(session) {
    if (this.sessions.length <= session.index) return
    if (this.sessions[session.index] !== session) return

    this.state.stats.sessionsClosed++

    // same as with the peer, this allows us to iterate while removing if iterating backwards
    const head = this.sessions.pop()
    if (head !== session) {
      head.index = session.index
      this.sessions[head.index] = head
    }

    this._bumpActivity()
    this._checkGC()
  }

  _bumpActivity() {
    let isActive = false

    for (let i = this.sessions.length - 1; i >= 0; i--) {
      if (this.sessions[i].isActive) {
        isActive = true
        break
      }
    }

    if (isActive) this.active()
    else this.inactive()
  }

  active() {
    if (this.isActive) return
    this.idleTicks = 0
    this.isActive = true
    this._updateActive(true)
  }

  inactive() {
    if (!this.isActive) return
    this.isActive = false
    this._updateActive(false)
  }

  _updateActive(active) {
    const info = { active }

    for (const peer of this.pendingPeers) {
      peer.topic.state.stats.wireInfo.tx++
      peer.wireInfo.send(info)
    }
    for (const peer of this.peers) {
      peer.topic.state.stats.wireInfo.tx++
      peer.wireInfo.send(info)
    }

    this._checkGC()

    if (active) this.state._onActive(this)
  }

  teardown() {
    if (this.destroyed) return
    this.destroyed = true

    this.state.stats.topicsGcd++

    for (let i = this.peers.length - 1; i >= 0; i--) {
      this.peers[i].channel.close()
    }

    for (let i = this.pendingPeers.length - 1; i >= 0; i--) {
      this.pendingPeers[i].channel.close()
    }

    const hex = b4a.toString(this.id, 'hex')

    this.gcing = false
    this.state.topics.delete(hex)
    this.state._removeGC(this)

    for (let i = this.sessions.length - 1; i >= 0; i--) {
      this.state.stats.sessionsClosed++
      const session = this.sessions[i]
      if (session.handlers.ondestroy) session.handlers.ondestroy(session)
    }
  }

  addStream(stream) {
    this._onopen(getMuxer(stream), false)
  }

  _proveCapabilityTo(stream) {
    return this._makeCapability(stream.isInitiator, stream.handshakeHash)
  }

  _makeCapability(isInitiator, handshakeHash) {
    return crypto.hash([isInitiator ? NS_INITATOR : NS_RESPONDER, this.capability, handshakeHash])
  }

  _addPeer(peer, open) {
    if (
      !b4a.equals(
        open.capability,
        this._makeCapability(!peer.stream.isInitiator, peer.stream.handshakeHash)
      )
    ) {
      peer.channel.close()
      return
    }

    if (peer.pending) {
      peer.unlink(this.pendingPeers)
    }

    peer.active = open.active
    peer.pending = false
    peer.index = this.peers.push(peer) - 1

    if (peer.active) {
      this.activePeers++
      this._checkGC()
    }

    for (let i = this.sessions.length - 1; i >= 0; i--) {
      const session = this.sessions[i]
      const handlers = session.handlers

      if (handlers.onpeeradd) handlers.onpeeradd(peer, session)
      if (peer.active && handlers.onpeeractive) handlers.onpeeractive(peer, session)
    }
  }

  _checkGC() {
    const shouldGC = this.activePeers === 0 && this.sessions.length === 0

    if (shouldGC) {
      if (!this.gcing) {
        this.gcing = true
        this.state._addGC(this)
      }
    } else {
      if (this.gcing) {
        this.gcing = false
        this.state._removeGC(this)
      }
    }
  }

  _removePeer(peer) {
    this.state.stats.peersRemoved++
    peer.removed = true
    this.peersByStream.delete(peer.stream)

    if (peer.pending) {
      peer.unlink(this.pendingPeers)
      return
    }

    const active = peer.active

    if (active) {
      peer.active = false
      this.activePeers--
      this._checkGC()
    }

    peer.unlink(this.peers)

    for (let i = this.sessions.length - 1; i >= 0; i--) {
      const session = this.sessions[i]
      const handlers = session.handlers

      if (active && handlers.onpeerinactive) handlers.onpeerinactive(peer, session)
      if (handlers.onpeerremove) handlers.onpeerremove(peer, session)
    }
  }

  _onannounce(wakeup, peer) {
    for (let i = this.sessions.length - 1; i >= 0; i--) {
      const session = this.sessions[i]
      const handlers = session.handlers

      if (handlers.onannounce) handlers.onannounce(wakeup, peer, session)
    }
  }

  _onlookup(req, peer) {
    for (let i = this.sessions.length - 1; i >= 0; i--) {
      const session = this.sessions[i]
      const handlers = session.handlers

      if (handlers.onlookup) handlers.onlookup(req, peer, session)
    }
  }

  _oninfo(info, peer) {
    if (info.active) {
      if (!peer.active) {
        peer.active = true
        this.activePeers++
        this._checkGC()

        for (let i = this.sessions.length - 1; i >= 0; i--) {
          const session = this.sessions[i]
          const handlers = session.handlers

          if (handlers.onpeeractive) handlers.onpeeractive(peer, session)
        }
      }
    } else {
      if (peer.active) {
        peer.active = false
        this.activePeers--
        this._checkGC()

        for (let i = this.sessions.length - 1; i >= 0; i--) {
          const session = this.sessions[i]
          const handlers = session.handlers

          if (handlers.onpeerinactive) handlers.onpeerinactive(peer, session)
        }
      }
    }
  }

  _onopen(muxer, unique) {
    if (!unique && this.peersByStream.has(muxer.stream)) return

    const peer = new WakeupPeer(this)
    const ch = muxer.createChannel({
      userData: peer,
      protocol: 'wakeup',
      id: this.id,
      handshake: Handshake,
      messages: [
        { encoding: Lookup, onmessage: onlookup },
        { encoding: Announce, onmessage: onannounce },
        { encoding: Info, onmessage: onchannelinfo }
      ],
      onopen: onchannelopen,
      onclose: onchannelclose
    })

    if (!ch) return

    this.state.stats.peersAdded++

    peer.channel = ch
    peer.stream = muxer.stream

    peer.wireLookup = ch.messages[0]
    peer.wireAnnounce = ch.messages[1]
    peer.wireInfo = ch.messages[2]

    peer.index = this.pendingPeers.push(peer) - 1
    this.peersByStream.set(muxer.stream, peer)

    ch.open({
      version: 0,
      capability: this._proveCapabilityTo(muxer.stream),
      active: this.isActive
    })
  }
}

function onchannelopen(open, channel) {
  const peer = channel.userData
  peer.topic._addPeer(peer, open)
}

function onchannelclose(close, channel) {
  const peer = channel.userData
  peer.topic._removePeer(peer)
}

function onlookup(req, channel) {
  const peer = channel.userData
  peer.topic.state.stats.wireLookup.rx++
  peer.topic._onlookup(req, peer)
}

function onannounce(wakeup, channel) {
  const peer = channel.userData
  peer.topic.state.stats.wireAnnounce.rx++
  peer.topic._onannounce(wakeup, peer)
}

function onchannelinfo(info, channel) {
  const peer = channel.userData
  peer.topic.state.stats.wireInfo.rx++
  peer.topic._oninfo(info, peer)
}

function getMuxer(stream) {
  if (Protomux.isProtomux(stream)) return stream
  if (stream.noiseStream.userData) return stream.noiseStream.userData
  const mux = Protomux.from(stream.noiseStream)
  stream.noiseStream.userData = mux
  return mux
}

function noop() {}
{
  "name": "protomux-wakeup",
  "version": "2.9.0",
  "description": "Wakeup protocol over protomux",
  "main": "index.js",
  "dependencies": {
    "b4a": "^1.6.7",
    "hypercore-crypto": "^3.5.0",
    "hyperschema": "^1.10.4",
    "protomux": "^3.10.1"
  },
  "devDependencies": {
    "@hyperswarm/secret-stream": "^6.8.1",
    "brittle": "^3.17.1",
    "lunte": "^1.0.0",
    "prettier": "^3.6.2",
    "prettier-config-holepunch": "^2.0.0",
    "prom-client": "^15.1.3"
  },
  "files": [
    "index.js",
    "spec/hyperschema/*.js"
  ],
  "scripts": {
    "format": "prettier --write .",
    "test": "prettier --check . && lunte && brittle test/*.js"
  },
  "repository": {
    "type": "git",
    "url": "https://github.com/holepunchto/protomux-wakeup.git"
  },
  "author": "Holepunch Inc.",
  "license": "Apache-2.0",
  "bugs": {
    "url": "https://github.com/holepunchto/protomux-wakeup/issues"
  },
  "homepage": "https://github.com/holepunchto/protomux-wakeup"
}
// This file is autogenerated by the hyperschema compiler
// Schema Version: 1
/* eslint-disable camelcase */
/* eslint-disable quotes */

const VERSION = 1
const { c } = require('hyperschema/runtime')

// eslint-disable-next-line no-unused-vars
let version = VERSION

// @wakeup/info
const encoding0 = {
  preencode(state, m) {
    state.end++ // max flag is 1 so always one byte
  },
  encode(state, m) {
    const flags = m.active ? 1 : 0

    c.uint.encode(state, flags)
  },
  decode(state) {
    const flags = c.uint.decode(state)

    return {
      active: (flags & 1) !== 0
    }
  }
}

// @wakeup/handshake
const encoding1 = {
  preencode(state, m) {
    c.uint.preencode(state, m.version)
    c.fixed32.preencode(state, m.capability)
    state.end++ // max flag is 1 so always one byte
  },
  encode(state, m) {
    const flags = m.active ? 1 : 0

    c.uint.encode(state, m.version)
    c.fixed32.encode(state, m.capability)
    c.uint.encode(state, flags)
  },
  decode(state) {
    const r0 = c.uint.decode(state)
    const r1 = c.fixed32.decode(state)
    const flags = c.uint.decode(state)

    return {
      version: r0,
      capability: r1,
      active: (flags & 1) !== 0
    }
  }
}

// @wakeup/writer
const encoding2 = {
  preencode(state, m) {
    c.fixed32.preencode(state, m.key)
    c.uint.preencode(state, m.length)
  },
  encode(state, m) {
    c.fixed32.encode(state, m.key)
    c.uint.encode(state, m.length)
  },
  decode(state) {
    const r0 = c.fixed32.decode(state)
    const r1 = c.uint.decode(state)

    return {
      key: r0,
      length: r1
    }
  }
}

// @wakeup/announce
const encoding3 = c.array(encoding2)

// @wakeup/lookup
const encoding4 = {
  preencode(state, m) {
    state.end++ // max flag is 1 so always one byte

    if (m.hash) c.fixed32.preencode(state, m.hash)
  },
  encode(state, m) {
    const flags = m.hash ? 1 : 0

    c.uint.encode(state, flags)

    if (m.hash) c.fixed32.encode(state, m.hash)
  },
  decode(state) {
    const flags = c.uint.decode(state)

    return {
      hash: (flags & 1) !== 0 ? c.fixed32.decode(state) : null
    }
  }
}

function setVersion(v) {
  version = v
}

function encode(name, value, v = VERSION) {
  version = v
  return c.encode(getEncoding(name), value)
}

function decode(name, buffer, v = VERSION) {
  version = v
  return c.decode(getEncoding(name), buffer)
}

function getEnum(name) {
  switch (name) {
    default:
      throw new Error('Enum not found ' + name)
  }
}

function getEncoding(name) {
  switch (name) {
    case '@wakeup/info':
      return encoding0
    case '@wakeup/handshake':
      return encoding1
    case '@wakeup/writer':
      return encoding2
    case '@wakeup/announce':
      return encoding3
    case '@wakeup/lookup':
      return encoding4
    default:
      throw new Error('Encoder not found ' + name)
  }
}

function getStruct(name, v = VERSION) {
  const enc = getEncoding(name)
  return {
    preencode(state, m) {
      version = v
      enc.preencode(state, m)
    },
    encode(state, m) {
      version = v
      enc.encode(state, m)
    },
    decode(state) {
      version = v
      return enc.decode(state)
    }
  }
}

const resolveStruct = getStruct // compat

module.exports = {
  resolveStruct,
  getStruct,
  getEnum,
  getEncoding,
  encode,
  decode,
  setVersion,
  version
}
const b4a = require('b4a')
const c = require('compact-encoding')
const queueTick = require('queue-tick')
const safetyCatch = require('safety-catch')
const unslab = require('unslab')

const MAX_BUFFERED = 32768
const MAX_BACKLOG = Infinity // TODO: impl "open" backpressure
const MAX_BATCH = 8 * 1024 * 1024

class Channel {
  constructor (mux, info, userData, protocol, aliases, id, handshake, messages, onopen, onclose, ondestroy, ondrain) {
    this.userData = userData
    this.protocol = protocol
    this.aliases = aliases
    this.id = id
    this.handshake = null
    this.messages = []

    this.opened = false
    this.closed = false
    this.destroyed = false

    this.onopen = onopen
    this.onclose = onclose
    this.ondestroy = ondestroy
    this.ondrain = ondrain

    this._handshake = handshake
    this._mux = mux
    this._info = info
    this._localId = 0
    this._remoteId = 0
    this._active = 0
    this._extensions = null

    this._decBound = this._dec.bind(this)
    this._decAndDestroyBound = this._decAndDestroy.bind(this)

    this._openedPromise = null
    this._openedResolve = null

    this._destroyedPromise = null
    this._destroyedResolve = null

    for (const m of messages) this.addMessage(m)
  }

  get drained () {
    return this._mux.drained
  }

  fullyOpened () {
    if (this.opened) return Promise.resolve(true)
    if (this.closed) return Promise.resolve(false)
    if (this._openedPromise) return this._openedPromise

    this._openedPromise = new Promise((resolve) => { this._openedResolve = resolve })
    return this._openedPromise
  }

  fullyClosed () {
    if (this.destroyed) return Promise.resolve()
    if (this._destroyedPromise) return this._destroyedPromise

    this._destroyedPromise = new Promise((resolve) => { this._destroyedResolve = resolve })
    return this._destroyedPromise
  }

  open (handshake) {
    const id = this._mux._free.length > 0
      ? this._mux._free.pop()
      : this._mux._local.push(null) - 1

    this._info.opened++
    this._info.lastChannel = this
    this._localId = id + 1
    this._mux._local[id] = this

    if (this._remoteId === 0) {
      this._info.outgoing.push(this._localId)
    }

    const state = { buffer: null, start: 2, end: 2 }

    c.uint.preencode(state, this._localId)
    c.string.preencode(state, this.protocol)
    c.buffer.preencode(state, this.id)
    if (this._handshake) this._handshake.preencode(state, handshake)

    state.buffer = this._mux._alloc(state.end)

    state.buffer[0] = 0
    state.buffer[1] = 1
    c.uint.encode(state, this._localId)
    c.string.encode(state, this.protocol)
    c.buffer.encode(state, this.id)
    if (this._handshake) this._handshake.encode(state, handshake)

    this._mux._write0(state.buffer)
  }

  _dec () {
    if (--this._active === 0 && this.closed === true) this._destroy()
  }

  _decAndDestroy (err) {
    this._dec()
    this._mux._safeDestroy(err)
  }

  _fullyOpenSoon () {
    this._mux._remote[this._remoteId - 1].session = this
    queueTick(this._fullyOpen.bind(this))
  }

  _fullyOpen () {
    if (this.opened === true || this.closed === true) return

    const remote = this._mux._remote[this._remoteId - 1]

    this.opened = true
    this.handshake = this._handshake ? this._handshake.decode(remote.state) : null
    this._track(this.onopen(this.handshake, this))

    remote.session = this
    remote.state = null
    if (remote.pending !== null) this._drain(remote)

    this._resolveOpen(true)
  }

  _resolveOpen (opened) {
    if (this._openedResolve !== null) {
      this._openedResolve(opened)
      this._openedResolve = this._openedPromise = null
    }
  }

  _resolveDestroyed () {
    if (this._destroyedResolve !== null) {
      this._destroyedResolve()
      this._destroyedResolve = this._destroyedPromise = null
    }
  }

  _drain (remote) {
    for (let i = 0; i < remote.pending.length; i++) {
      const p = remote.pending[i]
      this._mux._buffered -= byteSize(p.state)
      this._recv(p.type, p.state)
    }

    remote.pending = null
    this._mux._resumeMaybe()
  }

  _track (p) {
    if (isPromise(p) === true) {
      this._active++
      return p.then(this._decBound, this._decAndDestroyBound)
    }

    return null
  }

  _close (isRemote) {
    if (this.closed === true) return
    this.closed = true

    this._info.opened--
    if (this._info.lastChannel === this) this._info.lastChannel = null

    if (this._remoteId > 0) {
      this._mux._remote[this._remoteId - 1] = null
      this._remoteId = 0
      // If remote has acked, we can reuse the local id now
      // otherwise, we need to wait for the "ack" to arrive
      this._mux._free.push(this._localId - 1)
    }

    this._mux._local[this._localId - 1] = null
    this._localId = 0

    this._mux._gc(this._info)
    this._track(this.onclose(isRemote, this))

    if (this._active === 0) this._destroy()

    this._resolveOpen(false)
  }

  _destroy () {
    if (this.destroyed === true) return
    this.destroyed = true
    this._track(this.ondestroy(this))
    this._resolveDestroyed()
  }

  _recv (type, state) {
    if (type < this.messages.length) {
      const m = this.messages[type]
      const p = m.recv(state, this)
      if (m.autoBatch === true) return p
    }
    return null
  }

  cork () {
    this._mux.cork()
  }

  uncork () {
    this._mux.uncork()
  }

  close () {
    if (this.closed === true) return

    const state = { buffer: null, start: 2, end: 2 }

    c.uint.preencode(state, this._localId)

    state.buffer = this._mux._alloc(state.end)

    state.buffer[0] = 0
    state.buffer[1] = 3
    c.uint.encode(state, this._localId)

    this._close(false)
    this._mux._write0(state.buffer)
  }

  addMessage (opts) {
    if (!opts) return this._skipMessage()

    const type = this.messages.length
    const autoBatch = opts.autoBatch !== false
    const encoding = opts.encoding || c.raw
    const onmessage = opts.onmessage || noop

    const s = this
    const typeLen = encodingLength(c.uint, type)

    const m = {
      type,
      autoBatch,
      encoding,
      onmessage,
      recv (state, session) {
        return session._track(m.onmessage(encoding.decode(state), session))
      },
      send (m, session = s) {
        if (session.closed === true) return false

        const mux = session._mux
        const state = { buffer: null, start: 0, end: typeLen }

        if (mux._batch !== null) {
          encoding.preencode(state, m)
          state.buffer = mux._alloc(state.end)

          c.uint.encode(state, type)
          encoding.encode(state, m)

          mux._pushBatch(session._localId, state.buffer)
          return true
        }

        c.uint.preencode(state, session._localId)
        encoding.preencode(state, m)

        state.buffer = mux._alloc(state.end)

        c.uint.encode(state, session._localId)
        c.uint.encode(state, type)
        encoding.encode(state, m)

        mux.drained = mux.stream.write(state.buffer)

        return mux.drained
      }
    }

    this.messages.push(m)

    return m
  }

  _skipMessage () {
    const type = this.messages.length
    const m = {
      type,
      encoding: c.raw,
      onmessage: noop,
      recv (state, session) {},
      send (m, session) {}
    }

    this.messages.push(m)
    return m
  }
}

module.exports = class Protomux {
  constructor (stream, { alloc } = {}) {
    if (stream.userData === null) stream.userData = this

    this.isProtomux = true
    this.stream = stream
    this.corked = 0
    this.drained = true

    this._alloc = alloc || (typeof stream.alloc === 'function' ? stream.alloc.bind(stream) : b4a.allocUnsafe)
    this._safeDestroyBound = this._safeDestroy.bind(this)
    this._uncorkBound = this.uncork.bind(this)

    this._remoteBacklog = 0
    this._buffered = 0
    this._paused = false
    this._remote = []
    this._local = []
    this._free = []
    this._batch = null
    this._batchState = null

    this._infos = new Map()
    this._notify = new Map()

    this.stream.on('data', this._ondata.bind(this))
    this.stream.on('drain', this._ondrain.bind(this))
    this.stream.on('end', this._onend.bind(this))
    this.stream.on('error', noop) // we handle this in "close"
    this.stream.on('close', this._shutdown.bind(this))
  }

  static from (stream, opts) {
    if (stream.userData && stream.userData.isProtomux) return stream.userData
    if (stream.isProtomux) return stream
    return new this(stream, opts)
  }

  static isProtomux (mux) {
    return typeof mux === 'object' && mux.isProtomux === true
  }

  * [Symbol.iterator] () {
    for (const session of this._local) {
      if (session !== null) yield session
    }
  }

  isIdle () {
    return this._local.length === this._free.length
  }

  cork () {
    if (++this.corked === 1) {
      this._batch = []
      this._batchState = { buffer: null, start: 0, end: 1 }
    }
  }

  uncork () {
    if (--this.corked === 0) {
      this._sendBatch(this._batch, this._batchState)
      this._batch = null
      this._batchState = null
    }
  }

  getLastChannel ({ protocol, id = null }) {
    const key = toKey(protocol, id)
    const info = this._infos.get(key)
    if (info) return info.lastChannel
    return null
  }

  pair ({ protocol, id = null }, notify) {
    this._notify.set(toKey(protocol, id), notify)
  }

  unpair ({ protocol, id = null }) {
    this._notify.delete(toKey(protocol, id))
  }

  opened ({ protocol, id = null }) {
    const key = toKey(protocol, id)
    const info = this._infos.get(key)
    return info ? info.opened > 0 : false
  }

  createChannel ({ userData = null, protocol, aliases = [], id = null, unique = true, handshake = null, messages = [], onopen = noop, onclose = noop, ondestroy = noop, ondrain = noop }) {
    if (this.stream.destroyed) return null

    const info = this._get(protocol, id, aliases)
    if (unique && info.opened > 0) return null

    if (info.incoming.length === 0) {
      return new Channel(this, info, userData, protocol, aliases, id, handshake, messages, onopen, onclose, ondestroy, ondrain)
    }

    this._remoteBacklog--

    const remoteId = info.incoming.shift()
    const r = this._remote[remoteId - 1]
    if (r === null) return null

    const session = new Channel(this, info, userData, protocol, aliases, id, handshake, messages, onopen, onclose, ondestroy, ondrain)

    session._remoteId = remoteId
    session._fullyOpenSoon()

    return session
  }

  _pushBatch (localId, buffer) {
    if (this._batchState.end >= MAX_BATCH) {
      this._sendBatch(this._batch, this._batchState)
      this._batch = []
      this._batchState = { buffer: null, start: 0, end: 1 }
    }

    if (this._batch.length === 0 || this._batch[this._batch.length - 1].localId !== localId) {
      this._batchState.end++
      c.uint.preencode(this._batchState, localId)
    }
    c.buffer.preencode(this._batchState, buffer)
    this._batch.push({ localId, buffer })
  }

  _sendBatch (batch, state) {
    if (batch.length === 0) return

    let prev = batch[0].localId

    state.buffer = this._alloc(state.end)
    state.buffer[state.start++] = 0
    state.buffer[state.start++] = 0

    c.uint.encode(state, prev)

    for (let i = 0; i < batch.length; i++) {
      const b = batch[i]
      if (prev !== b.localId) {
        state.buffer[state.start++] = 0
        c.uint.encode(state, (prev = b.localId))
      }
      c.buffer.encode(state, b.buffer)
    }

    this.drained = this.stream.write(state.buffer)
  }

  _get (protocol, id, aliases = []) {
    const key = toKey(protocol, id)

    let info = this._infos.get(key)
    if (info) return info

    info = { key, protocol, aliases: [], id, pairing: 0, opened: 0, incoming: [], outgoing: [], lastChannel: null }
    this._infos.set(key, info)

    for (const alias of aliases) {
      const key = toKey(alias, id)
      info.aliases.push(key)

      this._infos.set(key, info)
    }

    return info
  }

  _gc (info) {
    if (info.opened === 0 && info.outgoing.length === 0 && info.incoming.length === 0) {
      this._infos.delete(info.key)

      for (const alias of info.aliases) this._infos.delete(alias)
    }
  }

  _ondata (buffer) {
    if (buffer.byteLength === 0) return // ignore empty frames...
    try {
      const state = { buffer, start: 0, end: buffer.byteLength }
      this._decode(c.uint.decode(state), state)
    } catch (err) {
      this._safeDestroy(err)
    }
  }

  _ondrain () {
    this.drained = true

    for (const s of this._local) {
      if (s !== null) s._track(s.ondrain(s))
    }
  }

  _onend () { // TODO: support half open mode for the users who wants that here
    this.stream.end()
  }

  _decode (remoteId, state) {
    const type = c.uint.decode(state)

    if (remoteId === 0) {
      return this._oncontrolsession(type, state)
    }

    const r = remoteId <= this._remote.length ? this._remote[remoteId - 1] : null

    // if the channel is closed ignore - could just be a pipeline message...
    if (r === null) return null

    if (r.pending !== null) {
      this._bufferMessage(r, type, state)
      return null
    }

    return r.session._recv(type, state)
  }

  _oncontrolsession (type, state) {
    switch (type) {
      case 0:
        this._onbatch(state)
        break

      case 1:
        // return the promise back up as this has sideeffects so we can batch reply
        return this._onopensession(state)

      case 2:
        this._onrejectsession(state)
        break

      case 3:
        this._onclosesession(state)
        break
    }

    return null
  }

  _bufferMessage (r, type, { buffer, start, end }) {
    const state = { buffer, start, end } // copy
    r.pending.push({ type, state })
    this._buffered += byteSize(state)
    this._pauseMaybe()
  }

  _pauseMaybe () {
    if (this._paused === true || this._buffered <= MAX_BUFFERED) return
    this._paused = true
    this.stream.pause()
  }

  _resumeMaybe () {
    if (this._paused === false || this._buffered > MAX_BUFFERED) return
    this._paused = false
    this.stream.resume()
  }

  _onbatch (state) {
    const end = state.end
    let remoteId = c.uint.decode(state)

    let waiting = null

    while (state.end > state.start) {
      const len = c.uint.decode(state)
      if (len === 0) {
        remoteId = c.uint.decode(state)
        continue
      }
      state.end = state.start + len
      // if batch contains more than one message, cork it so we reply back with a batch
      if (end !== state.end && waiting === null) {
        waiting = []
        this.cork()
      }
      const p = this._decode(remoteId, state)
      if (waiting !== null && p !== null) waiting.push(p)
      state.start = state.end
      state.end = end
    }

    if (waiting !== null) {
      // the waiting promises are not allowed to throw but we destroy the stream in case we are wrong
      Promise.all(waiting).then(this._uncorkBound, this._safeDestroyBound)
    }
  }

  _onopensession (state) {
    const remoteId = c.uint.decode(state)
    const protocol = c.string.decode(state)
    const id = unslab(c.buffer.decode(state))

    // remote tried to open the control session - auto reject for now
    // as we can use as an explicit control protocol declaration if we need to
    if (remoteId === 0) {
      this._rejectSession(0)
      return null
    }

    const rid = remoteId - 1
    const info = this._get(protocol, id)

    // allow the remote to grow the ids by one
    if (this._remote.length === rid) {
      this._remote.push(null)
    }

    if (rid >= this._remote.length || this._remote[rid] !== null) {
      throw new Error('Invalid open message')
    }

    if (info.outgoing.length > 0) {
      const localId = info.outgoing.shift()
      const session = this._local[localId - 1]

      if (session === null) { // we already closed the channel - ignore
        this._free.push(localId - 1)
        return null
      }

      this._remote[rid] = { state, pending: null, session: null }

      session._remoteId = remoteId
      session._fullyOpen()
      return null
    }

    const copyState = { buffer: state.buffer, start: state.start, end: state.end }
    this._remote[rid] = { state: copyState, pending: [], session: null }

    if (++this._remoteBacklog > MAX_BACKLOG) {
      throw new Error('Remote exceeded backlog')
    }

    info.pairing++
    info.incoming.push(remoteId)

    return this._requestSession(protocol, id, info).catch(this._safeDestroyBound)
  }

  _onrejectsession (state) {
    const localId = c.uint.decode(state)

    // TODO: can be done smarter...
    for (const info of this._infos.values()) {
      const i = info.outgoing.indexOf(localId)
      if (i === -1) continue

      info.outgoing.splice(i, 1)

      const session = this._local[localId - 1]

      this._free.push(localId - 1)
      if (session !== null) session._close(true)

      this._gc(info)
      return
    }

    throw new Error('Invalid reject message')
  }

  _onclosesession (state) {
    const remoteId = c.uint.decode(state)

    if (remoteId === 0) return // ignore

    const rid = remoteId - 1
    const r = rid < this._remote.length ? this._remote[rid] : null

    if (r === null) return

    if (r.session !== null) r.session._close(true)
  }

  async _requestSession (protocol, id, info) {
    const notify = this._notify.get(toKey(protocol, id)) || this._notify.get(toKey(protocol, null))

    if (notify) await notify(id)

    if (--info.pairing > 0) return

    while (info.incoming.length > 0) {
      this._rejectSession(info, info.incoming.shift())
    }

    this._gc(info)
  }

  _rejectSession (info, remoteId) {
    if (remoteId > 0) {
      const r = this._remote[remoteId - 1]

      if (r.pending !== null) {
        for (let i = 0; i < r.pending.length; i++) {
          this._buffered -= byteSize(r.pending[i].state)
        }
      }

      this._remote[remoteId - 1] = null
      this._resumeMaybe()
    }

    const state = { buffer: null, start: 2, end: 2 }

    c.uint.preencode(state, remoteId)

    state.buffer = this._alloc(state.end)

    state.buffer[0] = 0
    state.buffer[1] = 2
    c.uint.encode(state, remoteId)

    this._write0(state.buffer)
  }

  _write0 (buffer) {
    if (this._batch !== null) {
      this._pushBatch(0, buffer.subarray(1))
      return
    }

    this.drained = this.stream.write(buffer)
  }

  destroy (err) {
    this.stream.destroy(err)
  }

  _safeDestroy (err) {
    safetyCatch(err)
    this.stream.destroy(err)
  }

  _shutdown () {
    for (const s of this._local) {
      if (s !== null) s._close(true)
    }
  }
}

function noop () {}

function toKey (protocol, id) {
  return protocol + '##' + (id ? b4a.toString(id, 'hex') : '')
}

function byteSize (state) {
  return 512 + (state.end - state.start)
}

function isPromise (p) {
  return !!(p && typeof p.then === 'function')
}

function encodingLength (enc, val) {
  const state = { buffer: null, start: 0, end: 0 }
  enc.preencode(state, val)
  return state.end
}
{
  "name": "protomux",
  "version": "3.10.1",
  "description": "Multiplex multiple message oriented protocols over a stream",
  "main": "index.js",
  "files": [
    "index.js"
  ],
  "dependencies": {
    "b4a": "^1.3.1",
    "compact-encoding": "^2.5.1",
    "queue-tick": "^1.0.0",
    "safety-catch": "^1.0.1",
    "unslab": "^1.3.0"
  },
  "devDependencies": {
    "@hyperswarm/secret-stream": "^6.0.0",
    "brittle": "^3.0.0",
    "standard": "^16.0.4"
  },
  "scripts": {
    "test": "standard && brittle test.js"
  },
  "repository": {
    "type": "git",
    "url": "https://github.com/mafintosh/protomux.git"
  },
  "author": "Mathias Buus (@mafintosh)",
  "license": "MIT",
  "bugs": {
    "url": "https://github.com/mafintosh/protomux/issues"
  },
  "homepage": "https://github.com/mafintosh/protomux"
}
{
  "name": "queue-tick",
  "version": "1.0.1",
  "description": "Next tick shim that prefers process.nextTick over queueMicrotask for compat",
  "main": "./process-next-tick.js",
  "browser": "./queue-microtask.js",
  "dependencies": {},
  "devDependencies": {
    "standard": "^16.0.3",
    "tape": "^5.3.1"
  },
  "scripts": {
    "test": "standard && tape test.js"
  },
  "repository": {
    "type": "git",
    "url": "https://github.com/mafintosh/queue-tick.git"
  },
  "author": "Mathias Buus (@mafintosh)",
  "license": "MIT",
  "bugs": {
    "url": "https://github.com/mafintosh/queue-tick/issues"
  },
  "homepage": "https://github.com/mafintosh/queue-tick"
}
module.exports = (typeof process !== 'undefined' && typeof process.nextTick === 'function')
  ? process.nextTick.bind(process)
  : require('./queue-microtask')
module.exports = typeof queueMicrotask === 'function' ? queueMicrotask : (fn) => Promise.resolve().then(fn)
require.addon = require('require-addon')

module.exports = require.addon('.', __filename)
const binding = require('./binding')

exports.get = function get(field, bit) {
  const n = field.byteLength * 8

  if (bit < 0) bit += n
  if (bit < 0 || bit >= n) return false

  return binding.quickbit_napi_get(toBuffer(field), bit) !== 0
}

exports.set = function set(field, bit, value = true) {
  const n = field.byteLength * 8

  if (bit < 0) bit += n
  if (bit < 0 || bit >= n) return false

  return binding.quickbit_napi_set(toBuffer(field), bit, value ? 1 : 0) !== 0
}

exports.fill = function fill(
  field,
  value,
  start = 0,
  end = field.byteLength * 8
) {
  const n = field.byteLength * 8

  if (start < 0) start += n
  if (end < 0) end += n
  if (start < 0 || start >= field.byteLength * 8 || start >= end) return field

  binding.quickbit_napi_fill(toBuffer(field), value ? 1 : 0, start, end)
  return field
}

exports.clear = function clear(field, ...chunks) {
  binding.quickbit_napi_clear(toBuffer(field), chunks.map(toBufferChunk))
}

exports.findFirst = function findFirst(field, value, position = 0) {
  const n = field.byteLength * 8

  if (position < 0) position += n
  if (position < 0) position = 0
  if (position >= n) return -1

  return binding.quickbit_napi_find_first(
    toBuffer(field),
    value ? 1 : 0,
    position
  )
}

exports.findLast = function findLast(
  field,
  value,
  position = field.byteLength * 8 - 1
) {
  const n = field.byteLength * 8

  if (position < 0) position += n
  if (position < 0) return -1
  if (position >= n) position = n - 1

  return binding.quickbit_napi_find_last(
    toBuffer(field),
    value ? 1 : 0,
    position
  )
}

function toBuffer(field) {
  if (field.BYTES_PER_ELEMENT === 1) return field
  return new Uint8Array(field.buffer, field.byteOffset, field.byteLength)
}

function toBufferChunk(chunk) {
  return { field: toBuffer(chunk.field), offset: chunk.offset }
}

class Index {
  static from(fieldOrChunks, byteLength = -1) {
    if (Array.isArray(fieldOrChunks)) {
      return new SparseIndex(fieldOrChunks, byteLength)
    } else {
      return new DenseIndex(fieldOrChunks, byteLength)
    }
  }

  constructor(byteLength) {
    this._byteLength = byteLength
    this.handle = Buffer.allocUnsafe(binding.sizeof_quickbit_index_t)
  }

  get byteLength() {
    return this._byteLength
  }

  skipFirst(value, position = 0) {
    const n = this.byteLength * 8

    if (position < 0) position += n
    if (position < 0) position = 0
    if (position >= n) return n - 1

    return binding.quickbit_napi_skip_first(
      this.handle,
      this.byteLength,
      value ? 1 : 0,
      position
    )
  }

  skipLast(value, position = this.byteLength * 8 - 1) {
    const n = this.byteLength * 8

    if (position < 0) position += n
    if (position < 0) return 0
    if (position >= n) position = n - 1

    return binding.quickbit_napi_skip_last(
      this.handle,
      this.byteLength,
      value ? 1 : 0,
      position
    )
  }
}

exports.Index = Index

class DenseIndex extends Index {
  constructor(field, byteLength) {
    super(byteLength)
    this.field = field

    binding.quickbit_napi_index_init(this.handle, toBuffer(this.field))
  }

  get byteLength() {
    if (this._byteLength !== -1) return this._byteLength
    return this.field.byteLength
  }

  update(bit) {
    const n = this.byteLength * 8

    if (bit < 0) bit += n
    if (bit < 0 || bit >= n) return false

    return (
      binding.quickbit_napi_index_update(
        this.handle,
        toBuffer(this.field),
        bit
      ) !== 0
    )
  }
}

function selectChunk(chunks, offset) {
  for (let i = 0; i < chunks.length; i++) {
    const next = chunks[i]

    const start = next.offset
    const end = next.offset + next.field.byteLength

    if (offset >= start && offset + 16 <= end) {
      return next
    }
  }

  return null
}

class SparseIndex extends Index {
  constructor(chunks, byteLength) {
    super(byteLength)
    this.chunks = chunks

    binding.quickbit_napi_index_init_sparse(
      this.handle,
      this.chunks.map(toBufferChunk)
    )
  }

  get byteLength() {
    if (this._byteLength !== -1) return this._byteLength
    const last = this.chunks[this.chunks.length - 1]
    return last ? last.offset + last.field.byteLength : 0
  }

  update(bit) {
    const n = this.byteLength * 8

    if (bit < 0) bit += n
    if (bit < 0 || bit >= n) return false

    const j = Math.floor(bit / 128)

    const offset = j * 16

    const chunk = selectChunk(this.chunks, offset)

    if (chunk === null) return false

    return (
      binding.quickbit_napi_index_update_sparse(
        this.handle,
        toBuffer(chunk.field),
        chunk.offset,
        bit
      ) !== 0
    )
  }
}
{
  "name": "quickbit-native",
  "version": "2.4.8",
  "description": "libquickbit JavaScript bindings for Node.js",
  "main": "index.js",
  "files": [
    "index.js",
    "macros.h",
    "binding.cc",
    "binding.js",
    "CMakeLists.txt",
    "prebuilds"
  ],
  "addon": true,
  "scripts": {
    "test": "npm run lint && npm run test:bare && npm run test:node",
    "test:bare": "bare test.mjs",
    "test:node": "node test.mjs",
    "lint": "prettier . --check"
  },
  "repository": {
    "type": "git",
    "url": "git+https://github.com/holepunchto/quickbit-native.git"
  },
  "author": "Holepunch",
  "license": "Apache-2.0",
  "bugs": {
    "url": "https://github.com/holepunchto/quickbit-native/issues"
  },
  "homepage": "https://github.com/holepunchto/quickbit-native#readme",
  "dependencies": {
    "require-addon": "^1.1.0"
  },
  "devDependencies": {
    "b4a": "^1.6.7",
    "bare-compat-napi": "^1.3.2",
    "brittle": "^3.1.0",
    "cmake-bare": "^1.1.7",
    "cmake-fetch": "^1.1.0",
    "cmake-napi": "^1.0.2",
    "prettier": "^3.5.3",
    "prettier-config-standard": "^7.0.0"
  }
}
const simdle = require('simdle-universal')

const INDEX_LEN = (16 /* root */ + 128 * 16 /* children */) * 2

const get = exports.get = function get (field, bit) {
  const n = field.byteLength * 8

  if (bit < 0) bit += n
  if (bit < 0 || bit >= n) return false

  const m = field.BYTES_PER_ELEMENT * 8

  const offset = bit & (m - 1)
  const i = (bit - offset) / m

  return (field[i] & (1 << offset)) !== 0
}

const set = exports.set = function set (field, bit, value = true) {
  const n = field.byteLength * 8

  if (bit < 0) bit += n
  if (bit < 0 || bit >= n) return false

  const m = field.BYTES_PER_ELEMENT * 8

  const offset = bit & (m - 1)
  const i = (bit - offset) / m
  const mask = 1 << offset

  if (value) {
    if ((field[i] & mask) !== 0) return false
  } else {
    if ((field[i] & mask) === 0) return false
  }

  field[i] ^= mask

  return true
}

exports.fill = function fill (field, value, start = 0, end = field.byteLength * 8) {
  const n = field.byteLength * 8

  if (start < 0) start += n
  if (end < 0) end += n
  if (start < 0 || start >= field.byteLength * 8 || start >= end) return field

  const m = field.BYTES_PER_ELEMENT * 8

  let i, j

  {
    const offset = start & (m - 1)
    i = (start - offset) / m

    if (offset !== 0) {
      let shift = m - offset
      if (end - start < shift) shift = end - start

      const mask = ((1 << shift) - 1) << offset

      if (value) field[i] |= mask
      else field[i] &= ~mask

      i++
    }
  }

  {
    const offset = end & (m - 1)
    j = (end - offset) / m

    if (offset !== 0 && j >= i) {
      const mask = (1 << offset) - 1

      if (value) field[j] |= mask
      else field[j] &= ~mask
    }
  }

  if (i < j) field.fill(value ? (2 ** m) - 1 : 0, i, j)

  return field
}

exports.clear = function clear (field, ...chunks) {
  const n = field.byteLength

  for (const chunk of chunks) {
    if (chunk.offset >= n) continue

    const m = chunk.field.byteLength

    let i = chunk.offset
    let j = 0

    while (((i & 15) !== 0 || (j & 15) !== 0) && i < n && j < m) {
      field[i] = field[i] & ~chunk.field[j]
      i++
      j++
    }

    if (i + 15 < n && j + 15 < m) {
      const len = Math.min(n - (n & 15) - i, m - (m & 15) - j)

      simdle.clear(field.subarray(i, i + len), chunk.field.subarray(j, j + len), field.subarray(i, i + len))
    }

    while (i < n && j < m) {
      field[i] = field[i] & ~chunk.field[j]
      i++
      j++
    }
  }
}

function bitOffset (bit, offset) {
  return !bit ? offset : (INDEX_LEN * 8 / 2) + offset
}

function byteOffset (bit, offset) {
  return !bit ? offset : (INDEX_LEN / 2) + offset
}

exports.findFirst = function findFirst (field, value, position = 0) {
  const n = field.byteLength * 8

  if (position < 0) position += n
  if (position < 0) position = 0
  if (position >= n) return -1

  value = !!value

  for (let i = position; i < n; i++) {
    if (get(field, i) === value) return i
  }

  return -1
}

exports.findLast = function findLast (field, value, position = field.byteLength * 8 - 1) {
  const n = field.byteLength * 8

  if (position < 0) position += n
  if (position < 0) return -1
  if (position >= n) position = n - 1

  value = !!value

  for (let i = position; i >= 0; i--) {
    if (get(field, i) === value) return i
  }

  return -1
}

const Index = exports.Index = class Index {
  static from (fieldOrChunks, byteLength = -1) {
    if (Array.isArray(fieldOrChunks)) {
      return new SparseIndex(fieldOrChunks, byteLength)
    } else {
      return new DenseIndex(fieldOrChunks, byteLength)
    }
  }

  constructor (byteLength) {
    this._byteLength = byteLength
    this.handle = new Uint32Array(INDEX_LEN / 4)
  }

  get byteLength () {
    return this._byteLength
  }

  skipFirst (value, position = 0) {
    const n = this.byteLength * 8

    if (position < 0) position += n
    if (position < 0) position = 0
    if (position >= n) return n - 1

    let i = Math.floor(position / 16384)

    if (i > 127) return position

    while (i <= 127 && get(this.handle, bitOffset(value, i))) {
      i++
    }

    if (i === 128) return n - 1

    let k = i * 16384
    let j = 0

    if (position > k) j = Math.floor((position - k) / 128)

    while (j <= 127 && get(this.handle, bitOffset(value, i * 128 + j + 128))) {
      j++
      k += 128
    }

    if (j === 128 && i !== 127) return this.skipFirst(value, (i + 1) * 16384)

    if (k > position) position = k

    return position < n ? position : n - 1
  }

  skipLast (value, position = this.byteLength * 8 - 1) {
    const n = this.byteLength * 8

    if (position < 0) position += n
    if (position < 0) return 0
    if (position >= n) position = n - 1

    let i = Math.floor(position / 16384)

    if (i > 127) return position

    while (i >= 0 && get(this.handle, bitOffset(value, i))) {
      i--
    }

    if (i === -1) return 0

    let k = ((i + 1) * 16384) - 1
    let j = 127

    if (position < k) j = 128 - Math.ceil((k - position) / 128)

    while (j >= 0 && get(this.handle, bitOffset(value, i * 128 + j + 128))) {
      j--
      k -= 128
    }

    if (j === -1 && i !== 0) return this.skipLast(value, i * 16384 - 1)

    if (k < position) position = k

    return position
  }
}

class DenseIndex extends Index {
  constructor (field, byteLength) {
    super(byteLength)
    this.field = field

    const m = field.BYTES_PER_ELEMENT

    for (let i = 0; i < 128; i++) {
      for (let j = 0; j < 128; j++) {
        const offset = (i * 128 + j) * 16
        let allz = true
        let allo = false

        if (offset + 16 <= this.field.byteLength) {
          const vec = this.field.subarray(offset / m, (offset + 16) / m)

          allz = simdle.allz(vec)
          allo = simdle.allo(vec)
        }

        const k = i * 128 + 128 + j

        set(this.handle, bitOffset(false, k), allz)
        set(this.handle, bitOffset(true, k), allo)
      }

      {
        const offset = byteOffset(false, i * 16 + 16) / 4
        const allo = simdle.allo(this.handle.subarray(offset, offset + 4))

        set(this.handle, bitOffset(false, i), allo)
      }

      {
        const offset = byteOffset(true, i * 16 + 16) / 4
        const allo = simdle.allo(this.handle.subarray(offset, offset + 4))

        set(this.handle, bitOffset(true, i), allo)
      }
    }
  }

  get byteLength () {
    if (this._byteLength !== -1) return this._byteLength
    return this.field.byteLength
  }

  update (bit) {
    const n = this.byteLength * 8

    if (bit < 0) bit += n
    if (bit < 0 || bit >= n) return false

    const m = this.field.BYTES_PER_ELEMENT

    const i = Math.floor(bit / 16384)
    const j = Math.floor(bit / 128)

    const offset = (j * 16) / m
    const vec = this.field.subarray(offset, offset + (16 / m))

    const allz = simdle.allz(vec)
    const allo = simdle.allo(vec)

    let changed = false

    if (set(this.handle, bitOffset(false, 128 + j), allz)) {
      changed = true

      const offset = byteOffset(false, i * 16 + 16) / 4
      const allo = simdle.allo(this.handle.subarray(offset, offset + 4))

      set(this.handle, bitOffset(false, i), allo)
    }

    if (set(this.handle, bitOffset(true, 128 + j), allo)) {
      changed = true

      const offset = byteOffset(true, i * 16 + 16) / 4
      const allo = simdle.allo(this.handle.subarray(offset, offset + 4))

      set(this.handle, bitOffset(true, i), allo)
    }

    return changed
  }
}

function selectChunk (chunks, offset) {
  for (let i = 0; i < chunks.length; i++) {
    const next = chunks[i]

    const start = next.offset
    const end = next.offset + next.field.byteLength

    if (offset >= start && offset + 16 <= end) {
      return next
    }
  }

  return null
}

class SparseIndex extends Index {
  constructor (chunks, byteLength) {
    super(byteLength)
    this.chunks = chunks

    for (let i = 0; i < 128; i++) {
      for (let j = 0; j < 128; j++) {
        const offset = (i * 128 + j) * 16
        let allz = true
        let allo = false

        const chunk = selectChunk(this.chunks, offset)

        if (chunk !== null) {
          const m = chunk.field.BYTES_PER_ELEMENT

          const vec = chunk.field.subarray((offset - chunk.offset) / m, (offset - chunk.offset + 16) / m)

          allz = simdle.allz(vec)
          allo = simdle.allo(vec)
        }

        const k = i * 128 + 128 + j

        set(this.handle, bitOffset(false, k), allz)
        set(this.handle, bitOffset(true, k), allo)
      }

      {
        const offset = byteOffset(false, i * 16 + 16) / 4
        const allo = simdle.allo(this.handle.subarray(offset, offset + 4))

        set(this.handle, bitOffset(false, i), allo)
      }

      {
        const offset = byteOffset(true, i * 16 + 16) / 4
        const allo = simdle.allo(this.handle.subarray(offset, offset + 4))

        set(this.handle, bitOffset(true, i), allo)
      }
    }
  }

  get byteLength () {
    if (this._byteLength !== -1) return this._byteLength
    const last = this.chunks[this.chunks.length - 1]
    return last ? last.offset + last.field.byteLength : 0
  }

  update (bit) {
    const n = this.byteLength * 8

    if (bit < 0) bit += n
    if (bit < 0 || bit >= n) return false

    const i = Math.floor(bit / 16384)
    const j = Math.floor(bit / 128)

    const offset = j * 16

    const chunk = selectChunk(this.chunks, offset)

    if (chunk === null) return false

    const m = chunk.field.BYTES_PER_ELEMENT

    const vec = chunk.field.subarray((offset - chunk.offset) / m, (offset - chunk.offset + 16) / m)

    const allz = simdle.allz(vec)
    const allo = simdle.allo(vec)

    let changed = false

    if (set(this.handle, bitOffset(false, 128 + j), allz)) {
      changed = true

      const offset = byteOffset(false, i * 16 + 16) / 4
      const allo = simdle.allo(this.handle.subarray(offset, offset + 4))

      set(this.handle, bitOffset(false, i), allo)
    }

    if (set(this.handle, bitOffset(true, 128 + j), allo)) {
      changed = true

      const offset = byteOffset(true, i * 16 + 16) / 4
      const allo = simdle.allo(this.handle.subarray(offset, offset + 4))

      set(this.handle, bitOffset(true, i), allo)
    }

    return changed
  }
}
const fallback = require('./fallback')

try {
  const native = require('quickbit-native')

  // These functions are faster in JavaScript
  exports.get = fallback.get
  exports.set = fallback.set
  exports.fill = fallback.fill

  // These functions are faster in C
  exports.clear = native.clear
  exports.findFirst = native.findFirst
  exports.findLast = native.findLast
  exports.Index = native.Index
} catch {
  module.exports = fallback
}
{
  "name": "quickbit-universal",
  "version": "2.2.0",
  "description": "Universal wrapper for libquickbit with a JavaScript fallback",
  "main": "index.js",
  "files": [
    "fallback.js",
    "index.js"
  ],
  "browser": {
    "./index.js": "./fallback.js"
  },
  "scripts": {
    "test": "standard && brittle test.mjs"
  },
  "repository": {
    "type": "git",
    "url": "git+https://github.com/holepunchto/quickbit-universal.git"
  },
  "author": "Kasper Isager Dalsgar <kasper@funktionel.co>",
  "license": "ISC",
  "bugs": {
    "url": "https://github.com/holepunchto/quickbit-universal/issues"
  },
  "homepage": "https://github.com/holepunchto/quickbit-universal#readme",
  "dependencies": {
    "b4a": "^1.6.0",
    "simdle-universal": "^1.1.0"
  },
  "optionalDependencies": {
    "quickbit-native": "^2.2.0"
  },
  "devDependencies": {
    "brittle": "^3.1.0",
    "standard": "^17.0.0"
  }
}
class CacheEntry {
  constructor (key, index, map) {
    this.key = key
    this.index = index
    this.map = map
  }
}

class CacheValue {
  constructor (entry, value) {
    this.entry = entry
    this.value = value
  }
}

class Rache {
  constructor ({ maxSize = 65536, parent = null } = {}) {
    this.maxSize = parent?.maxSize || maxSize

    this._array = parent?._array || []
    this._map = new Map()
  }

  static from (cache) {
    return cache ? new this({ parent: cache }) : new this()
  }

  get globalSize () {
    return this._array.length
  }

  get size () {
    return this._map.size
  }

  sub () {
    return new Rache({ parent: this })
  }

  set (key, value) { // ~constant time
    const existing = this._map.get(key)
    if (existing !== undefined) {
      existing.value = value
      return
    }

    if (this._array.length >= this.maxSize) this._gc()

    const entry = new CacheEntry(key, this._array.length, this._map)
    this._array.push(entry)
    const cacheValue = new CacheValue(entry, value)
    this._map.set(key, cacheValue)
  }

  delete (key) {
    const existing = this._map.get(key)
    if (existing === undefined) return false

    this._delete(existing.entry.index)
    return true
  }

  get (key) {
    const existing = this._map.get(key)
    return existing === undefined ? undefined : existing.value
  }

  * [Symbol.iterator] () {
    for (const [key, { value }] of this._map) {
      yield [key, value]
    }
  }

  keys () {
    return this._map.keys()
  }

  * values () {
    for (const { value } of this._map.values()) {
      yield value
    }
  }

  clear () {
    // The entries in map linger on in _array,
    // so on top of clearing the map, we also kill the ref,
    // so that any gc running later on the old map won't interfere
    // (in case a new entry was added with the same key as a cleared entry)

    this._map.clear()
    this._map = new Map()
  }

  destroy () {
    this._map = null
    this._array = null
  }

  _gc () {
    this._delete(Math.floor(Math.random() * this._array.length))
  }

  _delete (index) { // ~constant time
    if (index >= this._array.length) throw new Error('Cannot delete unused index (logic bug?)')

    const head = this._array.pop()
    let removed = head

    if (index < this._array.length) {
      removed = this._array[index]
      head.index = index
      this._array[index] = head
    }

    removed.map.delete(removed.key)
  }
}

module.exports = Rache
{
  "name": "rache",
  "version": "1.0.0",
  "description": "Random-eviction cache",
  "main": "index.js",
  "scripts": {
    "test": "standard && brittle test.js --coverage"
  },
  "repository": {
    "type": "git",
    "url": "git+https://github.com/holepunchto/rache.git"
  },
  "author": "Holepunch",
  "license": "Apache-2.0",
  "bugs": {
    "url": "https://github.com/holepunchto/rache/issues"
  },
  "homepage": "https://github.com/holepunchto/rache#readme",
  "files": [
    "index.js"
  ],
  "dependencies": {},
  "devDependencies": {
    "brittle": "^3.5.2",
    "standard": "^17.1.0"
  }
}
module.exports = class RandomArrayIterator {
  constructor (values) {
    this.values = values
    this.start = 0
    this.length = this.values.length
  }

  next () {
    if (this.length === 0) {
      if (this.start === 0) return { done: true, value: undefined }
      this.length = this.start
      this.start = 0
    }

    const i = this.start + ((Math.random() * this.length) | 0)
    const j = this.start + --this.length
    const value = this.values[i]

    this.values[i] = this.values[j]
    this.values[j] = value

    return { done: false, value }
  }

  dequeue () {
    this.values[this.start + this.length] = this.values[this.values.length - 1]
    this.values.pop()
  }

  requeue () {
    const i = this.start + this.length
    const value = this.values[i]
    this.values[i] = this.values[this.start]
    this.values[this.start++] = value
  }

  restart () {
    this.start = 0
    this.length = this.values.length
    return this
  }

  [Symbol.iterator] () {
    return this
  }
}
{
  "name": "random-array-iterator",
  "version": "1.0.0",
  "description": "An iterator to iterate an array in random order with controls to requeue or dequeue elements during the iteration",
  "main": "index.js",
  "dependencies": {},
  "devDependencies": {
    "standard": "^16.0.3",
    "tape": "^5.0.1"
  },
  "scripts": {
    "test": "standard && tape test.js"
  },
  "repository": {
    "type": "git",
    "url": "https://github.com/mafintosh/random-array-iterator.git"
  },
  "author": "Mathias Buus (@mafintosh)",
  "license": "MIT",
  "bugs": {
    "url": "https://github.com/mafintosh/random-array-iterator/issues"
  },
  "homepage": "https://github.com/mafintosh/random-array-iterator"
}
const EventEmitter = require('events')

module.exports = class ReadyResource extends EventEmitter {
  constructor () {
    super()

    this.opening = null
    this.closing = null

    this.opened = false
    this.closed = false
  }

  ready () {
    if (this.opening !== null) return this.opening
    this.opening = open(this)
    return this.opening
  }

  close () {
    if (this.closing !== null) return this.closing
    this.closing = close(this)
    return this.closing
  }

  async _open () {
    // add impl here
  }

  async _close () {
    // add impl here
  }
}

async function open (self) {
  // open after close
  if (self.closing !== null) return

  try {
    await self._open()
  } catch (err) {
    self.close() // safe to run in bg
    throw err
  }

  self.opened = true
  self.emit('ready')
}

async function close (self) {
  try {
    if (self.opened === false && self.opening !== null) await self.opening
  } catch {
    // ignore errors on closing
  }
  if (self.opened === true || self.opening === null) await self._close()
  self.closed = true
  self.emit('close')
}
{
  "name": "ready-resource",
  "version": "1.2.0",
  "description": "Modern single resource management",
  "main": "index.js",
  "imports": {
    "events": {
      "bare": "bare-events",
      "default": "events"
    }
  },
  "types": "index.d.ts",
  "files": [
    "index.js",
    "index.d.ts"
  ],
  "scripts": {
    "test": "standard && brittle test.js"
  },
  "devDependencies": {
    "brittle": "^3.1.0",
    "standard": "^17.0.0"
  },
  "repository": {
    "type": "git",
    "url": "git+https://github.com/holepunchto/ready-resource.git"
  },
  "author": "Mathias Buus (@mafintosh)",
  "license": "MIT",
  "bugs": {
    "url": "https://github.com/holepunchto/ready-resource/issues"
  },
  "homepage": "https://github.com/holepunchto/ready-resource",
  "dependencies": {
    "bare-events": "^2.2.0"
  }
}
const b4a = require('b4a')

var EMPTY = []

module.exports = RecordCache

function RecordSet () {
  this.list = []
  this.map = new Map()
}

RecordSet.prototype.add = function (record, value) {
  var k = toString(record)
  var r = this.map.get(k)
  if (r) return false

  r = {index: this.list.length, record: value || record}
  this.list.push(r)
  this.map.set(k, r)
  return true
}

RecordSet.prototype.remove = function (record) {
  var k = toString(record)
  var r = this.map.get(k)
  if (!r) return false

  swap(this.list, r.index, this.list.length - 1)
  this.list.pop()
  this.map.delete(k)
  return true
}

function RecordStore () {
  this.records = new Map()
  this.size = 0
}

RecordStore.prototype.add = function (name, record, value) {
  var r = this.records.get(name)

  if (!r) {
    r = new RecordSet()
    this.records.set(name, r)
  }

  if (r.add(record, value)) {
    this.size++
    return true
  }

  return false
}

RecordStore.prototype.remove = function (name, record, value) {
  var r = this.records.get(name)
  if (!r) return false

  if (r.remove(record, value)) {
    this.size--
    if (!r.map.size) this.records.delete(name)
    return true
  }

  return false
}

RecordStore.prototype.get = function (name) {
  var r = this.records.get(name)
  return r ? r.list : EMPTY
}

function RecordCache (opts) {
  if (!(this instanceof RecordCache)) return new RecordCache(opts)
  if (!opts) opts = {}

  this.maxSize = opts.maxSize || Infinity
  this.maxAge = opts.maxAge || 0

  this._onstale = opts.onStale || opts.onstale || null
  this._fresh = new RecordStore()
  this._stale = new RecordStore()
  this._interval = null
  this._gced = false

  if (this.maxAge && this.maxAge < Infinity) {
    // 2/3 gives us a span of 0.66-1.33 maxAge or avg maxAge
    var tick = Math.ceil(2 / 3 * this.maxAge)
    this._interval = setInterval(this._gcAuto.bind(this), tick)
    if (this._interval.unref) this._interval.unref()
  }
}

Object.defineProperty(RecordCache.prototype, 'size', {
  get: function () {
    return this._fresh.size + this._stale.size
  }
})

RecordCache.prototype.add = function (name, record, value) {
  this._stale.remove(name, record, value)
  if (this._fresh.add(name, record, value) && this._fresh.size > this.maxSize) {
    this._gc()
  }
}

RecordCache.prototype.remove = function (name, record, value) {
  this._fresh.remove(name, record, value)
  this._stale.remove(name, record, value)
}

RecordCache.prototype.get = function (name, n) {
  var a = this._fresh.get(name)
  var b = this._stale.get(name)
  var aLen = a.length
  var bLen = b.length
  var len = aLen + bLen

  if (n > len || !n) n = len
  var result = new Array(n)

  for (var i = 0; i < n; i++) {
    var j = Math.floor(Math.random() * (aLen + bLen))
    if (j < aLen) {
      result[i] = a[j].record
      swap(a, j, --aLen)
    } else {
      j -= aLen
      result[i] = b[j].record
      swap(b, j, --bLen)
    }
  }

  return result
}

RecordCache.prototype._gcAuto = function () {
  if (!this._gced) this._gc()
  this._gced = false
}

RecordCache.prototype._gc = function () {
  if (this._onstale && this._stale.size > 0) this._onstale(this._stale)
  this._stale = this._fresh
  this._fresh = new RecordStore()
  this._gced = true
}

RecordCache.prototype.clear = function () {
  this._gc()
  this._gc()
}

RecordCache.prototype.destroy = function () {
  this.clear()
  clearInterval(this._interval)
  this._interval = null
}

function toString (record) {
  return b4a.isBuffer(record) ? b4a.toString(record, 'hex') : record
}

function swap (list, a, b) {
  var tmp = list[a]
  tmp.index = b
  list[b].index = a
  list[a] = list[b]
  list[b] = tmp
}
{
  "name": "record-cache",
  "version": "1.2.0",
  "description": "Cache optimised for record like things",
  "main": "index.js",
  "dependencies": {
    "b4a": "^1.3.1"
  },
  "devDependencies": {
    "standard": "^10.0.3",
    "tape": "^4.8.0"
  },
  "scripts": {
    "test": "standard && tape test.js"
  },
  "repository": {
    "type": "git",
    "url": "https://github.com/mafintosh/record-cache.git"
  },
  "author": "Mathias Buus (@mafintosh)",
  "license": "MIT",
  "bugs": {
    "url": "https://github.com/mafintosh/record-cache/issues"
  },
  "homepage": "https://github.com/mafintosh/record-cache"
}
module.exports = class RefCounter {
  constructor() {
    this.count = 0

    this._onidle = null
    this._idle = null
  }

  isIdle() {
    return this.count === 0
  }

  idle() {
    if (this.count === 0) return Promise.resolve()
    if (this._idle !== null) return this._idle

    this._idle = new Promise((resolve) => {
      this._onidle = resolve
    })

    return this._idle
  }

  inc() {
    this.count++
  }

  dec() {
    if (--this.count > 0) return

    if (this._onidle !== null) {
      const resolve = this._onidle
      this._idle = null
      this._onidle = null
      resolve()
    }
  }
}
{
  "name": "refcounter",
  "version": "1.0.0",
  "description": "Simple refcounter",
  "main": "index.js",
  "dependencies": {},
  "devDependencies": {},
  "repository": {
    "type": "git",
    "url": "https://github.com/holepunchto/refcounter.git"
  },
  "author": "Holepunch Inc",
  "license": "Apache-2.0",
  "bugs": {
    "url": "https://github.com/holepunchto/refcounter/issues"
  },
  "homepage": "https://github.com/holepunchto/refcounter"
}
module.exports = require.addon.bind(require)
{
  "name": "require-addon",
  "version": "1.2.0",
  "description": "Import native addons across JavaScript runtimes",
  "exports": {
    "./package": "./package.json",
    ".": {
      "bare": "./lib/bare.js",
      "node": "./lib/node.js",
      "default": "./lib/default.js"
    }
  },
  "imports": {
    "fs": {
      "bare": "bare-fs",
      "default": "fs"
    },
    "path": {
      "bare": "bare-path",
      "default": "path"
    },
    "url": {
      "bare": "bare-url",
      "default": "url"
    }
  },
  "files": [
    "lib"
  ],
  "scripts": {
    "test": "npm run lint && npm run test:bare && npm run test:node",
    "test:bare": "bare test.js",
    "test:node": "node test.js",
    "lint": "prettier --check .",
    "format": "prettier --write ."
  },
  "repository": {
    "type": "git",
    "url": "git+https://github.com/holepunchto/require-addon.git"
  },
  "author": "Holepunch",
  "license": "Apache-2.0",
  "bugs": {
    "url": "https://github.com/holepunchto/require-addon/issues"
  },
  "homepage": "https://github.com/holepunchto/require-addon#readme",
  "engines": {
    "bare": ">=1.10.0"
  },
  "dependencies": {
    "bare-addon-resolve": "^1.3.0"
  },
  "devDependencies": {
    "bare-bundle": "^1.8.1",
    "bare-bundle-evaluate": "^1.1.0",
    "bare-fs": "^4.0.0",
    "bare-path": "^3.0.0",
    "bare-url": "^2.1.0",
    "brittle": "^3.7.0",
    "prettier": "^3.6.2",
    "prettier-config-holepunch": "^2.0.0"
  }
}
let tmpResolve = null
let tmpReject = null

if (Promise.withResolvers) {
  module.exports = Promise.withResolvers.bind(Promise)
} else {
  module.exports = function resolveRejectPromise () {
    const promise = new Promise(setTmp)
    const result = { promise, resolve: tmpResolve, reject: tmpReject }
    tmpResolve = tmpReject = null
    return result
  }
}

function setTmp (resolve, reject) {
  tmpResolve = resolve
  tmpReject = reject
}
{
  "name": "resolve-reject-promise",
  "version": "1.1.0",
  "description": "Create an inverted promise with no function allocs",
  "main": "index.js",
  "scripts": {
    "test": "standard"
  },
  "devDependencies": {
    "standard": "^17.1.2"
  },
  "repository": {
    "type": "git",
    "url": "https://github.com/mafintosh/resolve-reject-promise.git"
  },
  "author": "Mathias Buus (@mafintosh)",
  "license": "MIT",
  "bugs": {
    "url": "https://github.com/mafintosh/resolve-reject-promise/issues"
  },
  "homepage": "https://github.com/mafintosh/resolve-reject-promise"
}
const resources = new Map()

exports.add = function(resource, teardown) {
  resources.set(resource, teardown)
}

exports.remove = function(resource) {
  resources.delete(resource)
}

if (global.Bare) global.Bare.on('exit', run)
else global.process.on('exit', run)

function run() {
  for (const [resource, teardown] of resources) {
    teardown(resource)
  }
}
{
  "name": "resource-on-exit",
  "version": "1.0.0",
  "description": "Register sync teardown handlers for a resource",
  "main": "index.js",
  "dependencies": {},
  "devDependencies": {},
  "repository": {
    "type": "git",
    "url": "https://github.com/holepunchto/bare-teardown.git"
  },
  "author": "Holepunch Inc.",
  "license": "Apache-2.0",
  "bugs": {
    "url": "https://github.com/holepunchto/bare-teardown/issues"
  },
  "homepage": "https://github.com/holepunchto/bare-teardown"
}
require.addon = require('require-addon')

module.exports = require.addon('.', __filename)
const ColumnFamily = require('./lib/column-family')
const Iterator = require('./lib/iterator')
const Snapshot = require('./lib/snapshot')
const State = require('./lib/state')
const { BloomFilterPolicy, RibbonFilterPolicy } = require('./lib/filter-policy')
const constants = require('./lib/constants')

class RocksDB {
  constructor(path, opts = {}) {
    const {
      columnFamily,
      state = new State(this, path, opts),
      snapshot = null,
      keyEncoding = null,
      valueEncoding = null
    } = opts

    this._state = state
    this._snapshot = snapshot
    this._columnFamily = state.getColumnFamily(columnFamily)
    this._keyEncoding = keyEncoding
    this._valueEncoding = valueEncoding
    this._index = -1

    this._state.addSession(this)
  }

  get opened() {
    return this._state.opened
  }

  get closed() {
    return this.isRoot() ? this._state.closed : this._index === -1
  }

  get path() {
    return this._state.path
  }

  get snapshotted() {
    return this._snapshot !== null
  }

  get defaultColumnFamily() {
    return this._columnFamily
  }

  session({
    columnFamily = this._columnFamily,
    snapshot = this._snapshot !== null,
    keyEncoding = this._keyEncoding,
    valueEncoding = this._valueEncoding
  } = {}) {
    maybeClosed(this)

    return new RocksDB(null, {
      state: this._state,
      columnFamily,
      snapshot: snapshot ? this._snapshot || new Snapshot(this._state) : null,
      keyEncoding,
      valueEncoding
    })
  }

  columnFamily(name, opts) {
    return this.session({ ...opts, columnFamily: name })
  }

  snapshot() {
    return this.session({ snapshot: true })
  }

  isRoot() {
    return this === this._state.db
  }

  ready() {
    return this._state.ready()
  }

  async close({ force } = {}) {
    if (!this._state.opened) await this._state.ready()

    if (this._index !== -1) this._state.removeSession(this)

    if (force) {
      while (this._state.sessions.length > 0) {
        await this._state.sessions[this._state.sessions.length - 1].close()
      }
    }

    return this.isRoot() ? this._state.close() : Promise.resolve()
  }

  suspend() {
    maybeClosed(this)

    return this._state.suspend()
  }

  resume() {
    maybeClosed(this)

    return this._state.resume()
  }

  isIdle() {
    return this._state.handles.isIdle()
  }

  idle() {
    return this._state.handles.idle()
  }

  iterator(range, opts) {
    maybeClosed(this)

    return new Iterator(this, { ...range, ...opts })
  }

  async *keys(range, opts) {
    for await (const { key } of this.iterator(range, {
      ...opts,
      values: false
    })) {
      yield key
    }
  }

  async peek(range, opts) {
    for await (const value of this.iterator({ ...range, ...opts, limit: 1 })) {
      return value
    }

    return null
  }

  read(opts) {
    maybeClosed(this)

    return this._state.createReadBatch(this, opts)
  }

  write(opts) {
    maybeClosed(this)

    return this._state.createWriteBatch(this, opts)
  }

  flush(opts) {
    maybeClosed(this)

    return this._state.flush(this, opts)
  }

  async get(key, opts) {
    const batch = this.read({ ...opts, capacity: 1, autoDestroy: true })
    const value = batch.get(key)
    batch.tryFlush()
    return value
  }

  async put(key, value, opts) {
    const batch = this.write({ ...opts, capacity: 1, autoDestroy: true })
    batch.tryPut(key, value)
    await batch.flush()
  }

  async delete(key, opts) {
    const batch = this.write({ ...opts, capacity: 1, autoDestroy: true })
    batch.tryDelete(key)
    await batch.flush()
  }

  async deleteRange(start, end, opts) {
    const batch = this.write({ ...opts, capacity: 1, autoDestroy: true })
    batch.tryDeleteRange(start, end)
    await batch.flush()
  }

  async compactRange(start = null, end = null, opts = {}) {
    if (typeof end === 'object' && end !== null) {
      opts = end
      end = null
    } else if (typeof start === 'object' && start !== null) {
      opts = start
      start = null
      end = null
    }

    await this._state.compactRange(this, start, end, opts)
  }

  async approximateSize(start, end, opts = {}) {
    return this._state.approximateSize(this, start, end, opts)
  }

  _ref() {
    if (this._snapshot) this._snapshot.ref()
    this._state.handles.inc()
  }

  _unref() {
    if (this._snapshot) this._snapshot.unref()
    this._state.handles.dec()
  }
}

module.exports = exports = RocksDB

exports.constants = constants

exports.ColumnFamily = ColumnFamily
exports.BloomFilterPolicy = BloomFilterPolicy
exports.RibbonFilterPolicy = RibbonFilterPolicy

function maybeClosed(db) {
  if (db._state.closing || db._index === -1) throw new Error('RocksDB session is closed')
}
const c = require('compact-encoding')
const binding = require('../binding')

const empty = Buffer.alloc(0)
const resolved = Promise.resolve()

class RocksDBBatch {
  constructor(db, opts = {}) {
    const { capacity = 8, autoDestroy = false } = opts

    db._ref()

    this._db = db
    this._destroyed = false
    this._capacity = capacity
    this._operations = []
    this._promises = []

    this._enqueuePromise = this._enqueuePromise.bind(this)

    this._request = null
    this._resolve = null
    this._reject = null

    this._handle = null
    this._buffer = null
    this._autoDestroy = autoDestroy

    if (db._state.opened === true) this.ready()
  }

  _reuse(db, opts = {}) {
    const { autoDestroy = false } = opts

    db._ref()

    this._db = db
    this._destroyed = false
    this._autoDestroy = autoDestroy
  }

  _onfinished(err) {
    const resolve = this._resolve
    const reject = this._reject

    if (this._request) this._db._state.io.dec()

    this._operations = []
    this._promises = []
    this._request = null
    this._resolve = null
    this._reject = null

    if (this._autoDestroy === true) this.destroy()

    if (reject !== null && err) reject(err)
    else if (resolve !== null) resolve()
  }

  _resize() {
    if (this._operations.length <= this._capacity) return false

    while (this._operations.length > this._capacity) {
      this._capacity *= 2
    }

    return true
  }

  async ready() {
    if (this._handle !== null) return

    if (this._db._state.opened === false) await this._db._state.ready()

    this._init()
  }

  destroy() {
    if (this._request) throw new Error('Request in progress')
    if (this._destroyed) return

    this._destroyed = true

    if (this._promises.length) this._abort()

    this._db._unref()
    this._onfree()
  }

  _onfree() {
    this._db._state.freeBatch(this, false)
    this._db = null
  }

  _abort() {
    for (let i = 0; i < this._promises.length; i++) {
      const promise = this._promises[i]
      if (promise !== null) promise.reject(new Error('Batch is destroyed'))
    }

    this._onfinished(new Error('Batch is destroyed'))
  }

  async flush() {
    if (this._request) throw new Error('Request in progress')
    if (this._destroyed) throw new Error('Batch is destroyed')

    this._request = new Promise((resolve, reject) => {
      this._resolve = resolve
      this._reject = reject
    })

    this._flush()

    return this._request
  }

  tryFlush() {
    if (this._request) throw new Error('Request in progress')
    if (this._destroyed) throw new Error('Batch is destroyed')

    this._request = resolved

    this._flush()
  }

  async _flush() {
    if (this._handle === null) await this.ready()

    this._db._state.io.inc()

    if (this._db._state.resumed !== null) {
      const resumed = await this._db._state.resumed.promise

      if (!resumed) {
        if (this._destroyed) {
          this._db._state.io.dec()
        } else {
          this._destroyed = true
          this._abort()
          this._db._unref()
        }
      }
    }
  }

  _enqueuePromise(resolve, reject) {
    this._promises.push({ resolve, reject })
  }

  _encodeKey(k) {
    if (this._db._keyEncoding) return c.encode(this._db._keyEncoding, k)
    if (typeof k === 'string') return Buffer.from(k)
    return k
  }

  _encodeValue(v) {
    if (this._db._valueEncoding) return c.encode(this._db._valueEncoding, v)
    if (v === null) return empty
    if (typeof v === 'string') return Buffer.from(v)
    return v
  }

  _decodeValue(b) {
    if (this._db._valueEncoding) return c.decode(this._db._valueEncoding, b)
    return b
  }
}

exports.ReadBatch = class RocksDBReadBatch extends RocksDBBatch {
  constructor(db, opts = {}) {
    super(db, opts)

    const { asyncIO = false, fillCache = true } = opts

    this._asyncIO = asyncIO
    this._fillCache = fillCache
  }

  _init() {
    this._handle = binding.readInit()
    this._buffer = binding.readBuffer(this._handle, this._capacity)
  }

  _resize() {
    if (super._resize() && this._handle !== null) {
      this._buffer = binding.readBuffer(this._handle, this._capacity)
    }
  }

  async _flush() {
    await super._flush()

    if (this._destroyed) return

    try {
      binding.read(
        this._db._state._handle,
        this._handle,
        this._operations,
        this._db._snapshot ? this._db._snapshot._handle : undefined,
        this._asyncIO,
        this._fillCache,
        this,
        this._onread
      )
    } catch (err) {
      this._db._state.io.dec()
      throw err
    }
  }

  _onread(errs, values) {
    let applied = true

    for (let i = 0, n = this._promises.length; i < n; i++) {
      const err = errs[i]
      if (err) applied = false

      const promise = this._promises[i]
      if (promise === null) continue

      if (err) promise.reject(new Error(err))
      else promise.resolve(values[i] ? this._decodeValue(Buffer.from(values[i])) : null)
    }

    this._onfinished(applied ? null : new Error('Batch was not applied'))
  }

  get(key) {
    if (this._request) throw new Error('Request already in progress')

    const promise = new Promise(this._enqueuePromise)

    this._operations.push(new RocksDBGet(this._encodeKey(key), this._db._columnFamily))

    this._resize()

    return promise
  }
}

exports.WriteBatch = class RocksDBWriteBatch extends RocksDBBatch {
  _init() {
    this._handle = binding.writeInit()
    this._buffer = binding.writeBuffer(this._handle, this._capacity)
  }

  _resize() {
    if (super._resize() && this._handle !== null) {
      this._buffer = binding.writeBuffer(this._handle, this._capacity)
    }
  }

  _onfree() {
    this._db._state.freeBatch(this, true)
  }

  async _flush() {
    await super._flush()

    if (this._destroyed) return

    try {
      binding.write(this._db._state._handle, this._handle, this._operations, this, this._onwrite)
    } catch (err) {
      this._db._state.io.dec()
      throw err
    }
  }

  _onwrite(err) {
    const applied = !err

    for (let i = 0, n = this._promises.length; i < n; i++) {
      const promise = this._promises[i]
      if (promise === null) continue

      if (err) promise.reject(new Error(err))
      else promise.resolve()
    }

    this._onfinished(applied ? null : new Error('Batch was not applied'))
  }

  put(key, value) {
    if (this._request) throw new Error('Request already in progress')

    const promise = new Promise(this._enqueuePromise)

    this._operations.push(
      new RocksDBPut(this._encodeKey(key), this._encodeValue(value), this._db._columnFamily)
    )

    this._resize()

    return promise
  }

  tryPut(key, value) {
    if (this._request) throw new Error('Request already in progress')

    this._operations.push(
      new RocksDBPut(this._encodeKey(key), this._encodeValue(value), this._db._columnFamily)
    )

    this._promises.push(null)

    this._resize()
  }

  delete(key) {
    if (this._request) throw new Error('Request already in progress')

    const promise = new Promise(this._enqueuePromise)

    this._operations.push(new RocksDBDelete(this._encodeKey(key), this._db._columnFamily))

    this._resize()

    return promise
  }

  tryDelete(key) {
    if (this._request) throw new Error('Request already in progress')

    this._operations.push(new RocksDBDelete(this._encodeKey(key), this._db._columnFamily))

    this._promises.push(null)

    this._resize()
  }

  deleteRange(start, end) {
    if (this._request) throw new Error('Request already in progress')

    const promise = new Promise(this._enqueuePromise)

    this._operations.push(
      new RocksDBDeleteRange(this._encodeKey(start), this._encodeKey(end), this._db._columnFamily)
    )

    this._resize()

    return promise
  }

  tryDeleteRange(start, end) {
    if (this._request) throw new Error('Request already in progress')

    this._operations.push(
      new RocksDBDeleteRange(this._encodeKey(start), this._encodeKey(end), this._db._columnFamily)
    )

    this._promises.push(null)

    this._resize()
  }
}

class RocksDBGet {
  constructor(key, columnFamily) {
    this.key = key
    this.columnFamily = columnFamily._handle
  }

  get type() {
    return binding.GET
  }
}

class RocksDBPut {
  constructor(key, value, columnFamily) {
    this.key = key
    this.value = value
    this.columnFamily = columnFamily._handle
  }

  get type() {
    return binding.PUT
  }
}

class RocksDBDelete {
  constructor(key, columnFamily) {
    this.key = key
    this.columnFamily = columnFamily._handle
  }

  get type() {
    return binding.DELETE
  }
}

class RocksDBDeleteRange {
  constructor(start, end, columnFamily) {
    this.start = start
    this.end = end
    this.columnFamily = columnFamily._handle
  }

  get type() {
    return binding.DELETE_RANGE
  }
}
const binding = require('../binding')
const constants = require('./constants')
const { BloomFilterPolicy } = require('./filter-policy')

class RocksDBColumnFamily {
  constructor(name, opts = {}) {
    const {
      // Blob options
      enableBlobFiles = false,
      minBlobSize = 0,
      blobFileSize = 0,
      enableBlobGarbageCollection = true,
      // Block table options
      tableBlockSize = 8192,
      tableCacheIndexAndFilterBlocks = true,
      tableFormatVersion = 6,
      optimizeFiltersForMemory = false,
      blockCache = true,
      filterPolicy = new BloomFilterPolicy(10),
      topLevelIndexPinningTier = constants.pinningTier.ALL,
      partitionPinningTier = constants.pinningTier.ALL,
      unpartitionedPinningTier = constants.pinningTier.ALL,
      optimizeFiltersForHits = false,
      numLevels = 7,
      maxWriteBufferNumber = 2
    } = opts

    this._name = name
    this._flushing = null
    this._options = {
      enableBlobFiles,
      minBlobSize,
      blobFileSize,
      enableBlobGarbageCollection,
      tableBlockSize,
      tableCacheIndexAndFilterBlocks,
      tableFormatVersion,
      optimizeFiltersForMemory,
      blockCache,
      filterPolicy,
      topLevelIndexPinningTier,
      partitionPinningTier,
      unpartitionedPinningTier,
      optimizeFiltersForHits,
      numLevels,
      maxWriteBufferNumber
    }

    const filterPolicyArguments = [0, 0, 0]

    if (filterPolicy !== null) {
      filterPolicyArguments[0] = filterPolicy.type

      switch (filterPolicy.type) {
        case 1: // Bloom filter policy
          filterPolicyArguments[1] = filterPolicy.bitsPerKey
          break
        case 2: // Ribbon filter policy
          filterPolicyArguments[1] = filterPolicy.bloomEquivalentBitsPerKey
          filterPolicyArguments[2] = filterPolicy.bloomBeforeLevel

          break
      }
    }

    this._handle = binding.columnFamilyInit(
      name,
      enableBlobFiles,
      minBlobSize,
      blobFileSize,
      enableBlobGarbageCollection,
      tableBlockSize,
      tableCacheIndexAndFilterBlocks,
      tableFormatVersion,
      optimizeFiltersForMemory,
      blockCache === false,
      ...filterPolicyArguments,
      topLevelIndexPinningTier,
      partitionPinningTier,
      unpartitionedPinningTier,
      optimizeFiltersForHits,
      numLevels,
      maxWriteBufferNumber
    )
  }

  cloneSettings(name) {
    return new RocksDBColumnFamily(name, this._options)
  }

  get name() {
    return this._name
  }

  destroy() {
    if (this._handle === null) return

    binding.columnFamilyDestroy(this._handle)

    this._handle = null
  }
}

module.exports = RocksDBColumnFamily
module.exports = {
  pinningTier: {
    NONE: 0,
    FLUSHED_AND_SIMILAR: 1,
    ALL: 2
  }
}
exports.BloomFilterPolicy = class RocksDBBloomFilterPolicy {
  get type() {
    return 1
  }

  constructor(bitsPerKey) {
    this.bitsPerKey = bitsPerKey
  }
}

exports.RibbonFilterPolicy = class RocksDBRibbonFilterPolicy {
  get type() {
    return 2
  }

  constructor(bloomEquivalentBitsPerKey, bloomBeforeLevel = 0) {
    this.bloomEquivalentBitsPerKey = bloomEquivalentBitsPerKey
    this.bloomBeforeLevel = bloomBeforeLevel
  }
}
const { Readable } = require('streamx')
const c = require('compact-encoding')
const binding = require('../binding')

const empty = Buffer.alloc(0)

module.exports = class RocksDBIterator extends Readable {
  constructor(db, opts = {}) {
    const {
      gt = null,
      gte = null,
      lt = null,
      lte = null,
      reverse = false,
      values = true,
      limit = Infinity,
      capacity = 8
    } = opts

    super()

    db._ref()

    this._db = db

    this._gt = gt ? this._encodeKey(gt) : empty
    this._gte = gte ? this._encodeKey(gte) : empty
    this._lt = lt ? this._encodeKey(lt) : empty
    this._lte = lte ? this._encodeKey(lte) : empty

    this._reverse = reverse
    this._values = values
    this._limit = limit < 0 ? Infinity : limit
    this._capacity = capacity
    this._opened = false

    this._pendingOpen = null
    this._pendingRead = null
    this._pendingDestroy = null

    this._buffer = null
    this._handle = null

    if (this._db._state.opened === true) this.ready()
  }

  _onopen(err) {
    const cb = this._pendingOpen
    this._pendingOpen = null
    this._opened = true
    this._db._state.io.dec()
    cb(err)
  }

  _onread(err, keys, values) {
    const cb = this._pendingRead
    this._pendingRead = null
    this._db._state.io.dec()
    if (err) return cb(err)

    const n = keys.length

    this._limit -= n

    for (let i = 0; i < n; i++) {
      this.push({
        key: this._decodeKey(Buffer.from(keys[i])),
        value: this._values ? this._decodeValue(Buffer.from(values[i])) : null
      })
    }

    if (n < this._capacity) this.push(null)

    cb(null)
  }

  _onclose(err) {
    const cb = this._pendingDestroy
    this._pendingDestroy = null
    this._db._state.io.dec()
    this._db._unref()
    cb(err)
  }

  _resize() {
    if (this._handle !== null) {
      this._buffer = binding.iteratorBuffer(this._handle, this._capacity)
    }
  }

  async ready() {
    if (this._handle !== null) return

    if (this._db._state.opened === false) await this._db._state.ready()

    this._init()
  }

  _init() {
    this._handle = binding.iteratorInit()
    this._buffer = binding.iteratorBuffer(this._handle, this._capacity)
  }

  async _open(cb) {
    await this.ready()

    this._db._state.io.inc()

    if (this._db._state.resumed !== null) {
      const resumed = await this._db._state.resumed.promise

      if (!resumed) {
        this._db._state.io.dec()

        return cb(new Error('RocksDB session is closed'))
      }
    }

    this._pendingOpen = cb

    try {
      binding.iteratorOpen(
        this._db._state._handle,
        this._handle,
        this._db._columnFamily._handle,
        this._gt,
        this._gte,
        this._lt,
        this._lte,
        this._reverse,
        !this._values, // Keys only
        this._db._snapshot ? this._db._snapshot._handle : undefined,
        this,
        this._onopen,
        this._onclose,
        this._onread
      )
    } catch (err) {
      this._db._state.io.dec()

      cb(err)
    }
  }

  async _read(cb) {
    this._db._state.io.inc()

    if (this._db._state.resumed !== null) {
      const resumed = await this._db._state.resumed.promise

      if (!resumed) {
        this._db._state.io.dec()

        return cb(new Error('RocksDB session is closed'))
      }
    }

    this._pendingRead = cb

    try {
      binding.iteratorRead(this._handle, Math.min(this._capacity, this._limit))
    } catch (err) {
      this._db._state.io.dec()

      cb(err)
    }
  }

  async _destroy(cb) {
    await this.ready()

    this._db._state.io.inc()

    if (this._opened === false) {
      this._db._state.io.dec()
      this._db._unref()

      return cb(null)
    }

    this._pendingDestroy = cb

    try {
      binding.iteratorClose(this._handle)
    } catch (err) {
      this._db._state.io.dec()
      this._db._unref()

      cb(err)
    }
  }

  _encodeKey(k) {
    if (this._db._keyEncoding !== null) return c.encode(this._db._keyEncoding, k)
    if (typeof k === 'string') return Buffer.from(k)
    return k
  }

  _decodeKey(b) {
    if (this._db._keyEncoding !== null) return c.decode(this._db._keyEncoding, b)
    return b
  }

  _decodeValue(b) {
    if (this._db._valueEncoding !== null) return c.decode(this._db._valueEncoding, b)
    return b
  }
}
const binding = require('../binding')

module.exports = class RocksDBSnapshot {
  constructor(state) {
    this._state = state

    this._handle = null
    this._refs = 0

    if (state.deferSnapshotInit === false) this._init()
  }

  _init() {
    this._handle = binding.snapshotCreate(this._state._handle)
  }

  ref() {
    this._refs++
  }

  unref() {
    if (--this._refs > 0) return

    if (this._handle === null) return

    binding.snapshotDestroy(this._handle)

    this._handle = null
  }
}
const ReadyResource = require('ready-resource')
const RefCounter = require('refcounter')
const rrp = require('resolve-reject-promise')
const SignalPromise = require('signal-promise')
const c = require('compact-encoding')
const { ReadBatch, WriteBatch } = require('./batch')
const ColumnFamily = require('./column-family')
const binding = require('../binding')

const MAX_BATCH_REUSE = 64
const empty = Buffer.alloc(0)

module.exports = class RocksDBState extends ReadyResource {
  constructor(db, path, opts) {
    super()

    const {
      columnFamily = new ColumnFamily('default', opts),
      columnFamilies = [],
      readOnly = false,
      createIfMissing = true,
      createMissingColumnFamilies = true,
      maxBackgroundJobs = 6,
      bytesPerSync = 1048576,
      maxOpenFiles = -1,
      useDirectReads = false,
      avoidUnnecessaryBlockingIO = false,
      skipStatsUpdateOnOpen = false,
      useDirectIOForFlushAndCompaction = false,
      maxFileOpeningThreads = 16,
      lock = null
    } = opts

    this.path = path
    this.db = db
    this.handles = new RefCounter()
    this.io = new RefCounter()
    this.sessions = []
    this.columnFamilies = [columnFamily]
    this.deferSnapshotInit = true
    this.resumed = null

    this._suspended = false
    this._suspending = false
    this._updating = false
    this._updatingSignal = new SignalPromise()
    this._columnsFlushed = false
    this._lock = lock
    this._readBatches = []
    this._writeBatches = []

    for (const columnFamily of columnFamilies) {
      this.columnFamilies.push(
        typeof columnFamily === 'string' ? new ColumnFamily(columnFamily, opts) : columnFamily
      )
    }

    this._handle = binding.init(
      readOnly,
      createIfMissing,
      createMissingColumnFamilies,
      maxBackgroundJobs,
      bytesPerSync,
      maxOpenFiles,
      useDirectReads,
      avoidUnnecessaryBlockingIO,
      skipStatsUpdateOnOpen,
      useDirectIOForFlushAndCompaction,
      maxFileOpeningThreads
    )
  }

  createReadBatch(db, opts) {
    if (this._readBatches.length === 0) return new ReadBatch(db, opts)
    const batch = this._readBatches.pop()
    batch._reuse(db, opts)
    return batch
  }

  createWriteBatch(db, opts) {
    if (this._writeBatches.length === 0) return new WriteBatch(db, opts)
    const batch = this._writeBatches.pop()
    batch._reuse(db, opts)
    return batch
  }

  freeBatch(batch, writable) {
    if (batch._capacity > 16) return
    const queue = writable ? this._writeBatches : this._readBatches
    if (queue.length >= MAX_BATCH_REUSE) return
    queue.push(batch)
  }

  addSession(db) {
    db._index = this.sessions.push(db) - 1
    if (db._snapshot) db._snapshot.ref()
  }

  removeSession(db) {
    const head = this.sessions.pop()
    if (head !== db) this.sessions[(head._index = db._index)] = head
    db._index = -1
    if (db._snapshot) db._snapshot.unref()
  }

  upsertColumnFamily(c) {
    if (typeof c === 'string') {
      let col = this.getColumnFamilyByName(c)
      if (col) return col
      col = this.columnFamilies[0].cloneSettings(c)
      this.columnFamilies.push(col)
      return col
    }

    if (this.columnFamilies.includes(c)) return c
    this.columnFamilies.push(c)
    return c
  }

  getColumnFamily(c) {
    if (!c) return this.columnFamilies[0]
    if (!this._columnsFlushed) return this.upsertColumnFamily(c)

    if (typeof c !== 'string') return c

    const col = this.getColumnFamilyByName(c)
    if (col === null) throw new Error('Unknown column family')
    return col
  }

  getColumnFamilyByName(name) {
    for (const col of this.columnFamilies) {
      if (col.name === name) return col
    }
    return null
  }

  async _open() {
    await Promise.resolve() // Allow column families to populate if on-demand

    if (this._lock) await this._lock.ready()

    const req = { resolve: null, reject: null, handle: null }

    const promise = new Promise((resolve, reject) => {
      req.resolve = resolve
      req.reject = reject
    })

    this._columnsFlushed = true

    const lock = this._lock === null ? -1 : this._lock.transfer()

    req.handle = binding.open(
      this._handle,
      this,
      this.path,
      this.columnFamilies.map((c) => c._handle),
      lock,
      req,
      onopen
    )

    await promise

    this.deferSnapshotInit = false

    for (const session of this.sessions) {
      if (session._snapshot) session._snapshot._init()
    }

    function onopen(err) {
      if (err) req.reject(new Error(err))
      else req.resolve()
    }
  }

  async _close() {
    while (this._updating) await this._updatingSignal.wait()
    if (this.resumed) this.resumed.resolve(false)

    while (!this.io.isIdle()) await this.io.idle()
    while (!this.handles.isIdle()) await this.handles.idle()

    while (this.sessions.length > 0) {
      await this.sessions[this.sessions.length - 1].close()
    }

    for (const columnFamily of this.columnFamilies) columnFamily.destroy()

    const req = { resolve: null, reject: null, handle: null }

    const promise = new Promise((resolve, reject) => {
      req.resolve = resolve
      req.reject = reject
    })

    req.handle = binding.close(this._handle, req, onclose)

    try {
      await promise
    } finally {
      if (this._lock) await this._lock.close()
    }

    function onclose(err) {
      if (err) req.reject(new Error(err))
      else req.resolve()
    }
  }

  async flush(db, opts) {
    if (this.opened === false) await this.ready()

    this.io.inc()

    if (this.resumed !== null) {
      const resumed = await this.resumed.promise

      if (!resumed) {
        this.io.dec()

        throw new Error('RocksDB session is closed')
      }
    }

    const req = { resolve: null, reject: null, handle: null }

    const promise = new Promise((resolve, reject) => {
      req.resolve = resolve
      req.reject = reject
    })

    try {
      req.handle = binding.flush(this._handle, db._columnFamily._handle, req, onflush)

      await promise
    } finally {
      this.io.dec()
    }

    function onflush(err) {
      if (err) req.reject(new Error(err))
      else req.resolve()
    }
  }

  suspend() {
    this._suspending = true
    return this.update()
  }

  resume() {
    this._suspending = false
    return this.update()
  }

  async update() {
    while (this._updating) await this._updatingSignal.wait()
    if (this._suspending === this._suspended || this.closing) return
    this._updating = true
    try {
      if (this._suspending) await this._suspend()
      else await this._resume()
    } finally {
      this._updating = false
      this._updatingSignal.notify()
    }
  }

  async _suspend() {
    if (this._suspended === true) return

    while (!this.io.isIdle()) await this.io.idle()

    this.io.inc()
    this.resumed = rrp()

    const req = { resolve: null, reject: null, handle: null }

    const promise = new Promise((resolve, reject) => {
      req.resolve = resolve
      req.reject = reject
    })

    try {
      req.handle = binding.suspend(this._handle, req, onsuspend)

      await promise

      this._suspended = true
    } finally {
      this.io.dec()
    }

    function onsuspend(err) {
      if (err) req.reject(new Error(err))
      else req.resolve()
    }
  }

  async _resume() {
    if (this._suspended === false) return

    const req = { resolve: null, reject: null, handle: null }

    const promise = new Promise((resolve, reject) => {
      req.resolve = resolve
      req.reject = reject
    })

    req.handle = binding.resume(this._handle, req, onresume)

    await promise

    this._suspended = false

    const resumed = this.resumed
    this.resumed = null
    resumed.resolve(true)

    function onresume(err) {
      if (err) req.reject(new Error(err))
      else req.resolve()
    }
  }

  async compactRange(db, start, end, opts) {
    if (this.opened === false) await this.ready()

    this.io.inc()

    const { exclusive = false } = opts

    start = this._encodeKey(start)
    end = this._encodeKey(end)

    const req = { resolve: null, reject: null, handle: null, start, end }

    const promise = new Promise((resolve, reject) => {
      req.resolve = resolve
      req.reject = reject
    })

    try {
      req.handle = binding.compactRange(
        this._handle,
        db._columnFamily._handle,
        start,
        end,
        exclusive,
        req,
        oncompactrange
      )

      await promise
    } finally {
      this.io.dec()
    }

    function oncompactrange(err) {
      if (err) req.reject(new Error(err))
      else req.resolve()
    }
  }

  async approximateSize(db, start, end, opts) {
    if (this.opened === false) await this.ready()

    this.io.inc()

    const { includeMemtables = false, includeFiles = true, filesSizeErrorMargin = -1 } = opts

    start = this._encodeKey(start)
    end = this._encodeKey(end)

    const req = { resolve: null, reject: null, handle: null, start, end }

    const promise = new Promise((resolve, reject) => {
      req.resolve = resolve
      req.reject = reject
    })

    try {
      req.handle = binding.approximateSize(
        this._handle,
        db._columnFamily._handle,
        start,
        end,
        includeMemtables,
        includeFiles,
        filesSizeErrorMargin,
        req,
        onapproximatesize
      )

      return await promise
    } finally {
      this.io.dec()
    }

    function onapproximatesize(err, result) {
      if (err) req.reject(new Error(err))
      else req.resolve(result)
    }
  }

  _encodeKey(k) {
    if (this.db._keyEncoding) return c.encode(this.db._keyEncoding, k)
    if (typeof k === 'string') return Buffer.from(k)
    if (k === null) return empty
    return k
  }
}
{
  "name": "rocksdb-native",
  "version": "3.11.4",
  "description": "librocksdb bindings for JavaScript",
  "exports": {
    ".": "./index.js",
    "./package": "./package.json"
  },
  "imports": {
    "fs": "bare-fs",
    "default": "fs"
  },
  "files": [
    "index.js",
    "binding.cc",
    "binding.js",
    "CMakeLists.txt",
    "lib",
    "prebuilds",
    "!prebuilds/android-*/**/*.node"
  ],
  "addon": true,
  "scripts": {
    "format": "prettier --write .",
    "lint": "prettier --check .",
    "test": "npm run lint && npm run test:bare && npm run test:node",
    "test:bare": "bare test.js",
    "test:node": "node test.js"
  },
  "repository": {
    "type": "git",
    "url": "git+https://github.com/holepunchto/rocksdb-native.git"
  },
  "author": "Holepunch Inc",
  "license": "Apache-2.0",
  "bugs": {
    "url": "https://github.com/holepunchto/rocksdb-native/issues"
  },
  "homepage": "https://github.com/holepunchto/rocksdb-native",
  "engines": {
    "bare": ">=1.16.0"
  },
  "dependencies": {
    "compact-encoding": "^2.15.0",
    "ready-resource": "^1.0.0",
    "refcounter": "^1.0.0",
    "require-addon": "^1.0.2",
    "resolve-reject-promise": "^1.1.0",
    "signal-promise": "^1.0.3",
    "streamx": "^2.16.1"
  },
  "devDependencies": {
    "bare-compat-napi": "^1.3.0",
    "bare-fs": "^4.5.0",
    "brittle": "^3.5.0",
    "cmake-bare": "^1.1.14",
    "cmake-fetch": "^1.0.1",
    "cmake-napi": "^1.0.6",
    "fd-lock": "^2.0.0",
    "prettier": "^3.6.2",
    "prettier-config-holepunch": "^2.0.0"
  }
}
module.exports = safetyCatch

function isActuallyUncaught (err) {
  if (!err) return false
  return err instanceof TypeError ||
    err instanceof SyntaxError ||
    err instanceof ReferenceError ||
    err instanceof EvalError ||
    err instanceof RangeError ||
    err instanceof URIError ||
    err.code === 'ERR_ASSERTION'
}

function throwErrorNT (err) {
  queueMicrotask(() => { throw err })
}

function safetyCatch (err) {
  if (isActuallyUncaught(err)) {
    throwErrorNT(err)
    throw err
  }
}
{
  "name": "safety-catch",
  "version": "1.0.2",
  "description": "Small module that makes sure your catch, caught an actual error and not a programming mistake or assertion",
  "main": "index.js",
  "dependencies": {},
  "devDependencies": {},
  "repository": {
    "type": "git",
    "url": "https://github.com/mafintosh/safety-catch.git"
  },
  "author": "Mathias Buus (@mafintosh)",
  "license": "MIT",
  "bugs": {
    "url": "https://github.com/mafintosh/safety-catch/issues"
  },
  "homepage": "https://github.com/mafintosh/safety-catch"
}
let tmpResolve = null

module.exports = class ScopeLock {
  constructor ({ debounce = false } = {}) {
    this.debounce = debounce
    this.waiting = []
    this.locked = false
    this.skip = 0
    this.destroyed = false
  }

  flush () {
    if (this.locked === false && this.waiting.length === 0) return Promise.resolve(this.destroyed === false)

    const promise = new Promise(setTmpResolve)
    const resolve = tmpResolve

    tmpResolve = null
    this.waiting.push({ lock: false, resolve })

    return promise
  }

  destroy () {
    this.destroyed = true
  }

  lock () {
    const promise = new Promise(setTmpResolve)
    const resolve = tmpResolve

    tmpResolve = null

    if (this.locked === true) {
      this.waiting.push({ lock: true, resolve })
      return promise
    }

    if (this.destroyed === true) {
      resolve(false)
      return promise
    }

    this.locked = true
    resolve(true)

    return promise
  }

  unlock () {
    if (this.destroyed === true) {
      for (let i = 0; i < this.waiting.length; i++) {
        this.waiting[i].resolve(false)
      }
      this.waiting = []
      this.skip = 0
      this.locked = false
      return
    }

    if (this.skip !== 0) {
      for (let i = 0; i < this.skip; i++) {
        const { lock, resolve } = this.waiting[i]
        resolve(lock === false)
      }

      this.waiting = this.waiting.slice(this.skip)
      this.skip = 0
    }

    while (this.waiting.length > 0 && this.waiting[0].lock === false) {
      this.waiting.shift().resolve(true)
    }

    if (this.waiting.length === 0) {
      this.locked = false
      return
    }

    const { resolve } = this.waiting.shift()
    if (this.debounce === true) this.skip = this.waiting.length

    resolve(true)
  }
}

function setTmpResolve (resolve) {
  tmpResolve = resolve
}
{
  "name": "scope-lock",
  "version": "1.2.4",
  "description": "Some concurrency semantics around entering scopes",
  "main": "index.js",
  "files": [
    "index.js"
  ],
  "devDependencies": {
    "brittle": "^3.7.0",
    "standard": "^17.1.2"
  },
  "scripts": {
    "test": "standard && brittle test.js"
  },
  "repository": {
    "type": "git",
    "url": "https://github.com/holepunchto/scope-lock.git"
  },
  "author": "Holepunch Inc.",
  "license": "Apache-2.0",
  "bugs": {
    "url": "https://github.com/holepunchto/scope-lock/issues"
  },
  "homepage": "https://github.com/holepunchto/scope-lock"
}
const set = require('unordered-set')

module.exports = opts => new ShuffledPriorityQueue(opts)

class ShuffledPriorityQueue {
  constructor (opts) {
    this.priorities = []
    this.equals = (opts && opts.equals) || null
  }

  get length () {
    return this.priorities.reduce(add, 0)
  }

  [Symbol.iterator] () {
    return new Iterator(this)
  }

  head () {
    for (let i = this.priorities.length - 1; i >= 0; i--) {
      const q = this.priorities[i]
      if (q.length) return shuffle(q, 0)
    }
    return null
  }

  tail () {
    for (let i = 0; i < this.priorities.length; i++) {
      const q = this.priorities[i]
      if (q.length) return shuffle(q, 0)
    }
    return null
  }

  prev (prev) {
    if (!prev) return this.tail()
    return next(this.priorities, prev, 1)
  }

  next (prev) {
    if (!prev) return this.head()
    return next(this.priorities, prev, -1)
  }

  shift () {
    return this.remove(this.head())
  }

  pop () {
    return this.remove(this.tail())
  }

  add (val) {
    const prio = val.priority || 0
    while (prio >= this.priorities.length) this.priorities.push([])
    set.add(this.priorities[prio], val)
    return val
  }

  remove (val) {
    if (!val) return null

    if (val._index === undefined) {
      val = this.find(val)
      if (!val) return null
    }

    return set.remove(this.priorities[val.priority || 0], val)
  }

  has (val) {
    if (val._index === undefined) return this.find(val)
    const priority = val.priority || 0
    if (priority >= this.priorities.length) return false
    return set.has(this.priorities[priority], val)
  }

  find (val) {
    if (val._index !== undefined) return val

    const prio = val.priority || 0
    const qs = this.priorities
    if (prio >= qs.length) return null

    const q = qs[prio]

    for (let i = 0; i < q.length; i++) {
      if (this.equals(q[i], val)) return q[i]
    }

    return null
  }
}

class Iterator {
  constructor (queue) {
    this.prev = null
    this.queue = queue
  }

  next () {
    const next = this.queue.next(this.prev)
    this.prev = next
    return { done: !next, value: next }
  }
}

function shuffle (q, i) {
  const ran = i + Math.floor(Math.random() * (q.length - i))
  set.swap(q, q[ran], q[i])
  return q[i]
}

function next (queues, prev, inc) {
  let i = prev.priority || 0
  let j = (prev._index || 0) + 1

  while (true) {
    if (i < 0 || i >= queues.length) return null
    const q = queues[i]

    if (j >= q.length) {
      i += inc
      j = 0
      continue
    }

    return shuffle(q, j)
  }
}

function add (len, b) {
  return len + b.length
}
{
  "name": "shuffled-priority-queue",
  "version": "2.1.0",
  "description": "A priority queue that shuffles elements with the same priority.",
  "main": "index.js",
  "scripts": {
    "test": "standard && tape test.js"
  },
  "dependencies": {
    "unordered-set": "^2.0.1"
  },
  "devDependencies": {
    "standard": "^12.0.1",
    "tape": "^4.9.1"
  },
  "repository": {
    "type": "git",
    "url": "https://github.com/mafintosh/shuffled-priority-queue.git"
  },
  "author": "Mathias Buus (@mafintosh)",
  "license": "MIT",
  "bugs": {
    "url": "https://github.com/mafintosh/shuffled-priority-queue/issues"
  },
  "homepage": "https://github.com/mafintosh/shuffled-priority-queue"
}
module.exports = class Signal {
  constructor () {
    this._resolve = null
    this._reject = null
    this._promise = null
    this._bind = bind.bind(this)
    this._onerror = clear.bind(this)
    this._onsuccess = clear.bind(this, null)
    this._timers = new Set()
  }

  wait (max) {
    if (!this._promise) {
      this._promise = new Promise(this._bind)
      this._promise.then(this._onsuccess).catch(this._onerror)
    }
    if (max) return this._sleep(max)
    return this._promise
  }

  _sleep (max) {
    const s = new Promise((resolve, reject) => {
      const done = () => {
        this._timers.delete(state)
        resolve(true)
      }
      const id = setTimeout(done, max)
      const state = { id, resolve, reject }
      this._timers.add(state)
    })

    return s
  }

  notify (err) {
    if (!this._promise) return
    const resolve = this._resolve
    const reject = this._reject
    this._promise = null
    if (err) reject(err)
    else resolve(true)
  }
}

function clear (err) {
  for (const { id, resolve, reject } of this._timers) {
    clearTimeout(id)
    if (err) reject(err)
    else resolve(true)
  }
  this._timers.clear()
}

function bind (resolve, reject) {
  this._resolve = resolve
  this._reject = reject
}
{
  "name": "signal-promise",
  "version": "1.0.3",
  "description": "Simple wait/notify promise with optional max wait time",
  "main": "index.js",
  "dependencies": {},
  "devDependencies": {},
  "repository": {
    "type": "git",
    "url": "https://github.com/mafintosh/signal-promise.git"
  },
  "author": "Mathias Buus (@mafintosh)",
  "license": "MIT",
  "bugs": {
    "url": "https://github.com/mafintosh/signal-promise/issues"
  },
  "homepage": "https://github.com/mafintosh/signal-promise"
}
var varint = require('varint')
exports.encode = function encode (v, b, o) {
  v = v >= 0 ? v*2 : v*-2 - 1
  var r = varint.encode(v, b, o)
  encode.bytes = varint.encode.bytes
  return r
}
exports.decode = function decode (b, o) {
  var v = varint.decode(b, o)
  decode.bytes = varint.decode.bytes
  return v & 1 ? (v+1) / -2 : v / 2
}

exports.encodingLength = function (v) {
  return varint.encodingLength(v >= 0 ? v*2 : v*-2 - 1)
}
{
  "name": "signed-varint",
  "description": "efficiently store signed integers in varint",
  "version": "2.0.1",
  "homepage": "https://github.com/dominictarr/signed-varint",
  "repository": {
    "type": "git",
    "url": "git://github.com/dominictarr/signed-varint.git"
  },
  "dependencies": {
    "varint": "~5.0.0"
  },
  "devDependencies": {
    "tape": "~2.12.3"
  },
  "scripts": {
    "test": "node test.js"
  },
  "author": "Dominic Tarr <dominic.tarr@gmail.com> (http://dominictarr.com)",
  "license": "MIT"
}
require.addon = require('require-addon')

module.exports = require.addon('.', __filename)
const binding = require('./binding')
const b4a = require('b4a')

function predicate(u8, u16, u32) {
  return function predicate(buf) {
    if (buf.byteLength % 16 !== 0) {
      throw new Error('Buffer length must be a multiple of 16')
    }

    const n = buf.BYTES_PER_ELEMENT

    if (n === 1) return u8(buf)
    if (n === 2) return u16(buf)
    return u32(buf)
  }
}

function unary(u8, u16, u32) {
  return function unary(buf, result = b4a.allocUnsafe(buf.byteLength)) {
    if (buf.byteLength % 16 !== 0) {
      throw new Error('Buffer length must be a multiple of 16')
    }

    if (buf.byteLength !== result.byteLength) {
      throw new Error('Length of result buffer is insufficient')
    }

    const n = buf.BYTES_PER_ELEMENT

    if (n === 1) u8(buf, result)
    else if (n === 2) u16(buf, result)
    else u32(buf, result)

    return result
  }
}

function binary(u8, u16, u32) {
  return function binary(a, b, result = b4a.allocUnsafe(a.byteLength)) {
    if (a.byteLength % 16 !== 0) {
      throw new Error('Buffer length must be a multiple of 16')
    }

    if (a.byteLength !== b.byteLength || a.byteLength !== result.byteLength) {
      throw new Error('Buffers must be the same length')
    }

    const n = a.BYTES_PER_ELEMENT

    if (n === 1) u8(a, b, result)
    else if (n === 2) u16(a, b, result)
    else u32(a, b, result)

    return result
  }
}

function reduce(u8, u16, u32) {
  return function reduce(buf) {
    if (buf.byteLength % 16 !== 0) {
      throw new Error('Buffer length must be a multiple of 16')
    }

    const n = buf.BYTES_PER_ELEMENT

    if (n === 1) return u8(buf)
    if (n === 2) return u16(buf)
    return u32(buf)
  }
}

exports.allo = predicate(
  binding.simdle_native_allo_v128_u8,
  binding.simdle_native_allo_v128_u16,
  binding.simdle_native_allo_v128_u32
)

exports.allz = predicate(
  binding.simdle_native_allz_v128_u8,
  binding.simdle_native_allz_v128_u16,
  binding.simdle_native_allz_v128_u32
)

exports.and = binary(
  binding.simdle_native_and_v128_u8,
  binding.simdle_native_and_v128_u16,
  binding.simdle_native_and_v128_u32
)

exports.clear = binary(
  binding.simdle_native_clear_v128_u8,
  binding.simdle_native_clear_v128_u16,
  binding.simdle_native_clear_v128_u32
)

exports.clo = unary(
  binding.simdle_native_clo_v128_u8,
  binding.simdle_native_clo_v128_u16,
  binding.simdle_native_clo_v128_u32
)

exports.clz = unary(
  binding.simdle_native_clz_v128_u8,
  binding.simdle_native_clz_v128_u16,
  binding.simdle_native_clz_v128_u32
)

exports.cnt = unary(
  binding.simdle_native_cnt_v128_u8,
  binding.simdle_native_cnt_v128_u16,
  binding.simdle_native_cnt_v128_u32
)

exports.cto = unary(
  binding.simdle_native_cto_v128_u8,
  binding.simdle_native_cto_v128_u16,
  binding.simdle_native_cto_v128_u32
)

exports.ctz = unary(
  binding.simdle_native_ctz_v128_u8,
  binding.simdle_native_ctz_v128_u16,
  binding.simdle_native_ctz_v128_u32
)

exports.not = unary(
  binding.simdle_native_not_v128_u8,
  binding.simdle_native_not_v128_u16,
  binding.simdle_native_not_v128_u32
)

exports.or = binary(
  binding.simdle_native_or_v128_u8,
  binding.simdle_native_or_v128_u16,
  binding.simdle_native_or_v128_u32
)

exports.sum = reduce(
  binding.simdle_native_sum_v128_u8,
  binding.simdle_native_sum_v128_u16,
  binding.simdle_native_sum_v128_u32
)

exports.xor = binary(
  binding.simdle_native_xor_v128_u8,
  binding.simdle_native_xor_v128_u16,
  binding.simdle_native_xor_v128_u32
)
{
  "name": "simdle-native",
  "version": "1.3.9",
  "description": "libsimdle JavaScript bindings for Node.js",
  "main": "index.js",
  "files": [
    "index.js",
    "binding.cc",
    "binding.js",
    "CMakeLists.txt",
    "prebuilds"
  ],
  "addon": true,
  "scripts": {
    "format": "prettier --write .",
    "lint": "prettier --check .",
    "test": "npm run lint && npm run test:bare && npm run test:node",
    "test:bare": "bare test.mjs",
    "test:node": "node test.mjs"
  },
  "repository": {
    "type": "git",
    "url": "git+https://github.com/holepunchto/simdle-native.git"
  },
  "author": "Holepunch",
  "license": "Apache-2.0",
  "bugs": {
    "url": "https://github.com/holepunchto/simdle-native/issues"
  },
  "homepage": "https://github.com/holepunchto/simdle-native#readme",
  "dependencies": {
    "b4a": "^1.6.0",
    "require-addon": "^1.1.0"
  },
  "devDependencies": {
    "bare-compat-napi": "^1.3.2",
    "brittle": "^3.1.0",
    "cmake-bare": "^1.1.7",
    "cmake-fetch": "^1.1.0",
    "cmake-napi": "^1.0.2",
    "prettier": "^3.6.2",
    "prettier-config-holepunch": "^1.0.0"
  }
}
const b4a = require('b4a')
const scalar = require('./scalar')

function view (buf, n) {
  if (n === buf.BYTES_PER_ELEMENT) return buf

  let TypedArray

  if (n === 1) TypedArray = Uint8Array
  else if (n === 2) TypedArray = Uint16Array
  else TypedArray = Uint32Array

  return new TypedArray(buf.buffer, buf.byteOffset, buf.byteLength / n)
}

function unary (u8, u16 = u8, u32 = u16) {
  return function unary (buf, result = b4a.allocUnsafe(buf.byteLength)) {
    if (buf.byteLength % 16 !== 0) {
      throw new Error('Buffer length must be a multiple of 16')
    }

    if (buf.byteLength !== result.byteLength) {
      throw new Error('Length of result buffer is insufficient')
    }

    const n = buf.BYTES_PER_ELEMENT

    if (n === 1) u8(buf, view(result, n))
    else if (n === 2) u16(buf, view(result, n))
    else u32(buf, view(result, n))

    return result
  }
}

function binary (u8, u16 = u8, u32 = u16) {
  return function binary (a, b, result = b4a.allocUnsafe(a.byteLength)) {
    if (a.byteLength % 16 !== 0) {
      throw new Error('Buffer length must be a multiple of 16')
    }

    if (a.byteLength !== b.byteLength || a.byteLength !== result.byteLength) {
      throw new Error('Buffers must be the same length')
    }

    const n = a.BYTES_PER_ELEMENT

    if (n === 1) u8(a, b, view(result, n))
    else if (n === 2) u16(a, b, view(result, n))
    else u32(a, b, view(result, n))

    return result
  }
}

function reduce (u8, u16 = u8, u32 = u16) {
  return function reduce (buf) {
    if (buf.byteLength % 16 !== 0) {
      throw new Error('Buffer length must be a multiple of 16')
    }

    const n = buf.BYTES_PER_ELEMENT

    if (n === 1) return u8(buf)
    if (n === 2) return u16(buf)
    return u32(buf)
  }
}

exports.allo = function allo (buf) {
  if (buf.byteLength % 16 !== 0) {
    throw new Error('Buffer length must be a multiple of 16')
  }

  const m = 2 ** (buf.BYTES_PER_ELEMENT * 8) - 1

  for (let i = 0, n = buf.length; i < n; i++) {
    if (buf[i] !== m) return false
  }

  return true
}

exports.allz = function allz (buf) {
  if (buf.byteLength % 16 !== 0) {
    throw new Error('Buffer length must be a multiple of 16')
  }

  for (let i = 0, n = buf.length; i < n; i++) {
    if (buf[i] !== 0) return false
  }

  return true
}

exports.and = binary(
  (a, b, result) => {
    for (let i = 0, n = result.length; i < n; i++) {
      result[i] = a[i] & b[i]
    }
  }
)

exports.clear = binary(
  (a, b, result) => {
    for (let i = 0, n = result.length; i < n; i++) {
      result[i] = a[i] & ~b[i]
    }
  }
)

exports.clo = unary(
  (buf, result) => {
    for (let i = 0, n = buf.length; i < n; i++) {
      result[i] = 24 - scalar.clo(buf[i])
    }
  },
  (buf, result) => {
    for (let i = 0, n = buf.length; i < n; i++) {
      result[i] = 16 - scalar.clo(buf[i])
    }
  },
  (buf, result) => {
    for (let i = 0, n = buf.length; i < n; i++) {
      result[i] = scalar.clo(buf[i])
    }
  }
)

exports.clz = unary(
  (buf, result) => {
    for (let i = 0, n = buf.length; i < n; i++) {
      result[i] = 24 - scalar.clz(buf[i])
    }
  },
  (buf, result) => {
    for (let i = 0, n = buf.length; i < n; i++) {
      result[i] = 16 - scalar.clz(buf[i])
    }
  },
  (buf, result) => {
    for (let i = 0, n = buf.length; i < n; i++) {
      result[i] = scalar.clz(buf[i])
    }
  }
)

exports.cnt = unary(
  (buf, result) => {
    for (let i = 0, n = buf.length; i < n; i++) {
      result[i] = scalar.cnt(buf[i]) & 0xff
    }
  },
  (buf, result) => {
    for (let i = 0, n = buf.length; i < n; i++) {
      result[i] = scalar.cnt(buf[i]) & 0xffff
    }
  },
  (buf, result) => {
    for (let i = 0, n = buf.length; i < n; i++) {
      result[i] = scalar.cnt(buf[i])
    }
  }
)

exports.cto = unary(
  (buf, result) => {
    for (let i = 0, n = buf.length; i < n; i++) {
      result[i] = Math.min(scalar.cto(buf[i]), 8)
    }
  },
  (buf, result) => {
    for (let i = 0, n = buf.length; i < n; i++) {
      result[i] = Math.min(scalar.cto(buf[i]), 16)
    }
  },
  (buf, result) => {
    for (let i = 0, n = buf.length; i < n; i++) {
      result[i] = scalar.cto(buf[i])
    }
  }
)

exports.ctz = unary(
  (buf, result) => {
    for (let i = 0, n = buf.length; i < n; i++) {
      result[i] = Math.min(scalar.ctz(buf[i]), 8)
    }
  },
  (buf, result) => {
    for (let i = 0, n = buf.length; i < n; i++) {
      result[i] = Math.min(scalar.ctz(buf[i]), 16)
    }
  },
  (buf, result) => {
    for (let i = 0, n = buf.length; i < n; i++) {
      result[i] = scalar.ctz(buf[i])
    }
  }
)

exports.not = unary(
  (buf, result) => {
    for (let i = 0, n = buf.length; i < n; i++) {
      result[i] = ~buf[i]
    }
  }
)

exports.or = binary(
  (a, b, result) => {
    for (let i = 0, n = result.length; i < n; i++) {
      result[i] = a[i] | b[i]
    }
  }
)

exports.sum = reduce(
  (buf) => {
    let result = 0n

    for (let i = 0, n = buf.length; i < n; i++) {
      result += BigInt(buf[i])
    }

    return result
  }
)

exports.xor = binary(
  (a, b, result) => {
    for (let i = 0, n = result.length; i < n; i++) {
      result[i] = a[i] ^ b[i]
    }
  }
)
try {
  module.exports = require('simdle-native')
} catch {
  module.exports = require('./fallback')
}
{
  "name": "simdle-universal",
  "version": "1.1.2",
  "description": "Universal wrapper for libsimdle with a JavaScript fallback",
  "main": "index.js",
  "files": [
    "fallback.js",
    "index.js",
    "scalar.js"
  ],
  "browser": {
    "./index.js": "./fallback.js"
  },
  "scripts": {
    "test": "standard && brittle test.mjs"
  },
  "repository": {
    "type": "git",
    "url": "git+https://github.com/holepunchto/simdle-universal.git"
  },
  "author": "Kasper Isager Dalsgar <kasper@funktionel.co>",
  "license": "ISC",
  "bugs": {
    "url": "https://github.com/holepunchto/simdle-universal/issues"
  },
  "homepage": "https://github.com/holepunchto/simdle-universal#readme",
  "dependencies": {
    "b4a": "^1.6.0"
  },
  "optionalDependencies": {
    "simdle-native": "^1.1.1"
  },
  "devDependencies": {
    "brittle": "^3.1.0",
    "standard": "^17.0.0"
  }
}
const clz = exports.clz = function clz (n) {
  return Math.clz32(n)
}

exports.clo = function clo (n) {
  return clz(~n)
}

const ctz = exports.ctz = function ctz (n) {
  return 32 - (n === 0 ? 0 : (clz(n & -n) + 1))
}

exports.cto = function cto (n) {
  return ctz(~n)
}

exports.cnt = function cnt (n) {
  n = n - ((n >>> 1) & 0x55555555)
  n = (n & 0x33333333) + ((n >>> 2) & 0x33333333)
  n = (n + (n >>> 4)) & 0x0f0f0f0f
  n = (n * 0x01010101) >>> 24
  return n
}
require.addon = require('require-addon')
module.exports = require.addon('.', __filename)
const binding = require('./binding')
const { isNode } = require('which-runtime')

const OPTIONAL = Buffer.from(new ArrayBuffer(0))

module.exports = exports = { ...binding }

// memory

exports.sodium_memzero = function (buf) {
  binding.sodium_memzero(buf)
}

exports.sodium_mlock = function (buf) {
  const res = binding.sodium_mlock(buf)
  if (res !== 0) throw new Error('memory lock failed')
}

exports.sodium_munlock = function (buf) {
  const res = binding.sodium_munlock(buf)
  if (res !== 0) throw new Error('memory unlock failed')
}

exports.sodium_malloc = function (size) {
  if (size < 0) throw new Error('invalid size')
  const buf = Buffer.from(binding.sodium_malloc(size))
  buf.secure = true

  return buf
}

exports.sodium_free = function (buf) {
  if (!buf?.secure) return

  binding.sodium_free(buf.buffer)
}

exports.sodium_mprotect_noaccess = function (buf) {
  const res = binding.sodium_mprotect_noaccess(buf.buffer)

  if (res !== 0) throw new Error('failed to lock buffer')
}

exports.sodium_mprotect_readonly = function (buf) {
  const res = binding.sodium_mprotect_readonly(buf.buffer)

  if (res !== 0) throw new Error('failed to unlock buffer')
}

exports.sodium_mprotect_readwrite = function (buf) {
  const res = binding.sodium_mprotect_readwrite(buf.buffer)

  if (res !== 0) throw new Error('failed to unlock buffer')
}

// crypto_randombytes

exports.randombytes_buf = function (buffer) {
  binding.randombytes_buf(buffer.buffer, buffer.byteOffset, buffer.byteLength)
}

exports.randombytes_buf_deterministic = function (buffer, seed) {
  binding.randombytes_buf_deterministic(
    buffer.buffer,
    buffer.byteOffset,
    buffer.byteLength,

    seed.buffer,
    seed.byteOffset,
    seed.byteLength
  )
}

// sodium_helpers

exports.sodium_memcmp = function (a, b) {
  if (a?.byteLength !== b?.byteLength)
    throw new Error('buffers must be of same length"')
  return binding.sodium_memcmp(a, b)
}

exports.sodium_add = function (a, b) {
  if (a?.byteLength !== b?.byteLength)
    throw new Error('buffers must be of same length"')
  binding.sodium_add(a, b)
}

exports.sodium_sub = function (a, b) {
  if (a?.byteLength !== b?.byteLength)
    throw new Error('buffers must be of same length"')
  binding.sodium_sub(a, b)
}

exports.sodium_compare = function (a, b) {
  if (a?.byteLength !== b?.byteLength)
    throw new Error('buffers must be of same length"')
  return binding.sodium_compare(a, b)
}

exports.sodium_is_zero = function (buffer, length) {
  if (!buffer) throw new Error('invalid buffer')
  length ??= buffer.byteLength
  if (length > buffer.byteLength || length < 0)
    throw new Error('invalid length')

  return binding.sodium_is_zero(buffer, length)
}

exports.sodium_pad = function (buffer, unpaddedBuflen, blockSize) {
  if (unpaddedBuflen > buffer.byteLength)
    throw new Error('unpadded length cannot exceed buffer length')
  if (blockSize > buffer.byteLength)
    throw new Error('block size cannot exceed buffer length')
  if (blockSize < 1) throw new Error('block sizemust be at least 1 byte')
  if (
    buffer?.byteLength <
    unpaddedBuflen + (blockSize - (unpaddedBuflen % blockSize))
  )
    throw new Error('buf not long enough')

  return binding.sodium_pad(buffer, unpaddedBuflen, blockSize)
}

exports.sodium_unpad = function (buffer, paddedBuflen, blockSize) {
  if (paddedBuflen > buffer.byteLength)
    throw new Error('unpadded length cannot exceed buffer length')
  if (blockSize > buffer.byteLength)
    throw new Error('block size cannot exceed buffer length')
  if (blockSize < 1) throw new Error('block size must be at least 1 byte')

  return binding.sodium_unpad(buffer, paddedBuflen, blockSize)
}

// crypto_sign

exports.crypto_sign_keypair = function (pk, sk) {
  if (pk?.byteLength !== binding.crypto_sign_PUBLICKEYBYTES)
    throw new Error('pk')

  const res = binding.crypto_sign_keypair(pk, sk)

  if (res !== 0) throw new Error('status: ' + res)
}

exports.crypto_sign_seed_keypair = function (pk, sk, seed) {
  if (pk?.byteLength !== binding.crypto_sign_PUBLICKEYBYTES)
    throw new Error('pk')

  const res = binding.crypto_sign_seed_keypair(pk, sk, seed)

  if (res !== 0) throw new Error('status: ' + res)
}

exports.crypto_sign = function (sm, m, sk) {
  if (sm?.byteLength !== binding.crypto_sign_BYTES + m.byteLength)
    throw new Error('sm must be "m.byteLength + crypto_sign_BYTES" bytes')
  if (sk?.byteLength !== binding.crypto_sign_SECRETKEYBYTES)
    throw new Error('sk')

  const res = binding.crypto_sign(sm, m, sk)

  if (res !== 0) throw new Error('status: ' + res)
}

exports.crypto_sign_open = function (m, sm, pk) {
  if (sm?.byteLength < binding.crypto_sign_BYTES) throw new Error('sm')
  if (m?.byteLength !== sm.byteLength - binding.crypto_sign_BYTES)
    throw new Error('m must be "sm.byteLength - crypto_sign_BYTES" bytes')
  if (pk?.byteLength !== binding.crypto_sign_PUBLICKEYBYTES)
    throw new Error('pk')

  const res = binding.crypto_sign_open(m, sm, pk)

  if (res !== 0) throw new Error('status: ' + res)
}

exports.crypto_sign_open = function (m, sm, pk) {
  if (sm?.byteLength < binding.crypto_sign_BYTES) throw new Error('sm')
  if (m?.byteLength !== sm.byteLength - binding.crypto_sign_BYTES)
    throw new Error('m must be "sm.byteLength - crypto_sign_BYTES" bytes')
  if (pk?.byteLength !== binding.crypto_sign_PUBLICKEYBYTES)
    throw new Error('pk')

  return binding.crypto_sign_open(m, sm, pk)
}

exports.crypto_sign_detached = function (sig, m, sk) {
  if (sig?.byteLength !== binding.crypto_sign_BYTES) throw new Error('sig')
  if (sk?.byteLength !== binding.crypto_sign_SECRETKEYBYTES)
    throw new Error('sk')

  const res = binding.crypto_sign_detached(sig, m, sk)

  if (res !== 0) throw new Error('status: ' + res)
}

exports.crypto_sign_verify_detached = function (sig, m, pk) {
  return binding.crypto_sign_verify_detached(
    sig.buffer,
    sig.byteOffset,
    sig.byteLength,

    m.buffer,
    m.byteOffset,
    m.byteLength,

    pk.buffer,
    pk.byteOffset,
    pk.byteLength
  )
}

exports.crypto_sign_ed25519_sk_to_pk = function (pk, sk) {
  if (pk?.byteLength !== binding.crypto_sign_PUBLICKEYBYTES)
    throw new Error('pk')
  if (sk?.byteLength !== binding.crypto_sign_SECRETKEYBYTES)
    throw new Error('sk')

  const res = binding.crypto_sign_ed25519_sk_to_pk(pk, sk)

  if (res !== 0) throw new Error('status: ' + res)
}

exports.crypto_sign_ed25519_pk_to_curve25519 = function (x25519pk, ed25519pk) {
  if (x25519pk?.byteLength !== binding.crypto_box_PUBLICKEYBYTES)
    throw new Error('x25519_pk')
  if (ed25519pk?.byteLength !== binding.crypto_sign_PUBLICKEYBYTES)
    throw new Error('ed25519_pk')

  const res = binding.crypto_sign_ed25519_pk_to_curve25519(x25519pk, ed25519pk)

  if (res !== 0) throw new Error('status: ' + res)
}

exports.crypto_sign_ed25519_sk_to_curve25519 = function (x25519sk, ed25519sk) {
  if (x25519sk?.byteLength !== binding.crypto_box_SECRETKEYBYTES)
    throw new Error('x25519_sk')

  const edLen = ed25519sk.byteLength

  if (
    edLen !== binding.crypto_sign_SECRETKEYBYTES &&
    edLen !== binding.crypto_box_SECRETKEYBYTES
  ) {
    throw new Error(
      "ed25519_sk should either be 'crypto_sign_SECRETKEYBYTES' bytes or 'crypto_sign_SECRETKEYBYTES - crypto_sign_PUBLICKEYBYTES' bytes"
    )
  }

  const res = binding.crypto_sign_ed25519_sk_to_curve25519(x25519sk, ed25519sk)

  if (res !== 0) throw new Error('status: ' + res)
}

// crypto_box

exports.crypto_box_keypair = function (pk, sk) {
  if (pk?.byteLength !== binding.crypto_box_PUBLICKEYBYTES)
    throw new Error('pk')
  if (sk?.byteLength !== binding.crypto_box_SECRETKEYBYTES)
    throw new Error('sk')

  const res = binding.crypto_box_keypair(pk, sk)

  if (res !== 0) throw new Error('status: ' + res)
}

exports.crypto_box_seed_keypair = function (pk, sk, seed) {
  if (pk?.byteLength !== binding.crypto_box_PUBLICKEYBYTES)
    throw new Error('pk')
  if (sk?.byteLength !== binding.crypto_box_SECRETKEYBYTES)
    throw new Error('sk')

  const res = binding.crypto_box_seed_keypair(pk, sk, seed)

  if (res !== 0) throw new Error('status: ' + res)
}

exports.crypto_box_easy = function (c, m, n, pk, sk) {
  const res = binding.crypto_box_easy(c, m, n, pk, sk)

  if (res !== 0) throw new Error('status: ' + res)
}

exports.crypto_box_detached = function (c, mac, m, n, pk, sk) {
  const res = binding.crypto_box_detached(c, mac, m, n, pk, sk)

  if (res !== 0) throw new Error('status: ' + res)
}

exports.crypto_box_seal = function (c, m, pk) {
  const res = binding.crypto_box_seal(c, m, pk)

  if (res !== 0) throw new Error('status: ' + res)
}

exports.crypto_box_seal_open = function (m, c, pk, sk) {
  return binding.crypto_box_seal_open(
    m.buffer,
    m.byteOffset,
    m.byteLength,

    c.buffer,
    c.byteOffset,
    c.byteLength,

    pk.buffer,
    pk.byteOffset,
    pk.byteLength,

    sk.buffer,
    sk.byteOffset,
    sk.byteLength
  )
}

// crypto_secretbox

exports.crypto_secretbox_easy = function (c, m, n, k) {
  if (c?.byteLength !== m.byteLength + binding.crypto_secretbox_MACBYTES)
    throw new Error(
      'c must be "m.byteLength + crypto_secretbox_MACBYTES" bytes'
    )
  if (n?.byteLength !== binding.crypto_secretbox_NONCEBYTES)
    throw new Error('n')
  if (k?.byteLength !== binding.crypto_secretbox_KEYBYTES) throw new Error('k')

  const res = binding.crypto_secretbox_easy(c, m, n, k)

  if (res !== 0) throw new Error('status: ' + res)
}

exports.crypto_secretbox_open_easy = function (m, c, n, k) {
  if (m?.byteLength !== c.byteLength - binding.crypto_secretbox_MACBYTES)
    throw new Error('m must be "c - crypto_secretbox_MACBYTES" bytes')
  if (c?.byteLength < binding.crypto_secretbox_MACBYTES) throw new Error('c')
  if (n?.byteLength !== binding.crypto_secretbox_NONCEBYTES)
    throw new Error('n')
  if (k?.byteLength !== binding.crypto_secretbox_KEYBYTES) throw new Error('k')

  return binding.crypto_secretbox_open_easy(m, c, n, k)
}

exports.crypto_secretbox_detached = function (c, mac, m, n, k) {
  if (c?.byteLength !== m.byteLength)
    throw new Error('c must "m.byteLength" bytes')
  if (mac?.byteLength !== binding.crypto_secretbox_MACBYTES)
    throw new Error('mac')
  if (n?.byteLength !== binding.crypto_secretbox_NONCEBYTES)
    throw new Error('n')
  if (k?.byteLength !== binding.crypto_secretbox_KEYBYTES) throw new Error('k')

  const res = binding.crypto_secretbox_detached(c, mac, m, n, k)

  if (res !== 0) throw new Error('status: ' + res)
}

exports.crypto_secretbox_open_detached = function (m, c, mac, n, k) {
  if (m?.byteLength !== c.byteLength)
    throw new Error('m must be "c.byteLength" bytes')
  if (mac?.byteLength !== binding.crypto_secretbox_MACBYTES)
    throw new Error('mac')
  if (n?.byteLength !== binding.crypto_secretbox_NONCEBYTES)
    throw new Error('n')
  if (k?.byteLength !== binding.crypto_secretbox_KEYBYTES) throw new Error('k')

  return binding.crypto_secretbox_open_detached(m, c, mac, n, k)
}

// crypto_generichash

exports.crypto_generichash = function (output, input, key = OPTIONAL) {
  const res = binding.crypto_generichash(
    output.buffer,
    output.byteOffset,
    output.byteLength,

    input.buffer,
    input.byteOffset,
    input.byteLength,

    key.buffer,
    key.byteOffset,
    key.byteLength
  )

  if (res !== 0) throw new Error('status: ' + res)
}

exports.crypto_generichash_batch = function (output, batch, key) {
  if (isNode || batch.length < 4) {
    const res = binding.crypto_generichash_batch(
      output,
      batch,
      !!key,
      key || OPTIONAL
    )
    if (res !== 0) throw new Error('status: ' + res)
  } else {
    const state = Buffer.alloc(binding.crypto_generichash_STATEBYTES)

    exports.crypto_generichash_init(state, key, output.byteLength)

    for (const buf of batch) {
      exports.crypto_generichash_update(state, buf)
    }

    exports.crypto_generichash_final(state, output)
  }
}

exports.crypto_generichash_keygen = function (key) {
  const res = binding.crypto_generichash_keygen(
    key.buffer,
    key.byteOffset,
    key.byteLength
  )
  if (res !== 0) throw new Error('status: ' + res)
}

exports.crypto_generichash_init = function (state, key, outputLength) {
  key ||= OPTIONAL

  const res = binding.crypto_generichash_init(
    state.buffer,
    state.byteOffset,
    state.byteLength,

    key.buffer,
    key.byteOffset,
    key.byteLength,

    outputLength
  )

  if (res !== 0) throw new Error('status: ' + res)
}

exports.crypto_generichash_update = function (state, input) {
  const res = binding.crypto_generichash_update(
    state.buffer,
    state.byteOffset,
    state.byteLength,

    input.buffer,
    input.byteOffset,
    input.byteLength
  )

  if (res !== 0) throw new Error('status: ' + res)
}

exports.crypto_generichash_final = function (state, output) {
  const res = binding.crypto_generichash_final(
    state.buffer,
    state.byteOffset,
    state.byteLength,

    output.buffer,
    output.byteOffset,
    output.byteLength
  )

  if (res !== 0) throw new Error('status: ' + res)
}

// secretstream

exports.crypto_secretstream_xchacha20poly1305_keygen = function (k) {
  binding.crypto_secretstream_xchacha20poly1305_keygen(
    k.buffer,
    k.byteOffset,
    k.byteLength
  )
}

exports.crypto_secretstream_xchacha20poly1305_init_push = function (
  state,
  header,
  k
) {
  const res = binding.crypto_secretstream_xchacha20poly1305_init_push(
    state.buffer,
    state.byteOffset,
    state.byteLength,

    header.buffer,
    header.byteOffset,
    header.byteLength,

    k.buffer,
    k.byteOffset,
    k.byteLength
  )

  if (res !== 0) throw new Error('status: ' + res)
}

exports.crypto_secretstream_xchacha20poly1305_init_pull = function (
  state,
  header,
  k
) {
  const res = binding.crypto_secretstream_xchacha20poly1305_init_pull(
    state.buffer,
    state.byteOffset,
    state.byteLength,

    header.buffer,
    header.byteOffset,
    header.byteLength,

    k.buffer,
    k.byteOffset,
    k.byteLength
  )

  if (res !== 0) throw new Error('status: ' + res)
}

exports.crypto_secretstream_xchacha20poly1305_push = function (
  state,
  c,
  m,
  ad,
  tag
) {
  ad ||= OPTIONAL

  const res = binding.crypto_secretstream_xchacha20poly1305_push(
    state.buffer,
    state.byteOffset,
    state.byteLength,

    c.buffer,
    c.byteOffset,
    c.byteLength,

    m.buffer,
    m.byteOffset,
    m.byteLength,

    ad.buffer,
    ad.byteOffset,
    ad.byteLength,

    tag
  )

  if (res < 0) throw new Error('push failed')

  return res
}

exports.crypto_secretstream_xchacha20poly1305_pull = function (
  state,
  m,
  tag,
  c,
  ad
) {
  ad ||= OPTIONAL

  if (c?.byteLength < binding.crypto_secretstream_xchacha20poly1305_ABYTES)
    throw new Error('invalid cipher length')
  if (
    m?.byteLength !==
    c.byteLength - binding.crypto_secretstream_xchacha20poly1305_ABYTES
  )
    throw new Error('invalid message length')

  const res = binding.crypto_secretstream_xchacha20poly1305_pull(
    state.buffer,
    state.byteOffset,
    state.byteLength,

    m.buffer,
    m.byteOffset,
    m.byteLength,

    tag.buffer,
    tag.byteOffset,
    tag.byteLength,

    c.buffer,
    c.byteOffset,
    c.byteLength,

    ad.buffer,
    ad.byteOffset,
    ad.byteLength
  )

  if (res < 0) throw new Error('pull failed')

  return res
}

exports.crypto_secretstream_xchacha20poly1305_rekey = function (state) {
  binding.crypto_secretstream_xchacha20poly1305_rekey(
    state.buffer,
    state.byteOffset,
    state.byteLength
  )
}

// crypto_stream

exports.crypto_stream = function (c, n, k) {
  if (n?.byteLength !== binding.crypto_stream_NONCEBYTES) throw new Error('n')
  if (k?.byteLength !== binding.crypto_stream_KEYBYTES) throw new Error('k')

  const res = binding.crypto_stream(c, n, k)

  if (res !== 0) throw new Error('status: ' + res)
}

exports.crypto_stream_xor = function (c, m, n, k) {
  const res = binding.crypto_stream_xor(
    c.buffer,
    c.byteOffset,
    c.byteLength,

    m.buffer,
    m.byteOffset,
    m.byteLength,

    n.buffer,
    n.byteOffset,
    n.byteLength,

    k.buffer,
    k.byteOffset,
    k.byteLength
  )

  if (res !== 0) throw new Error('status: ' + res)
}

exports.crypto_stream_chacha20 = function (c, n, k) {
  if (n?.byteLength !== binding.crypto_stream_chacha20_NONCEBYTES)
    throw new Error('n')
  if (k?.byteLength !== binding.crypto_stream_chacha20_KEYBYTES)
    throw new Error('k')

  const res = binding.crypto_stream_chacha20(c, n, k)

  if (res !== 0) throw new Error('status: ' + res)
}

exports.crypto_stream_chacha20_xor = function (c, m, n, k) {
  if (c?.byteLength !== m.byteLength)
    throw new Error('m must be "c.byteLength" bytes')
  if (n?.byteLength !== binding.crypto_stream_chacha20_NONCEBYTES)
    throw new Error('n')
  if (k?.byteLength !== binding.crypto_stream_chacha20_KEYBYTES)
    throw new Error('k')

  const res = binding.crypto_stream_chacha20_xor(c, m, n, k)

  if (res !== 0) throw new Error('status: ' + res)
}

exports.crypto_stream_chacha20_xor_ic = function (c, m, n, ic, k) {
  if (c?.byteLength !== m.byteLength)
    throw new Error('m must be "c.byteLength" bytes')
  if (n?.byteLength !== binding.crypto_stream_chacha20_NONCEBYTES)
    throw new Error('n')
  if (k?.byteLength !== binding.crypto_stream_chacha20_KEYBYTES)
    throw new Error('k')

  const res = binding.crypto_stream_chacha20_xor_ic(c, m, n, ic, k)

  if (res !== 0) throw new Error('status: ' + res)
}

exports.crypto_stream_chacha20_ietf = function (c, n, k) {
  if (n?.byteLength !== binding.crypto_stream_chacha20_ietf_NONCEBYTES)
    throw new Error('n')
  if (k?.byteLength !== binding.crypto_stream_chacha20_ietf_KEYBYTES)
    throw new Error('k')

  const res = binding.crypto_stream_chacha20_ietf(c, n, k)

  if (res !== 0) throw new Error('status: ' + res)
}

exports.crypto_stream_chacha20_ietf_xor = function (c, m, n, k) {
  if (c?.byteLength !== m.byteLength)
    throw new Error('m must be "c.byteLength" bytes')
  if (n?.byteLength !== binding.crypto_stream_chacha20_ietf_NONCEBYTES)
    throw new Error('n')
  if (k?.byteLength !== binding.crypto_stream_chacha20_ietf_KEYBYTES)
    throw new Error('k')

  const res = binding.crypto_stream_chacha20_ietf_xor(c, m, n, k)

  if (res !== 0) throw new Error('status: ' + res)
}

exports.crypto_stream_chacha20_ietf_xor_ic = function (c, m, n, ic, k) {
  if (c?.byteLength !== m.byteLength)
    throw new Error('m must be "c.byteLength" bytes')
  if (n?.byteLength !== binding.crypto_stream_chacha20_ietf_NONCEBYTES)
    throw new Error('n')
  if (k?.byteLength !== binding.crypto_stream_chacha20_ietf_KEYBYTES)
    throw new Error('k')

  const res = binding.crypto_stream_chacha20_ietf_xor_ic(c, m, n, ic, k)

  if (res !== 0) throw new Error('status: ' + res)
}

exports.crypto_stream_xchacha20 = function (c, n, k) {
  if (n?.byteLength !== binding.crypto_stream_xchacha20_NONCEBYTES)
    throw new Error('n')
  if (k?.byteLength !== binding.crypto_stream_xchacha20_KEYBYTES)
    throw new Error('k')

  const res = binding.crypto_stream_xchacha20(c, n, k)

  if (res !== 0) throw new Error('status: ' + res)
}

exports.crypto_stream_xchacha20_xor = function (c, m, n, k) {
  if (c?.byteLength !== m.byteLength)
    throw new Error('m must be "c.byteLength" bytes')
  if (n?.byteLength !== binding.crypto_stream_xchacha20_NONCEBYTES)
    throw new Error('n')
  if (k?.byteLength !== binding.crypto_stream_xchacha20_KEYBYTES)
    throw new Error('k')

  const res = binding.crypto_stream_xchacha20_xor(c, m, n, k)

  if (res !== 0) throw new Error('status: ' + res)
}

exports.crypto_stream_xchacha20_xor_ic = function (c, m, n, ic, k) {
  if (c?.byteLength !== m.byteLength)
    throw new Error('m must be "c.byteLength" bytes')
  if (n?.byteLength !== binding.crypto_stream_xchacha20_NONCEBYTES)
    throw new Error('n')
  if (k?.byteLength !== binding.crypto_stream_xchacha20_KEYBYTES)
    throw new Error('k')

  const res = binding.crypto_stream_xchacha20_xor_ic(c, m, n, ic, k)

  if (res !== 0) throw new Error('status: ' + res)
}

exports.crypto_stream_salsa20 = function (c, n, k) {
  if (n?.byteLength !== binding.crypto_stream_salsa20_NONCEBYTES)
    throw new Error('n')
  if (k?.byteLength !== binding.crypto_stream_salsa20_KEYBYTES)
    throw new Error('k')

  const res = binding.crypto_stream_salsa20(c, n, k)

  if (res !== 0) throw new Error('status: ' + res)
}

exports.crypto_stream_salsa20_xor = function (c, m, n, k) {
  if (c?.byteLength !== m.byteLength)
    throw new Error('m must be "c.byteLength" bytes')
  if (n?.byteLength !== binding.crypto_stream_salsa20_NONCEBYTES)
    throw new Error('n')
  if (k?.byteLength !== binding.crypto_stream_salsa20_KEYBYTES)
    throw new Error('k')

  const res = binding.crypto_stream_salsa20_xor(c, m, n, k)

  if (res !== 0) throw new Error('status: ' + res)
}

exports.crypto_stream_salsa20_xor_ic = function (c, m, n, ic, k) {
  if (c?.byteLength !== m.byteLength)
    throw new Error('m must be "c.byteLength" bytes')
  if (n?.byteLength !== binding.crypto_stream_salsa20_NONCEBYTES)
    throw new Error('n')
  if (k?.byteLength !== binding.crypto_stream_salsa20_KEYBYTES)
    throw new Error('k')

  const res = binding.crypto_stream_salsa20_xor_ic(c, m, n, ic, k)

  if (res !== 0) throw new Error('status: ' + res)
}

// crypto_auth

exports.crypto_auth = function (out, input, k) {
  if (out?.byteLength !== binding.crypto_auth_BYTES) throw new Error('out')
  if (k?.byteLength !== binding.crypto_auth_KEYBYTES) throw new Error('k')

  const res = binding.crypto_auth(out, input, k)

  if (res !== 0) throw new Error('status: ' + res)
}

exports.crypto_auth_verify = function (h, input, k) {
  if (h?.byteLength !== binding.crypto_auth_BYTES) throw new Error('h')
  if (k?.byteLength !== binding.crypto_auth_KEYBYTES) throw new Error('k')

  return binding.crypto_auth_verify(h, input, k)
}

// crypto_onetimeauth

exports.crypto_onetimeauth = function (out, input, k) {
  if (out?.byteLength !== binding.crypto_onetimeauth_BYTES)
    throw new Error('out')
  if (k?.byteLength !== binding.crypto_onetimeauth_KEYBYTES)
    throw new Error('k')

  const res = binding.crypto_onetimeauth(out, input, k)

  if (res !== 0) throw new Error('status: ' + res)
}

exports.crypto_onetimeauth_init = function (state, k) {
  if (state?.byteLength !== binding.crypto_onetimeauth_STATEBYTES)
    throw new Error("state must be 'crypto_onetimeauth_STATEBYTES' bytes")
  if (k?.byteLength !== binding.crypto_onetimeauth_KEYBYTES)
    throw new Error('k')

  const res = binding.crypto_onetimeauth_init(state, k)

  if (res !== 0) throw new Error('status: ' + res)
}

exports.crypto_onetimeauth_update = function (state, input) {
  if (state?.byteLength !== binding.crypto_onetimeauth_STATEBYTES)
    throw new Error("state must be 'crypto_onetimeauth_STATEBYTES' bytes")

  const res = binding.crypto_onetimeauth_update(state, input)

  if (res !== 0) throw new Error('status: ' + res)
}

exports.crypto_onetimeauth_final = function (state, out) {
  if (state?.byteLength !== binding.crypto_onetimeauth_STATEBYTES)
    throw new Error("state must be 'crypto_onetimeauth_STATEBYTES' bytes")
  if (out?.byteLength !== binding.crypto_onetimeauth_BYTES)
    throw new Error('out')

  const res = binding.crypto_onetimeauth_final(state, out)

  if (res !== 0) throw new Error('status: ' + res)
}

exports.crypto_onetimeauth_verify = function (h, input, k) {
  if (h?.byteLength !== binding.crypto_onetimeauth_BYTES) throw new Error('h')
  if (k?.byteLength !== binding.crypto_onetimeauth_KEYBYTES)
    throw new Error('k')

  return binding.crypto_onetimeauth_verify(h, input, k)
}

// crypto_pwhash

exports.crypto_pwhash = function (out, passwd, salt, opslimit, memlimit, alg) {
  if (out?.byteLength < binding.crypto_pwhash_BYTES_MIN) throw new Error('out')
  if (out?.byteLength > binding.crypto_pwhash_BYTES_MAX) throw new Error('out')
  if (salt?.byteLength !== binding.crypto_pwhash_SALTBYTES)
    throw new Error('salt')
  if (opslimit < binding.crypto_pwhash_OPSLIMIT_MIN) throw new Error('opslimit')
  if (opslimit > binding.crypto_pwhash_OPSLIMIT_MAX) throw new Error('opslimit')
  if (memlimit < binding.crypto_pwhash_MEMLIMIT_MIN) throw new Error('memlimit')
  if (memlimit > binding.crypto_pwhash_MEMLIMIT_MAX) throw new Error('memlimit')
  if (alg < 1 || alg > 2)
    throw new Error('alg must be either Argon2i 1.3 or Argon2id 1.3')

  const res = binding.crypto_pwhash(out, passwd, salt, opslimit, memlimit, alg)

  if (res !== 0) throw new Error('status: ' + res)
}

exports.crypto_pwhash_async = function (
  out,
  passwd,
  salt,
  opslimit,
  memlimit,
  alg,
  callback = undefined
) {
  if (out?.byteLength < binding.crypto_pwhash_BYTES_MIN) throw new Error('out')
  if (out?.byteLength > binding.crypto_pwhash_BYTES_MAX) throw new Error('out')
  if (salt?.byteLength !== binding.crypto_pwhash_SALTBYTES)
    throw new Error('salt')
  if (opslimit < binding.crypto_pwhash_OPSLIMIT_MIN) throw new Error('opslimit')
  if (opslimit > binding.crypto_pwhash_OPSLIMIT_MAX) throw new Error('opslimit')
  if (memlimit < binding.crypto_pwhash_MEMLIMIT_MIN) throw new Error('memlimit')
  if (memlimit > binding.crypto_pwhash_MEMLIMIT_MAX) throw new Error('memlimit')
  if (alg < 1 || alg > 2)
    throw new Error('alg must be either Argon2i 1.3 or Argon2id 1.3')

  const [done, promise] = checkStatus(callback)

  binding.crypto_pwhash_async(
    out.buffer,
    out.byteOffset,
    out.byteLength,

    passwd.buffer,
    passwd.byteOffset,
    passwd.byteLength,

    salt.buffer,
    salt.byteOffset,
    salt.byteLength,

    opslimit,
    memlimit,
    alg,

    done
  )

  return promise
}

exports.crypto_pwhash_str = function (out, passwd, opslimit, memlimit) {
  if (out?.byteLength !== binding.crypto_pwhash_STRBYTES) throw new Error('out')
  if (typeof opslimit !== 'number') throw new Error('opslimit')
  if (opslimit < binding.crypto_pwhash_OPSLIMIT_MIN) throw new Error('opslimit')
  if (opslimit > binding.crypto_pwhash_OPSLIMIT_MAX) throw new Error('opslimit')
  if (typeof memlimit !== 'number') throw new Error('memlimit')
  if (memlimit < binding.crypto_pwhash_MEMLIMIT_MIN) throw new Error('memlimit')
  if (memlimit > binding.crypto_pwhash_MEMLIMIT_MAX) throw new Error('memlimit')

  const res = binding.crypto_pwhash_str(out, passwd, opslimit, memlimit)

  if (res !== 0) throw new Error('status: ' + res)
}

exports.crypto_pwhash_str_async = function (
  out,
  passwd,
  opslimit,
  memlimit,
  callback = undefined
) {
  if (out?.byteLength !== binding.crypto_pwhash_STRBYTES) throw new Error('out')
  if (!passwd?.byteLength) throw new Error('passwd')
  if (typeof opslimit !== 'number') throw new Error('opslimit')
  if (opslimit < binding.crypto_pwhash_OPSLIMIT_MIN) throw new Error('opslimit')
  if (opslimit > binding.crypto_pwhash_OPSLIMIT_MAX) throw new Error('opslimit')
  if (typeof memlimit !== 'number') throw new Error('memlimit')
  if (memlimit < binding.crypto_pwhash_MEMLIMIT_MIN) throw new Error('memlimit')
  if (memlimit > binding.crypto_pwhash_MEMLIMIT_MAX) throw new Error('memlimit')

  const [done, promise] = checkStatus(callback)

  binding.crypto_pwhash_str_async(
    out.buffer,
    out.byteOffset,
    out.byteLength,

    passwd.buffer,
    passwd.byteOffset,
    passwd.byteLength,

    opslimit,
    memlimit,

    done
  )

  return promise
}

exports.crypto_pwhash_str_verify = function (str, passwd) {
  if (str?.byteLength !== binding.crypto_pwhash_STRBYTES) throw new Error('str')

  return binding.crypto_pwhash_str_verify(str, passwd)
}

exports.crypto_pwhash_str_verify_async = function (
  str,
  passwd,
  callback = undefined
) {
  if (str?.byteLength !== binding.crypto_pwhash_STRBYTES) throw new Error('str')
  if (!passwd?.byteLength) throw new Error('passwd')

  const [done, promise] = checkStatus(callback, true)

  binding.crypto_pwhash_str_verify_async(
    str.buffer,
    str.byteOffset,
    str.byteLength,

    passwd.buffer,
    passwd.byteOffset,
    passwd.byteLength,

    done
  )

  return promise
}

exports.crypto_pwhash_str_needs_rehash = function (str, opslimit, memlimit) {
  if (str?.byteLength !== binding.crypto_pwhash_STRBYTES) throw new Error('str')
  if (opslimit < binding.crypto_pwhash_OPSLIMIT_MIN) throw new Error('opslimit')
  if (opslimit > binding.crypto_pwhash_OPSLIMIT_MAX) throw new Error('opslimit')
  if (memlimit < binding.crypto_pwhash_MEMLIMIT_MIN) throw new Error('memlimit')
  if (memlimit > binding.crypto_pwhash_MEMLIMIT_MAX) throw new Error('memlimit')

  return binding.crypto_pwhash_str_needs_rehash(str, opslimit, memlimit)
}

exports.crypto_pwhash_scryptsalsa208sha256 = function (
  out,
  passwd,
  salt,
  opslimit,
  memlimit
) {
  if (out?.byteLength < binding.crypto_pwhash_scryptsalsa208sha256_BYTES_MIN)
    throw new Error('out')
  if (out?.byteLength > binding.crypto_pwhash_scryptsalsa208sha256_BYTES_MAX)
    throw new Error('out')
  if (salt?.byteLength !== binding.crypto_pwhash_scryptsalsa208sha256_SALTBYTES)
    throw new Error('salt')
  if (opslimit < binding.crypto_pwhash_scryptsalsa208sha256_OPSLIMIT_MIN)
    throw new Error('opslimit')
  if (opslimit > binding.crypto_pwhash_scryptsalsa208sha256_OPSLIMIT_MAX)
    throw new Error('opslimit')
  if (memlimit < binding.crypto_pwhash_scryptsalsa208sha256_MEMLIMIT_MIN)
    throw new Error('memlimit')
  if (memlimit > binding.crypto_pwhash_scryptsalsa208sha256_MEMLIMIT_MAX)
    throw new Error('memlimit')

  const res = binding.crypto_pwhash_scryptsalsa208sha256(
    out,
    passwd,
    salt,
    opslimit,
    memlimit
  )

  if (res !== 0) throw new Error('status: ' + res)
}

exports.crypto_pwhash_scryptsalsa208sha256_async = function (
  out,
  passwd,
  salt,
  opslimit,
  memlimit,
  callback = undefined
) {
  if (out?.byteLength < binding.crypto_pwhash_scryptsalsa208sha256_BYTES_MIN)
    throw new Error('out')
  if (out?.byteLength > binding.crypto_pwhash_scryptsalsa208sha256_BYTES_MAX)
    throw new Error('out')
  if (!passwd?.byteLength) throw new Error('passwd')
  if (salt?.byteLength !== binding.crypto_pwhash_scryptsalsa208sha256_SALTBYTES)
    throw new Error('salt')
  if (opslimit < binding.crypto_pwhash_scryptsalsa208sha256_OPSLIMIT_MIN)
    throw new Error('opslimit')
  if (opslimit > binding.crypto_pwhash_scryptsalsa208sha256_OPSLIMIT_MAX)
    throw new Error('opslimit')
  if (memlimit < binding.crypto_pwhash_scryptsalsa208sha256_MEMLIMIT_MIN)
    throw new Error('memlimit')
  if (memlimit > binding.crypto_pwhash_scryptsalsa208sha256_MEMLIMIT_MAX)
    throw new Error('memlimit')

  const [done, promise] = checkStatus(callback)

  binding.crypto_pwhash_scryptsalsa208sha256_async(
    out.buffer,
    out.byteOffset,
    out.byteLength,

    passwd.buffer,
    passwd.byteOffset,
    passwd.byteLength,

    salt.buffer,
    salt.byteOffset,
    salt.byteLength,

    opslimit,
    memlimit,

    done
  )

  return promise
}

exports.crypto_pwhash_scryptsalsa208sha256_str_async = function (
  out,
  passwd,
  opslimit,
  memlimit,
  callback = undefined
) {
  if (out?.byteLength !== binding.crypto_pwhash_scryptsalsa208sha256_STRBYTES)
    throw new Error('out')
  if (!passwd?.byteLength) throw new Error('passwd')
  if (opslimit < binding.crypto_pwhash_scryptsalsa208sha256_OPSLIMIT_MIN)
    throw new Error('opslimit')
  if (opslimit > binding.crypto_pwhash_scryptsalsa208sha256_OPSLIMIT_MAX)
    throw new Error('opslimit')
  if (memlimit < binding.crypto_pwhash_scryptsalsa208sha256_MEMLIMIT_MIN)
    throw new Error('memlimit')
  if (memlimit > binding.crypto_pwhash_scryptsalsa208sha256_MEMLIMIT_MAX)
    throw new Error('memlimit')

  const [done, promise] = checkStatus(callback)

  binding.crypto_pwhash_scryptsalsa208sha256_str_async(
    out.buffer,
    out.byteOffset,
    out.byteLength,

    passwd.buffer,
    passwd.byteOffset,
    passwd.byteLength,

    opslimit,
    memlimit,

    done
  )

  return promise
}

exports.crypto_pwhash_scryptsalsa208sha256_str = function (
  out,
  passwd,
  opslimit,
  memlimit
) {
  if (out?.byteLength !== binding.crypto_pwhash_scryptsalsa208sha256_STRBYTES)
    throw new Error('out')
  if (!passwd?.byteLength) throw new Error('passwd')
  if (opslimit < binding.crypto_pwhash_scryptsalsa208sha256_OPSLIMIT_MIN)
    throw new Error('opslimit')
  if (opslimit > binding.crypto_pwhash_scryptsalsa208sha256_OPSLIMIT_MAX)
    throw new Error('opslimit')
  if (memlimit < binding.crypto_pwhash_scryptsalsa208sha256_MEMLIMIT_MIN)
    throw new Error('memlimit')
  if (memlimit > binding.crypto_pwhash_scryptsalsa208sha256_MEMLIMIT_MAX)
    throw new Error('memlimit')

  const res = binding.crypto_pwhash_scryptsalsa208sha256_str(
    out,
    passwd,
    opslimit,
    memlimit
  )

  if (res !== 0) throw new Error('status: ' + res)
}

exports.crypto_pwhash_scryptsalsa208sha256_str_verify_async = function (
  str,
  passwd,
  callback = undefined
) {
  if (str?.byteLength !== binding.crypto_pwhash_scryptsalsa208sha256_STRBYTES)
    throw new Error('str')
  if (!passwd?.byteLength) throw new Error('passwd')

  const [done, promise] = checkStatus(callback, true)

  binding.crypto_pwhash_scryptsalsa208sha256_str_verify_async(
    str.buffer,
    str.byteOffset,
    str.byteLength,

    passwd.buffer,
    passwd.byteOffset,
    passwd.byteLength,

    done
  )

  return promise
}

exports.crypto_pwhash_scryptsalsa208sha256_str_verify = function (str, passwd) {
  if (str?.byteLength !== binding.crypto_pwhash_scryptsalsa208sha256_STRBYTES)
    throw new Error('str')
  if (!passwd?.byteLength) throw new Error('passwd')

  return binding.crypto_pwhash_scryptsalsa208sha256_str_verify(str, passwd)
}

exports.crypto_pwhash_scryptsalsa208sha256_str_needs_rehash = function (
  str,
  opslimit,
  memlimit
) {
  if (str?.byteLength !== binding.crypto_pwhash_scryptsalsa208sha256_STRBYTES)
    throw new Error('str')
  if (opslimit < binding.crypto_pwhash_OPSLIMIT_MIN) throw new Error('opslimit')
  if (opslimit > binding.crypto_pwhash_OPSLIMIT_MAX) throw new Error('opslimit')
  if (memlimit < binding.crypto_pwhash_MEMLIMIT_MIN) throw new Error('memlimit')
  if (memlimit > binding.crypto_pwhash_MEMLIMIT_MAX) throw new Error('memlimit')

  return binding.crypto_pwhash_scryptsalsa208sha256_str_needs_rehash(
    str,
    opslimit,
    memlimit
  )
}

// crypto_kx

exports.crypto_kx_keypair = function (pk, sk) {
  if (pk?.byteLength !== binding.crypto_kx_PUBLICKEYBYTES) throw new Error('pk')
  if (sk?.byteLength !== binding.crypto_kx_SECRETKEYBYTES) throw new Error('sk')

  const res = binding.crypto_kx_keypair(pk, sk)

  if (res !== 0) throw new Error('status: ' + res)
}

exports.crypto_kx_seed_keypair = function (pk, sk, seed) {
  if (pk?.byteLength !== binding.crypto_kx_PUBLICKEYBYTES) throw new Error('pk')
  if (sk?.byteLength !== binding.crypto_kx_SECRETKEYBYTES) throw new Error('sk')
  if (seed?.byteLength !== binding.crypto_kx_SEEDBYTES) throw new Error('seed')

  const res = binding.crypto_kx_seed_keypair(pk, sk, seed)

  if (res !== 0) throw new Error('status: ' + res)
}

exports.crypto_kx_client_session_keys = function (
  rx,
  tx,
  clientPk,
  clientSk,
  serverPk
) {
  // match `std::optional` by coercing null to undefined
  rx ??= undefined
  tx ??= undefined

  if (!rx && !tx) throw new Error('at least one session key must be specified')

  if (rx) {
    if (rx?.byteLength !== binding.crypto_kx_SESSIONKEYBYTES)
      throw new Error(
        'receiving key buffer must be "crypto_kx_SESSIONKEYBYTES" bytes or null'
      )
  } else {
    if (tx?.byteLength !== binding.crypto_kx_SESSIONKEYBYTES)
      throw new Error(
        'transmitting key buffer must be "crypto_kx_SESSIONKEYBYTES" bytes or null'
      )
  }

  if (clientPk?.byteLength !== binding.crypto_kx_PUBLICKEYBYTES)
    throw new Error('client_pk')
  if (clientSk?.byteLength !== binding.crypto_kx_SECRETKEYBYTES)
    throw new Error('client_sk')
  if (serverPk?.byteLength !== binding.crypto_kx_PUBLICKEYBYTES)
    throw new Error('server_pk')

  const res = binding.crypto_kx_client_session_keys(
    rx,
    tx,
    clientPk,
    clientSk,
    serverPk
  )

  if (res !== 0) throw new Error('status: ' + res)
}

exports.crypto_kx_server_session_keys = function (
  rx,
  tx,
  serverPk,
  serverSk,
  clientPk
) {
  rx ??= undefined
  tx ??= undefined

  if (!rx && !tx) throw new Error('at least one session key must be specified')

  if (rx) {
    if (rx?.byteLength !== binding.crypto_kx_SESSIONKEYBYTES)
      throw new Error(
        'receiving key buffer must be "crypto_kx_SESSIONKEYBYTES" bytes or null'
      )
  } else {
    if (tx?.byteLength !== binding.crypto_kx_SESSIONKEYBYTES)
      throw new Error(
        'transmitting key buffer must be "crypto_kx_SESSIONKEYBYTES" bytes or null'
      )
  }

  if (serverPk?.byteLength !== binding.crypto_kx_PUBLICKEYBYTES)
    throw new Error('server_pk')
  if (serverSk?.byteLength !== binding.crypto_kx_SECRETKEYBYTES)
    throw new Error('server_sk')
  if (clientPk?.byteLength !== binding.crypto_kx_PUBLICKEYBYTES)
    throw new Error('client_pk')

  const res = binding.crypto_kx_server_session_keys(
    rx,
    tx,
    serverPk,
    serverSk,
    clientPk
  )

  if (res !== 0) throw new Error('status: ' + res)
}

// crypto_scalarmult

exports.crypto_scalarmult_base = function (q, n) {
  if (q?.byteLength !== binding.crypto_scalarmult_BYTES) throw new Error('q')
  if (n?.byteLength !== binding.crypto_scalarmult_SCALARBYTES)
    throw new Error('n')

  const res = binding.crypto_scalarmult_base(q, n)

  if (res !== 0) throw new Error('status: ' + res)
}

exports.crypto_scalarmult = function (q, n, p) {
  if (q?.byteLength !== binding.crypto_scalarmult_BYTES) throw new Error('q')
  if (n?.byteLength !== binding.crypto_scalarmult_SCALARBYTES)
    throw new Error('n')
  if (p?.byteLength !== binding.crypto_scalarmult_BYTES) throw new Error('p')

  const res = binding.crypto_scalarmult(q, n, p)

  if (res !== 0) throw new Error('status: ' + res)
}

exports.crypto_scalarmult_ed25519_base = function (q, n) {
  if (q?.byteLength !== binding.crypto_scalarmult_ed25519_BYTES)
    throw new Error('q')
  if (n?.byteLength !== binding.crypto_scalarmult_ed25519_SCALARBYTES)
    throw new Error('n')

  const res = binding.crypto_scalarmult_ed25519_base(q, n)

  if (res !== 0) throw new Error('status: ' + res)
}

exports.crypto_scalarmult_ed25519 = function (q, n, p) {
  if (q?.byteLength !== binding.crypto_scalarmult_ed25519_BYTES)
    throw new Error('q')
  if (n?.byteLength !== binding.crypto_scalarmult_ed25519_SCALARBYTES)
    throw new Error('n')
  if (p?.byteLength !== binding.crypto_scalarmult_ed25519_BYTES)
    throw new Error('p')

  const res = binding.crypto_scalarmult_ed25519(q, n, p)

  if (res !== 0) throw new Error('status: ' + res)
}

exports.crypto_core_ed25519_is_valid_point = function (p) {
  if (p?.byteLength !== binding.crypto_core_ed25519_BYTES) throw new Error('p')

  return binding.crypto_core_ed25519_is_valid_point(p)
}

exports.crypto_core_ed25519_from_uniform = function (p, r) {
  if (p?.byteLength !== binding.crypto_core_ed25519_BYTES) throw new Error('p')
  if (r?.byteLength !== binding.crypto_core_ed25519_UNIFORMBYTES)
    throw new Error('r')

  const res = binding.crypto_core_ed25519_from_uniform(p, r)

  if (res !== 0) throw new Error('status: ' + res)
}

exports.crypto_scalarmult_ed25519_base_noclamp = function (q, n) {
  if (q?.byteLength !== binding.crypto_scalarmult_ed25519_BYTES)
    throw new Error('q')
  if (n?.byteLength !== binding.crypto_scalarmult_ed25519_SCALARBYTES)
    throw new Error('n')

  const res = binding.crypto_scalarmult_ed25519_base_noclamp(q, n)

  if (res !== 0) throw new Error('status: ' + res)
}

exports.crypto_scalarmult_ed25519_noclamp = function (q, n, p) {
  if (q?.byteLength !== binding.crypto_scalarmult_ed25519_BYTES)
    throw new Error('q')
  if (n?.byteLength !== binding.crypto_scalarmult_ed25519_SCALARBYTES)
    throw new Error('n')
  if (p?.byteLength !== binding.crypto_scalarmult_ed25519_BYTES)
    throw new Error('p')

  const res = binding.crypto_scalarmult_ed25519_noclamp(q, n, p)

  if (res !== 0) throw new Error('status: ' + res)
}

// crypto_core

exports.crypto_core_ed25519_add = function (r, p, q) {
  if (r?.byteLength !== binding.crypto_core_ed25519_BYTES) throw new Error('r')
  if (p?.byteLength !== binding.crypto_core_ed25519_BYTES) throw new Error('p')
  if (q?.byteLength !== binding.crypto_core_ed25519_BYTES) throw new Error('q')

  const res = binding.crypto_core_ed25519_add(r, p, q)

  if (res !== 0) throw new Error('status: ' + res)
}

exports.crypto_core_ed25519_sub = function (r, p, q) {
  if (r?.byteLength !== binding.crypto_core_ed25519_BYTES) throw new Error('r')
  if (p?.byteLength !== binding.crypto_core_ed25519_BYTES) throw new Error('p')
  if (q?.byteLength !== binding.crypto_core_ed25519_BYTES) throw new Error('q')

  const res = binding.crypto_core_ed25519_sub(r, p, q)

  if (res !== 0) throw new Error('status: ' + res)
}

exports.crypto_core_ed25519_scalar_random = function (r) {
  if (r?.byteLength !== binding.crypto_core_ed25519_SCALARBYTES)
    throw new Error('r')

  binding.crypto_core_ed25519_scalar_random(r)
}

exports.crypto_core_ed25519_scalar_reduce = function (r, s) {
  if (r?.byteLength !== binding.crypto_core_ed25519_SCALARBYTES)
    throw new Error('r')
  if (s?.byteLength !== binding.crypto_core_ed25519_NONREDUCEDSCALARBYTES)
    throw new Error('s')

  binding.crypto_core_ed25519_scalar_reduce(r, s)
}

exports.crypto_core_ed25519_scalar_invert = function (recip, s) {
  if (recip?.byteLength !== binding.crypto_core_ed25519_SCALARBYTES)
    throw new Error('recip')
  if (s?.byteLength !== binding.crypto_core_ed25519_SCALARBYTES)
    throw new Error('s')

  binding.crypto_core_ed25519_scalar_invert(recip, s)
}

exports.crypto_core_ed25519_scalar_negate = function (neg, s) {
  if (neg?.byteLength !== binding.crypto_core_ed25519_SCALARBYTES)
    throw new Error('neg')
  if (s?.byteLength !== binding.crypto_core_ed25519_SCALARBYTES)
    throw new Error('s')

  binding.crypto_core_ed25519_scalar_negate(neg, s)
}

exports.crypto_core_ed25519_scalar_complement = function (comp, s) {
  if (comp?.byteLength !== binding.crypto_core_ed25519_SCALARBYTES)
    throw new Error('comp')
  if (s?.byteLength !== binding.crypto_core_ed25519_SCALARBYTES)
    throw new Error('s')

  binding.crypto_core_ed25519_scalar_complement(comp, s)
}

exports.crypto_core_ed25519_scalar_add = function (z, x, y) {
  if (z?.byteLength !== binding.crypto_core_ed25519_SCALARBYTES)
    throw new Error('z')
  if (x?.byteLength !== binding.crypto_core_ed25519_SCALARBYTES)
    throw new Error('x')
  if (y?.byteLength !== binding.crypto_core_ed25519_SCALARBYTES)
    throw new Error('y')

  binding.crypto_core_ed25519_scalar_add(z, x, y)
}

exports.crypto_core_ed25519_scalar_sub = function (z, x, y) {
  if (z?.byteLength !== binding.crypto_core_ed25519_SCALARBYTES)
    throw new Error('z')
  if (x?.byteLength !== binding.crypto_core_ed25519_SCALARBYTES)
    throw new Error('x')
  if (y?.byteLength !== binding.crypto_core_ed25519_SCALARBYTES)
    throw new Error('y')

  binding.crypto_core_ed25519_scalar_sub(z, x, y)
}

// crypto_shorthash

exports.crypto_shorthash = function (out, input, k) {
  if (out?.byteLength !== binding.crypto_shorthash_BYTES) throw new Error('out')
  if (k?.byteLength !== binding.crypto_shorthash_KEYBYTES) throw new Error('k')

  const res = binding.crypto_shorthash(out, input, k)

  if (res !== 0) throw new Error('status: ' + res)
}

// crypto_kdf

exports.crypto_kdf_keygen = function (key) {
  if (key?.byteLength !== binding.crypto_kdf_KEYBYTES) throw new Error('key')

  binding.crypto_kdf_keygen(key)
}

exports.crypto_kdf_derive_from_key = function (subkey, subkeyId, ctx, key) {
  if (subkey?.byteLength < binding.crypto_kdf_BYTES_MIN)
    throw new Error('subkey')
  if (subkey?.byteLength > binding.crypto_kdf_BYTES_MAX)
    throw new Error('subkey')
  if (ctx?.byteLength !== binding.crypto_kdf_CONTEXTBYTES)
    throw new Error('ctx')
  if (key?.byteLength !== binding.crypto_kdf_KEYBYTES) throw new Error('key')

  const res = binding.crypto_kdf_derive_from_key(subkey, subkeyId, ctx, key)

  if (res !== 0) throw new Error('status: ' + res)
}

// crypto_hash

exports.crypto_hash = function (out, input) {
  if (out?.byteLength !== binding.crypto_hash_BYTES) throw new Error('out')

  const res = binding.crypto_hash(out, input)

  if (res !== 0) throw new Error('status: ' + res)
}

exports.crypto_hash_sha256 = function (out, input) {
  if (out?.byteLength !== binding.crypto_hash_sha256_BYTES)
    throw new Error('out')

  const res = binding.crypto_hash_sha256(out, input)

  if (res !== 0) throw new Error('status: ' + res)
}

exports.crypto_hash_sha256_init = function (state) {
  if (state?.byteLength !== binding.crypto_hash_sha256_STATEBYTES) {
    throw new Error("state must be 'crypto_hash_sha256_STATEBYTES' bytes")
  }

  const res = binding.crypto_hash_sha256_init(state)

  if (res !== 0) throw new Error('status: ' + res)
}

exports.crypto_hash_sha256_update = function (state, input) {
  if (state?.byteLength !== binding.crypto_hash_sha256_STATEBYTES) {
    throw new Error("state must be 'crypto_hash_sha256_STATEBYTES' bytes")
  }

  const res = binding.crypto_hash_sha256_update(state, input)

  if (res !== 0) throw new Error('status: ' + res)
}

exports.crypto_hash_sha256_final = function (state, out) {
  if (state?.byteLength !== binding.crypto_hash_sha256_STATEBYTES) {
    throw new Error("state must be 'crypto_hash_sha256_STATEBYTES' bytes")
  }
  if (out?.byteLength !== binding.crypto_hash_sha256_BYTES)
    throw new Error('state')

  const res = binding.crypto_hash_sha256_final(state, out)

  if (res !== 0) throw new Error('status: ' + res)
}

exports.crypto_hash_sha512 = function (out, input) {
  if (out?.byteLength !== binding.crypto_hash_sha512_BYTES)
    throw new Error('out')

  const res = binding.crypto_hash_sha512(out, input)

  if (res !== 0) throw new Error('status: ' + res)
}

exports.crypto_hash_sha512_init = function (state) {
  if (state?.byteLength !== binding.crypto_hash_sha512_STATEBYTES) {
    throw new Error("state must be 'crypto_hash_sha512_STATEBYTES' bytes")
  }

  const res = binding.crypto_hash_sha512_init(state)

  if (res !== 0) throw new Error('status: ' + res)
}

exports.crypto_hash_sha512_update = function (state, input) {
  if (state?.byteLength !== binding.crypto_hash_sha512_STATEBYTES) {
    throw new Error("state must be 'crypto_hash_sha512_STATEBYTES' bytes")
  }

  const res = binding.crypto_hash_sha512_update(state, input)

  if (res !== 0) throw new Error('status: ' + res)
}

exports.crypto_hash_sha512_final = function (state, out) {
  if (state?.byteLength !== binding.crypto_hash_sha512_STATEBYTES) {
    throw new Error("state must be 'crypto_hash_sha512_STATEBYTES' bytes")
  }
  if (out?.byteLength !== binding.crypto_hash_sha512_BYTES)
    throw new Error('out')

  const res = binding.crypto_hash_sha512_final(state, out)

  if (res !== 0) throw new Error('status: ' + res)
}

// crypto_aead

exports.crypto_aead_xchacha20poly1305_ietf_keygen = function (k) {
  if (k?.byteLength !== binding.crypto_aead_xchacha20poly1305_ietf_KEYBYTES)
    throw new Error('k')

  binding.crypto_aead_xchacha20poly1305_ietf_keygen(k)
}

exports.crypto_aead_xchacha20poly1305_ietf_encrypt = function (
  c,
  m,
  ad,
  nsec = null,
  npub,
  k
) {
  ad ??= undefined
  if (nsec !== null) throw new Error('nsec must always be set to null')
  if (
    c?.byteLength !==
    m.byteLength + binding.crypto_aead_xchacha20poly1305_ietf_ABYTES
  )
    throw new Error(
      'c must "m.byteLength + crypto_aead_xchacha20poly1305_ietf_ABYTES" bytes'
    )
  if (c?.byteLength > 0xffffffff)
    throw new Error('c.byteLength must be a 32bit integer')
  if (npub?.byteLength !== binding.crypto_aead_xchacha20poly1305_ietf_NPUBBYTES)
    throw new Error('npub')
  if (k?.byteLength !== binding.crypto_aead_xchacha20poly1305_ietf_KEYBYTES)
    throw new Error('k')

  const res = binding.crypto_aead_xchacha20poly1305_ietf_encrypt(
    c,
    m,
    ad,
    npub,
    k
  )
  if (res < 0) throw new Error('could not encrypt data')

  return res
}

exports.crypto_aead_xchacha20poly1305_ietf_decrypt = function (
  m,
  nsec = null,
  c,
  ad,
  npub,
  k
) {
  ad ??= undefined
  if (nsec !== null) throw new Error('nsec must always be set to null')
  if (
    m?.byteLength !==
    c.byteLength - binding.crypto_aead_xchacha20poly1305_ietf_ABYTES
  )
    throw new Error(
      'm must "c.byteLength - crypto_aead_xchacha20poly1305_ietf_ABYTES" bytes'
    )
  if (m?.byteLength > 0xffffffff)
    throw new Error('m.byteLength must be a 32bit integer')
  if (npub?.byteLength !== binding.crypto_aead_xchacha20poly1305_ietf_NPUBBYTES)
    throw new Error('npub')
  if (k?.byteLength !== binding.crypto_aead_xchacha20poly1305_ietf_KEYBYTES)
    throw new Error('k')

  const res = binding.crypto_aead_xchacha20poly1305_ietf_decrypt(
    m,
    c,
    ad,
    npub,
    k
  )
  if (res < 0) throw new Error('could not verify data')

  return res
}

exports.crypto_aead_xchacha20poly1305_ietf_encrypt_detached = function (
  c,
  mac,
  m,
  ad,
  nsec = null,
  npub,
  k
) {
  ad ??= undefined
  if (nsec !== null) throw new Error('nsec must always be set to null')
  if (c?.byteLength !== m.byteLength)
    throw new Error('c must be "m.byteLength" bytes')
  if (mac?.byteLength !== binding.crypto_aead_xchacha20poly1305_ietf_ABYTES)
    throw new Error('mac')
  if (npub?.byteLength !== binding.crypto_aead_xchacha20poly1305_ietf_NPUBBYTES)
    throw new Error('npub')
  if (k?.byteLength !== binding.crypto_aead_xchacha20poly1305_ietf_KEYBYTES)
    throw new Error('k')

  const res = binding.crypto_aead_xchacha20poly1305_ietf_encrypt_detached(
    c,
    mac,
    m,
    ad,
    npub,
    k
  )
  if (res < 0) throw new Error('could not encrypt data')

  return res
}

exports.crypto_aead_xchacha20poly1305_ietf_decrypt_detached = function (
  m,
  nsec = null,
  c,
  mac,
  ad,
  npub,
  k
) {
  ad ??= undefined
  if (nsec !== null) throw new Error('nsec must always be set to null')
  if (m?.byteLength !== c.byteLength)
    throw new Error('m must be "c.byteLength" bytes')
  if (mac?.byteLength !== binding.crypto_aead_xchacha20poly1305_ietf_ABYTES)
    throw new Error('mac')
  if (npub?.byteLength !== binding.crypto_aead_xchacha20poly1305_ietf_NPUBBYTES)
    throw new Error('npub')
  if (k?.byteLength !== binding.crypto_aead_xchacha20poly1305_ietf_KEYBYTES)
    throw new Error('k')

  const res = binding.crypto_aead_xchacha20poly1305_ietf_decrypt_detached(
    m,
    c,
    mac,
    ad,
    npub,
    k
  )
  if (res !== 0) throw new Error('could not verify data')
}

exports.crypto_aead_chacha20poly1305_ietf_keygen = function (k) {
  if (k?.byteLength !== binding.crypto_aead_chacha20poly1305_ietf_KEYBYTES)
    throw new Error('k')

  binding.crypto_aead_chacha20poly1305_ietf_keygen(k)
}

exports.crypto_aead_chacha20poly1305_ietf_encrypt = function (
  c,
  m,
  ad,
  nsec = null,
  npub,
  k
) {
  ad ??= undefined
  if (nsec !== null) throw new Error('nsec must always be set to null')
  if (
    c?.byteLength !==
    m.byteLength + binding.crypto_aead_chacha20poly1305_ietf_ABYTES
  )
    throw new Error(
      'c must "m.byteLength + crypto_aead_chacha20poly1305_ietf_ABYTES" bytes'
    )
  if (c?.byteLength > 0xffffffff)
    throw new Error('c.byteLength must be a 32bit integer')
  if (npub?.byteLength !== binding.crypto_aead_chacha20poly1305_ietf_NPUBBYTES)
    throw new Error('npub')
  if (k?.byteLength !== binding.crypto_aead_chacha20poly1305_ietf_KEYBYTES)
    throw new Error('k')

  const res = binding.crypto_aead_chacha20poly1305_ietf_encrypt(
    c,
    m,
    ad,
    npub,
    k
  )
  if (res < 0) throw new Error('could not encrypt data')

  return res
}

exports.crypto_aead_chacha20poly1305_ietf_decrypt = function (
  m,
  nsec = null,
  c,
  ad,
  npub,
  k
) {
  ad ??= undefined
  if (nsec !== null) throw new Error('nsec must always be set to null')
  if (
    m?.byteLength !==
    c.byteLength - binding.crypto_aead_chacha20poly1305_ietf_ABYTES
  )
    throw new Error(
      'm must "c.byteLength - crypto_aead_chacha20poly1305_ietf_ABYTES" bytes'
    )
  if (m?.byteLength > 0xffffffff)
    throw new Error('m.byteLength must be a 32bit integer')
  if (npub?.byteLength !== binding.crypto_aead_chacha20poly1305_ietf_NPUBBYTES)
    throw new Error('npub')
  if (k?.byteLength !== binding.crypto_aead_chacha20poly1305_ietf_KEYBYTES)
    throw new Error('k')

  const res = binding.crypto_aead_chacha20poly1305_ietf_decrypt(
    m,
    c,
    ad,
    npub,
    k
  )
  if (res < 0) throw new Error('could not verify data')

  return res
}

exports.crypto_aead_chacha20poly1305_ietf_encrypt_detached = function (
  c,
  mac,
  m,
  ad,
  nsec = null,
  npub,
  k
) {
  ad ??= undefined
  if (nsec !== null) throw new Error('nsec must always be set to null')
  if (c?.byteLength !== m.byteLength)
    throw new Error('c must be "m.byteLength" bytes')
  if (mac?.byteLength !== binding.crypto_aead_chacha20poly1305_ietf_ABYTES)
    throw new Error('mac')
  if (npub?.byteLength !== binding.crypto_aead_chacha20poly1305_ietf_NPUBBYTES)
    throw new Error('npub')
  if (k?.byteLength !== binding.crypto_aead_chacha20poly1305_ietf_KEYBYTES)
    throw new Error('k')

  const res = binding.crypto_aead_chacha20poly1305_ietf_encrypt_detached(
    c,
    mac,
    m,
    ad,
    npub,
    k
  )
  if (res < 0) throw new Error('could not encrypt data')

  return res
}

exports.crypto_aead_chacha20poly1305_ietf_decrypt_detached = function (
  m,
  nsec = null,
  c,
  mac,
  ad,
  npub,
  k
) {
  ad ??= undefined
  if (nsec !== null) throw new Error('nsec must always be set to null')
  if (m?.byteLength !== c.byteLength)
    throw new Error('m must be "c.byteLength" bytes')
  if (mac?.byteLength !== binding.crypto_aead_chacha20poly1305_ietf_ABYTES)
    throw new Error('mac')
  if (npub?.byteLength !== binding.crypto_aead_chacha20poly1305_ietf_NPUBBYTES)
    throw new Error('npub')
  if (k?.byteLength !== binding.crypto_aead_chacha20poly1305_ietf_KEYBYTES)
    throw new Error('k')

  const res = binding.crypto_aead_chacha20poly1305_ietf_decrypt_detached(
    m,
    c,
    mac,
    ad,
    npub,
    k
  )
  if (res !== 0) throw new Error('could not verify data')
}

// crypto_stream

exports.crypto_stream_xor_wrap_init = function (state, n, k) {
  if (state?.byteLength !== binding.sn_crypto_stream_xor_STATEBYTES) {
    throw new Error("state must be 'sn_crypto_stream_xor_STATEBYTES' bytes")
  }
  if (n?.byteLength !== binding.crypto_stream_NONCEBYTES) throw new Error('n')
  if (k?.byteLength !== binding.crypto_stream_KEYBYTES) throw new Error('k')

  binding.crypto_stream_xor_wrap_init(state, n, k)
}

exports.crypto_stream_xor_wrap_update = function (state, c, m) {
  if (state?.byteLength !== binding.sn_crypto_stream_xor_STATEBYTES) {
    throw new Error("state must be 'sn_crypto_stream_xor_STATEBYTES' bytes")
  }
  if (c?.byteLength !== m.byteLength)
    throw new Error('c must be "m.byteLength" bytes')

  binding.crypto_stream_xor_wrap_update(state, c, m)
}

exports.crypto_stream_xor_wrap_final = function (state) {
  if (state?.byteLength !== binding.sn_crypto_stream_xor_STATEBYTES) {
    throw new Error("state must be 'sn_crypto_stream_xor_STATEBYTES' bytes")
  }

  binding.crypto_stream_xor_wrap_final(state)
}

exports.crypto_stream_chacha20_xor_wrap_init = function (state, n, k) {
  if (state?.byteLength !== binding.crypto_stream_chacha20_xor_STATEBYTES) {
    throw new Error(
      "state must be 'crypto_stream_chacha20_xor_STATEBYTES' bytes"
    )
  }
  if (n?.byteLength !== binding.crypto_stream_chacha20_NONCEBYTES)
    throw new Error('n')
  if (k?.byteLength !== binding.crypto_stream_chacha20_KEYBYTES)
    throw new Error('k')

  binding.crypto_stream_chacha20_xor_wrap_init(state, n, k)
}

exports.crypto_stream_chacha20_xor_wrap_update = function (state, c, m) {
  if (state?.byteLength !== binding.crypto_stream_chacha20_xor_STATEBYTES) {
    throw new Error(
      "state must be 'crypto_stream_chacha20_xor_STATEBYTES' bytes"
    )
  }
  if (c?.byteLength !== m.byteLength)
    throw new Error('c must be "m.byteLength" bytes')

  binding.crypto_stream_chacha20_xor_wrap_update(state, c, m)
}

exports.crypto_stream_chacha20_xor_wrap_final = function (state) {
  if (state?.byteLength !== binding.crypto_stream_chacha20_xor_STATEBYTES) {
    throw new Error(
      "state must be 'crypto_stream_chacha20_xor_STATEBYTES' bytes"
    )
  }

  binding.crypto_stream_chacha20_xor_wrap_final(state)
}

exports.crypto_stream_chacha20_ietf_xor_wrap_init = function (state, n, k) {
  if (
    state?.byteLength !== binding.crypto_stream_chacha20_ietf_xor_STATEBYTES
  ) {
    throw new Error(
      "state must be 'crypto_stream_chacha20_ietf_xor_STATEBYTES' bytes"
    )
  }
  if (n?.byteLength !== binding.crypto_stream_chacha20_ietf_NONCEBYTES)
    throw new Error('n')
  if (k?.byteLength !== binding.crypto_stream_chacha20_ietf_KEYBYTES)
    throw new Error('k')

  binding.crypto_stream_chacha20_ietf_xor_wrap_init(state, n, k)
}

exports.crypto_stream_chacha20_ietf_xor_wrap_update = function (state, c, m) {
  if (
    state?.byteLength !== binding.crypto_stream_chacha20_ietf_xor_STATEBYTES
  ) {
    throw new Error(
      "state must be 'crypto_stream_chacha20_ietf_xor_STATEBYTES' bytes"
    )
  }
  if (c?.byteLength !== m.byteLength)
    throw new Error('c must be "m.byteLength" bytes')

  binding.crypto_stream_chacha20_ietf_xor_wrap_update(state, c, m)
}

exports.crypto_stream_chacha20_ietf_xor_wrap_final = function (state) {
  if (
    state?.byteLength !== binding.crypto_stream_chacha20_ietf_xor_STATEBYTES
  ) {
    throw new Error(
      "state must be 'crypto_stream_chacha20_ietf_xor_STATEBYTES' bytes"
    )
  }

  binding.crypto_stream_chacha20_ietf_xor_wrap_final(state)
}

exports.crypto_stream_xchacha20_xor_wrap_init = function (state, n, k) {
  if (state?.byteLength !== binding.crypto_stream_xchacha20_xor_STATEBYTES) {
    throw new Error(
      "state must be 'crypto_stream_xchacha20_xor_STATEBYTES' bytes"
    )
  }
  if (n?.byteLength !== binding.crypto_stream_xchacha20_NONCEBYTES)
    throw new Error('n')
  if (k?.byteLength !== binding.crypto_stream_xchacha20_KEYBYTES)
    throw new Error('k')

  binding.crypto_stream_xchacha20_xor_wrap_init(state, n, k)
}

exports.crypto_stream_xchacha20_xor_wrap_update = function (state, c, m) {
  if (state?.byteLength !== binding.crypto_stream_xchacha20_xor_STATEBYTES) {
    throw new Error(
      "state must be 'crypto_stream_xchacha20_xor_STATEBYTES' bytes"
    )
  }
  if (c?.byteLength !== m.byteLength)
    throw new Error('c must be "m.byteLength" bytes')

  binding.crypto_stream_xchacha20_xor_wrap_update(state, c, m)
}

exports.crypto_stream_xchacha20_xor_wrap_final = function (state) {
  if (state?.byteLength !== binding.crypto_stream_xchacha20_xor_STATEBYTES) {
    throw new Error(
      "state must be 'crypto_stream_xchacha20_xor_STATEBYTES' bytes"
    )
  }

  binding.crypto_stream_xchacha20_xor_wrap_final(state)
}

exports.crypto_stream_salsa20_xor_wrap_init = function (state, n, k) {
  if (state?.byteLength !== binding.crypto_stream_salsa20_xor_STATEBYTES) {
    throw new Error(
      "state must be 'crypto_stream_salsa20_xor_STATEBYTES' bytes"
    )
  }
  if (n?.byteLength !== binding.crypto_stream_salsa20_NONCEBYTES)
    throw new Error('n')
  if (k?.byteLength !== binding.crypto_stream_salsa20_KEYBYTES)
    throw new Error('k')

  binding.crypto_stream_salsa20_xor_wrap_init(state, n, k)
}

exports.crypto_stream_salsa20_xor_wrap_update = function (state, c, m) {
  if (state?.byteLength !== binding.crypto_stream_salsa20_xor_STATEBYTES) {
    throw new Error(
      "state must be 'crypto_stream_salsa20_xor_STATEBYTES' bytes"
    )
  }
  if (c?.byteLength !== m.byteLength)
    throw new Error('c must be "m.byteLength" bytes')

  binding.crypto_stream_salsa20_xor_wrap_update(state, c, m)
}

exports.crypto_stream_salsa20_xor_wrap_final = function (state) {
  if (state?.byteLength !== binding.crypto_stream_salsa20_xor_STATEBYTES) {
    throw new Error(
      "state must be 'crypto_stream_salsa20_xor_STATEBYTES' bytes"
    )
  }

  binding.crypto_stream_salsa20_xor_wrap_final(state)
}

// experimental

exports.extension_tweak_ed25519_base = function (n, p, ns) {
  if (n?.byteLength !== binding.extension_tweak_ed25519_SCALARBYTES)
    throw new Error('n')
  if (p?.byteLength !== binding.extension_tweak_ed25519_BYTES)
    throw new Error('p')

  binding.extension_tweak_ed25519_base(n, p, ns)
}

exports.extension_tweak_ed25519_sign_detached = function (sig, m, scalar, pk) {
  if (sig?.byteLength !== binding.crypto_sign_BYTES) throw new Error('sig')
  if (scalar?.byteLength !== binding.extension_tweak_ed25519_SCALARBYTES)
    throw new Error('scalar')
  if (pk && pk.byteLength !== binding.crypto_sign_PUBLICKEYBYTES)
    throw new Error('pk')

  const res = binding.extension_tweak_ed25519_sign_detached(sig, m, scalar, pk)
  if (res !== 0) throw new Error('failed to compute signature')
}

exports.extension_tweak_ed25519_sk_to_scalar = function (n, sk) {
  if (n?.byteLength !== binding.extension_tweak_ed25519_SCALARBYTES)
    throw new Error('n')
  if (sk?.byteLength !== binding.crypto_sign_SECRETKEYBYTES)
    throw new Error('sk')

  binding.extension_tweak_ed25519_sk_to_scalar(n, sk)
}

exports.extension_tweak_ed25519_scalar = function (scalarOut, scalar, ns) {
  if (scalarOut?.byteLength !== binding.extension_tweak_ed25519_SCALARBYTES)
    throw new Error('scalar_out')
  if (scalar?.byteLength !== binding.extension_tweak_ed25519_SCALARBYTES)
    throw new Error('scalar')

  binding.extension_tweak_ed25519_scalar(scalarOut, scalar, ns)
}

exports.extension_tweak_ed25519_pk = function (tpk, pk, ns) {
  if (tpk?.byteLength !== binding.crypto_sign_PUBLICKEYBYTES)
    throw new Error('tpk')
  if (pk?.byteLength !== binding.crypto_sign_PUBLICKEYBYTES)
    throw new Error('pk')

  const res = binding.extension_tweak_ed25519_pk(tpk, pk, ns)
  if (res !== 0) throw new Error('failed to tweak public key')
}

exports.extension_tweak_ed25519_keypair = function (
  pk,
  scalarOut,
  scalarIn,
  ns
) {
  if (pk?.byteLength !== binding.extension_tweak_ed25519_BYTES)
    throw new Error('pk')
  if (scalarOut?.byteLength !== binding.extension_tweak_ed25519_SCALARBYTES)
    throw new Error('scalar_out')
  if (scalarIn?.byteLength !== binding.extension_tweak_ed25519_SCALARBYTES)
    throw new Error('scalar_in')

  binding.extension_tweak_ed25519_keypair(pk, scalarOut, scalarIn, ns)
}

exports.extension_tweak_ed25519_scalar_add = function (scalarOut, scalar, n) {
  if (scalarOut?.byteLength !== binding.extension_tweak_ed25519_SCALARBYTES)
    throw new Error('scalar_out')
  if (scalar?.byteLength !== binding.extension_tweak_ed25519_SCALARBYTES)
    throw new Error('scalar')
  if (n?.byteLength !== binding.extension_tweak_ed25519_SCALARBYTES)
    throw new Error('n')

  binding.extension_tweak_ed25519_scalar_add(scalarOut, scalar, n)
}

exports.extension_tweak_ed25519_pk_add = function (tpk, pk, p) {
  if (tpk?.byteLength !== binding.crypto_sign_PUBLICKEYBYTES)
    throw new Error('tpk')
  if (pk?.byteLength !== binding.crypto_sign_PUBLICKEYBYTES)
    throw new Error('pk')
  if (p?.byteLength !== binding.crypto_sign_PUBLICKEYBYTES) throw new Error('p')

  const res = binding.extension_tweak_ed25519_pk_add(tpk, pk, p)
  if (res !== 0) throw new Error('failed to add tweak to public key')
}

exports.extension_tweak_ed25519_keypair_add = function (
  pk,
  scalarOut,
  scalarIn,
  tweak
) {
  if (pk?.byteLength !== binding.extension_tweak_ed25519_BYTES)
    throw new Error('pk')
  if (scalarOut?.byteLength !== binding.extension_tweak_ed25519_SCALARBYTES)
    throw new Error('scalar_out')
  if (scalarIn?.byteLength !== binding.extension_tweak_ed25519_SCALARBYTES)
    throw new Error('scalar_in')
  if (tweak?.byteLength !== binding.extension_tweak_ed25519_SCALARBYTES)
    throw new Error('tweak')

  const res = binding.extension_tweak_ed25519_keypair_add(
    pk,
    scalarOut,
    scalarIn,
    tweak
  )
  if (res !== 0) throw new Error('failed to add tweak to keypair')
}

exports.extension_pbkdf2_sha512_async = function (
  out,
  passwd,
  salt,
  iter,
  outlen,
  callback = undefined
) {
  if (iter < binding.extension_pbkdf2_sha512_ITERATIONS_MIN)
    throw new Error('iterations')
  if (outlen > binding.extension_pbkdf2_sha512_BYTES_MAX)
    throw new Error('outlen')
  if (out?.byteLength < outlen) throw new Error('out')
  if (!out?.byteLength) throw new Error('out')
  if (!passwd?.byteLength) throw new Error('passwd')
  if (!salt?.byteLength) throw new Error('salt')

  const [done, promise] = checkStatus(callback)

  binding.extension_pbkdf2_sha512_async(
    out.buffer,
    out.byteOffset,
    out.byteLength,

    passwd.buffer,
    passwd.byteOffset,
    passwd.byteLength,

    salt.buffer,
    salt.byteOffset,
    salt.byteLength,

    iter,
    outlen,

    done
  )

  return promise
}

exports.extension_pbkdf2_sha512 = function (out, passwd, salt, iter, outlen) {
  if (iter < binding.extension_pbkdf2_sha512_ITERATIONS_MIN)
    throw new Error('iterations')
  if (outlen > binding.extension_pbkdf2_sha512_BYTES_MAX)
    throw new Error('outlen')
  if (out?.byteLength < outlen) throw new Error('out')

  const res = binding.extension_pbkdf2_sha512(out, passwd, salt, iter, outlen)

  if (res !== 0) throw new Error('failed to add tweak to public key')
}

function checkStatus(callback, booleanResult = false) {
  let done, promise

  if (typeof callback === 'function') {
    done = function (status) {
      if (booleanResult) callback(null, status === 0)
      else if (status === 0) callback(null)
      else callback(new Error('status: ' + status))
    }
  } else {
    promise = new Promise(function (resolve, reject) {
      done = function (status) {
        if (booleanResult) resolve(status === 0)
        else if (status === 0) resolve()
        else reject(new Error('status: ' + status))
      }
    })
  }

  return [done, promise]
}
{
  "name": "sodium-native",
  "version": "5.0.10",
  "description": "Low level bindings for libsodium",
  "main": "index.js",
  "files": [
    "index.js",
    "binding.cc",
    "binding.js",
    "extensions",
    "prebuilds",
    "CMakeLists.txt"
  ],
  "addon": true,
  "dependencies": {
    "require-addon": "^1.1.0",
    "which-runtime": "^1.2.1"
  },
  "devDependencies": {
    "bare-compat-napi": "^1.3.5",
    "brittle": "^3.16.2",
    "cmake-bare": "^1.6.1",
    "cmake-fetch": "^1.4.3",
    "cmake-napi": "^1.2.1",
    "prettier": "^3.6.2",
    "prettier-config-holepunch": "^1.0.0"
  },
  "scripts": {
    "test": "prettier . --check && npm run test:node && npm run test:bare",
    "test:node": "node test/all.js",
    "test:bare": "bare test/all.js"
  },
  "standard": {
    "ignore": [
      "/test/fixtures/*.js"
    ]
  },
  "engines": {
    "bare": ">=1.16.0"
  },
  "repository": {
    "type": "git",
    "url": "git+https://github.com/holepunchto/sodium-native.git"
  },
  "contributors": [
    "Emil Bay <github@tixz.dk> (http://bayes.dk)",
    "Mathias Buus <mathiasbuus@gmail.com> (https://mafinto.sh)",
    "Christophe Diederichs <chm-diederichs@hyperdivision.dk>"
  ],
  "license": "MIT",
  "bugs": {
    "url": "https://github.com/holepunchto/sodium-native/issues"
  },
  "homepage": "https://github.com/holepunchto/sodium-native"
}
const sodium = require('sodium-universal')
const b4a = require('b4a')

const ABYTES = sodium.crypto_secretstream_xchacha20poly1305_ABYTES
const TAG_MESSAGE = sodium.crypto_secretstream_xchacha20poly1305_TAG_MESSAGE
const TAG_FINAL = sodium.crypto_secretstream_xchacha20poly1305_TAG_FINAL
const STATEBYTES = sodium.crypto_secretstream_xchacha20poly1305_STATEBYTES
const HEADERBYTES = sodium.crypto_secretstream_xchacha20poly1305_HEADERBYTES
const KEYBYTES = sodium.crypto_secretstream_xchacha20poly1305_KEYBYTES
const TAG_FINAL_BYTE = b4a.isBuffer(TAG_FINAL) ? TAG_FINAL[0] : TAG_FINAL

const EMPTY = b4a.alloc(0)
const TAG = b4a.alloc(1)

class Push {
  constructor (key, state = b4a.allocUnsafeSlow(STATEBYTES), header = b4a.allocUnsafeSlow(HEADERBYTES)) {
    if (!TAG_FINAL) throw new Error('JavaScript sodium version needs to support crypto_secretstream_xchacha20poly')

    this.key = key
    this.state = state
    this.header = header

    sodium.crypto_secretstream_xchacha20poly1305_init_push(this.state, this.header, this.key)
  }

  next (message, cipher = b4a.allocUnsafe(message.byteLength + ABYTES)) {
    sodium.crypto_secretstream_xchacha20poly1305_push(this.state, cipher, message, null, TAG_MESSAGE)
    return cipher
  }

  final (message = EMPTY, cipher = b4a.allocUnsafe(ABYTES)) {
    sodium.crypto_secretstream_xchacha20poly1305_push(this.state, cipher, message, null, TAG_FINAL)
    return cipher
  }
}

class Pull {
  constructor (key, state = b4a.allocUnsafeSlow(STATEBYTES)) {
    if (!TAG_FINAL) throw new Error('JavaScript sodium version needs to support crypto_secretstream_xchacha20poly')

    this.key = key
    this.state = state
    this.final = false
  }

  init (header) {
    sodium.crypto_secretstream_xchacha20poly1305_init_pull(this.state, header, this.key)
  }

  next (cipher, message = b4a.allocUnsafe(cipher.byteLength - ABYTES)) {
    sodium.crypto_secretstream_xchacha20poly1305_pull(this.state, message, TAG, cipher, null)
    this.final = TAG[0] === TAG_FINAL_BYTE
    return message
  }
}

function keygen (buf = b4a.alloc(KEYBYTES)) {
  sodium.crypto_secretstream_xchacha20poly1305_keygen(buf)
  return buf
}

module.exports = {
  keygen,
  KEYBYTES,
  ABYTES,
  STATEBYTES,
  HEADERBYTES,
  Push,
  Pull
}
{
  "name": "sodium-secretstream",
  "version": "1.2.0",
  "description": "Wraps libsodiums secretstream in a higher level abstraction",
  "main": "index.js",
  "dependencies": {
    "b4a": "^1.1.1",
    "sodium-universal": "^5.0.0"
  },
  "devDependencies": {
    "standard": "^17.0.0"
  },
  "scripts": {
    "test": "standard"
  },
  "repository": {
    "type": "git",
    "url": "https://github.com/mafintosh/sodium-secretstream.git"
  },
  "author": "Mathias Buus (@mafintosh)",
  "license": "MIT",
  "bugs": {
    "url": "https://github.com/mafintosh/sodium-secretstream/issues"
  },
  "homepage": "https://github.com/mafintosh/sodium-secretstream"
}
module.exports = require('sodium-native')
{
  "name": "sodium-universal",
  "version": "5.0.1",
  "description": "Universal wrapper for sodium-javascript and sodium-native working in Node.js and the Browser",
  "main": "index.js",
  "dependencies": {
    "sodium-native": "^5.0.1"
  },
  "peerDependencies": {
    "sodium-javascript": "~0.8.0"
  },
  "peerDependenciesMeta": {
    "sodium-javascript": {
      "optional": true
    }
  },
  "scripts": {
    "prepublish": "./build-scripts/generate.js"
  },
  "browser": {
    "sodium-native": "sodium-javascript"
  },
  "browserify": {
    "transform": [
      "./build-scripts/transform.js"
    ]
  },
  "repository": {
    "type": "git",
    "url": "git+https://github.com/holepunchto/sodium-universal.git"
  },
  "keywords": [
    "libsodium",
    "sodium",
    "sodium-native",
    "sodium-javascript",
    "browserify"
  ],
  "contributors": [
    "Emil Bay <github@tixz.dk> (http://bayes.dk)",
    "Mathias Buus <mathiasbuus@gmail.com> (https://mafinto.sh)",
    "Christophe Diederichs <chm-diederichs@hyperdivision.dk>"
  ],
  "license": "MIT",
  "bugs": {
    "url": "https://github.com/holepunchto/sodium-universal/issues"
  },
  "homepage": "https://github.com/holepunchto/sodium-universal#readme"
}
const { EventEmitter } = require('events-universal')
const STREAM_DESTROYED = new Error('Stream was destroyed')
const PREMATURE_CLOSE = new Error('Premature close')

const FIFO = require('fast-fifo')
const TextDecoder = require('text-decoder')

// if we do a future major, expect queue microtask to be there always, for now a bit defensive
const qmt = typeof queueMicrotask === 'undefined' ? fn => global.process.nextTick(fn) : queueMicrotask

/* eslint-disable no-multi-spaces */

// 29 bits used total (4 from shared, 14 from read, and 11 from write)
const MAX = ((1 << 29) - 1)

// Shared state
const OPENING       = 0b0001
const PREDESTROYING = 0b0010
const DESTROYING    = 0b0100
const DESTROYED     = 0b1000

const NOT_OPENING = MAX ^ OPENING
const NOT_PREDESTROYING = MAX ^ PREDESTROYING

// Read state (4 bit offset from shared state)
const READ_ACTIVE           = 0b00000000000001 << 4
const READ_UPDATING         = 0b00000000000010 << 4
const READ_PRIMARY          = 0b00000000000100 << 4
const READ_QUEUED           = 0b00000000001000 << 4
const READ_RESUMED          = 0b00000000010000 << 4
const READ_PIPE_DRAINED     = 0b00000000100000 << 4
const READ_ENDING           = 0b00000001000000 << 4
const READ_EMIT_DATA        = 0b00000010000000 << 4
const READ_EMIT_READABLE    = 0b00000100000000 << 4
const READ_EMITTED_READABLE = 0b00001000000000 << 4
const READ_DONE             = 0b00010000000000 << 4
const READ_NEXT_TICK        = 0b00100000000000 << 4
const READ_NEEDS_PUSH       = 0b01000000000000 << 4
const READ_READ_AHEAD       = 0b10000000000000 << 4

// Combined read state
const READ_FLOWING = READ_RESUMED | READ_PIPE_DRAINED
const READ_ACTIVE_AND_NEEDS_PUSH = READ_ACTIVE | READ_NEEDS_PUSH
const READ_PRIMARY_AND_ACTIVE = READ_PRIMARY | READ_ACTIVE
const READ_EMIT_READABLE_AND_QUEUED = READ_EMIT_READABLE | READ_QUEUED
const READ_RESUMED_READ_AHEAD = READ_RESUMED | READ_READ_AHEAD

const READ_NOT_ACTIVE             = MAX ^ READ_ACTIVE
const READ_NON_PRIMARY            = MAX ^ READ_PRIMARY
const READ_NON_PRIMARY_AND_PUSHED = MAX ^ (READ_PRIMARY | READ_NEEDS_PUSH)
const READ_PUSHED                 = MAX ^ READ_NEEDS_PUSH
const READ_PAUSED                 = MAX ^ READ_RESUMED
const READ_NOT_QUEUED             = MAX ^ (READ_QUEUED | READ_EMITTED_READABLE)
const READ_NOT_ENDING             = MAX ^ READ_ENDING
const READ_PIPE_NOT_DRAINED       = MAX ^ READ_FLOWING
const READ_NOT_NEXT_TICK          = MAX ^ READ_NEXT_TICK
const READ_NOT_UPDATING           = MAX ^ READ_UPDATING
const READ_NO_READ_AHEAD          = MAX ^ READ_READ_AHEAD
const READ_PAUSED_NO_READ_AHEAD   = MAX ^ READ_RESUMED_READ_AHEAD

// Write state (18 bit offset, 4 bit offset from shared state and 14 from read state)
const WRITE_ACTIVE     = 0b00000000001 << 18
const WRITE_UPDATING   = 0b00000000010 << 18
const WRITE_PRIMARY    = 0b00000000100 << 18
const WRITE_QUEUED     = 0b00000001000 << 18
const WRITE_UNDRAINED  = 0b00000010000 << 18
const WRITE_DONE       = 0b00000100000 << 18
const WRITE_EMIT_DRAIN = 0b00001000000 << 18
const WRITE_NEXT_TICK  = 0b00010000000 << 18
const WRITE_WRITING    = 0b00100000000 << 18
const WRITE_FINISHING  = 0b01000000000 << 18
const WRITE_CORKED     = 0b10000000000 << 18

const WRITE_NOT_ACTIVE    = MAX ^ (WRITE_ACTIVE | WRITE_WRITING)
const WRITE_NON_PRIMARY   = MAX ^ WRITE_PRIMARY
const WRITE_NOT_FINISHING = MAX ^ (WRITE_ACTIVE | WRITE_FINISHING)
const WRITE_DRAINED       = MAX ^ WRITE_UNDRAINED
const WRITE_NOT_QUEUED    = MAX ^ WRITE_QUEUED
const WRITE_NOT_NEXT_TICK = MAX ^ WRITE_NEXT_TICK
const WRITE_NOT_UPDATING  = MAX ^ WRITE_UPDATING
const WRITE_NOT_CORKED    = MAX ^ WRITE_CORKED

// Combined shared state
const ACTIVE = READ_ACTIVE | WRITE_ACTIVE
const NOT_ACTIVE = MAX ^ ACTIVE
const DONE = READ_DONE | WRITE_DONE
const DESTROY_STATUS = DESTROYING | DESTROYED | PREDESTROYING
const OPEN_STATUS = DESTROY_STATUS | OPENING
const AUTO_DESTROY = DESTROY_STATUS | DONE
const NON_PRIMARY = WRITE_NON_PRIMARY & READ_NON_PRIMARY
const ACTIVE_OR_TICKING = WRITE_NEXT_TICK | READ_NEXT_TICK
const TICKING = ACTIVE_OR_TICKING & NOT_ACTIVE
const IS_OPENING = OPEN_STATUS | TICKING

// Combined shared state and read state
const READ_PRIMARY_STATUS = OPEN_STATUS | READ_ENDING | READ_DONE
const READ_STATUS = OPEN_STATUS | READ_DONE | READ_QUEUED
const READ_ENDING_STATUS = OPEN_STATUS | READ_ENDING | READ_QUEUED
const READ_READABLE_STATUS = OPEN_STATUS | READ_EMIT_READABLE | READ_QUEUED | READ_EMITTED_READABLE
const SHOULD_NOT_READ = OPEN_STATUS | READ_ACTIVE | READ_ENDING | READ_DONE | READ_NEEDS_PUSH | READ_READ_AHEAD
const READ_BACKPRESSURE_STATUS = DESTROY_STATUS | READ_ENDING | READ_DONE
const READ_UPDATE_SYNC_STATUS = READ_UPDATING | OPEN_STATUS | READ_NEXT_TICK | READ_PRIMARY
const READ_NEXT_TICK_OR_OPENING = READ_NEXT_TICK | OPENING

// Combined write state
const WRITE_PRIMARY_STATUS = OPEN_STATUS | WRITE_FINISHING | WRITE_DONE
const WRITE_QUEUED_AND_UNDRAINED = WRITE_QUEUED | WRITE_UNDRAINED
const WRITE_QUEUED_AND_ACTIVE = WRITE_QUEUED | WRITE_ACTIVE
const WRITE_DRAIN_STATUS = WRITE_QUEUED | WRITE_UNDRAINED | OPEN_STATUS | WRITE_ACTIVE
const WRITE_STATUS = OPEN_STATUS | WRITE_ACTIVE | WRITE_QUEUED | WRITE_CORKED
const WRITE_PRIMARY_AND_ACTIVE = WRITE_PRIMARY | WRITE_ACTIVE
const WRITE_ACTIVE_AND_WRITING = WRITE_ACTIVE | WRITE_WRITING
const WRITE_FINISHING_STATUS = OPEN_STATUS | WRITE_FINISHING | WRITE_QUEUED_AND_ACTIVE | WRITE_DONE
const WRITE_BACKPRESSURE_STATUS = WRITE_UNDRAINED | DESTROY_STATUS | WRITE_FINISHING | WRITE_DONE
const WRITE_UPDATE_SYNC_STATUS = WRITE_UPDATING | OPEN_STATUS | WRITE_NEXT_TICK | WRITE_PRIMARY
const WRITE_DROP_DATA = WRITE_FINISHING | WRITE_DONE | DESTROY_STATUS

const asyncIterator = Symbol.asyncIterator || Symbol('asyncIterator')

class WritableState {
  constructor (stream, { highWaterMark = 16384, map = null, mapWritable, byteLength, byteLengthWritable } = {}) {
    this.stream = stream
    this.queue = new FIFO()
    this.highWaterMark = highWaterMark
    this.buffered = 0
    this.error = null
    this.pipeline = null
    this.drains = null // if we add more seldomly used helpers we might them into a subobject so its a single ptr
    this.byteLength = byteLengthWritable || byteLength || defaultByteLength
    this.map = mapWritable || map
    this.afterWrite = afterWrite.bind(this)
    this.afterUpdateNextTick = updateWriteNT.bind(this)
  }

  get ended () {
    return (this.stream._duplexState & WRITE_DONE) !== 0
  }

  push (data) {
    if ((this.stream._duplexState & WRITE_DROP_DATA) !== 0) return false
    if (this.map !== null) data = this.map(data)

    this.buffered += this.byteLength(data)
    this.queue.push(data)

    if (this.buffered < this.highWaterMark) {
      this.stream._duplexState |= WRITE_QUEUED
      return true
    }

    this.stream._duplexState |= WRITE_QUEUED_AND_UNDRAINED
    return false
  }

  shift () {
    const data = this.queue.shift()

    this.buffered -= this.byteLength(data)
    if (this.buffered === 0) this.stream._duplexState &= WRITE_NOT_QUEUED

    return data
  }

  end (data) {
    if (typeof data === 'function') this.stream.once('finish', data)
    else if (data !== undefined && data !== null) this.push(data)
    this.stream._duplexState = (this.stream._duplexState | WRITE_FINISHING) & WRITE_NON_PRIMARY
  }

  autoBatch (data, cb) {
    const buffer = []
    const stream = this.stream

    buffer.push(data)
    while ((stream._duplexState & WRITE_STATUS) === WRITE_QUEUED_AND_ACTIVE) {
      buffer.push(stream._writableState.shift())
    }

    if ((stream._duplexState & OPEN_STATUS) !== 0) return cb(null)
    stream._writev(buffer, cb)
  }

  update () {
    const stream = this.stream

    stream._duplexState |= WRITE_UPDATING

    do {
      while ((stream._duplexState & WRITE_STATUS) === WRITE_QUEUED) {
        const data = this.shift()
        stream._duplexState |= WRITE_ACTIVE_AND_WRITING
        stream._write(data, this.afterWrite)
      }

      if ((stream._duplexState & WRITE_PRIMARY_AND_ACTIVE) === 0) this.updateNonPrimary()
    } while (this.continueUpdate() === true)

    stream._duplexState &= WRITE_NOT_UPDATING
  }

  updateNonPrimary () {
    const stream = this.stream

    if ((stream._duplexState & WRITE_FINISHING_STATUS) === WRITE_FINISHING) {
      stream._duplexState = stream._duplexState | WRITE_ACTIVE
      stream._final(afterFinal.bind(this))
      return
    }

    if ((stream._duplexState & DESTROY_STATUS) === DESTROYING) {
      if ((stream._duplexState & ACTIVE_OR_TICKING) === 0) {
        stream._duplexState |= ACTIVE
        stream._destroy(afterDestroy.bind(this))
      }
      return
    }

    if ((stream._duplexState & IS_OPENING) === OPENING) {
      stream._duplexState = (stream._duplexState | ACTIVE) & NOT_OPENING
      stream._open(afterOpen.bind(this))
    }
  }

  continueUpdate () {
    if ((this.stream._duplexState & WRITE_NEXT_TICK) === 0) return false
    this.stream._duplexState &= WRITE_NOT_NEXT_TICK
    return true
  }

  updateCallback () {
    if ((this.stream._duplexState & WRITE_UPDATE_SYNC_STATUS) === WRITE_PRIMARY) this.update()
    else this.updateNextTick()
  }

  updateNextTick () {
    if ((this.stream._duplexState & WRITE_NEXT_TICK) !== 0) return
    this.stream._duplexState |= WRITE_NEXT_TICK
    if ((this.stream._duplexState & WRITE_UPDATING) === 0) qmt(this.afterUpdateNextTick)
  }
}

class ReadableState {
  constructor (stream, { highWaterMark = 16384, map = null, mapReadable, byteLength, byteLengthReadable } = {}) {
    this.stream = stream
    this.queue = new FIFO()
    this.highWaterMark = highWaterMark === 0 ? 1 : highWaterMark
    this.buffered = 0
    this.readAhead = highWaterMark > 0
    this.error = null
    this.pipeline = null
    this.byteLength = byteLengthReadable || byteLength || defaultByteLength
    this.map = mapReadable || map
    this.pipeTo = null
    this.afterRead = afterRead.bind(this)
    this.afterUpdateNextTick = updateReadNT.bind(this)
  }

  get ended () {
    return (this.stream._duplexState & READ_DONE) !== 0
  }

  pipe (pipeTo, cb) {
    if (this.pipeTo !== null) throw new Error('Can only pipe to one destination')
    if (typeof cb !== 'function') cb = null

    this.stream._duplexState |= READ_PIPE_DRAINED
    this.pipeTo = pipeTo
    this.pipeline = new Pipeline(this.stream, pipeTo, cb)

    if (cb) this.stream.on('error', noop) // We already error handle this so supress crashes

    if (isStreamx(pipeTo)) {
      pipeTo._writableState.pipeline = this.pipeline
      if (cb) pipeTo.on('error', noop) // We already error handle this so supress crashes
      pipeTo.on('finish', this.pipeline.finished.bind(this.pipeline)) // TODO: just call finished from pipeTo itself
    } else {
      const onerror = this.pipeline.done.bind(this.pipeline, pipeTo)
      const onclose = this.pipeline.done.bind(this.pipeline, pipeTo, null) // onclose has a weird bool arg
      pipeTo.on('error', onerror)
      pipeTo.on('close', onclose)
      pipeTo.on('finish', this.pipeline.finished.bind(this.pipeline))
    }

    pipeTo.on('drain', afterDrain.bind(this))
    this.stream.emit('piping', pipeTo)
    pipeTo.emit('pipe', this.stream)
  }

  push (data) {
    const stream = this.stream

    if (data === null) {
      this.highWaterMark = 0
      stream._duplexState = (stream._duplexState | READ_ENDING) & READ_NON_PRIMARY_AND_PUSHED
      return false
    }

    if (this.map !== null) {
      data = this.map(data)
      if (data === null) {
        stream._duplexState &= READ_PUSHED
        return this.buffered < this.highWaterMark
      }
    }

    this.buffered += this.byteLength(data)
    this.queue.push(data)

    stream._duplexState = (stream._duplexState | READ_QUEUED) & READ_PUSHED

    return this.buffered < this.highWaterMark
  }

  shift () {
    const data = this.queue.shift()

    this.buffered -= this.byteLength(data)
    if (this.buffered === 0) this.stream._duplexState &= READ_NOT_QUEUED
    return data
  }

  unshift (data) {
    const pending = [this.map !== null ? this.map(data) : data]
    while (this.buffered > 0) pending.push(this.shift())

    for (let i = 0; i < pending.length - 1; i++) {
      const data = pending[i]
      this.buffered += this.byteLength(data)
      this.queue.push(data)
    }

    this.push(pending[pending.length - 1])
  }

  read () {
    const stream = this.stream

    if ((stream._duplexState & READ_STATUS) === READ_QUEUED) {
      const data = this.shift()
      if (this.pipeTo !== null && this.pipeTo.write(data) === false) stream._duplexState &= READ_PIPE_NOT_DRAINED
      if ((stream._duplexState & READ_EMIT_DATA) !== 0) stream.emit('data', data)
      return data
    }

    if (this.readAhead === false) {
      stream._duplexState |= READ_READ_AHEAD
      this.updateNextTick()
    }

    return null
  }

  drain () {
    const stream = this.stream

    while ((stream._duplexState & READ_STATUS) === READ_QUEUED && (stream._duplexState & READ_FLOWING) !== 0) {
      const data = this.shift()
      if (this.pipeTo !== null && this.pipeTo.write(data) === false) stream._duplexState &= READ_PIPE_NOT_DRAINED
      if ((stream._duplexState & READ_EMIT_DATA) !== 0) stream.emit('data', data)
    }
  }

  update () {
    const stream = this.stream

    stream._duplexState |= READ_UPDATING

    do {
      this.drain()

      while (this.buffered < this.highWaterMark && (stream._duplexState & SHOULD_NOT_READ) === READ_READ_AHEAD) {
        stream._duplexState |= READ_ACTIVE_AND_NEEDS_PUSH
        stream._read(this.afterRead)
        this.drain()
      }

      if ((stream._duplexState & READ_READABLE_STATUS) === READ_EMIT_READABLE_AND_QUEUED) {
        stream._duplexState |= READ_EMITTED_READABLE
        stream.emit('readable')
      }

      if ((stream._duplexState & READ_PRIMARY_AND_ACTIVE) === 0) this.updateNonPrimary()
    } while (this.continueUpdate() === true)

    stream._duplexState &= READ_NOT_UPDATING
  }

  updateNonPrimary () {
    const stream = this.stream

    if ((stream._duplexState & READ_ENDING_STATUS) === READ_ENDING) {
      stream._duplexState = (stream._duplexState | READ_DONE) & READ_NOT_ENDING
      stream.emit('end')
      if ((stream._duplexState & AUTO_DESTROY) === DONE) stream._duplexState |= DESTROYING
      if (this.pipeTo !== null) this.pipeTo.end()
    }

    if ((stream._duplexState & DESTROY_STATUS) === DESTROYING) {
      if ((stream._duplexState & ACTIVE_OR_TICKING) === 0) {
        stream._duplexState |= ACTIVE
        stream._destroy(afterDestroy.bind(this))
      }
      return
    }

    if ((stream._duplexState & IS_OPENING) === OPENING) {
      stream._duplexState = (stream._duplexState | ACTIVE) & NOT_OPENING
      stream._open(afterOpen.bind(this))
    }
  }

  continueUpdate () {
    if ((this.stream._duplexState & READ_NEXT_TICK) === 0) return false
    this.stream._duplexState &= READ_NOT_NEXT_TICK
    return true
  }

  updateCallback () {
    if ((this.stream._duplexState & READ_UPDATE_SYNC_STATUS) === READ_PRIMARY) this.update()
    else this.updateNextTick()
  }

  updateNextTickIfOpen () {
    if ((this.stream._duplexState & READ_NEXT_TICK_OR_OPENING) !== 0) return
    this.stream._duplexState |= READ_NEXT_TICK
    if ((this.stream._duplexState & READ_UPDATING) === 0) qmt(this.afterUpdateNextTick)
  }

  updateNextTick () {
    if ((this.stream._duplexState & READ_NEXT_TICK) !== 0) return
    this.stream._duplexState |= READ_NEXT_TICK
    if ((this.stream._duplexState & READ_UPDATING) === 0) qmt(this.afterUpdateNextTick)
  }
}

class TransformState {
  constructor (stream) {
    this.data = null
    this.afterTransform = afterTransform.bind(stream)
    this.afterFinal = null
  }
}

class Pipeline {
  constructor (src, dst, cb) {
    this.from = src
    this.to = dst
    this.afterPipe = cb
    this.error = null
    this.pipeToFinished = false
  }

  finished () {
    this.pipeToFinished = true
  }

  done (stream, err) {
    if (err) this.error = err

    if (stream === this.to) {
      this.to = null

      if (this.from !== null) {
        if ((this.from._duplexState & READ_DONE) === 0 || !this.pipeToFinished) {
          this.from.destroy(this.error || new Error('Writable stream closed prematurely'))
        }
        return
      }
    }

    if (stream === this.from) {
      this.from = null

      if (this.to !== null) {
        if ((stream._duplexState & READ_DONE) === 0) {
          this.to.destroy(this.error || new Error('Readable stream closed before ending'))
        }
        return
      }
    }

    if (this.afterPipe !== null) this.afterPipe(this.error)
    this.to = this.from = this.afterPipe = null
  }
}

function afterDrain () {
  this.stream._duplexState |= READ_PIPE_DRAINED
  this.updateCallback()
}

function afterFinal (err) {
  const stream = this.stream
  if (err) stream.destroy(err)
  if ((stream._duplexState & DESTROY_STATUS) === 0) {
    stream._duplexState |= WRITE_DONE
    stream.emit('finish')
  }
  if ((stream._duplexState & AUTO_DESTROY) === DONE) {
    stream._duplexState |= DESTROYING
  }

  stream._duplexState &= WRITE_NOT_FINISHING

  // no need to wait the extra tick here, so we short circuit that
  if ((stream._duplexState & WRITE_UPDATING) === 0) this.update()
  else this.updateNextTick()
}

function afterDestroy (err) {
  const stream = this.stream

  if (!err && this.error !== STREAM_DESTROYED) err = this.error
  if (err) stream.emit('error', err)
  stream._duplexState |= DESTROYED
  stream.emit('close')

  const rs = stream._readableState
  const ws = stream._writableState

  if (rs !== null && rs.pipeline !== null) rs.pipeline.done(stream, err)

  if (ws !== null) {
    while (ws.drains !== null && ws.drains.length > 0) ws.drains.shift().resolve(false)
    if (ws.pipeline !== null) ws.pipeline.done(stream, err)
  }
}

function afterWrite (err) {
  const stream = this.stream

  if (err) stream.destroy(err)
  stream._duplexState &= WRITE_NOT_ACTIVE

  if (this.drains !== null) tickDrains(this.drains)

  if ((stream._duplexState & WRITE_DRAIN_STATUS) === WRITE_UNDRAINED) {
    stream._duplexState &= WRITE_DRAINED
    if ((stream._duplexState & WRITE_EMIT_DRAIN) === WRITE_EMIT_DRAIN) {
      stream.emit('drain')
    }
  }

  this.updateCallback()
}

function afterRead (err) {
  if (err) this.stream.destroy(err)
  this.stream._duplexState &= READ_NOT_ACTIVE
  if (this.readAhead === false && (this.stream._duplexState & READ_RESUMED) === 0) this.stream._duplexState &= READ_NO_READ_AHEAD
  this.updateCallback()
}

function updateReadNT () {
  if ((this.stream._duplexState & READ_UPDATING) === 0) {
    this.stream._duplexState &= READ_NOT_NEXT_TICK
    this.update()
  }
}

function updateWriteNT () {
  if ((this.stream._duplexState & WRITE_UPDATING) === 0) {
    this.stream._duplexState &= WRITE_NOT_NEXT_TICK
    this.update()
  }
}

function tickDrains (drains) {
  for (let i = 0; i < drains.length; i++) {
    // drains.writes are monotonic, so if one is 0 its always the first one
    if (--drains[i].writes === 0) {
      drains.shift().resolve(true)
      i--
    }
  }
}

function afterOpen (err) {
  const stream = this.stream

  if (err) stream.destroy(err)

  if ((stream._duplexState & DESTROYING) === 0) {
    if ((stream._duplexState & READ_PRIMARY_STATUS) === 0) stream._duplexState |= READ_PRIMARY
    if ((stream._duplexState & WRITE_PRIMARY_STATUS) === 0) stream._duplexState |= WRITE_PRIMARY
    stream.emit('open')
  }

  stream._duplexState &= NOT_ACTIVE

  if (stream._writableState !== null) {
    stream._writableState.updateCallback()
  }

  if (stream._readableState !== null) {
    stream._readableState.updateCallback()
  }
}

function afterTransform (err, data) {
  if (data !== undefined && data !== null) this.push(data)
  this._writableState.afterWrite(err)
}

function newListener (name) {
  if (this._readableState !== null) {
    if (name === 'data') {
      this._duplexState |= (READ_EMIT_DATA | READ_RESUMED_READ_AHEAD)
      this._readableState.updateNextTick()
    }
    if (name === 'readable') {
      this._duplexState |= READ_EMIT_READABLE
      this._readableState.updateNextTick()
    }
  }

  if (this._writableState !== null) {
    if (name === 'drain') {
      this._duplexState |= WRITE_EMIT_DRAIN
      this._writableState.updateNextTick()
    }
  }
}

class Stream extends EventEmitter {
  constructor (opts) {
    super()

    this._duplexState = 0
    this._readableState = null
    this._writableState = null

    if (opts) {
      if (opts.open) this._open = opts.open
      if (opts.destroy) this._destroy = opts.destroy
      if (opts.predestroy) this._predestroy = opts.predestroy
      if (opts.signal) {
        opts.signal.addEventListener('abort', abort.bind(this))
      }
    }

    this.on('newListener', newListener)
  }

  _open (cb) {
    cb(null)
  }

  _destroy (cb) {
    cb(null)
  }

  _predestroy () {
    // does nothing
  }

  get readable () {
    return this._readableState !== null ? true : undefined
  }

  get writable () {
    return this._writableState !== null ? true : undefined
  }

  get destroyed () {
    return (this._duplexState & DESTROYED) !== 0
  }

  get destroying () {
    return (this._duplexState & DESTROY_STATUS) !== 0
  }

  destroy (err) {
    if ((this._duplexState & DESTROY_STATUS) === 0) {
      if (!err) err = STREAM_DESTROYED
      this._duplexState = (this._duplexState | DESTROYING) & NON_PRIMARY

      if (this._readableState !== null) {
        this._readableState.highWaterMark = 0
        this._readableState.error = err
      }
      if (this._writableState !== null) {
        this._writableState.highWaterMark = 0
        this._writableState.error = err
      }

      this._duplexState |= PREDESTROYING
      this._predestroy()
      this._duplexState &= NOT_PREDESTROYING

      if (this._readableState !== null) this._readableState.updateNextTick()
      if (this._writableState !== null) this._writableState.updateNextTick()
    }
  }
}

class Readable extends Stream {
  constructor (opts) {
    super(opts)

    this._duplexState |= OPENING | WRITE_DONE | READ_READ_AHEAD
    this._readableState = new ReadableState(this, opts)

    if (opts) {
      if (this._readableState.readAhead === false) this._duplexState &= READ_NO_READ_AHEAD
      if (opts.read) this._read = opts.read
      if (opts.eagerOpen) this._readableState.updateNextTick()
      if (opts.encoding) this.setEncoding(opts.encoding)
    }
  }

  setEncoding (encoding) {
    const dec = new TextDecoder(encoding)
    const map = this._readableState.map || echo
    this._readableState.map = mapOrSkip
    return this

    function mapOrSkip (data) {
      const next = dec.push(data)
      return next === '' && (data.byteLength !== 0 || dec.remaining > 0) ? null : map(next)
    }
  }

  _read (cb) {
    cb(null)
  }

  pipe (dest, cb) {
    this._readableState.updateNextTick()
    this._readableState.pipe(dest, cb)
    return dest
  }

  read () {
    this._readableState.updateNextTick()
    return this._readableState.read()
  }

  push (data) {
    this._readableState.updateNextTickIfOpen()
    return this._readableState.push(data)
  }

  unshift (data) {
    this._readableState.updateNextTickIfOpen()
    return this._readableState.unshift(data)
  }

  resume () {
    this._duplexState |= READ_RESUMED_READ_AHEAD
    this._readableState.updateNextTick()
    return this
  }

  pause () {
    this._duplexState &= (this._readableState.readAhead === false ? READ_PAUSED_NO_READ_AHEAD : READ_PAUSED)
    return this
  }

  static _fromAsyncIterator (ite, opts) {
    let destroy

    const rs = new Readable({
      ...opts,
      read (cb) {
        ite.next().then(push).then(cb.bind(null, null)).catch(cb)
      },
      predestroy () {
        destroy = ite.return()
      },
      destroy (cb) {
        if (!destroy) return cb(null)
        destroy.then(cb.bind(null, null)).catch(cb)
      }
    })

    return rs

    function push (data) {
      if (data.done) rs.push(null)
      else rs.push(data.value)
    }
  }

  static from (data, opts) {
    if (isReadStreamx(data)) return data
    if (data[asyncIterator]) return this._fromAsyncIterator(data[asyncIterator](), opts)
    if (!Array.isArray(data)) data = data === undefined ? [] : [data]

    let i = 0
    return new Readable({
      ...opts,
      read (cb) {
        this.push(i === data.length ? null : data[i++])
        cb(null)
      }
    })
  }

  static isBackpressured (rs) {
    return (rs._duplexState & READ_BACKPRESSURE_STATUS) !== 0 || rs._readableState.buffered >= rs._readableState.highWaterMark
  }

  static isPaused (rs) {
    return (rs._duplexState & READ_RESUMED) === 0
  }

  [asyncIterator] () {
    const stream = this

    let error = null
    let promiseResolve = null
    let promiseReject = null

    this.on('error', (err) => { error = err })
    this.on('readable', onreadable)
    this.on('close', onclose)

    return {
      [asyncIterator] () {
        return this
      },
      next () {
        return new Promise(function (resolve, reject) {
          promiseResolve = resolve
          promiseReject = reject
          const data = stream.read()
          if (data !== null) ondata(data)
          else if ((stream._duplexState & DESTROYED) !== 0) ondata(null)
        })
      },
      return () {
        return destroy(null)
      },
      throw (err) {
        return destroy(err)
      }
    }

    function onreadable () {
      if (promiseResolve !== null) ondata(stream.read())
    }

    function onclose () {
      if (promiseResolve !== null) ondata(null)
    }

    function ondata (data) {
      if (promiseReject === null) return
      if (error) promiseReject(error)
      else if (data === null && (stream._duplexState & READ_DONE) === 0) promiseReject(STREAM_DESTROYED)
      else promiseResolve({ value: data, done: data === null })
      promiseReject = promiseResolve = null
    }

    function destroy (err) {
      stream.destroy(err)
      return new Promise((resolve, reject) => {
        if (stream._duplexState & DESTROYED) return resolve({ value: undefined, done: true })
        stream.once('close', function () {
          if (err) reject(err)
          else resolve({ value: undefined, done: true })
        })
      })
    }
  }
}

class Writable extends Stream {
  constructor (opts) {
    super(opts)

    this._duplexState |= OPENING | READ_DONE
    this._writableState = new WritableState(this, opts)

    if (opts) {
      if (opts.writev) this._writev = opts.writev
      if (opts.write) this._write = opts.write
      if (opts.final) this._final = opts.final
      if (opts.eagerOpen) this._writableState.updateNextTick()
    }
  }

  cork () {
    this._duplexState |= WRITE_CORKED
  }

  uncork () {
    this._duplexState &= WRITE_NOT_CORKED
    this._writableState.updateNextTick()
  }

  _writev (batch, cb) {
    cb(null)
  }

  _write (data, cb) {
    this._writableState.autoBatch(data, cb)
  }

  _final (cb) {
    cb(null)
  }

  static isBackpressured (ws) {
    return (ws._duplexState & WRITE_BACKPRESSURE_STATUS) !== 0
  }

  static drained (ws) {
    if (ws.destroyed) return Promise.resolve(false)
    const state = ws._writableState
    const pending = (isWritev(ws) ? Math.min(1, state.queue.length) : state.queue.length)
    const writes = pending + ((ws._duplexState & WRITE_WRITING) ? 1 : 0)
    if (writes === 0) return Promise.resolve(true)
    if (state.drains === null) state.drains = []
    return new Promise((resolve) => {
      state.drains.push({ writes, resolve })
    })
  }

  write (data) {
    this._writableState.updateNextTick()
    return this._writableState.push(data)
  }

  end (data) {
    this._writableState.updateNextTick()
    this._writableState.end(data)
    return this
  }
}

class Duplex extends Readable { // and Writable
  constructor (opts) {
    super(opts)

    this._duplexState = OPENING | (this._duplexState & READ_READ_AHEAD)
    this._writableState = new WritableState(this, opts)

    if (opts) {
      if (opts.writev) this._writev = opts.writev
      if (opts.write) this._write = opts.write
      if (opts.final) this._final = opts.final
    }
  }

  cork () {
    this._duplexState |= WRITE_CORKED
  }

  uncork () {
    this._duplexState &= WRITE_NOT_CORKED
    this._writableState.updateNextTick()
  }

  _writev (batch, cb) {
    cb(null)
  }

  _write (data, cb) {
    this._writableState.autoBatch(data, cb)
  }

  _final (cb) {
    cb(null)
  }

  write (data) {
    this._writableState.updateNextTick()
    return this._writableState.push(data)
  }

  end (data) {
    this._writableState.updateNextTick()
    this._writableState.end(data)
    return this
  }
}

class Transform extends Duplex {
  constructor (opts) {
    super(opts)
    this._transformState = new TransformState(this)

    if (opts) {
      if (opts.transform) this._transform = opts.transform
      if (opts.flush) this._flush = opts.flush
    }
  }

  _write (data, cb) {
    if (this._readableState.buffered >= this._readableState.highWaterMark) {
      this._transformState.data = data
    } else {
      this._transform(data, this._transformState.afterTransform)
    }
  }

  _read (cb) {
    if (this._transformState.data !== null) {
      const data = this._transformState.data
      this._transformState.data = null
      cb(null)
      this._transform(data, this._transformState.afterTransform)
    } else {
      cb(null)
    }
  }

  destroy (err) {
    super.destroy(err)
    if (this._transformState.data !== null) {
      this._transformState.data = null
      this._transformState.afterTransform()
    }
  }

  _transform (data, cb) {
    cb(null, data)
  }

  _flush (cb) {
    cb(null)
  }

  _final (cb) {
    this._transformState.afterFinal = cb
    this._flush(transformAfterFlush.bind(this))
  }
}

class PassThrough extends Transform {}

function transformAfterFlush (err, data) {
  const cb = this._transformState.afterFinal
  if (err) return cb(err)
  if (data !== null && data !== undefined) this.push(data)
  this.push(null)
  cb(null)
}

function pipelinePromise (...streams) {
  return new Promise((resolve, reject) => {
    return pipeline(...streams, (err) => {
      if (err) return reject(err)
      resolve()
    })
  })
}

function pipeline (stream, ...streams) {
  const all = Array.isArray(stream) ? [...stream, ...streams] : [stream, ...streams]
  const done = (all.length && typeof all[all.length - 1] === 'function') ? all.pop() : null

  if (all.length < 2) throw new Error('Pipeline requires at least 2 streams')

  let src = all[0]
  let dest = null
  let error = null

  for (let i = 1; i < all.length; i++) {
    dest = all[i]

    if (isStreamx(src)) {
      src.pipe(dest, onerror)
    } else {
      errorHandle(src, true, i > 1, onerror)
      src.pipe(dest)
    }

    src = dest
  }

  if (done) {
    let fin = false

    const autoDestroy = isStreamx(dest) || !!(dest._writableState && dest._writableState.autoDestroy)

    dest.on('error', (err) => {
      if (error === null) error = err
    })

    dest.on('finish', () => {
      fin = true
      if (!autoDestroy) done(error)
    })

    if (autoDestroy) {
      dest.on('close', () => done(error || (fin ? null : PREMATURE_CLOSE)))
    }
  }

  return dest

  function errorHandle (s, rd, wr, onerror) {
    s.on('error', onerror)
    s.on('close', onclose)

    function onclose () {
      if (rd && s._readableState && !s._readableState.ended) return onerror(PREMATURE_CLOSE)
      if (wr && s._writableState && !s._writableState.ended) return onerror(PREMATURE_CLOSE)
    }
  }

  function onerror (err) {
    if (!err || error) return
    error = err

    for (const s of all) {
      s.destroy(err)
    }
  }
}

function echo (s) {
  return s
}

function isStream (stream) {
  return !!stream._readableState || !!stream._writableState
}

function isStreamx (stream) {
  return typeof stream._duplexState === 'number' && isStream(stream)
}

function isEnded (stream) {
  return !!stream._readableState && stream._readableState.ended
}

function isFinished (stream) {
  return !!stream._writableState && stream._writableState.ended
}

function getStreamError (stream, opts = {}) {
  const err = (stream._readableState && stream._readableState.error) || (stream._writableState && stream._writableState.error)

  // avoid implicit errors by default
  return (!opts.all && err === STREAM_DESTROYED) ? null : err
}

function isReadStreamx (stream) {
  return isStreamx(stream) && stream.readable
}

function isDisturbed (stream) {
  return (stream._duplexState & OPENING) !== OPENING || (stream._duplexState & ACTIVE_OR_TICKING) !== 0
}

function isTypedArray (data) {
  return typeof data === 'object' && data !== null && typeof data.byteLength === 'number'
}

function defaultByteLength (data) {
  return isTypedArray(data) ? data.byteLength : 1024
}

function noop () {}

function abort () {
  this.destroy(new Error('Stream aborted.'))
}

function isWritev (s) {
  return s._writev !== Writable.prototype._writev && s._writev !== Duplex.prototype._writev
}

module.exports = {
  pipeline,
  pipelinePromise,
  isStream,
  isStreamx,
  isEnded,
  isFinished,
  isDisturbed,
  getStreamError,
  Stream,
  Writable,
  Readable,
  Duplex,
  Transform,
  // Export PassThrough for compatibility with Node.js core's stream module
  PassThrough
}
{
  "name": "streamx",
  "version": "2.23.0",
  "description": "An iteration of the Node.js core streams with a series of improvements",
  "main": "index.js",
  "dependencies": {
    "events-universal": "^1.0.0",
    "fast-fifo": "^1.3.2",
    "text-decoder": "^1.1.0"
  },
  "devDependencies": {
    "b4a": "^1.6.6",
    "brittle": "^3.1.1",
    "end-of-stream": "^1.4.4",
    "standard": "^17.0.0"
  },
  "files": [
    "index.js"
  ],
  "scripts": {
    "test": "standard && node test/all.js",
    "test:bare": "standard && bare test/all.js"
  },
  "repository": {
    "type": "git",
    "url": "https://github.com/mafintosh/streamx.git"
  },
  "author": "Mathias Buus (@mafintosh)",
  "license": "MIT",
  "bugs": {
    "url": "https://github.com/mafintosh/streamx/issues"
  },
  "homepage": "https://github.com/mafintosh/streamx"
}
const codecs = require('codecs')
const b = require('b4a')

const SEP = b.alloc(1)
const SEP_BUMPED = b.from([0x1])
const EMPTY = b.alloc(0)

module.exports = class SubEncoder {
  constructor (prefix, encoding, parent = null) {
    this.userEncoding = codecs(encoding)
    this.prefix = prefix != null ? createPrefix(prefix, parent) : null
    this.lt = this.prefix && b.concat([this.prefix.subarray(0, this.prefix.byteLength - 1), SEP_BUMPED])
  }

  _encodeRangeUser (r) {
    if (this.userEncoding.encodeRange) return this.userEncoding.encodeRange(r)

    const res = {}
    if (r.gt != null) res.gt = this.userEncoding.encode(r.gt)
    if (r.gte != null) res.gte = this.userEncoding.encode(r.gte)
    if (r.lte != null) res.lte = this.userEncoding.encode(r.lte)
    if (r.lt != null) res.lt = this.userEncoding.encode(r.lt)

    return res
  }

  _addPrefix (key) {
    return this.prefix ? b.concat([this.prefix, key]) : key
  }

  encode (key) {
    return this._addPrefix(this.userEncoding.encode(key))
  }

  encodeRange (range) {
    const r = this._encodeRangeUser(range)

    if (r.gt) r.gt = this._addPrefix(r.gt)
    else if (r.gte) r.gte = this._addPrefix(r.gte)
    else if (this.prefix) r.gte = this.prefix

    if (r.lt) r.lt = this._addPrefix(r.lt)
    else if (r.lte) r.lte = this._addPrefix(r.lte)
    else if (this.prefix) r.lt = this.lt

    return r
  }

  decode (key) {
    return this.userEncoding.decode(this.prefix ? key.subarray(this.prefix.byteLength) : key)
  }

  sub (prefix, encoding) {
    return new SubEncoder(prefix || EMPTY, compat(encoding), this.prefix)
  }
}

function createPrefix (prefix, parent) {
  prefix = typeof prefix === 'string' ? b.from(prefix) : prefix

  if (prefix && parent) return b.concat([parent, prefix, SEP])
  if (prefix) return b.concat([prefix, SEP])
  if (parent) return b.concat([parent, SEP])
  return SEP
}

function compat (enc) {
  if (enc && enc.keyEncoding) return enc.keyEncoding
  return enc
}
{
  "name": "sub-encoder",
  "version": "2.1.3",
  "description": "Generate sub encodings for key/value stores",
  "main": "index.js",
  "scripts": {
    "test": "standard && brittle test/*.js"
  },
  "repository": {
    "type": "git",
    "url": "git+https://github.com/holepunchto/sub-encoder.git"
  },
  "keywords": [
    "kv-store",
    "encoding",
    "hyperbee"
  ],
  "author": "Andrew Osheroff <andrewosh@gmail.com>",
  "license": "Apache-2.0",
  "bugs": {
    "url": "https://github.com/holepunchto/sub-encoder/issues"
  },
  "homepage": "https://github.com/holepunchto/sub-encoder#readme",
  "devDependencies": {
    "brittle": "^3.1.1",
    "compact-encoding": "^2.12.0",
    "hyperbee": "^2.11.0",
    "hypercore": "^10.3.2",
    "index-encoder": "^3.0.0",
    "random-access-memory": "^6.0.0",
    "standard": "^17.0.0"
  },
  "dependencies": {
    "b4a": "^1.6.0",
    "codecs": "^3.1.0"
  }
}
const PassThroughDecoder = require('./lib/pass-through-decoder')
const UTF8Decoder = require('./lib/utf8-decoder')

module.exports = class TextDecoder {
  constructor (encoding = 'utf8') {
    this.encoding = normalizeEncoding(encoding)

    switch (this.encoding) {
      case 'utf8':
        this.decoder = new UTF8Decoder()
        break
      case 'utf16le':
      case 'base64':
        throw new Error('Unsupported encoding: ' + this.encoding)
      default:
        this.decoder = new PassThroughDecoder(this.encoding)
    }
  }

  get remaining () {
    return this.decoder.remaining
  }

  push (data) {
    if (typeof data === 'string') return data
    return this.decoder.decode(data)
  }

  // For Node.js compatibility
  write (data) {
    return this.push(data)
  }

  end (data) {
    let result = ''
    if (data) result = this.push(data)
    result += this.decoder.flush()
    return result
  }
}

function normalizeEncoding (encoding) {
  encoding = encoding.toLowerCase()

  switch (encoding) {
    case 'utf8':
    case 'utf-8':
      return 'utf8'
    case 'ucs2':
    case 'ucs-2':
    case 'utf16le':
    case 'utf-16le':
      return 'utf16le'
    case 'latin1':
    case 'binary':
      return 'latin1'
    case 'base64':
    case 'ascii':
    case 'hex':
      return encoding
    default:
      throw new Error('Unknown encoding: ' + encoding)
  }
};
const b4a = require('b4a')

module.exports = class PassThroughDecoder {
  constructor (encoding) {
    this.encoding = encoding
  }

  get remaining () {
    return 0
  }

  decode (tail) {
    return b4a.toString(tail, this.encoding)
  }

  flush () {
    return ''
  }
}
const b4a = require('b4a')

/**
 * https://encoding.spec.whatwg.org/#utf-8-decoder
 */
module.exports = class UTF8Decoder {
  constructor () {
    this.codePoint = 0
    this.bytesSeen = 0
    this.bytesNeeded = 0
    this.lowerBoundary = 0x80
    this.upperBoundary = 0xbf
  }

  get remaining () {
    return this.bytesSeen
  }

  decode (data) {
    // If we have a fast path, just sniff if the last part is a boundary
    if (this.bytesNeeded === 0) {
      let isBoundary = true

      for (let i = Math.max(0, data.byteLength - 4), n = data.byteLength; i < n && isBoundary; i++) {
        isBoundary = data[i] <= 0x7f
      }

      if (isBoundary) return b4a.toString(data, 'utf8')
    }

    let result = ''

    for (let i = 0, n = data.byteLength; i < n; i++) {
      const byte = data[i]

      if (this.bytesNeeded === 0) {
        if (byte <= 0x7f) {
          result += String.fromCharCode(byte)
        } else {
          this.bytesSeen = 1

          if (byte >= 0xc2 && byte <= 0xdf) {
            this.bytesNeeded = 2
            this.codePoint = byte & 0x1f
          } else if (byte >= 0xe0 && byte <= 0xef) {
            if (byte === 0xe0) this.lowerBoundary = 0xa0
            else if (byte === 0xed) this.upperBoundary = 0x9f
            this.bytesNeeded = 3
            this.codePoint = byte & 0xf
          } else if (byte >= 0xf0 && byte <= 0xf4) {
            if (byte === 0xf0) this.lowerBoundary = 0x90
            if (byte === 0xf4) this.upperBoundary = 0x8f
            this.bytesNeeded = 4
            this.codePoint = byte & 0x7
          } else {
            result += '\ufffd'
          }
        }

        continue
      }

      if (byte < this.lowerBoundary || byte > this.upperBoundary) {
        this.codePoint = 0
        this.bytesNeeded = 0
        this.bytesSeen = 0
        this.lowerBoundary = 0x80
        this.upperBoundary = 0xbf

        result += '\ufffd'

        continue
      }

      this.lowerBoundary = 0x80
      this.upperBoundary = 0xbf

      this.codePoint = (this.codePoint << 6) | (byte & 0x3f)
      this.bytesSeen++

      if (this.bytesSeen !== this.bytesNeeded) continue

      result += String.fromCodePoint(this.codePoint)

      this.codePoint = 0
      this.bytesNeeded = 0
      this.bytesSeen = 0
    }

    return result
  }

  flush () {
    const result = this.bytesNeeded > 0 ? '\ufffd' : ''

    this.codePoint = 0
    this.bytesNeeded = 0
    this.bytesSeen = 0
    this.lowerBoundary = 0x80
    this.upperBoundary = 0xbf

    return result
  }
}
{
  "name": "text-decoder",
  "version": "1.2.3",
  "description": "Streaming text decoder that preserves multibyte Unicode characters",
  "main": "index.js",
  "files": [
    "index.js",
    "lib"
  ],
  "browser": {
    "./lib/pass-through-decoder.js": "./lib/browser-decoder.js",
    "./lib/utf8-decoder.js": "./lib/browser-decoder.js"
  },
  "react-native": {
    "./lib/pass-through-decoder.js": "./lib/pass-through-decoder.js",
    "./lib/utf8-decoder.js": "./lib/utf8-decoder.js"
  },
  "scripts": {
    "test": "standard && brittle test.js"
  },
  "repository": {
    "type": "git",
    "url": "git+https://github.com/holepunchto/text-decoder.git"
  },
  "author": "Holepunch",
  "license": "Apache-2.0",
  "bugs": {
    "url": "https://github.com/holepunchto/text-decoder/issues"
  },
  "homepage": "https://github.com/holepunchto/text-decoder#readme",
  "dependencies": {
    "b4a": "^1.6.4"
  },
  "devDependencies": {
    "brittle": "^3.3.2",
    "standard": "^17.0.0"
  }
}
module.exports = class TimeOrderedSet {
  constructor () {
    this.oldest = null
    this.latest = null
    this.length = 0
  }

  has (node) {
    return !!(node.next || node.prev) || node === this.oldest
  }

  add (node) {
    if (this.has(node)) this.remove(node)

    if (!this.latest && !this.oldest) {
      this.latest = this.oldest = node
      node.prev = node.next = null
    } else {
      this.latest.next = node
      node.prev = this.latest
      node.next = null
      this.latest = node
    }

    this.length++

    return node
  }

  remove (node) {
    if (!this.has(node)) return node

    if (this.oldest !== node && this.latest !== node) {
      node.prev.next = node.next
      node.next.prev = node.prev
    } else {
      if (this.oldest === node) {
        this.oldest = node.next
        if (this.oldest) this.oldest.prev = null
      }
      if (this.latest === node) {
        this.latest = node.prev
        if (this.latest) this.latest.next = null
      }
    }

    node.next = node.prev = null
    this.length--

    return node
  }

  toArray ({ limit = Infinity, reverse = false } = {}) {
    const list = []

    if (reverse) {
      let node = this.latest
      while (node && limit--) {
        list.push(node)
        node = node.prev
      }
    } else {
      let node = this.oldest
      while (node && limit--) {
        list.push(node)
        node = node.next
      }
    }

    return list
  }
}
{
  "name": "time-ordered-set",
  "version": "2.0.1",
  "description": "Efficiently maintain a set of nodes ordered by the time they were added to the set",
  "main": "index.js",
  "files": [
    "index.js"
  ],
  "devDependencies": {
    "brittle": "^3.0.0",
    "standard": "^17.1.2"
  },
  "scripts": {
    "test": "standard && brittle test.js"
  },
  "repository": {
    "type": "git",
    "url": "https://github.com/mafintosh/time-ordered-set.git"
  },
  "author": "Mathias Buus (@mafintosh)",
  "license": "MIT",
  "bugs": {
    "url": "https://github.com/mafintosh/time-ordered-set/issues"
  },
  "homepage": "https://github.com/mafintosh/time-ordered-set"
}
module.exports = class TimerBrowser {
  constructor (ms, fn, ctx = null, interval = false) {
    this.ms = ms
    this.ontimeout = fn
    this.context = ctx || null
    this.interval = interval
    this.done = false

    this._timer = interval
      ? setInterval(callInterval, ms, this)
      : setTimeout(callTimeout, ms, this)
  }

  unref () {}

  ref () {}

  refresh () {
    if (this.done) return

    if (this.interval) {
      clearInterval(this._timer)
      this._timer = setInterval(callInterval, this.ms, this)
    } else {
      clearTimeout(this._timer)
      this._timer = setTimeout(callTimeout, this.ms, this)
    }
  }

  destroy () {
    this.done = true
    this.ontimeout = null

    if (this.interval) clearInterval(this._timer)
    else clearTimeout(this._timer)
  }

  static once (ms, fn, ctx) {
    return new this(ms, fn, ctx, false)
  }

  static on (ms, fn, ctx) {
    return new this(ms, fn, ctx, true)
  }
}

function callTimeout (self) {
  self.done = true
  self.ontimeout.call(self.context)
}

function callInterval (self) {
  self.ontimeout.call(self.context)
}
module.exports = isNode()
  ? require('./node')
  : require('./browser')

function isNode () {
  const to = setTimeout(function () {}, 1000)
  clearTimeout(to)
  return !!to.refresh
}
module.exports = class Timer {
  constructor (ms, fn, ctx = null, interval = false) {
    this.ms = ms
    this.ontimeout = fn
    this.context = ctx
    this.interval = interval
    this.done = false

    this._timer = interval
      ? setInterval(callInterval, ms, this)
      : setTimeout(callTimeout, ms, this)
  }

  unref () {
    this._timer.unref()
  }

  ref () {
    this._timer.ref()
  }

  refresh () {
    if (this.done !== true) this._timer.refresh()
  }

  destroy () {
    this.done = true
    this.ontimeout = null
    if (this.interval) clearInterval(this._timer)
    else clearTimeout(this._timer)
  }

  static once (ms, fn, ctx) {
    return new this(ms, fn, ctx, false)
  }

  static on (ms, fn, ctx) {
    return new this(ms, fn, ctx, true)
  }
}

function callTimeout (self) {
  self.done = true
  self.ontimeout.call(self.context)
}

function callInterval (self) {
  self.ontimeout.call(self.context)
}
{
  "name": "timeout-refresh",
  "version": "2.0.1",
  "description": "Efficiently refresh a timer",
  "main": "index.js",
  "dependencies": {},
  "devDependencies": {
    "standard": "^16.0.4",
    "tape": "^5.5.2"
  },
  "browser": "./browser.js",
  "scripts": {
    "test": "standard && tape test.js"
  },
  "repository": {
    "type": "git",
    "url": "https://github.com/mafintosh/timeout-refresh.git"
  },
  "author": "Mathias Buus (@mafintosh)",
  "license": "MIT",
  "bugs": {
    "url": "https://github.com/mafintosh/timeout-refresh/issues"
  },
  "homepage": "https://github.com/mafintosh/timeout-refresh"
}
const b4a = require('b4a')

module.exports = class BufferMap {
  constructor (other) {
    this.m = other ? new Map([...other.m]) : new Map()
  }

  get size () {
    return this.m.size
  }

  get (key) {
    if (b4a.isBuffer(key)) key = b4a.toString(key, 'hex')
    return this.m.get(key)
  }

  set (key, value) {
    if (b4a.isBuffer(key)) key = b4a.toString(key, 'hex')
    return this.m.set(key, value)
  }

  delete (key) {
    if (b4a.isBuffer(key)) key = b4a.toString(key, 'hex')
    return this.m.delete(key)
  }

  has (key) {
    if (b4a.isBuffer(key)) key = b4a.toString(key, 'hex')
    return this.m.has(key)
  }

  * [Symbol.iterator] () {
    for (const [key, value] of this.m) {
      yield [b4a.from(key, 'hex'), value]
    }
  }

  * keys () {
    for (const key of this.m.keys()) {
      yield b4a.from(key, 'hex')
    }
  }

  values () {
    return this.m.values()
  }

  clear () {
    return this.m.clear()
  }
}
{
  "name": "tiny-buffer-map",
  "version": "1.1.1",
  "description": "A very simple map for Buffers",
  "main": "index.js",
  "scripts": {
    "test": "brittle test.js"
  },
  "repository": {
    "type": "git",
    "url": "git+https://github.com/andrewosh/tiny-buffer-map.git"
  },
  "keywords": [
    "buffer",
    "map"
  ],
  "author": "Andrew Osheroff <andrewosh@gmail.com>",
  "license": "MIT",
  "bugs": {
    "url": "https://github.com/andrewosh/tiny-buffer-map/issues"
  },
  "homepage": "https://github.com/andrewosh/tiny-buffer-map#readme",
  "dependencies": {
    "b4a": "^1.6.0"
  },
  "devDependencies": {
    "brittle": "^3.0.4"
  }
}
require.addon = require('require-addon')

module.exports = require.addon('.', __filename)
// Copyright Joyent, Inc. and other Node contributors.
//
// Permission is hereby granted, free of charge, to any person obtaining a
// copy of this software and associated documentation files (the
// "Software"), to deal in the Software without restriction, including
// without limitation the rights to use, copy, modify, merge, publish,
// distribute, sublicense, and/or sell copies of the Software, and to permit
// persons to whom the Software is furnished to do so, subject to the
// following conditions:
//
// The above copyright notice and this permission notice shall be included
// in all copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS
// OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
// MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN
// NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,
// DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR
// OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE
// USE OR OTHER DEALINGS IN THE SOFTWARE.

const v4Seg = '(?:[0-9]|[1-9][0-9]|1[0-9][0-9]|2[0-4][0-9]|25[0-5])'
const v4Str = `(${v4Seg}[.]){3}${v4Seg}`
const IPv4Pattern = new RegExp(`^${v4Str}$`)

const v6Seg = '(?:[0-9a-fA-F]{1,4})'
const IPv6Pattern = new RegExp(
  '^(' +
    `(?:${v6Seg}:){7}(?:${v6Seg}|:)|` +
    `(?:${v6Seg}:){6}(?:${v4Str}|:${v6Seg}|:)|` +
    `(?:${v6Seg}:){5}(?::${v4Str}|(:${v6Seg}){1,2}|:)|` +
    `(?:${v6Seg}:){4}(?:(:${v6Seg}){0,1}:${v4Str}|(:${v6Seg}){1,3}|:)|` +
    `(?:${v6Seg}:){3}(?:(:${v6Seg}){0,2}:${v4Str}|(:${v6Seg}){1,4}|:)|` +
    `(?:${v6Seg}:){2}(?:(:${v6Seg}){0,3}:${v4Str}|(:${v6Seg}){1,5}|:)|` +
    `(?:${v6Seg}:){1}(?:(:${v6Seg}){0,4}:${v4Str}|(:${v6Seg}){1,6}|:)|` +
    `(?::((?::${v6Seg}){0,5}:${v4Str}|(?::${v6Seg}){1,7}|:))` +
    ')(%[0-9a-zA-Z-.:]{1,})?$'
)

const isIPv4 = (exports.isIPv4 = function isIPv4(host) {
  return IPv4Pattern.test(host)
})

const isIPv6 = (exports.isIPv6 = function isIPv6(host) {
  return IPv6Pattern.test(host)
})

exports.isIP = function isIP(host) {
  if (isIPv4(host)) return 4
  if (isIPv6(host)) return 6
  return 0
}
const events = require('events')
const b4a = require('b4a')
const binding = require('../binding')

module.exports = class NetworkInterfaces extends events.EventEmitter {
  constructor(udx) {
    super()

    this._handle = b4a.alloc(binding.sizeof_udx_napi_interface_event_t)
    this._watching = false
    this._destroying = null

    binding.udx_napi_interface_event_init(
      udx._handle,
      this._handle,
      this,
      this._onevent,
      this._onclose
    )

    this.interfaces = binding.udx_napi_interface_event_get_addrs(this._handle)
  }

  _onclose() {
    this.emit('close')
  }

  _onevent() {
    this.interfaces = binding.udx_napi_interface_event_get_addrs(this._handle)

    this.emit('change', this.interfaces)
  }

  watch() {
    if (this._watching) return this
    this._watching = true

    binding.udx_napi_interface_event_start(this._handle)

    return this
  }

  unwatch() {
    if (!this._watching) return this
    this._watching = false

    binding.udx_napi_interface_event_stop(this._handle)

    return this
  }

  async destroy() {
    if (this._destroying) return this._destroying
    this._destroying = events.once(this, 'close')

    binding.udx_napi_interface_event_close(this._handle)

    return this._destroying
  }

  [Symbol.iterator]() {
    return this.interfaces[Symbol.iterator]()
  }
}
const events = require('events')
const b4a = require('b4a')
const binding = require('../binding')
const ip = require('./ip')

module.exports = class UDXSocket extends events.EventEmitter {
  constructor(udx, opts = {}) {
    super()

    this.udx = udx

    this._handle = b4a.alloc(binding.sizeof_udx_napi_socket_t)
    this._inited = false
    this._host = null
    this._family = 0
    this._ipv6Only = opts.ipv6Only === true
    this._reuseAddress = opts.reuseAddress === true
    this._port = 0
    this._reqs = []
    this._free = []
    this._closing = null
    this._closed = false

    this._view64 = new BigUint64Array(
      this._handle.buffer,
      this._handle.byteOffset,
      this._handle.byteLength >> 3
    )

    this.streams = new Set()

    this.userData = null
  }

  get bound() {
    return this._port !== 0
  }

  get closing() {
    return this._closing !== null
  }

  get idle() {
    return this.streams.size === 0
  }

  get busy() {
    return this.streams.size > 0
  }

  get bytesTransmitted() {
    if (this._inited !== true) return 0
    return Number(this._view64[binding.offsetof_udx_socket_t_bytes_tx >> 3])
  }

  get packetsTransmitted() {
    if (this._inited !== true) return 0
    return Number(this._view64[binding.offsetof_udx_socket_t_packets_tx >> 3])
  }

  get bytesReceived() {
    if (this._inited !== true) return 0
    return Number(this._view64[binding.offsetof_udx_socket_t_bytes_rx >> 3])
  }

  get packetsReceived() {
    if (this._inited !== true) return 0
    return Number(this._view64[binding.offsetof_udx_socket_t_packets_rx >> 3])
  }

  get packetsDroppedByKernel() {
    if (this._inited !== true) return 0
    return Number(this._view64[binding.offsetof_udx_socket_t_packets_dropped_by_kernel >> 3])
  }

  toJSON() {
    return {
      bound: this.bound,
      closing: this.closing,
      streams: this.streams.size,
      address: this.address(),
      ipv6Only: this._ipv6Only,
      reuseAddress: this._reuseAddress,
      idle: this.idle,
      busy: this.busy
    }
  }

  _init() {
    if (this._inited) return

    binding.udx_napi_socket_init(
      this.udx._handle,
      this._handle,
      this,
      this._onsend,
      this._onmessage,
      this._onclose,
      this._reallocMessage
    )

    this._inited = true
  }

  _onsend(id, err) {
    const req = this._reqs[id]

    const onflush = req.onflush

    req.buffer = null
    req.onflush = null

    this._free.push(id)

    onflush(err >= 0)

    // gc the free list
    if (this._free.length >= 16 && this._free.length === this._reqs.length) {
      this._free = []
      this._reqs = []
    }
  }

  _onmessage(len, port, host, family) {
    this.emit('message', this.udx._consumeMessage(len), { host, family, port })
    return this.udx._buffer
  }

  _onclose() {
    this.emit('close')
  }

  _reallocMessage() {
    return this.udx._reallocMessage()
  }

  _onidle() {
    this.emit('idle')
  }

  _onbusy() {
    this.emit('busy')
  }

  _addStream(stream) {
    if (this.streams.has(stream)) return false
    this.streams.add(stream)
    if (this.streams.size === 1) this._onbusy()
    return true
  }

  _removeStream(stream) {
    if (!this.streams.has(stream)) return false
    this.streams.delete(stream)
    const closed = this._closeMaybe()
    if (this.idle && !closed) this._onidle()
    return true
  }

  address() {
    if (!this.bound) return null
    return { host: this._host, family: this._family, port: this._port }
  }

  bind(port, host) {
    if (this.bound) throw new Error('Already bound')
    if (this.closing) throw new Error('Socket is closed')

    if (!port) port = 0

    let flags = 0
    if (this._ipv6Only) flags |= binding.UV_UDP_IPV6ONLY
    if (this._reuseAddress) flags |= binding.UV_UDP_REUSEADDR

    let family

    if (host) {
      family = ip.isIP(host)
      if (!family) throw new Error(`${host} is not a valid IP address`)

      if (!this._inited) this._init()

      this._port = binding.udx_napi_socket_bind(this._handle, port, host, family, flags)
    } else {
      if (!this._inited) this._init()

      try {
        host = '::'
        family = 6
        this._port = binding.udx_napi_socket_bind(this._handle, port, host, family, flags)
      } catch {
        host = '0.0.0.0'
        family = 4
        this._port = binding.udx_napi_socket_bind(this._handle, port, host, family, flags)
      }
    }

    this._host = host
    this._family = family

    this.emit('listening')
  }

  async close() {
    if (this._closing) return this._closing
    this._closing = new Promise((resolve) => this.once('close', resolve))
    this._closeMaybe()
    return this._closing
  }

  _closeMaybe() {
    if (this._closed || this._closing === null) return this._closed

    if (!this._inited) {
      this._closed = true
      this.emit('close')
      return true
    }

    if (this.idle) {
      binding.udx_napi_socket_close(this._handle)
      this._closed = true
    }

    return this._closed
  }

  setTTL(ttl) {
    if (!this._inited) throw new Error('Socket not active')
    binding.udx_napi_socket_set_ttl(this._handle, ttl)
  }

  getRecvBufferSize() {
    if (!this._inited) throw new Error('Socket not active')
    return binding.udx_napi_socket_get_recv_buffer_size(this._handle)
  }

  setRecvBufferSize(size) {
    if (!this._inited) throw new Error('Socket not active')
    return binding.udx_napi_socket_set_recv_buffer_size(this._handle, size)
  }

  getSendBufferSize() {
    if (!this._inited) throw new Error('Socket not active')
    return binding.udx_napi_socket_get_send_buffer_size(this._handle)
  }

  setSendBufferSize(size) {
    if (!this._inited) throw new Error('Socket not active')
    return binding.udx_napi_socket_set_send_buffer_size(this._handle, size)
  }

  addMembership(group, ifaceAddress) {
    if (!this._inited) throw new Error('Socket not active')
    return binding.udx_napi_socket_set_membership(this._handle, group, ifaceAddress || '', true)
  }

  dropMembership(group, ifaceAddress) {
    if (!this._inited) throw new Error('Socket not active')
    return binding.udx_napi_socket_set_membership(this._handle, group, ifaceAddress || '', false)
  }

  async send(buffer, port, host, ttl) {
    if (this.closing) return false

    if (!host) host = '127.0.0.1'

    const family = ip.isIP(host)
    if (!family) throw new Error(`${host} is not a valid IP address`)

    if (!this.bound) this.bind(0)

    const id = this._allocSend()
    const req = this._reqs[id]

    req.buffer = buffer

    const promise = new Promise((resolve) => {
      req.onflush = resolve
    })

    binding.udx_napi_socket_send_ttl(
      this._handle,
      req.handle,
      id,
      buffer,
      port,
      host,
      family,
      ttl || 0
    )

    return promise
  }

  trySend(buffer, port, host, ttl) {
    if (this.closing) return

    if (!host) host = '127.0.0.1'

    const family = ip.isIP(host)
    if (!family) throw new Error(`${host} is not a valid IP address`)

    if (!this.bound) this.bind(0)

    const id = this._allocSend()
    const req = this._reqs[id]

    req.buffer = buffer
    req.onflush = noop

    binding.udx_napi_socket_send_ttl(
      this._handle,
      req.handle,
      id,
      buffer,
      port,
      host,
      family,
      ttl || 0
    )
  }

  _allocSend() {
    if (this._free.length > 0) return this._free.pop()
    const handle = b4a.allocUnsafe(binding.sizeof_udx_socket_send_t)
    return this._reqs.push({ handle, buffer: null, onflush: null }) - 1
  }
}

function noop() {}
const streamx = require('streamx')
const b4a = require('b4a')
const binding = require('../binding')
const ip = require('./ip')

const MAX_PACKET = 2048
const BUFFER_SIZE = 65536 + MAX_PACKET

module.exports = class UDXStream extends streamx.Duplex {
  constructor(udx, id, opts = {}) {
    super({ mapWritable: toBuffer, eagerOpen: true })

    this.udx = udx
    this.socket = null

    this._handle = b4a.alloc(binding.sizeof_udx_napi_stream_t)
    this._view = new Uint32Array(
      this._handle.buffer,
      this._handle.byteOffset,
      this._handle.byteLength >> 2
    )
    this._view16 = new Uint16Array(
      this._handle.buffer,
      this._handle.byteOffset,
      this._handle.byteLength >> 1
    )
    this._view64 = new BigUint64Array(
      this._handle.buffer,
      this._handle.byteOffset,
      this._handle.byteLength >> 3
    )

    this._wreqs = []
    this._wfree = []

    this._sreqs = []
    this._sfree = []
    this._closed = false

    this._flushing = 0
    this._flushes = []

    this._buffer = null
    this._reallocData()

    this._onwrite = null
    this._ondestroy = null
    this._firewall = opts.firewall || firewallAll

    this._remoteChanging = null
    this._previousSocket = null

    this.id = id
    this.remoteId = 0
    this.remoteHost = null
    this.remoteFamily = 0
    this.remotePort = 0

    this.userData = null

    binding.udx_napi_stream_init(
      this.udx._handle,
      this._handle,
      id,
      opts.framed ? 1 : 0,
      this,
      this._ondata,
      this._onend,
      this._ondrain,
      this._onack,
      this._onsend,
      this._onmessage,
      this._onclose,
      this._onfirewall,
      this._onremotechanged,
      this._reallocData,
      this._reallocMessage
    )

    if (opts.seq) binding.udx_napi_stream_set_seq(this._handle, opts.seq)

    binding.udx_napi_stream_recv_start(this._handle, this._buffer)
  }

  get connected() {
    return this.socket !== null
  }

  get mtu() {
    return this._view16[binding.offsetof_udx_stream_t_mtu >> 1]
  }

  get rtt() {
    return this._view[binding.offsetof_udx_stream_t_srtt >> 2]
  }

  get cwnd() {
    return this._view[binding.offsetof_udx_stream_t_cwnd >> 2]
  }

  get rtoCount() {
    return this._view16[binding.offsetof_udx_stream_t_rto_count >> 1]
  }

  get retransmits() {
    return this._view16[binding.offsetof_udx_stream_t_retransmit_count >> 1]
  }

  get fastRecoveries() {
    return this._view16[binding.offsetof_udx_stream_t_fast_recovery_count >> 1]
  }

  get inflight() {
    return this._view[binding.offsetof_udx_stream_t_inflight >> 2]
  }

  get bytesTransmitted() {
    return Number(this._view64[binding.offsetof_udx_stream_t_bytes_tx >> 3])
  }

  get packetsTransmitted() {
    return Number(this._view64[binding.offsetof_udx_stream_t_packets_tx >> 3])
  }

  get bytesReceived() {
    return Number(this._view64[binding.offsetof_udx_stream_t_bytes_rx >> 3])
  }

  get packetsReceived() {
    return Number(this._view64[binding.offsetof_udx_stream_t_packets_rx >> 3])
  }

  get localHost() {
    return this.socket ? this.socket.address().host : null
  }

  get localFamily() {
    return this.socket ? this.socket.address().family : 0
  }

  get localPort() {
    return this.socket ? this.socket.address().port : 0
  }

  setInteractive(bool) {
    if (!this._closed) return
    binding.udx_napi_stream_set_mode(this._handle, bool ? 0 : 1)
  }

  connect(socket, remoteId, port, host, opts = {}) {
    if (this._closed) return

    if (this.connected) throw new Error('Already connected')
    if (socket.closing) throw new Error('Socket is closed')

    if (typeof host === 'object') {
      opts = host
      host = null
    }

    if (!host) host = '127.0.0.1'

    const family = ip.isIP(host)
    if (!family) throw new Error(`${host} is not a valid IP address`)
    if (!(port > 0 && port < 65536)) throw new Error(`${port} is not a valid port`)

    if (!socket.bound) socket.bind(0)

    this.remoteId = remoteId
    this.remotePort = port
    this.remoteHost = host
    this.remoteFamily = family
    this.socket = socket

    if (opts.ack) binding.udx_napi_stream_set_ack(this._handle, opts.ack)

    binding.udx_napi_stream_connect(this._handle, socket._handle, remoteId, port, host, family)

    this.socket._addStream(this)

    this.emit('connect')
  }

  changeRemote(socket, remoteId, port, host) {
    if (this._remoteChanging) throw new Error('Remote already changing')

    if (!this.connected) throw new Error('Not yet connected')
    if (socket.closing) throw new Error('Socket is closed')

    if (this.socket.udx !== socket.udx) {
      throw new Error('Cannot change to a socket on another UDX instance')
    }

    if (!host) host = '127.0.0.1'

    const family = ip.isIP(host)
    if (!family) throw new Error(`${host} is not a valid IP address`)
    if (!(port > 0 && port < 65536)) throw new Error(`${port} is not a valid port`)

    if (this.socket !== socket) this._previousSocket = this.socket

    this.remoteId = remoteId
    this.remotePort = port
    this.remoteHost = host
    this.remoteFamily = family
    this.socket = socket

    this._remoteChanging = new Promise((resolve, reject) => {
      const onchanged = () => {
        this.off('close', onclose)
        resolve()
      }

      const onclose = () => {
        this.off('remote-changed', onchanged)
        reject(new Error('Stream is closed'))
      }

      this.once('remote-changed', onchanged).once('close', onclose)
    })

    binding.udx_napi_stream_change_remote(
      this._handle,
      socket._handle,
      remoteId,
      port,
      host,
      family
    )

    this.socket._addStream(this)

    return this._remoteChanging
  }

  relayTo(destination) {
    if (this._closed) return

    binding.udx_napi_stream_relay_to(this._handle, destination._handle)
  }

  async send(buffer) {
    if (!this.connected || this._closed) return false

    const id = this._allocSend()
    const req = this._sreqs[id]

    req.buffer = buffer

    const promise = new Promise((resolve) => {
      req.onflush = resolve
    })

    binding.udx_napi_stream_send(this._handle, req.handle, id, buffer)

    return promise
  }

  trySend(buffer) {
    if (!this.connected || this._closed) return

    const id = this._allocSend()
    const req = this._sreqs[id]

    req.buffer = buffer
    req.onflush = noop

    binding.udx_napi_stream_send(this._handle, req.handle, id, buffer)
  }

  async flush() {
    if ((await streamx.Writable.drained(this)) === false) return false
    if (this.destroying) return false

    const missing = this._wreqs.length - this._wfree.length
    if (missing === 0) return true

    return new Promise((resolve) => {
      this._flushes.push({ flush: this._flushing++, missing, resolve })
    })
  }

  toJSON() {
    return {
      id: this.id,
      connected: this.connected,
      destroying: this.destroying,
      destroyed: this.destroyed,
      remoteId: this.remoteId,
      remoteHost: this.remoteHost,
      remoteFamily: this.remoteFamily,
      remotePort: this.remotePort,
      mtu: this.mtu,
      rtt: this.rtt,
      cwnd: this.cwnd,
      inflight: this.inflight,
      socket: this.socket ? this.socket.toJSON() : null
    }
  }

  _read(cb) {
    cb(null)
  }

  _writeContinue(err) {
    if (this._onwrite === null) return
    const cb = this._onwrite
    this._onwrite = null
    cb(err)
  }

  _destroyContinue(err) {
    if (this._ondestroy === null) return
    const cb = this._ondestroy
    this._ondestroy = null
    cb(err)
  }

  _writev(buffers, cb) {
    if (!this.connected)
      throw customError('Writing while not connected not currently supported', 'ERR_ASSERTION')

    let drained = true

    if (buffers.length === 1) {
      const id = this._allocWrite(1)
      const req = this._wreqs[id]

      req.flush = this._flushing
      req.buffer = buffers[0]

      drained = binding.udx_napi_stream_write(this._handle, req.handle, id, req.buffer) !== 0
    } else {
      const id = this._allocWrite(nextBatchSize(buffers.length))
      const req = this._wreqs[id]

      req.flush = this._flushing
      req.buffers = buffers

      drained = binding.udx_napi_stream_writev(this._handle, req.handle, id, req.buffers) !== 0
    }

    if (drained) cb(null)
    else this._onwrite = cb
  }

  _final(cb) {
    const id = this._allocWrite(1)
    const req = this._wreqs[id]

    req.flush = this._flushes
    req.buffer = b4a.allocUnsafe(0)

    const drained =
      binding.udx_napi_stream_write_end(this._handle, req.handle, id, req.buffer) !== 0

    if (drained) cb(null)
    else this._onwrite = cb
  }

  _predestroy() {
    if (!this._closed) binding.udx_napi_stream_destroy(this._handle)
    this._closed = true
    this._writeContinue(null)
  }

  _destroy(cb) {
    if (this.connected) this._ondestroy = cb
    else cb(null)
  }

  _ondata(read) {
    this.push(this._consumeData(read))
    return this._buffer
  }

  _onend(read) {
    if (read > 0) this.push(this._consumeData(read))
    this.push(null)
  }

  _ondrain() {
    this._writeContinue(null)
  }

  _flushAck(flush) {
    for (let i = this._flushes.length - 1; i >= 0; i--) {
      const f = this._flushes[i]
      if (f.flush < flush) break
      f.missing--
    }

    while (this._flushes.length > 0 && this._flushes[0].missing === 0) {
      this._flushes.shift().resolve(true)
    }
  }

  _onack(id) {
    const req = this._wreqs[id]

    req.buffers = req.buffer = null
    this._wfree.push(id)

    if (this._flushes.length > 0) this._flushAck(req.flush)

    // gc the free list
    if (this._wfree.length >= 64 && this._wfree.length === this._wreqs.length) {
      this._wfree = []
      this._wreqs = []
    }
  }

  _onsend(id, err) {
    const req = this._sreqs[id]

    const onflush = req.onflush

    req.buffer = null
    req.onflush = null

    this._sfree.push(id)

    onflush(err >= 0)

    // gc the free list
    if (this._sfree.length >= 16 && this._sfree.length === this._sreqs.length) {
      this._sfree = []
      this._sreqs = []
    }
  }

  _onmessage(len) {
    this.emit('message', this.udx._consumeMessage(len))
    return this.udx._buffer
  }

  _onclose(err) {
    this._closed = true

    if (this.socket) {
      this.socket._removeStream(this)
      this.socket = null
    }

    if (this._previousSocket) {
      this._previousSocket._removeStream(this)
      this._previousSocket = null
    }

    // no error, we don't need to do anything
    if (!err) return this._destroyContinue(null)

    if (this._ondestroy === null) this.destroy(err)
    else this._destroyContinue(err)
  }

  _onfirewall(socket, port, host, family) {
    return this._firewall(socket, port, host, family) ? 1 : 0
  }

  _onremotechanged() {
    if (this._previousSocket) {
      this._previousSocket._removeStream(this)
      this._previousSocket = null
    }

    this._remoteChanging = null
    this.emit('remote-changed')
  }

  _consumeData(len) {
    const next = this._buffer.subarray(0, len)
    this._buffer = this._buffer.subarray(len)
    if (this._buffer.byteLength < MAX_PACKET) this._reallocData()
    return next
  }

  _reallocData() {
    this._buffer = b4a.allocUnsafe(BUFFER_SIZE)
    return this._buffer
  }

  _reallocMessage() {
    return this.udx._reallocMessage()
  }

  _allocWrite(size) {
    if (this._wfree.length === 0) {
      const handle = b4a.allocUnsafe(binding.udx_napi_stream_write_sizeof(size))
      return (
        this._wreqs.push({
          handle,
          size,
          buffers: null,
          buffer: null,
          flush: 0
        }) - 1
      )
    }

    const free = this._wfree.pop()
    if (size === 1) return free

    const next = this._wreqs[free]
    if (next.size < size) {
      next.handle = b4a.allocUnsafe(binding.udx_napi_stream_write_sizeof(size))
      next.size = size
    }

    return free
  }

  _allocSend() {
    if (this._sfree.length > 0) return this._sfree.pop()
    const handle = b4a.allocUnsafe(binding.sizeof_udx_stream_send_t)
    return this._sreqs.push({ handle, buffer: null, resolve: null, reject: null }) - 1
  }
}

function noop() {}

function toBuffer(data) {
  return typeof data === 'string' ? b4a.from(data) : data
}

function firewallAll(socket, port, host) {
  return true
}

function customError(message, code) {
  const error = new Error(message)
  error.code = code
  return error
}

function nextBatchSize(n) {
  // try to coerce the the writevs into sameish size
  if (n === 1) return 1
  // group all < 8 to the same size, low mem overhead but save some small allocs
  if (n < 8) return 8
  if (n < 16) return 16
  if (n < 32) return 32
  if (n < 64) return 64
  return n
}
const b4a = require('b4a')
const binding = require('../binding')
const ip = require('./ip')
const Socket = require('./socket')
const Stream = require('./stream')
const NetworkInterfaces = require('./network-interfaces')

const MAX_MESSAGE = 4096
const BUFFER_SIZE = 65536 + MAX_MESSAGE

module.exports = class UDX {
  constructor() {
    this._handle = b4a.alloc(binding.sizeof_udx_napi_t)
    this._watchers = new Set()
    this._view64 = new BigUint64Array(
      this._handle.buffer,
      this._handle.byteOffset,
      this._handle.byteLength >> 3
    )

    this._buffer = null
    this._reallocMessage()

    binding.udx_napi_init(this._handle, this._buffer)
  }

  static isIPv4(host) {
    return ip.isIPv4(host)
  }

  static isIPv6(host) {
    return ip.isIPv6(host)
  }

  static isIP(host) {
    return ip.isIP(host)
  }

  get bytesTransmitted() {
    return Number(this._view64[binding.offsetof_udx_t_bytes_tx >> 3])
  }

  get packetsTransmitted() {
    return Number(this._view64[binding.offsetof_udx_t_packets_tx >> 3])
  }

  get bytesReceived() {
    return Number(this._view64[binding.offsetof_udx_t_bytes_rx >> 3])
  }

  get packetsReceived() {
    return Number(this._view64[binding.offsetof_udx_t_packets_rx >> 3])
  }

  get packetsDroppedByKernel() {
    return Number(this._view64[binding.offsetof_udx_t_packets_dropped_by_kernel >> 3])
  }

  _consumeMessage(len) {
    const next = this._buffer.subarray(0, len)
    this._buffer = this._buffer.subarray(len)
    if (this._buffer.byteLength < MAX_MESSAGE) this._reallocMessage()
    return next
  }

  _reallocMessage() {
    // TODO: move reallocation to native
    this._buffer = b4a.allocUnsafe(BUFFER_SIZE)
    return this._buffer
  }

  createSocket(opts) {
    return new Socket(this, opts)
  }

  createStream(id, opts) {
    return new Stream(this, id, opts)
  }

  networkInterfaces() {
    let [watcher = null] = this._watchers
    if (watcher) return watcher.interfaces

    watcher = new NetworkInterfaces(this)
    watcher.destroy()

    return watcher.interfaces
  }

  watchNetworkInterfaces(onchange) {
    const watcher = new NetworkInterfaces(this)

    this._watchers.add(watcher)
    watcher.on('close', () => {
      this._watchers.delete(watcher)
    })

    if (onchange) watcher.on('change', onchange)

    return watcher.watch()
  }

  async lookup(host, opts = {}) {
    const { family = 0 } = opts

    const req = b4a.alloc(binding.sizeof_udx_napi_lookup_t)
    const ctx = {
      req,
      resolve: null,
      reject: null
    }

    const promise = new Promise((resolve, reject) => {
      ctx.resolve = resolve
      ctx.reject = reject
    })

    binding.udx_napi_lookup(this._handle, req, host, family, ctx, onlookup)

    return promise
  }
}

function onlookup(err, host, family) {
  if (err) this.reject(err)
  else this.resolve({ host, family })
}
{
  "name": "udx-native",
  "version": "1.19.2",
  "description": "udx is reliable, multiplexed, and congestion-controlled streams over udp",
  "main": "lib/udx.js",
  "files": [
    "lib",
    "prebuilds",
    "binding.cc",
    "binding.js",
    "CMakeLists.txt"
  ],
  "imports": {
    "events": {
      "bare": "bare-events",
      "default": "events"
    }
  },
  "addon": true,
  "scripts": {
    "format": "prettier --write .",
    "lint": "prettier --check .",
    "test": "npm run lint && npm run test:bare && npm run test:node",
    "test:node": "node test/all.js",
    "test:bare": "bare test/all.js",
    "test:all": "brittle test/*.js test/slow/*.js",
    "test:generate": "brittle -r test/all.js test/*.js",
    "bench": "brittle test/bench/*.js"
  },
  "repository": {
    "type": "git",
    "url": "git+https://github.com/holepunchto/udx-native.git"
  },
  "keywords": [
    "tcp",
    "udp",
    "stream",
    "reliable"
  ],
  "author": "Holepunch Inc.",
  "license": "Apache-2.0",
  "bugs": {
    "url": "https://github.com/holepunchto/udx-native/issues"
  },
  "homepage": "https://github.com/holepunchto/udx-native#readme",
  "engines": {
    "bare": ">=1.17.4"
  },
  "dependencies": {
    "b4a": "^1.5.0",
    "bare-events": "^2.2.0",
    "require-addon": "^1.1.0",
    "streamx": "^2.22.0"
  },
  "devDependencies": {
    "bare-compat-napi": "^1.3.0",
    "brittle": "^3.1.0",
    "cmake-bare": "^1.1.10",
    "cmake-fetch": "^1.0.1",
    "cmake-napi": "^1.0.5",
    "is-ci": "^3.0.1",
    "prettier": "^3.6.2",
    "prettier-config-holepunch": "^2.0.0",
    "tiny-byte-size": "^1.1.0"
  }
}
exports.add = add
exports.has = has
exports.remove = remove
exports.swap = swap

function add (list, item) {
  if (has(list, item)) return item
  item._index = list.length
  list.push(item)
  return item
}

function has (list, item) {
  return item._index < list.length && list[item._index] === item
}

function remove (list, item) {
  if (!has(list, item)) return null

  var last = list.pop()
  if (last !== item) {
    list[item._index] = last
    last._index = item._index
  }

  return item
}

function swap (list, a, b) {
  if (!has(list, a) || !has(list, b)) return
  var tmp = a._index
  a._index = b._index
  list[a._index] = a
  b._index = tmp
  list[b._index] = b
}
{
  "name": "unordered-set",
  "version": "2.0.1",
  "description": "A couple of functions that make it easy to maintain an unordered set as an array in an efficient way",
  "main": "index.js",
  "dependencies": {},
  "devDependencies": {
    "standard": "^6.0.4",
    "tape": "^4.4.0"
  },
  "scripts": {
    "test": "standard && tape test.js"
  },
  "repository": {
    "type": "git",
    "url": "https://github.com/mafintosh/unordered-set.git"
  },
  "author": "Mathias Buus (@mafintosh)",
  "license": "MIT",
  "bugs": {
    "url": "https://github.com/mafintosh/unordered-set/issues"
  },
  "homepage": "https://github.com/mafintosh/unordered-set"
}
const b4a = require('b4a')

unslab.all = all
unslab.is = is

module.exports = unslab

function unslab (buf) {
  if (buf === null || buf.buffer.byteLength === buf.byteLength) return buf
  const copy = b4a.allocUnsafeSlow(buf.byteLength)
  copy.set(buf, 0)
  return copy
}

function is (buf) {
  return buf.buffer.byteLength !== buf.byteLength
}

function all (list) {
  let size = 0
  for (let i = 0; i < list.length; i++) {
    const buf = list[i]
    size += buf === null || buf.buffer.byteLength === buf.byteLength ? 0 : buf.byteLength
  }

  const copy = b4a.allocUnsafeSlow(size)
  const result = new Array(list.length)

  let offset = 0
  for (let i = 0; i < list.length; i++) {
    let buf = list[i]

    if (buf !== null && buf.buffer.byteLength !== buf.byteLength) {
      copy.set(buf, offset)
      buf = copy.subarray(offset, offset += buf.byteLength)
    }

    result[i] = buf
  }

  return result
}
{
  "name": "unslab",
  "version": "1.3.0",
  "description": "Unslab some slab'ed buffers",
  "main": "index.js",
  "files": [
    "index.js"
  ],
  "dependencies": {
    "b4a": "^1.6.6"
  },
  "devDependencies": {
    "brittle": "^3.5.2",
    "standard": "^17.1.0"
  },
  "scripts": {
    "test": "standard && brittle test.js"
  },
  "repository": {
    "type": "git",
    "url": "https://github.com/holepunchto/unslab.git"
  },
  "author": "Holepunch",
  "license": "Apache-2.0",
  "bugs": {
    "url": "https://github.com/holepunchto/unslab/issues"
  },
  "homepage": "https://github.com/holepunchto/unslab"
}
module.exports = read

var MSB = 0x80
  , REST = 0x7F

function read(buf, offset) {
  var res    = 0
    , offset = offset || 0
    , shift  = 0
    , counter = offset
    , b
    , l = buf.length

  do {
    if (counter >= l) {
      read.bytes = 0
      throw new RangeError('Could not decode varint')
    }
    b = buf[counter++]
    res += shift < 28
      ? (b & REST) << shift
      : (b & REST) * Math.pow(2, shift)
    shift += 7
  } while (b >= MSB)

  read.bytes = counter - offset

  return res
}
module.exports = encode

var MSB = 0x80
  , REST = 0x7F
  , MSBALL = ~REST
  , INT = Math.pow(2, 31)

function encode(num, out, offset) {
  out = out || []
  offset = offset || 0
  var oldOffset = offset

  while(num >= INT) {
    out[offset++] = (num & 0xFF) | MSB
    num /= 128
  }
  while(num & MSBALL) {
    out[offset++] = (num & 0xFF) | MSB
    num >>>= 7
  }
  out[offset] = num | 0
  
  encode.bytes = offset - oldOffset + 1
  
  return out
}
module.exports = {
    encode: require('./encode.js')
  , decode: require('./decode.js')
  , encodingLength: require('./length.js')
}

var N1 = Math.pow(2,  7)
var N2 = Math.pow(2, 14)
var N3 = Math.pow(2, 21)
var N4 = Math.pow(2, 28)
var N5 = Math.pow(2, 35)
var N6 = Math.pow(2, 42)
var N7 = Math.pow(2, 49)
var N8 = Math.pow(2, 56)
var N9 = Math.pow(2, 63)

module.exports = function (value) {
  return (
    value < N1 ? 1
  : value < N2 ? 2
  : value < N3 ? 3
  : value < N4 ? 4
  : value < N5 ? 5
  : value < N6 ? 6
  : value < N7 ? 7
  : value < N8 ? 8
  : value < N9 ? 9
  :              10
  )
}
{
  "name": "varint",
  "version": "5.0.0",
  "description": "protobuf-style varint bytes - use msb to create integer values of varying sizes",
  "main": "index.js",
  "scripts": {
    "test": "node test.js"
  },
  "repository": {
    "type": "git",
    "url": "git://github.com/chrisdickinson/varint.git"
  },
  "keywords": [
    "varint",
    "protobuf",
    "encode",
    "decode"
  ],
  "author": "Chris Dickinson <chris@neversaw.us>",
  "license": "MIT",
  "devDependencies": {
    "tape": "~2.12.3"
  }
}
const { runtime, platform, arch } = typeof Bare !== 'undefined'
  ? { runtime: 'bare', platform: global.Bare.platform, arch: global.Bare.arch }
  : typeof process !== 'undefined'
    ? { runtime: 'node', platform: global.process.platform, arch: global.process.arch }
    : typeof Window !== 'undefined'
      ? { runtime: 'browser', platform: 'unknown', arch: 'unknown' }
      : { runtime: 'unknown', platform: 'unknown', arch: 'unknown' }

exports.runtime = runtime
exports.platform = platform
exports.arch = arch
exports.isBare = runtime === 'bare'
exports.isBareKit = exports.isBare && typeof BareKit !== 'undefined'
exports.isPear = typeof Pear !== 'undefined'
exports.isNode = runtime === 'node'
exports.isBrowser = runtime === 'browser'
exports.isWindows = platform === 'win32'
exports.isLinux = platform === 'linux'
exports.isMac = platform === 'darwin'
exports.isIOS = platform === 'ios' || platform === 'ios-simulator'
exports.isAndroid = platform === 'android'
exports.isElectron = typeof process !== 'undefined' && !!global.process.versions?.electron
exports.isElectronRenderer = exports.isElectron && global.process.type === 'renderer'
exports.isElectronWorker = exports.isElectron && global.process.type === 'worker'
{
  "name": "which-runtime",
  "version": "1.3.2",
  "description": "Detect if you are in Bare or Node and which os etc",
  "main": "index.js",
  "files": [
    "index.js"
  ],
  "dependencies": {},
  "devDependencies": {
    "standard": "^17.0.0"
  },
  "scripts": {
    "test": "standard"
  },
  "repository": {
    "type": "git",
    "url": "https://github.com/holepunchto/which-runtime.git"
  },
  "author": "Holepunch Inc.",
  "license": "Apache-2.0",
  "bugs": {
    "url": "https://github.com/holepunchto/which-runtime/issues"
  },
  "homepage": "https://github.com/holepunchto/which-runtime"
}
module.exports = class MaxCache {
  constructor ({ maxSize, maxAge, createMap, ongc }) {
    this.maxSize = maxSize
    this.maxAge = maxAge
    this.ongc = ongc || null

    this._createMap = createMap || defaultCreateMap
    this._latest = this._createMap()
    this._oldest = this._createMap()
    this._retained = this._createMap()
    this._gced = false
    this._interval = null

    if (this.maxAge > 0 && this.maxAge < Infinity) {
      const tick = Math.ceil(2 / 3 * this.maxAge)
      this._interval = setInterval(this._gcAuto.bind(this), tick)
      if (this._interval.unref) this._interval.unref()
    }
  }

  * [Symbol.iterator] () {
    for (const it of [this._latest, this._oldest, this._retained]) {
      yield * it
    }
  }

  * keys () {
    for (const it of [this._latest, this._oldest, this._retained]) {
      yield * it.keys()
    }
  }

  * values () {
    for (const it of [this._latest, this._oldest, this._retained]) {
      yield * it.values()
    }
  }

  destroy () {
    this.clear()
    clearInterval(this._interval)
    this._interval = null
  }

  clear () {
    this._gced = true
    this._latest.clear()
    this._oldest.clear()
    this._retained.clear()
  }

  set (k, v) {
    if (this._retained.has(k)) return this
    this._latest.set(k, v)
    this._oldest.delete(k) || this._retained.delete(k)
    if (this._latest.size >= this.maxSize) this._gc()
    return this
  }

  retain (k, v) {
    this._retained.set(k, v)
    this._latest.delete(k) || this._oldest.delete(k)
    return this
  }

  delete (k) {
    return this._latest.delete(k) || this._oldest.delete(k) || this._retained.delete(k)
  }

  has (k) {
    return this._latest.has(k) || this._oldest.has(k) || this._retained.has(k)
  }

  get (k) {
    if (this._latest.has(k)) {
      return this._latest.get(k)
    }

    if (this._oldest.has(k)) {
      const v = this._oldest.get(k)
      this._latest.set(k, v)
      this._oldest.delete(k)
      return v
    }

    if (this._retained.has(k)) {
      return this._retained.get(k)
    }

    return null
  }

  _gcAuto () {
    if (!this._gced) this._gc()
    this._gced = false
  }

  _gc () {
    this._gced = true
    if (this.ongc !== null && this._oldest.size > 0) this.ongc(this._oldest)
    this._oldest = this._latest
    this._latest = this._createMap()
  }
}

function defaultCreateMap () {
  return new Map()
}
{
  "name": "xache",
  "version": "1.2.1",
  "description": "Yet another auto expiring, max sizable cache",
  "main": "index.js",
  "files": [
    "index.js"
  ],
  "devDependencies": {
    "brittle": "^3.3.2",
    "standard": "^17.1.0"
  },
  "scripts": {
    "test": "standard && brittle test.js"
  },
  "repository": {
    "type": "git",
    "url": "https://github.com/mafintosh/xache.git"
  },
  "author": "Mathias Buus (@mafintosh)",
  "license": "MIT",
  "bugs": {
    "url": "https://github.com/mafintosh/xache/issues"
  },
  "homepage": "https://github.com/mafintosh/xache"
}
const b4a = require('b4a')

const ALPHABET = 'ybndrfg8ejkmcpqxot1uwisza345h769'
const MIN = 0x31 // 1
const MAX = 0x7a // z
const REVERSE = new Int8Array(1 + MAX - MIN)

REVERSE.fill(-1)

for (let i = 0; i < ALPHABET.length; i++) {
  const v = ALPHABET.charCodeAt(i) - MIN
  REVERSE[v] = i
}

exports.encode = encode
exports.decode = decode
exports.ALPHABET = ALPHABET

function decode (s, out) {
  let pb = 0
  let ps = 0

  const r = s.length & 7
  const q = (s.length - r) / 8

  if (!out) out = b4a.allocUnsafe(Math.ceil(s.length * 5 / 8))

  // 0 5 2 7 4 1 6 3 (+5 mod 8)
  for (let i = 0; i < q; i++) {
    const a = quintet(s, ps++)
    const b = quintet(s, ps++)
    const c = quintet(s, ps++)
    const d = quintet(s, ps++)
    const e = quintet(s, ps++)
    const f = quintet(s, ps++)
    const g = quintet(s, ps++)
    const h = quintet(s, ps++)

    out[pb++] = (a << 3) | (b >>> 2)
    out[pb++] = ((b & 0b11) << 6) | (c << 1) | (d >>> 4)
    out[pb++] = ((d & 0b1111) << 4) | (e >>> 1)
    out[pb++] = ((e & 0b1) << 7) | (f << 2) | (g >>> 3)
    out[pb++] = ((g & 0b111) << 5) | h
  }

  if (r === 0) return out.subarray(0, pb)

  const a = quintet(s, ps++)
  const b = quintet(s, ps++)

  out[pb++] = (a << 3) | (b >>> 2)

  if (r <= 2) return out.subarray(0, pb)

  const c = quintet(s, ps++)
  const d = quintet(s, ps++)

  out[pb++] = ((b & 0b11) << 6) | (c << 1) | (d >>> 4)

  if (r <= 4) return out.subarray(0, pb)

  const e = quintet(s, ps++)

  out[pb++] = ((d & 0b1111) << 4) | (e >>> 1)

  if (r <= 5) return out.subarray(0, pb)

  const f = quintet(s, ps++)
  const g = quintet(s, ps++)

  out[pb++] = ((e & 0b1) << 7) | (f << 2) | (g >>> 3)

  if (r <= 7) return out.subarray(0, pb)

  const h = quintet(s, ps++)

  out[pb++] = ((g & 0b111) << 5) | h

  return out.subarray(0, pb)
}

function encode (buf) {
  if (typeof buf === 'string') buf = b4a.from(buf)

  const max = buf.byteLength * 8

  let s = ''

  for (let p = 0; p < max; p += 5) {
    const i = p >>> 3
    const j = p & 7

    if (j <= 3) {
      s += ALPHABET[(buf[i] >>> (3 - j)) & 0b11111]
      continue
    }

    const of = j - 3
    const h = (buf[i] << of) & 0b11111
    const l = (i >= buf.byteLength ? 0 : buf[i + 1]) >>> (8 - of)

    s += ALPHABET[h | l]
  }

  return s
}

function quintet (s, i) {
  if (i > s.length) {
    return 0
  }

  const v = s.charCodeAt(i)

  if (v < MIN || v > MAX) {
    throw Error('Invalid character in base32 input: "' + s[i] + '" at position ' + i)
  }

  const bits = REVERSE[v - MIN]

  if (bits === -1) {
    throw Error('Invalid character in base32 input: "' + s[i] + '" at position ' + i)
  }

  return bits
}
{
  "name": "z32",
  "version": "1.1.0",
  "description": "Encode & decode z-base32",
  "main": "index.js",
  "dependencies": {
    "b4a": "^1.5.3"
  },
  "devDependencies": {
    "base-x": "^4.0.0",
    "base32": "0.0.7",
    "brittle": "^3.1.3",
    "nanobench": "^3.0.0",
    "rfc4648": "^1.5.2",
    "standard": "^17.0.0"
  },
  "repository": {
    "type": "git",
    "url": "https://github.com/mafintosh/z32.git"
  },
  "scripts": {
    "test": "standard && brittle test.js",
    "bench": "node benchmark.js"
  },
  "author": "Mathias Buus (@mafintosh)",
  "license": "MIT",
  "bugs": {
    "url": "https://github.com/mafintosh/z32/issues"
  },
  "homepage": "https://github.com/mafintosh/z32"
}
{
  "private": true,
  "name": "listam",
  "description": "A minimalistic p2p list app",
  "main": "expo-router/entry",
  "scripts": {
    "start": "expo start",
    "android": "npm run bundle:backend:android && expo run:android",
    "ios": "npm run bundle:backend:ios && expo run:ios --device",
    "bundle:backend:ios": "npx bare-pack --target ios --linked --out app/app.bundle.ios.mjs backend/backend.mjs",
    "bundle:backend:android": "npx bare-pack --target android --linked --out app/app.android.mjs backend/backend.mjs"
  },
  "dependencies": {
    "@inquirer/prompts": "^7.8.1",
    "@react-native-clipboard/clipboard": "1.16.3",
    "autobase": "^7.20.0",
    "autopass": "^2.2.1",
    "b4a": "^1.7.1",
    "bare-crypto": "^1.12.0",
    "bare-fs": "^4.4.4",
    "bare-rpc": "^0.2.12",
    "blind-peering": "^1.13.0",
    "corestore": "^7.5.0",
    "expo": "54.0.30",
    "expo-build-properties": "1.0.10",
    "expo-constants": "18.0.12",
    "expo-linking": "8.0.11",
    "expo-router": "6.0.21",
    "expo-system-ui": "6.0.9",
    "hypercore-crypto": "^3.6.1",
    "hyperdb": "^4.19.0",
    "hyperdispatch": "^1.4.4",
    "hyperschema": "^1.17.0",
    "hyperswarm": "^4.12.1",
    "lucide-react-native": "^0.553.0",
    "make-dir": "^5.1.0",
    "protomux-wakeup": "^2.6.1",
    "react": "19.2.3",
    "react-dom": "19.2.3",
    "react-native": "0.83.0",
    "react-native-b4a": "0.0.4",
    "react-native-bare-kit": "0.11.5",
    "react-native-safe-area-context": "5.6.2"
  },
  "devDependencies": {
    "@react-native-community/cli": "latest",
    "@types/b4a": "1.6.5",
    "@types/react": "19.2.7",
    "bare-pack": "1.5.1",
    "prettier": "3.7.4",
    "prettier-config-standard": "^7.0.0",
    "typescript": "5.9.3"
  }
}
export const RPC_RESET = 0
export const RPC_MESSAGE = 1
export const RPC_ADD = 2
export const RPC_UPDATE = 3
export const RPC_DELETE = 4
export const RPC_GET_KEY = 5
export const SYNC_LIST = 6
export const RPC_JOIN_KEY = 7
export const RPC_ADD_FROM_BACKEND = 8
export const RPC_UPDATE_FROM_BACKEND = 9
export const RPC_DELETE_FROM_BACKEND = 10
